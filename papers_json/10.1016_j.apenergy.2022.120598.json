{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85146093540",
    "originalText": "serial JL 271429 291210 291702 291731 291787 291877 291878 291881 31 Applied Energy APPLIEDENERGY 2023-01-10 2023-01-10 2023-01-10 2023-01-10 2023-07-21T18:59:57 1-s2.0-S0306261922018554 S0306-2619(22)01855-4 S0306261922018554 10.1016/j.apenergy.2022.120598 S300 S300.1 FULL-TEXT 1-s2.0-S0306261922X00263 2024-01-01T13:32:31.319396Z 0 0 20230301 2023 2023-01-10T13:49:37.323151Z absattachment articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast highlightsabst nomenclature orcid primabst ref specialabst 0306-2619 03062619 true 333 333 C Volume 333 39 120598 120598 120598 20230301 1 March 2023 2023-03-01 2023 Research Papers article fla © 2023 Elsevier Ltd. All rights reserved. ONLINETRANSFERLEARNINGSTRATEGYFORENHANCINGSCALABILITYDEPLOYMENTDEEPREINFORCEMENTLEARNINGCONTROLINSMARTBUILDINGS CORACI D Nomenclature 1 Introduction 1.1 Related works on TL applications for advanced controllers in buildings 1.2 Novelty and contributions of the paper 2 Methods 2.1 Transfer learning 2.1.1 Transfer learning for RL applications 3 Control problem formulation 4 Methodology 4.1 Design of control problem 4.1.1 Design of rule-based controller 4.1.2 Design of DRL controller 4.2 DRL training phase on source building 4.3 Transfer of DRL control policy 4.4 Performance benchmarking on target buildings 4.4.1 Offline deep reinforcement learning 4.4.2 Online deep reinforcement learning 4.4.3 Online transfer learning strategy 5 Implementation 5.1 Implementation details on source and target building configurations 5.2 Simulation environment 5.3 Rule-based control strategy 5.4 Design of DRL control strategy 5.4.1 Design of state\u2013space 5.4.2 Design of action-space 5.4.3 Design of reward function 5.5 Training setup of DRL agent on source building 5.6 Implementation details on online transfer learning and DRL learning strategies 6 Results 6.1 Training of DRL agent on source building 6.2 Performance benchmarking of online transfer learning with RBC and DRL control strategies on target buildings 7 Discussion 8 Conclusion CRediT authorship contribution statement References YANG 2015 577 586 L MARTINOPOULOS 2018 687 699 G BANIASADI 2020 101186 A WANG 2020 115036 Z GENG 1993 819 824 G PROCEEDINGSIEEEINTERNATIONALCONFERENCECONTROLAPPLICATIONS PERFORMANCETUNINGPIDCONTROLLERSINHVACSYSTEMS SALSBURY 2005 90 100 T BRANDI 2022 104128 S SERALE 2018 G SERALE 2018 438 449 G DRGONA 2020 J OLDEWURTEL 2012 15 27 F HENZE 1997 233 264 G CORACI 2021 D KONTES 2018 G SUTTON 2018 R REINFORCEMENTLEARNINGINTRODUCTION BRANDI 2022 1550 1567 S MNIH 2015 529 533 V VALLADARES 2019 105 117 W ZOU 2020 106535 Z DU 2021 116117 Y WANG 2017 Y ZHANG 2019 472 490 Z BRANDI 2020 110225 S VAZQUEZCANTELI 2019 243 257 J PINTO 2021 120725 G PINTO 2021 117642 G ASSOCIATION 2000 M MODELICAAUNIFIEDOBJECTORIENTEDLANGUAGEFORPHYSICALSYSTEMSMODELINGTUTORIAL CRAWLEY 2001 319 331 D PINTO 2022 100084 G PAN 2010 1345 1359 S DASILVA 2019 645 703 F PEIRELINCK 2022 100126 T HIMEUR 2022 104059 Y LEONMALPARTIDA 2018 1 4 J 2018IEEEXXVINTERNATIONALCONFERENCEELECTRONICSELECTRICALENGINEERINGCOMPUTING ANEWMETHODCLASSIFICATIONREJECTIONAPPLIEDBUILDINGIMAGESRECOGNITIONBASEDTRANSFERLEARNING SIMONYAN 2014 K DEEPCONVOLUTIONALNETWORKSFORLARGESCALEIMAGERECOGNITION SILVER 2016 484 489 D SINGH 1999 S FAN 2022 122775 C OLIVEIRA 2021 P FANG 2021 119208 X CHEN 2017 897 904 W PARDAMEAN 2019 1 12 B CHEN 2020 119866 Y DEMIANENKO 2021 M PINTO 2022 112530 G FAN 2021 L LISSA 2021 100044 P ARGALL 2009 469 483 B XU 2022 100018. J LISSA 2020 1 12 P FANG 2022 125679. X XU 2020 230 239 S PROCEEDINGS7THACMINTERNATIONALCONFERENCESYSTEMSFORENERGYEFFICIENTBUILDINGSCITIESTRANSPORTATION ONEFORMANYTRANSFERLEARNINGFORBUILDINGHVACCONTROL ZHANG 2022 556 564 T PROCEEDINGSTHIRTEENTHACMINTERNATIONALCONFERENCEFUTUREENERGYSYSTEMS DIVERSITYFORTRANSFERINLEARNINGBASEDCONTROLBUILDINGS TSANG 2019 1 7 N 2019IEEEINTERNATIONALCONFERENCEENGINEERINGTECHNOLOGYINNOVATION AUTONOMOUSHOUSEHOLDENERGYMANAGEMENTUSINGDEEPREINFORCEMENTLEARNING MBUWIR 2020 1 6 B 2020IEEEINTERNATIONALCONFERENCECOMMUNICATIONSCONTROLCOMPUTINGTECHNOLOGIESFORSMARTGRIDSSMARTGRIDCOMM TRANSFERLEARNINGFOROPERATIONALPLANNINGBATTERIESINCOMMERCIALBUILDINGS ZHAO 2014 76 102 P GRUBINGER 2017 63 71 T CHRISTODOULOU 2019 P SOFTACTORCRITICFORDISCRETEACTIONSETTINGS AKIBA 2019 2623 2631 T PROCEEDINGS25THACMSIGKDDINTERNATIONALCONFERENCEKNOWLEDGEDISCOVERYDATAMINING OPTUNAANEXTGENERATIONHYPERPARAMETEROPTIMIZATIONFRAMEWORK BELLMAN 1966 34 37 R HAARNOJA 2019 T SOFTACTORCRITICALGORITHMSAPPLICATIONS TAYLOR 2009 1633 1685 M ZHU 2020 Z TRANSFERLEARNINGINDEEPREINFORCEMENTLEARNINGASURVEY 2022 ARERAANDAMENTODELPREZZODELLENERGIAELETTRICAPERILCONSUMATOREDOMESTICOTIPOINMAGGIORTUTELA 2016 D22EUROPEANCLIMATEZONESBIOCLIMATICDESIGNREQUIREMENTSREPORTPVSITESWP2T21D22M03BEAR20160831V01 TSIKALOUDAKI 2012 32 44 K 2022 AUSTINENERGYELECTRICITYTARIFFPILOTPROGRAMS 2015 MINISTRYECONOMICDEVELOPMENTINTERMINISTERIALDECREE26JUNE2015APPLICATIONENERGYPERFORMANCECALCULATIONMETHODOLOGIESDEFINITIONPRESCRIPTIONSMINIMUMREQUIREMENTSFORBUILDINGSAPPENDIXAGENERALCRITERIAREQUIREMENTSFORENERGYPERFORMANCEBUILDINGS 2015 MINISTRYECONOMICDEVELOPMENTINTERMINISTERIALDECREE26JUNE2015APPLICATIONENERGYPERFORMANCECALCULATIONMETHODOLOGIESDEFINITIONPRESCRIPTIONSMINIMUMREQUIREMENTSFORBUILDINGSAPPENDIXBSPECIFICREQUIREMENTSFOREXISTINGBUILDINGSSUBJECTENERGYREHABILITATION BIENVENIDOHUERTAS 2019 D HUYNH 2021 A BROCKMAN 2016 G OPENAIGYM BERGSTRA 2011 2546 2554 J PROCEEDINGS24THINTERNATIONALCONFERENCENEURALINFORMATIONPROCESSINGSYSTEMS ALGORITHMSFORHYPERPARAMETEROPTIMIZATION XIN 2013 203 296 Q DIESELENGINESYSTEMDESIGN 3OPTIMIZATIONTECHNIQUESINDIESELENGINESYSTEMDESIGN ZELANY 1974 479 496 M SMITH 2017 S DONTDECAYLEARNINGRATEINCREASEBATCHSIZE WETTER 2020 M 2020BUILDINGPERFORMANCEMODELINGCONFERENCESIMBUILDCOORGANIZEDBYASHRAEIBPSAUSA LIFTINGGARAGEDOORSPAWNOPENSOURCEBEMCONTROLSENGINE CORACIX2023X120598 CORACIX2023X120598XD 2025-01-10T00:00:00.000Z 2025-01-10T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2023 Elsevier Ltd. All rights reserved. https://doi.org/10.15223/policy-017 https://doi.org/10.15223/policy-037 https://doi.org/10.15223/policy-012 https://doi.org/10.15223/policy-029 https://doi.org/10.15223/policy-004 item S0306-2619(22)01855-4 S0306261922018554 1-s2.0-S0306261922018554 10.1016/j.apenergy.2022.120598 271429 2024-01-01T13:32:31.319396Z 2023-03-01 1-s2.0-S0306261922018554-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/MAIN/application/pdf/466471910911fdfaa8a83dcadbea5b8e/main.pdf main.pdf pdf true 6242118 MAIN 24 1-s2.0-S0306261922018554-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/PREVIEW/image/png/52d02df0d9263b0e466c5162cb04d64d/main_1.png main_1.png png 52856 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0306261922018554-ga1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/ga1/DOWNSAMPLED/image/jpeg/3512739d1b05a23eafcc2dd902cf25ca/ga1.jpg ga1 true ga1.jpg jpg 41932 122 301 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr10/DOWNSAMPLED/image/jpeg/f14f3cd9175e7334c989e22092a4b3be/gr10.jpg gr10 gr10.jpg jpg 154972 715 642 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr1/DOWNSAMPLED/image/jpeg/ee5a527a8f363ababf4f733c2b48947a/gr1.jpg gr1 gr1.jpg jpg 66101 420 596 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr2/DOWNSAMPLED/image/jpeg/fbc02186b730d2bbac17057547cfd1fb/gr2.jpg gr2 gr2.jpg jpg 32832 173 624 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr3/DOWNSAMPLED/image/jpeg/00450644381118ef63658caac1bf0361/gr3.jpg gr3 gr3.jpg jpg 63142 349 653 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr4/DOWNSAMPLED/image/jpeg/253b875aa01619d0b2c8ffb2147ec6e4/gr4.jpg gr4 gr4.jpg jpg 93179 235 653 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr5/DOWNSAMPLED/image/jpeg/daa5cedb973dd736123be3ab6df8a869/gr5.jpg gr5 gr5.jpg jpg 108834 664 627 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr6/DOWNSAMPLED/image/jpeg/8837ee135c41b11faeeab16f90d98e46/gr6.jpg gr6 gr6.jpg jpg 122827 604 636 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr7/DOWNSAMPLED/image/jpeg/d783cb549dbb5b44a00f61bbed1f1d3f/gr7.jpg gr7 gr7.jpg jpg 57916 257 602 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-fx1002.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/fx1002/DOWNSAMPLED/image/jpeg/5df82f0808e4a8038b7180cfebc862ff/fx1002.jpg fx1002 fx1002.jpg jpg 107225 388 541 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr8/DOWNSAMPLED/image/jpeg/bde3e1ebf5f24744b812faa516240aaf/gr8.jpg gr8 gr8.jpg jpg 74055 435 602 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-gr14.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr14/DOWNSAMPLED/image/jpeg/ca3aa7ad299b84aab80ece6368f9996c/gr14.jpg gr14 gr14.jpg jpg 103417 733 641 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-fx1001.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/fx1001/DOWNSAMPLED/image/jpeg/24f5cdc2f2e47673cc7d93c312171121/fx1001.jpg fx1001 fx1001.jpg jpg 90562 367 646 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr9/DOWNSAMPLED/image/jpeg/7b41315f023ffdbebc1d534a76af0ca3/gr9.jpg gr9 gr9.jpg jpg 70082 260 638 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-gr13.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr13/DOWNSAMPLED/image/jpeg/19e48dc04446252559d05d0d6c8e27d7/gr13.jpg gr13 gr13.jpg jpg 101276 733 641 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-gr12.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr12/DOWNSAMPLED/image/jpeg/61c6eae5e893484bb25daf7de20d3ab0/gr12.jpg gr12 gr12.jpg jpg 150881 801 639 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-gr11.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr11/DOWNSAMPLED/image/jpeg/ab96f6159fcbb96fcfc1ecc9e4b2554b/gr11.jpg gr11 gr11.jpg jpg 126970 804 639 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-gr15.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr15/DOWNSAMPLED/image/jpeg/b085b5efd1670cf9ac87254ebc856a45/gr15.jpg gr15 gr15.jpg jpg 116416 362 595 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922018554-ga1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/ga1/THUMBNAIL/image/gif/3ce3f2df2e10cdb409f163b2920e158b/ga1.sml ga1 true ga1.sml sml 22249 89 219 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr10/THUMBNAIL/image/gif/e22ca304e4470fae718210a6cd036134/gr10.sml gr10 gr10.sml sml 25729 164 147 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr1/THUMBNAIL/image/gif/811a7c807a137aba14de85b25477cb66/gr1.sml gr1 gr1.sml sml 17788 154 219 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr2/THUMBNAIL/image/gif/66cfe5d65f5e346a66d553691528052a/gr2.sml gr2 gr2.sml sml 11675 61 219 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr3/THUMBNAIL/image/gif/797a7ed20b0ade28c6d754eb0067daa0/gr3.sml gr3 gr3.sml sml 14506 117 219 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr4/THUMBNAIL/image/gif/522dec7d1efa92b0e0bb3b6225b51cb4/gr4.sml gr4 gr4.sml sml 20700 79 219 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr5/THUMBNAIL/image/gif/041f9365750d1dac6cde302ea4f365d6/gr5.sml gr5 gr5.sml sml 17170 164 155 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr6/THUMBNAIL/image/gif/d1bbb733e915b1ded26d9cdbd7b1f141/gr6.sml gr6 gr6.sml sml 20447 164 173 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr7/THUMBNAIL/image/gif/3de3d89376e808cabf99263b95f9324d/gr7.sml gr7 gr7.sml sml 15974 93 219 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-fx1002.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/fx1002/THUMBNAIL/image/gif/3ba12b39879f15aaf5ee970adf782870/fx1002.sml fx1002 fx1002.sml sml 24610 157 219 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr8/THUMBNAIL/image/gif/c551d96f75c4ef0e3ccc404c6353bcd4/gr8.sml gr8 gr8.sml sml 18301 158 219 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-gr14.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr14/THUMBNAIL/image/gif/d5528618bed4e3f5e9bf4d690fcdb3aa/gr14.sml gr14 gr14.sml sml 15157 164 143 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-fx1001.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/fx1001/THUMBNAIL/image/gif/d4118e43dd7cfca8ccfb7ce480c3d131/fx1001.sml fx1001 fx1001.sml sml 17212 124 219 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr9/THUMBNAIL/image/gif/f1c74aa640ee3e2f4f5a2c83094691c5/gr9.sml gr9 gr9.sml sml 16803 89 219 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-gr13.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr13/THUMBNAIL/image/gif/4d17dfd8b80eaca34125af9daf058e9e/gr13.sml gr13 gr13.sml sml 15131 164 143 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-gr12.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr12/THUMBNAIL/image/gif/33fbd9b701c93c68215e2818229d3ac0/gr12.sml gr12 gr12.sml sml 18457 164 131 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-gr11.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr11/THUMBNAIL/image/gif/c2d13cc8ff37870e4d3228ca0a237467/gr11.sml gr11 gr11.sml sml 17267 164 130 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-gr15.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/gr15/THUMBNAIL/image/gif/ab03b7b2157fa29410560893609fea7e/gr15.sml gr15 gr15.sml sml 70900 133 219 IMAGE-THUMBNAIL 1-s2.0-S0306261922018554-ga1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/cbad44d0076d4794a773d4a238de26de/ga1_lrg.jpg ga1 true ga1_lrg.jpg jpg 239732 539 1333 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/36c2e55e78ec2014c9edbd06ceee31db/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 1214353 3170 2845 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/45cb73f37a8dbf03ac86de1c4da54f23/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 484274 1859 2641 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/0893845f4094a5f14a9026dc2365e330/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 168124 766 2765 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/c5dc9dc37d1f8ca403850965f37f7a86/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 332696 1547 2891 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/4d3f82550449e1473d05c60f8fe4cf88/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 516270 1041 2891 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/83555c017275cac15f06d917cbd97fc4/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 607097 2939 2776 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/bcd7058f97804392d1bf0327cb2f4f57/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 692938 2672 2815 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/0eea4cbd5edb88280769ed189c30ede3/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 295777 1137 2667 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-fx1002_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/65e69acfc2485e10a3f72876d747c28d/fx1002_lrg.jpg fx1002 fx1002_lrg.jpg jpg 633671 1718 2395 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/73573439af6dae75c51067b94382f152/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 453314 1927 2667 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-gr14_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/51c43b7bd6a63df07b3ca2b84cbeeaf4/gr14_lrg.jpg gr14 gr14_lrg.jpg jpg 646583 3245 2838 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-fx1001_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/649654dd278b36a99b76c8713721587b/fx1001_lrg.jpg fx1001 fx1001_lrg.jpg jpg 539640 1623 2860 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/ea7e13026bc421dd61febb0d6802b619/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 488337 1153 2824 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-gr13_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/98e6885d943666df720c73f116c8f206/gr13_lrg.jpg gr13 gr13_lrg.jpg jpg 617151 3246 2838 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-gr12_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/74d28facfec53f1e12d7830f4eb2c201/gr12_lrg.jpg gr12 gr12_lrg.jpg jpg 847443 3547 2831 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-gr11_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/917f848e5b1850f4bc5781bf7b33a49a/gr11_lrg.jpg gr11 gr11_lrg.jpg jpg 707436 3561 2830 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-gr15_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/HIGHRES/image/jpeg/e9ab0002b168fe9ce0f2823f2ee6e248/gr15_lrg.jpg gr15 gr15_lrg.jpg jpg 372344 1603 2637 IMAGE-HIGH-RES 1-s2.0-S0306261922018554-si10.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/805ae412336f5a8978d71d31c91544b4/si10.svg si10 si10.svg svg 2412 ALTIMG 1-s2.0-S0306261922018554-si104.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/94fb5a3bc539f65e397e189b0cde58f8/si104.svg si104 si104.svg svg 1434 ALTIMG 1-s2.0-S0306261922018554-si105.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/25dbfb791401978d60a27511c225ac79/si105.svg si105 si105.svg svg 6378 ALTIMG 1-s2.0-S0306261922018554-si106.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/28b25930c1ddd7c35d45e05635497d6a/si106.svg si106 si106.svg svg 14922 ALTIMG 1-s2.0-S0306261922018554-si11.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/41ef94f1b74f4af8684c777476206123/si11.svg si11 si11.svg svg 2483 ALTIMG 1-s2.0-S0306261922018554-si111.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/dfd01ddd7362e103ddf34940723e3eab/si111.svg si111 si111.svg svg 9902 ALTIMG 1-s2.0-S0306261922018554-si112.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/d3be661a445142acd8b28eb86d1f43d7/si112.svg si112 si112.svg svg 17044 ALTIMG 1-s2.0-S0306261922018554-si116.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/8710e9168d6f0f96e1f62778c1c66db9/si116.svg si116 si116.svg svg 3552 ALTIMG 1-s2.0-S0306261922018554-si117.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/f76508e795723653022b24026fb3aa9c/si117.svg si117 si117.svg svg 7901 ALTIMG 1-s2.0-S0306261922018554-si118.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/b5b95e0a5b13a3a1417a523d7ae488cb/si118.svg si118 si118.svg svg 4633 ALTIMG 1-s2.0-S0306261922018554-si119.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/89a668772f2937d05b755c645bd6017e/si119.svg si119 si119.svg svg 8784 ALTIMG 1-s2.0-S0306261922018554-si12.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/cee9cdc3999c1e5f69a1d19f2a61b030/si12.svg si12 si12.svg svg 2093 ALTIMG 1-s2.0-S0306261922018554-si120.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/87d7f4379c550f7e30fdef383f594bb0/si120.svg si120 si120.svg svg 13331 ALTIMG 1-s2.0-S0306261922018554-si121.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/ab5bac03cbc9f57456768a8fcb388f3b/si121.svg si121 si121.svg svg 9424 ALTIMG 1-s2.0-S0306261922018554-si123.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/6d5fbc01e9bb8ea04093e83147a5af00/si123.svg si123 si123.svg svg 8650 ALTIMG 1-s2.0-S0306261922018554-si124.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/aded2e54ae9460e6139207e94fe0f1d4/si124.svg si124 si124.svg svg 13332 ALTIMG 1-s2.0-S0306261922018554-si125.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/a04cca6766b9e6ecb5d5c47efedbd7f3/si125.svg si125 si125.svg svg 7711 ALTIMG 1-s2.0-S0306261922018554-si13.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/8909f28acc06a2ccf4dd75891475c901/si13.svg si13 si13.svg svg 7873 ALTIMG 1-s2.0-S0306261922018554-si131.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/5d279d6b8f0074b59f79ba9e96a6f68a/si131.svg si131 si131.svg svg 19013 ALTIMG 1-s2.0-S0306261922018554-si133.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/a3aa3797d39abeafa88236c785fb8991/si133.svg si133 si133.svg svg 12183 ALTIMG 1-s2.0-S0306261922018554-si134.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/a83ce4701c7ee3640a18bf3f687f88f6/si134.svg si134 si134.svg svg 5216 ALTIMG 1-s2.0-S0306261922018554-si137.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/4bd67e258584c46904b0ac54ca28fa7f/si137.svg si137 si137.svg svg 6786 ALTIMG 1-s2.0-S0306261922018554-si138.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/c8b2f39f0a9afbceb97a7e4a8768d437/si138.svg si138 si138.svg svg 11297 ALTIMG 1-s2.0-S0306261922018554-si14.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/34f052dc215cc4cad19b773e0673f479/si14.svg si14 si14.svg svg 4550 ALTIMG 1-s2.0-S0306261922018554-si140.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/7d2d9ab563cc1fe20d5de58bf13c50d5/si140.svg si140 si140.svg svg 6380 ALTIMG 1-s2.0-S0306261922018554-si141.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/b4fbf9695bdc36497f22991daf91d592/si141.svg si141 si141.svg svg 6457 ALTIMG 1-s2.0-S0306261922018554-si142.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/33d01defa4d254620d015f3b098fd9d0/si142.svg si142 si142.svg svg 10972 ALTIMG 1-s2.0-S0306261922018554-si143.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/bc47bbc78d93b4218c6d21222d5bb94f/si143.svg si143 si143.svg svg 1147 ALTIMG 1-s2.0-S0306261922018554-si144.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/e55b45c71a74352d8f3ca1f3c7216550/si144.svg si144 si144.svg svg 1365 ALTIMG 1-s2.0-S0306261922018554-si145.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/201138e2acb2cd174f47bcf5a589da2d/si145.svg si145 si145.svg svg 1381 ALTIMG 1-s2.0-S0306261922018554-si146.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/a9e7143a16aa3848a32cc94fff7aad2a/si146.svg si146 si146.svg svg 1686 ALTIMG 1-s2.0-S0306261922018554-si15.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/d11dcd742512236da2d4e55aaf4e02ad/si15.svg si15 si15.svg svg 5020 ALTIMG 1-s2.0-S0306261922018554-si156.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/69760f8029515489197b2423349dde4c/si156.svg si156 si156.svg svg 11048 ALTIMG 1-s2.0-S0306261922018554-si157.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/14f214f554470d635a9b417e2905c80c/si157.svg si157 si157.svg svg 10945 ALTIMG 1-s2.0-S0306261922018554-si158.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/261bccfdaa81b9e535a0cee1cdd7bb6e/si158.svg si158 si158.svg svg 10861 ALTIMG 1-s2.0-S0306261922018554-si159.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/fdb8696b09c4e1f3847b2b02e35d40d1/si159.svg si159 si159.svg svg 11856 ALTIMG 1-s2.0-S0306261922018554-si16.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/1080e6c415d7c3765e45fcc99f511ce5/si16.svg si16 si16.svg svg 3033 ALTIMG 1-s2.0-S0306261922018554-si161.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/1fddd95860652b1cac236adc9134e0b8/si161.svg si161 si161.svg svg 3132 ALTIMG 1-s2.0-S0306261922018554-si162.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/f76c2b73f03698eed0d57c073dd774ab/si162.svg si162 si162.svg svg 2545 ALTIMG 1-s2.0-S0306261922018554-si163.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/2bc206eba3d4e740abf80b3ddda43096/si163.svg si163 si163.svg svg 2550 ALTIMG 1-s2.0-S0306261922018554-si164.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/c398509090737bda76fc5c31b239ee4a/si164.svg si164 si164.svg svg 2550 ALTIMG 1-s2.0-S0306261922018554-si165.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/9cd0d22af322337207664c49d677ef2b/si165.svg si165 si165.svg svg 2550 ALTIMG 1-s2.0-S0306261922018554-si18.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/3a3344e1671c732a844280557bbc3aa0/si18.svg si18 si18.svg svg 4499 ALTIMG 1-s2.0-S0306261922018554-si19.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/ed2d4b8461d8e8b56178666b69907d1a/si19.svg si19 si19.svg svg 2796 ALTIMG 1-s2.0-S0306261922018554-si20.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/a736c008a4b3e2e87c7358366fcf7353/si20.svg si20 si20.svg svg 2425 ALTIMG 1-s2.0-S0306261922018554-si21.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/bbcc67880b40311bf6e074abee27f467/si21.svg si21 si21.svg svg 1995 ALTIMG 1-s2.0-S0306261922018554-si23.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/7aa837954e6deaf15a0d52793b30e190/si23.svg si23 si23.svg svg 5511 ALTIMG 1-s2.0-S0306261922018554-si24.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/f7a77134ef0b0efd49e4f1781986be1e/si24.svg si24 si24.svg svg 3391 ALTIMG 1-s2.0-S0306261922018554-si25.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/bd6cb7d4c14b7be2a2c08cae2766266e/si25.svg si25 si25.svg svg 2390 ALTIMG 1-s2.0-S0306261922018554-si27.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/2f1abcf33103cb4f98305f1d5a412bbd/si27.svg si27 si27.svg svg 5252 ALTIMG 1-s2.0-S0306261922018554-si28.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/5f84b49c0e7daff472cf18c64f427b7c/si28.svg si28 si28.svg svg 3217 ALTIMG 1-s2.0-S0306261922018554-si29.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/754f3527be162e0ad322b739fb37b902/si29.svg si29 si29.svg svg 3747 ALTIMG 1-s2.0-S0306261922018554-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/dcbbf762a6126c948067807ecf651746/si3.svg si3 si3.svg svg 1391 ALTIMG 1-s2.0-S0306261922018554-si30.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/7fc780037e7941f0c8d982e2e031f7d6/si30.svg si30 si30.svg svg 3858 ALTIMG 1-s2.0-S0306261922018554-si31.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/f0865cc453e112be44430c2354e4d432/si31.svg si31 si31.svg svg 6448 ALTIMG 1-s2.0-S0306261922018554-si32.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/ecaec5eb48875172aa1fdd91c637f4b0/si32.svg si32 si32.svg svg 6287 ALTIMG 1-s2.0-S0306261922018554-si33.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/036f520ee4ce195d88ca90282fa174f4/si33.svg si33 si33.svg svg 2415 ALTIMG 1-s2.0-S0306261922018554-si34.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/589dd77780b83949903dcafc9185f64f/si34.svg si34 si34.svg svg 2025 ALTIMG 1-s2.0-S0306261922018554-si35.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/57674e917f9dacf68d4c4d432ad806ce/si35.svg si35 si35.svg svg 3541 ALTIMG 1-s2.0-S0306261922018554-si36.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/37723cf06931d8909a2c572433adf6c4/si36.svg si36 si36.svg svg 4555 ALTIMG 1-s2.0-S0306261922018554-si37.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/02145b6a5a8120a0b1b50e59926d4b10/si37.svg si37 si37.svg svg 5691 ALTIMG 1-s2.0-S0306261922018554-si38.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/f39229e47ce5886e1d94c3f927551203/si38.svg si38 si38.svg svg 3284 ALTIMG 1-s2.0-S0306261922018554-si39.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/47ea5b029290e6e9cf5f30ee1577e426/si39.svg si39 si39.svg svg 3532 ALTIMG 1-s2.0-S0306261922018554-si40.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/b10cec684eaace666b32c9464d5a0da1/si40.svg si40 si40.svg svg 3735 ALTIMG 1-s2.0-S0306261922018554-si42.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/75c5d336d12997672b8808205dd594c1/si42.svg si42 si42.svg svg 2700 ALTIMG 1-s2.0-S0306261922018554-si43.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/fe2d1071c3c5724e89a6726e9a157c33/si43.svg si43 si43.svg svg 2395 ALTIMG 1-s2.0-S0306261922018554-si44.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/8795de086c478cbe76fb69041c87c667/si44.svg si44 si44.svg svg 5076 ALTIMG 1-s2.0-S0306261922018554-si46.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/ee72ed2adb21bee54e41f7db6daad684/si46.svg si46 si46.svg svg 3563 ALTIMG 1-s2.0-S0306261922018554-si47.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/c48c6abaf8803437f5a001b76b08d595/si47.svg si47 si47.svg svg 3258 ALTIMG 1-s2.0-S0306261922018554-si48.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/d3396cabe63b70ede9696549449da3cc/si48.svg si48 si48.svg svg 5476 ALTIMG 1-s2.0-S0306261922018554-si49.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/dcbefea4600e5e98488c1fd3cd041697/si49.svg si49 si49.svg svg 5171 ALTIMG 1-s2.0-S0306261922018554-si5.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/3e2fa92509748309f0e2e8fe922d5ef4/si5.svg si5 si5.svg svg 2315 ALTIMG 1-s2.0-S0306261922018554-si51.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/c7adb16b17948c71435fcf4ea1572119/si51.svg si51 si51.svg svg 3173 ALTIMG 1-s2.0-S0306261922018554-si52.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/94548720ef1d4d08fcf29e59dfd26d33/si52.svg si52 si52.svg svg 2868 ALTIMG 1-s2.0-S0306261922018554-si53.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/e776b7d32c84b748fcd78ba6c96f26ca/si53.svg si53 si53.svg svg 4654 ALTIMG 1-s2.0-S0306261922018554-si54.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/8385a6155f6063608169226b56341a9f/si54.svg si54 si54.svg svg 4349 ALTIMG 1-s2.0-S0306261922018554-si70.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/75051bc0eca95836525775ffc6d28475/si70.svg si70 si70.svg svg 1626 ALTIMG 1-s2.0-S0306261922018554-si71.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/d4cb4bdfccf1a984df120a9d94c1137d/si71.svg si71 si71.svg svg 1321 ALTIMG 1-s2.0-S0306261922018554-si76.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/2b73e9a7651c3281faa03fd1008154eb/si76.svg si76 si76.svg svg 11798 ALTIMG 1-s2.0-S0306261922018554-si77.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/ee005c9ad4496635b3d49bb34dd1d4a1/si77.svg si77 si77.svg svg 11657 ALTIMG 1-s2.0-S0306261922018554-si78.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/c7051e7fe8c008530eeb2adb4cfab6b8/si78.svg si78 si78.svg svg 12048 ALTIMG 1-s2.0-S0306261922018554-si79.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/49c3621ff798c1cca666dcc4cb243bd4/si79.svg si79 si79.svg svg 1783 ALTIMG 1-s2.0-S0306261922018554-si84.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/47085db07ded56d4615d4e358055ad1e/si84.svg si84 si84.svg svg 5688 ALTIMG 1-s2.0-S0306261922018554-si85.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/4879b616835ae60f4eaa1fcea1e182f7/si85.svg si85 si85.svg svg 5331 ALTIMG 1-s2.0-S0306261922018554-si86.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/a5c3dbec5ea0c78579fba45d88f7b4eb/si86.svg si86 si86.svg svg 10101 ALTIMG 1-s2.0-S0306261922018554-si87.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/b057871be618b7e5cbee73ba0f9e3dd5/si87.svg si87 si87.svg svg 5806 ALTIMG 1-s2.0-S0306261922018554-si88.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/6a59e4e9d6740fe496424d41693d1df7/si88.svg si88 si88.svg svg 9616 ALTIMG 1-s2.0-S0306261922018554-si89.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/4d4ae2419fa7bb4443113b89b59d2323/si89.svg si89 si89.svg svg 5079 ALTIMG 1-s2.0-S0306261922018554-si9.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/1837149944a24a2d880a78f6e3f8d566/si9.svg si9 si9.svg svg 2072 ALTIMG 1-s2.0-S0306261922018554-si90.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/0b63e3083aba541dd92f06b8cffccaa3/si90.svg si90 si90.svg svg 9453 ALTIMG 1-s2.0-S0306261922018554-si91.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/29fc3b4b23a7c1b9854a59fc2a48f67b/si91.svg si91 si91.svg svg 3514 ALTIMG 1-s2.0-S0306261922018554-si92.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922018554/image/svg+xml/c4b36369d08a4b8e565e50ae4127f630/si92.svg si92 si92.svg svg 9572 ALTIMG 1-s2.0-S0306261922018554-am.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:1003W9Z5C1G/MAIN/application/pdf/aeda56b0355d7b53d9f635645bb2dc60/am.pdf am am.pdf pdf false 10225480 AAM-PDF APEN 120598 120598 S0306-2619(22)01855-4 10.1016/j.apenergy.2022.120598 Elsevier Ltd Fig. 1 Model parameter-based TL for RL agents: comparison between feature-extraction and weight-initialization. Fig. 2 Simplified scheme of the cooling energy system. Fig. 3 Operation modes of the cooling energy system. Fig. 4 Methodological framework adopted in this work. Fig. 5 Learning strategies deployed in target buildings: (a) online transfer learning, (b) online deep reinforcement learning and (c) offline deep reinforcement learning. Fig. 6 Source building and target building configurations. Fig. 7 Electricity price schedules employed for target buildings. Fig. 8 Architecture of the simulation environment. Modified from [16]. Fig. 9 Indoor temperature profile with RBC and DRL for the source building. Fig. 10 Chiller energy consumption and storage SOC evolution with RBC and DRL for the source building. Fig. 11 Total electricity cost and cumulated sum of temperature violations for target buildings with TOU pricing schedule. Fig. 12 Total electricity cost and cumulated sum of temperature violations for target buildings with on\u2013off peak pricing schedule. Fig. 13 Daily cumulative curve of electricity cost and temperature violation for target buildings in Turin and Paris. Fig. 14 Daily cumulative curve of electricity cost and temperature violation for target buildings in Palermo and Helsinki. Fig. 15 Number of training episodes required by offline DRL to reach OTL performance. Table 1 Opaque and transparent envelope features for target buildings. Efficiency configuration U O P [ W / ( m 2 ∗ K ) ] χ i [ k J / ( m 2 ∗ K ) ] U T R [ W / ( m 2 ∗ K ) ] Solar factor g 0 0.16 38.9 0.5 0.49 1 0.45 48.0 2.5 0.35 2 0.34 44.2 1.9 0.65 3 0.18 40.0 1 0.5 4 0.3 43.5 1.3 0.35 Table 2 Time period and indoor temperature conditions in R B C C F for the starting phase. Combination Time period Indoor temperature 1 4 : 00 ≤ t < 5 : 00 T I N T − T U P P ≥ 3 ° C 2 5 : 00 ≤ t < 6 : 00 T I N T − T U P P ≥ 2 ° C 3 6 : 00 ≤ t < 7 : 00 T I N T − T U P P ≥ 1 ° C 4 t ≥ 7 : 00 T I N T − T U P P ≥ 0 ° C Table 3 Variables included in the state\u2013space. Variable Unit Timestep Δ T Indoor Setpoint - Mean Indoor Air °C t, t-1, t-2 S O C T E S \u2013 t, t-1, t-2 Outdoor air temperature °C t Time of the day h t Day of the week \u2013 t Electricity price \u20ac/kWh t, t+1, \u2026,t+24 Occupants\u2019 presence status \u2013 t, t+1, \u2026,t+24 Table 4 Details on action-space. Action Operation mode Cooling fraction 0 0 0 1 1 0 2 −1 −1 3 0 −1 4 1 −1 Table 5 Ranges of DRL hyperparameter values involved in the optimization. Hyperparameter Value # Hidden layers [2, 4] # Neurons per layer [64, 128] Batch size [64, 128] Discount factor γ [0.9, 0.95, 0.99] Actor/Critic learning rate μ [0.00025, 0.0005, 0.00075, 0.001] Reward electricity cost-term weight factor δ [2, 4, 6, 8, 10, 12] Reward temperature-term weight factor β [0.015, 0.03, 0.045, 0.06, 0.075, 0.09] Table 6 Hyperparameters selected for offline DRL, online DRL and OTL. Hyperparameter Offline DRL Online DRL OTL Batch size 128 32 32 Actor/Critic learning rate μ 0.001 0.001 0.0005 Training Episodes 30 1 1 Learning step 30 min Every 3 days Every 3 days Gradient steps 1 20 10 Table 7 Configurations of DRL hyperparameters involved in the optimization. Table 8 Total electricity cost and cumulated sum of temperature violations for all investigated control strategies for each target building. Online transfer learning strategy for enhancing the scalability and deployment of deep reinforcement learning control in smart buildings Davide Coraci Conceptualization Methodology Software Investigation Formal analysis Data curation Writing \u2013 original draft Visualization a Silvio Brandi Conceptualization Methodology Investigation Writing \u2013 review & editing a Tianzhen Hong Methodology Validation Writing \u2013 review & editing b Alfonso Capozzoli Conceptualization Methodology Validation Writing \u2013 review & editing Supervision a \u204e a Politecnico di Torino, Department of Energy, TEBE research group, BAEDA Lab, Corso Duca degli Abruzzi 24 Torino, 10129, Italy Politecnico di Torino, Department of Energy, TEBE research group, BAEDA Lab Corso Duca degli Abruzzi 24 Torino 10129 Italy Politecnico di Torino, Department of Energy, TEBE research group, BAEDA Lab,Department and Organization Corso Duca degli Abruzzi 24, Torino, 10129, Italy b Building Technology and Urban Systems Division, Lawrence Berkeley National Laboratory, One Cyclotron Road Berkeley, CA 94720, USA Building Technology and Urban Systems Division, Lawrence Berkeley National Laboratory One Cyclotron Road Berkeley CA 94720 USA Building Technology and Urban Systems Division, Lawrence Berkeley National Laboratory,Department and Organization One Cyclotron Road, Berkeley, CA 94720, USA \u204e Corresponding author. In recent years, advanced control strategies based on Deep Reinforcement Learning (DRL) proved to be effective in optimizing the management of integrated energy systems in buildings, reducing energy costs and improving indoor comfort conditions when compared to traditional reactive controllers. However, the scalability and implementation of DRL controllers are still limited since they require a considerable amount of time before converging to a near-optimal solution. This issue is currently addressed in literature through the offline pre-training of the DRL agent. However this solution results in two main critical issues: (1) the need to develop a building surrogate model to perform the training task, and (2) the need to perform a fine-tuning process over several training episodes to obtain a near-optimal control policy. In this context, this paper introduces an Online Transfer Learning (OTL) strategy that exploits two knowledge-sharing techniques, weight-initialization and imitation learning, to transfer a DRL control policy from a source office building to various target buildings in a simulation environment coupling EnergyPlus and Python. A DRL controller based on discrete Soft Actor\u2013Critic (SAC) is trained on the source building to manage the operation of a cooling system consisting of a chiller and a thermal storage. Several target buildings are defined to benchmark the performance of the OTL strategy with that of a Rule-Based Controller (RBC) and two DRL-based control strategies, deployed in offline and online fashion. The strategy adopted for OTL emulates the real world implementation with a simulation process by implementing the transferred DRL agent for a single episode in the target buildings. Target buildings have the same geometrical features and are served by the same energy system as the source building, but differ in terms of weather conditions, electricity price schedules, occupancy patterns, and building envelope efficiency levels. The results show that the OTL strategy can reduce the cumulated sum of temperature violations on average by 50% and 80% respectively when compared to RBC and online DRL while enhancing the energy system operation with electricity cost savings ranging between 20% and 40%. The OTL agent performs slightly worse than the offline DRL controller but it does not require any modeling effort and can be implemented directly on target buildings emulating a real-world implementation. Graphical abstract Keywords Online transfer learning Homogeneous transfer learning Intra-agent transfer learning Building adaptive control Deep reinforcement learning Energy efficiency Data availability Data will be made available on request. Nomenclature α Boltzmann temperature coefficient β Temperature term weight of reward function χ i Internal heat capacity [kJ/m2K] δ Electricity cost term weight of reward function γ Discount factor μ Learning rate A t Control action at control time step t c E Electricity buying price [\u20ac/kWh] D S Source domain D T Target domain E C H I L L E R Chiller energy consumption [kWh] E c o s t Electricity cost [\u20ac] E P U M P Circulation pumps energy consumption [kWh] f ( ⋅ ) Objective predictive function g Solar heat gain coefficient Q c a p Capacity of chiller [kW] R E Electricity cost term of reward function R T Temperature term of reward function r t Reward at control time step t R B C C F Rule-based controller part choosing whether to supply cooling energy to the building R B C O M Rule-based controller part choosing the operation mode of the energy system S t + 1 Environment state at control time step t+1 S t Environment state at control time step t S O C T E S State-Of-Charge of the water storage S P I N T Indoor air temperature setpoint [°C] T c h Chiller supply temperature [°C] T I N T Indoor air temperature [°C] T L O W Lower threshold limit of temperature comfort range [°C] T s , m a x Storage temperature upper boundary [°C] T s , m i n Storage temperature lower boundary [°C] T S Source task T T Target task T U P P Upper threshold limit of temperature comfort range [°C] T v i o l Temperature violation [°C] T w x y z Target building configuration code U O P Thermal transmittance of the opaque envelope [W/m2K] U T R Thermal transmittance of the transparent envelope [W/m2K] Acronyms AHUs Air Handling Units AI Artificial Intelligence BESS Battery Energy Storage System BCVTB Building Control Virtual Test Bed CF Cooling Fraction COP Coefficient of Performance DDPG Deep Deterministic Policy Gradient DNNs Deep Neural Networks DRL Deep Reinforcement Learning EV Electric Vehicle HVAC Heating, Ventilation and Air Conditioning IES Integrated Energy Systems IL Imitation Learning KPIs Key Performance Indicators LfD Learning from Demonstration MILP Mixed-Integer Linear Programming ML Machine Learning MPC Model Predictive Control OM Operation Mode OTL Online Transfer Learning PID Proportional\u2013Integrative\u2013Derivative PV Photovoltaic RBC Rule-Based Controller RES Renewable Energy Sources RL Reinforcement Learning SAC Soft Actor\u2013Critic SC Self-Consumption SOC State-Of-Charge TES Thermal Energy Storage TL Transfer Learning TOU Time-Of-Use TPE Tree-structured Parzen Estimator VAV Variable Air Volume 1 Introduction Building energy consumption currently amounts to approximately 40% of global primary energy, of which more than 50% is related to the use of Heating, Ventilation and Air Conditioning (HVAC) systems [1]. In this context, the introduction of advanced energy management strategies is required to support the widespread adoption of Integrated Energy Systems (IES), consisting of Renewable Energy Sources (RES), such as Photovoltaic (PV) system [2], and Battery Energy Storage System (BESS) that aims to improve the Self-Consumption (SC) of the energy produced on-site from PV [3]. Moreover, buildings can exploit other flexibility sources on the thermal side, such as Thermal Energy Storage (TES) and building thermal inertia, that allows to shift or curtail energy demands for HVAC. However, such IES requires appropriate management [4], due to the need of adapting their operation to exogenous factors continuously evolving, as weather conditions, occupancy patterns or electricity tariffs. Although the commonly implemented ON/OFF and Proportional\u2013Integrative\u2013Derivative (PID) control strategies in buildings can be easily implemented and exhibit robust operation, they are reactive and not capable to adapt to changes in the environment to be controlled [5,6]. To overcome such limitations, researchers have explored the application of advanced control strategies that enable the management of energy systems by optimizing multi-objective functions [7]. Among advanced controllers, Model Predictive Control (MPC) is the most widely investigated in recent applications, as it can automatically adapt to changes in boundary conditions thanks to its predictive capabilities [8,9]. MPC has gained considerable attention in the building industry [10] since it has demonstrated a remarkable ability in managing energy systems to enhance indoor comfort conditions while reducing energy consumption [11,12]. However, the generalized implementation of MPC in buildings fails to emerge as its operation relies on the definition of a model for the optimization of the control problem [13], which is a time-intensive process. As a result, MPC deployment is limited in the building industry [14]. In that context, Reinforcement Learning (RL) emerges as a promising technique due to its model-free and data-driven nature, as the agent directly learns the optimal control policy by interacting with the system through a trial-and-error approach [15]. One particular family of algorithms, named Deep Reinforcement Learning (DRL), couples RL with Deep Neural Networks (DNNs) [16]. In this framework, DNNs are employed to approximate RL policy functions and enable the resolution of real-world problems, which are complex and require the definition of a large number of states and actions to properly represent the control problem [17]. In the context of building energy management, RL-based controllers have been implemented in HVAC systems to regulate the fan speed [18,19] or to manage the indoor temperature set-point [20,21] and the supply water temperature at generation level [13,22,23]. Furthermore, RL exhibited excellent capabilities in managing thermal storage by controlling their temperature setpoint [24] or charge/discharge process at single [16,21] and multiple building scale [25,26]. Certain applications in literature evaluate the use of an online training technique to emulate the direct implementation of RL controllers without offline pre-training. The online RL control strategy requires that the optimal control policy is learned while the system is actively controlled [7]. However, this strategy is inefficient since the initial performance of the controller is usually very poor and, as a consequence, a significant training time is required to interact with the environment and achieve a near-optimal control policy. Conversely, the strategy mostly explored in literature foresees an offline pre-training setup of the RL controller before its deployment. Such approach involves the definition of a building surrogate model that can be either data-driven [19] (i.e., building dynamics approximated by means of neural networks) or physics-based [23], developed with modeling software such as Modelica [27] or EnergyPlus [28]. However, the offline pre-training of DRL controllers cannot be performed in buildings with no available (e.g., new buildings) or limited amount of data [29], since a considerable amount of data is required to build the surrogate model of the building. It results that these control strategies are less scalable and generalizable, performing properly only for specific building configurations. In addition, the definition of a model is needed for each building to be controlled, as in the case of the MPC. To address this gap, Transfer Learning (TL) emerges as a promising technique for increasing the scalability of advanced controllers in buildings. TL is a Machine Learning (ML) method that allows the sharing of pre-acquired knowledge for a particular task (i.e., source task) in a different but related problem (i.e., target task) with similar or different domains [30]. The implementation of TL results in a dramatic reduction of the training time required by machine learning models to converge towards a near-optimal solution and is usually applied at the beginning of the training process in the target domain, mainly in the context of supervised machine learning applications [31]. Since ML algorithms suffer from some issues (e.g., the lack of data to adequately train the models) [32], training machine learning-based models to address different tasks (e.g., load forecasting) is challenging [33]. Initial applications of TL are related to image recognition [34,35], game playing [36] and natural language processing [37,38]. However, reusing previous knowledge from various sources could be beneficial in the context of smart buildings. Therefore, in recent years TL was implemented in smart buildings in the context of load prediction [39\u201341], occupancy detection and activity recognition [42,43], building dynamics [44\u201346] and system control [47,48]. In the next section, reference studies about the use of the TL for the sharing of the control policy in buildings are reviewed. Furthermore, the motivations and the novelty of the present contribution are provided. 1.1 Related works on TL applications for advanced controllers in buildings Applications of TL to system control are limited if compared to the others investigated in the context of smart buildings and are mainly dated back to the last three years. Moreover, the implementation of TL in the control field mainly refers to agents based RL, as in the case of robotics [49] or automotive [50]. In the context of building system control, the implementation of TL provides multiple advantages, since it allows the transfer of information between advanced controllers, easing the deployment of such algorithms that are commonly tailored for specific control problems and scaling up the application of such algorithms in buildings with a limited amount of historical data. In addition, TL techniques could lead to the online implementation of RL-based controllers, ensuring acceptable performance from the early stages of deployment. However, the current state of the art concerning TL applications for DRL controllers evaluates the performance of the controller by applying a fine-tuning process performed over several episodes on the target building, as highlighted in the few published papers related to the application of TL for sharing control policy in buildings. Lissa et al. [51] proposed a methodology based on TL to enable the sharing of a RL control policy operating on a HVAC system between different rooms in the same building. In particular, a series of experiments were carried out to evaluate the variance in RL performance compared to the case without TL, as a function of the geometrical and geographical differences of the various rooms, as well as the different sizes of the HVAC system. This approach improved the indoor comfort conditions by reducing the discomfort time of the occupants. A similar methodology was employed by Fang et al. [52] to investigate the cross temporal\u2013spatial transferability of a DRL controller in a HVAC system consisting of a chiller and three Air Handling Units (AHUs) to enhance indoor temperature conditions while reducing energy consumption. In detail, the authors develop a TL methodology to assess the effect of the climate and the number of neural network layers on the knowledge sharing process. As a result, the transfer process is effective when the DRL agent is transferred between buildings located in similar climatic conditions, exhibiting better performance by sharing two of the five layers of the neural network that approximates the control policy in the source building. Furthermore, Xu et al. [53] evaluate TL performances when a RL control policy was transferred from source to target buildings in different climates and with different envelope features and HVAC configurations. This study is the only one in which the use of heterogeneous TL was evaluated since it assessed the possibility of transferring a control policy between buildings with different numbers of thermal zones. Zhang et al. [54] implemented a strategy to transfer a library of RL multi-agent control policies from a multi-zone source building to a target building. Before transferring the control policy, the authors designed a strategy to choose the best pre-trained RL policy among those obtained on the source building for the management of zone temperature setpoints in a Variable Air Volume (VAV) system. As a result, 40% of the energy consumed by the HVAC system was saved on the target building compared to the baseline controller and 50% compared to the RL controller trained from scratch over 5000 episodes. Tsang et al. [55] developed a framework to share the DRL optimal control policy between agents managing the electrical devices in an autonomous household. In detail, dependent devices are grouped to avoid scalability issues and the source controller knowledge is shared with the devices in the same group to advise the control action choice. This approach ensured a reduction of the training time of target device controllers by around 25%. Similarly, the transfer of a RL agent controlling appliances was investigated by Zhang et al. [56]. In this case, the use of TL resulted in a reduction of the RL controller training time on the target buildings, improving its performance from the early stages of implementation compared to the case without transfer. The RL control agent was trained on a benchmark home with the same number and type of appliances and then fine-tuned on the target buildings. TL was evaluated for transferring battery management control policies between similar buildings with an integrated energy system in [57]. In particular, the building similarity was assessed by considering K-shape clustering to group buildings according to their energy consumption patterns. The operation of batteries was planned using a RL controller and transferred to target buildings in the same cluster. This approach allowed to achieve performances in target buildings similar to those of a Mixed-Integer Linear Programming (MILP) controller in 10 days. To conclude, the policy transfer for RL controllers was evaluated at microgrid scale in [47,48]. Fan et al. [47] have developed a methodology to transfer between microgrids a Deep Deterministic Policy Gradient (DDPG) controller, reducing the training time in target building to achieve a near-optimal control policy. The optimal control policy is learned during a training phase developed in the source building to reduce operating costs by learning an optimal scheduling microgrid strategy. Lissa et al. [48] proposed an intra-transfer learning method, named parallel transfer learning, which allowed knowledge to be shared between five different agents during their training process without waiting until the end. This transfer approach was implemented in a microgrid with five homes, each with its energy system consisting of a PV system and a heat pump, and with its DRL controller managing the heat pump for minimizing energy costs. As a result, training time was reduced by a factor of 5 and energy savings of 10% were achieved compared to the case without transfer. 1.2 Novelty and contributions of the paper A fundamental gap emerged from the analysis of the current scientific literature about the application of TL to control policies in buildings. Analyzed applications evaluate the performance of a transferred control policy in a target building by applying a fine-tuning process over multiple episodes. In typical energy and building applications one episode usually represents an entire season (either heating or cooling) if not even a whole year. Thus, following this approach in a real-world context, a transferred control agent could still require multiple episodes before converging to acceptable solution which could translate in several years of implementation. In order to effectively enhance the scalability of DRL controllers in buildings it is desirable that a transferred agent is capable to achieve acceptable performance shortly after its implementation in the target building. Therefore, it is required to develop an Online Transfer Learning (OTL) approach capable to rapidly perform the fine-tuning of the control policy pre-trained in the source building while its already implemented in the target building. A similar approach was previously exploited only to transfer supervised learning models. In [58], the authors shared the knowledge of a pre-trained offline classifier on a source scenario to different targets. In [59], the authors transferred a classification and regression model to predict the building dynamics. To the best of our knowledge, an online transfer learning strategy was not yet explored in the context of building control policy transfer. Following these considerations, the present paper proposes an online transfer learning methodology to share the control policy of a DRL agent, based on a formulation of Soft Actor\u2013Critic (SAC) introduced by Christodoulou [60] capable of handling discrete action spaces. The SAC agent was firstly pre-trained on a source building to minimize electricity cost and enhance indoor temperature conditions. Since the performance of DRL controllers is strongly influenced by hyperparameters, their implementation requires the definition of a method to obtain the optimal set of hyperparameters. Therefore, during the training phase of the DRL agent on source building an automated procedure was carried out to optimize the set of hyperparameters using Optuna [61]. Then, the best source DRL controller was transferred to several target buildings, derived from the source building by varying the weather conditions, electricity price schedules, occupancy schedules and building thermophysical properties. The pre-trained control agent was implemented and fine-tuned in each target building through the proposed methodology. The proposed controller was benchmarked in terms of electricity cost and temperature violations against an online DRL controller and an offline pre-trained DRL controller. Moreover, an additional RBC strategy was introduced as a benchmark to provide a comparison with a traditional control strategy. The experiments were carried out by means of a simulation environment combining EnergyPlus and Python, as in [13,23]. According to the TL classifications discussed in [29,30], the knowledge-sharing methodology developed in this paper is classified as homogeneous transductive transfer learning, as the transfer process is implemented in buildings where DRL controllers operate in a similar domain (i.e., same geometry and energy systems) and with an identical objective function. The knowledge sharing was performed exploiting a model parameter-based TL technique, named weight-initialization, since the target model weights are initialized using the pre-trained model weights. Furthermore, according to [31], our TL method is labeled as intra-agent transfer learning, since the target agents do not know the possible future implications of a new training process for the source agent after the knowledge sharing process. Based on the literature review on transfer learning of DRL controllers in buildings, the main innovative contributions of this paper can be summarized as follows: \u2022 An online transfer learning strategy, based on homogeneous transductive TL, was developed to transfer a DRL controller pre-trained on a source office building to minimize electricity cost while enhancing indoor temperature conditions. The review of the literature shows that transfer learning approaches have been poorly explored for DRL control systems in buildings and the knowledge sharing approaches adopted have been rarely identified with respect to theoretical statements of transfer learning. Moreover, to the best of the authors\u2019 knowledge, an online transfer learning approach has not been explored in the framework of building control systems. \u2022 A pre-trained DRL agent transferred on several target buildings and online deployed was benchmarked with other two DRL control strategies, offline and online deployed without any prior knowledge of the environment to be controlled. Moreover an additional RBC strategy was introduced to benchmark OTL performances. To the best of our knowledge, the implementation of transfer learning in the context of control systems in buildings has not been compared yet with an online DRL strategy. The comparison among OTL and online DRL was the fairest to highlight the benefits of applying transfer learning. In fact the online DRL controller was developed to emulate the direct implementation of a controller that does not require the development of a simulation model of the controlled environment to perform pre-training. \u2022 Several target building configurations were designed, where the best controller pre-trained on the source building was transferred to speed up the training process in target controllers. The source and target buildings differ in terms of weather conditions, electricity price schedules, occupancy schedules and building thermophysical properties, but have the same geometry and energy system. As a result, the effect of each variable on the performance of transfer learning can be independently quantified. The rest of this paper is organized as follows. In Section 2 the theoretical foundations of DRL controllers and TL are described. Section 3 introduces the formulation of the control problem while Section 4 describes the methodological framework. Implementation details concerning source and target buildings, the online transfer learning and the controllers developed in this paper are provided in Section 5. Section 6 outlines the results obtained while Section 7 discusses them before providing conclusions and future directions in Section 8. 2 Methods This section describes the methods adopted in this paper. However, only theoretical foundations regarding transfer learning are described in detail. Theoretical aspects concerning DRL and the discrete SAC algorithm applied in this paper can be found in [15,60,62,63]. 2.1 Transfer learning Transfer Learning is a machine learning method, emerging as a promising technique to reuse the knowledge acquired in a particular task for improving performances in a different but related problem [29]. Knowledge sharing happens at the beginning of the learning process, to accelerate the convergence process of machine learning models over the situation in which learning is performed from the beginning without prior knowledge. The mathematical definition of TL requires the description of the concepts of domain and task, as described by [30]. Specifically, the domain consists of a feature space X and a marginal distribution probability P ( X ) , while the task consists of the label space Y and an objective predictive function f ( ⋅ ) . This function is learned from the training data (represented by a pair ( x i , y i )) and used to approximate the conditional probability P ( y | x ) as well as to predict the label of new instances. The transfer process can occur between multiple domains, however research has focused on the case where knowledge sharing occurs between a source domain D S = ( x S 1 , y S 1 ), \u2026, ( x S n S , y S n S ) and a target domain D T = ( x T 1 , y T 1 ), \u2026, ( x T n T , y T n T ). Thus, according to [29,30], TL is defined as the process that improves the learning of the predictive function in the target domain D T with learning task T T , using the acquired knowledge in the source domain D S with task T S . In general, domains and tasks can be the same or different. TL foresees that knowledge can be shared where source and target have different or similar domains, tasks and solutions. In this regard, according to [29], it is possible to identify some classifications concerning the similarity of tasks (i.e., label classification), features and labels (i.e., space classification), and knowledge sharing modalities (i.e., solution classification). The label classification splits into three categories TL depending on task similarity and label availability: \u2022 inductive transfer learning, considering the availability of labeled data in source and target domains and that source and target tasks are different, without any interest in domain differences; \u2022 transductive transfer learning, considering that source and target domains are different but with the same tasks. In this case labeled data are available only for source domain; \u2022 unsupervised transfer learning, considering the unavailability of labeled data in source and target domains (that could be different or not) and different tasks between source and target. The differences in source and target features (i.e., spaces) and labels are accounted within the space classification. In this case, TL is classified as homogeneous when source and target spaces and labels are identical. Otherwise, TL is classified as heterogeneous when spaces and/or labels differ between source and target. Moreover, TL is classified according to the knowledge sharing method adopted in solution classification: instance-based TL, feature representation-based TL, relation knowledge-based TL and model parameter-based TL. Details about the first three categories are provided in Pinto et al. [29], being outside the scope of this work since it was implemented the model parameter-based TL. This kind of knowledge sharing is widely used for neural networks and involves the sharing of certain parameters or their distributions between source and target tasks, such as model weights. The model parameter-based TL can be divided into 3 sub-categories according to model parameter sharing modes: \u2022 feature-extraction, where weights from pre-trained model are used for some layers that are not domain dependent and do not require further fine-tuning; \u2022 weight-initialization, where the target model weights are initialized using the pre-trained model weights. In this case, an additional fine-tuning process could be performed; \u2022 relational knowledge-based, considering the sharing of data relationship in case of similarity among the source and target datasets. 2.1.1 Transfer learning for RL applications In the context of RL, [31,64,65] give some indications about possible applications of TL for this algorithm. However, to classify TL applications for RL according to the previous categories, it is necessary to establish the correspondence between domain, label space and task defined for generic machine learning problems and state\u2013space, action-space and reward function in the case of RL. In this case, the input feature space (i.e., domain) corresponds to the state\u2013space in the RL framework, while the label space is equivalent to the RL action space. As indicated in [29], in RL the task corresponds to the combination of action space, reward function and transition function. Using this definition, RL applications can be categorized using labels and space classification for general machine learning problems. In detail, this paper explores the use of homogeneous transductive transfer learning, as the state and action spaces are the same between the source and target buildings, while the transition function changes due to differences between the domains in climatic conditions, electricity price schedules, occupancy schedules and building thermophysical properties. Furthermore, the knowledge sharing between RL agents is carried out considering model parameter-based TL: a comparison between feature-extraction and weight-initialization is reported in Fig. 1. In this paper, the weight-initialization TL method is employed as knowledge-sharing strategy between source and target agents. The differences associated with the source of knowledge, availability and required domain knowledge enable the identification of an additional classification for transfer learning. According to Da Silva and Costa [31], it is possible to define: \u2022 Intra-Agent TL, representing transfer methods that do not require explicit communication for accessing to internal knowledge of the agents. In this case, it transferred the knowledge acquired by the source agent up to that moment (whether or not the training process has been completed) without the target agent knowing the possible future implications of new training process for the source agent. \u2022 Inter-Agent TL, describing transfer methods to reuse the knowledge from communication with other agents. In this case, it transferred the knowledge already available from another agent at any time during the training process of the source agent, and the knowledge may be transferred bidirectionally, from target to source and vice-versa. According to this classification, our work is categorized as Intra-Agent transfer learning, with the adoption of knowledge transfer named Mentor/Observer [31]: the target agent (i.e., observer) learns an optimal policy by observing the successful optimization of the control problem performed by the source controller (i.e., mentor). To conclude, [31,65] indicate different settings of knowledge reuse in addition to transfer learning, such as: \u2022 Imitation Learning (IL), where the agent in the target domain learns an optimal strategy on a particular task by observing an expert (e.g., RBC) that optimizes the same task. In this case, the target agent is aware of the transitions from the expert (and could store them in a buffer) but is not informed about the chosen set of actions and reward signals. \u2022 Learning from Demonstration (LfD), similar to imitation learning, but in this case the expert controller could inform the target agent about the chosen set of actions and the target policy could be improved by accessing reward signals. In this work, the source agent knowledge is reused by combining Transfer Learning and Imitation Learning, since the target agent policy is initialized using weights from pre-trained source policy, and the target agent buffer is initialized with the transition from RBC warm-up. 3 Control problem formulation In this paper, homogeneous transfer learning is implemented to share the control policy between a source office building in Turin, Italy, and different target buildings. Source and target buildings are characterized by the same geometry and the same energy system. However, targets derived from the source building are located in different weather conditions and have different electricity price schedules, occupancy patterns and envelope features: further details are provided in Section 5.1. The proposed application is carried out for a cooling season lasting 3 months (i.e., from 1 June to 29 August). The case study is conceived to benchmark the performance of transfer learning for a DRL-based controller system with that of RBC and two particular advanced control strategies, offline DRL and online DRL. The building is served by a cooling system consisting of an air-to-water chiller and a cold thermal storage acting as a buffer between the building and chiller. The energy system was modeled using EnergyPlus with available features for chiller and TES. The energy system provides cooling energy to the building through the electric chiller, operating at constant cold water temperature setpoint T c h , or the TES, operating at constant cold water flow rate. The thermal storage operates between a minimum temperature T s , m i n and a maximum temperature T s , m a x , which match respectively the maximum and minimum TES State-Of-Charge (SOC). The cooling energy is delivered to the environment using zone terminals, connected to the carrier fluid circuit in which the cold water flows by means of circulation pumps. Moreover, thermostatic control is considered in this case study, since the supply of energy to the building depends on indoor temperature conditions. A simplified scheme of the analyzed energy system is shown in Fig. 2. The DRL controller is developed to reduce electricity cost associated with the operation of the chiller and circulation auxiliary while maintaining the indoor temperature inside an acceptability range defined between [25, 27] °C, which corresponds to [−1, +1] from the desired indoor temperature setpoint of 26°C by: \u2022 optimal managing the cooling system by choosing between different operation modes as detailed below; \u2022 choosing whether or not to supply cooling energy to the thermal zones. The controller can manage the energy system according to three cooling operation modes, as shown in Fig. 3: 1. Discharging mode (operation mode = -1), where the cooling energy required by the building is delivered by discharging the thermal storage. In this setting, the energy system operates at variable supply water temperature. 2. Chiller mode (operation mode = 0), where cooling energy is provided to the building exclusively by the chiller. In this operation mode, the energy system operates at constant supply water temperature. 3. Charging mode (operation mode = 1), where cooling energy is provided simultaneously to the storage and the building (if needed). In this setting, the energy system operates at constant supply water temperature. The system operation modes are conceived to avoid that cooling energy is supplied to the building by both the chiller and the cold thermal energy storage simultaneously. 4 Methodology This section outlines the methodological framework employed in this paper, offering insights concerning the TL process and the advanced controllers adopted to benchmark TL performance. The methodological process is organized into four stages, as shown in Fig. 4. 4.1 Design of control problem The first stage of the methodological process involves the development of the RBC and the DRL controller implemented in the source building. 4.1.1 Design of rule-based controller The RBC is made up of two parts, one that chooses whether to supply cooling energy to the building (i.e., R B C C F ) and the other which decides the operating mode of the energy system (i.e., R B C O M ). These two agents are not independent, since the mode of operation of the energy system depends on whether cooling energy is delivered to the zone. Further details on the RBC design are provided in Section 5.3. 4.1.2 Design of DRL controller DRL controller is developed to reduce electrical cost and maintain adequate indoor temperature conditions during occupancy hours. The development of the DRL controller involves the definition of its main components, i.e. the action space (which includes all possible actions to be selected by the control agent), the state space (containing all the observations required by the agent to optimize the control policy) and the reward function, intended to be representative of the control problem objective. 4.2 DRL training phase on source building During the second methodological step, the DRL controller is trained on the source building in an offline manner. Details about this training method are provided in Section 4.4.1. During the DRL agent training process, an automated procedure is performed via Optuna [61] to find the optimal configuration of control algorithm hyperparameters since the performance of DRL controllers is considerably influenced by the choice of such variables. As a result, it is identified the best control agent among the analyzed controllers: this is employed during the next stage involving the transfer of the control policy to the target buildings. Further details on the DRL training phase are provided in Section 5.5. 4.3 Transfer of DRL control policy The third step of the framework involves the development of a methodology for the implementation of TL for control policy sharing. In this way, it is shared the optimal control policy of the best agent trained on source building to the controllers to be implemented in the target buildings. The TL strategy employed in this paper is categorized as homogeneous transductive TL according to Section 2.1, since controllers implemented on the source and target buildings address the same task and the same state and action spaces but different probability distribution for state space (i.e., different domains). Furthermore, the transfer strategy employed is the weight-initialization as the knowledge transfer is linked to the sharing of neural network parameters between source and target controllers. In detail, the weights of Actor and Critic networks of the target controllers are initialized using the weights of the pre-trained source agent. Then, a fine-tuning process is performed to enable the updating of the neural network weights allowing the agent to adapt the control policy to the new conditions existing in the target buildings. However, TL approaches available in literature evaluate a fine-tuning process carried out in an offline manner, i.e., repeating this procedure several episodes consecutively. As a result, the scalability of the TL process is limited since it is required the development of a model for each target building to which source agent control policy will be transferred. Therefore, an online transfer learning methodology, described in Section 4.4.3, is developed to transfer the source optimal control policy. 4.4 Performance benchmarking on target buildings In the last methodological step, a robust benchmark with two advanced strategies is provided for OTL with the offline and online (not transferred) DRL controllers which belong to the same family of advanced controllers. Moreover, the RBC was introduced to provide a benchmark with a traditional control strategy that is commonly implemented in real buildings. The offline DRL strategy corresponds to that used during the DRL agent training phase on the source building (i.e., offline deep reinforcement learning), while the online strategy is designed to emulate the direct real-world DRL controller implementation without having any knowledge of the environment to be controlled. Further details about these DRL training strategies are provided in Sections 4.4.1 and 4.4.2. The performances of these controllers are assessed during their implementation on different target buildings, derived from the source building by changing boundary conditions. In detail, nineteen target buildings are evaluated, accounting for different weather conditions, electricity price schedules, occupancy patterns, and envelope efficiencies (i.e., changing the thermophysical characteristics of the opaque and transparent envelope). Thus, a sensitivity analysis is performed to evaluate the effectiveness of TL as a function of the differences between the source and the target buildings. Further details on target building configurations are provided in Section 5.1. 4.4.1 Offline deep reinforcement learning The offline training strategy for a DRL agent, shown in Fig. 5(c), foresees that the training period, named training episode, is repeated multiple times to ensure a stable control policy for the agent. However, this process exhibits a remarkable weakness although it guarantees a stable control policy: in case of changes in the environment to be controlled, controller retraining is required. This recursive training process is difficult to implement in practice, as it would require several episodes (e.g., corresponding to several cooling seasons in this study) before the agent would be able to upgrade the control policy, as well as a significant modeling effort to obtain a model of the building to be controlled. 4.4.2 Online deep reinforcement learning The online DRL training strategy requires that the control agent converges to the optimal policy while actively controlling the system [7]. To imitate a direct real-time implementation, the training of the DRL agent is carried out on a single episode and not for several episodes as in the case of offline DRL. The advantage of this strategy relies on its model-free nature, as it is not necessary to generate a model of the building to be controlled. However, during the early stages of the training period the agent does not have any knowledge of the control problem and the risk that the actions chosen by the controller result in poor performance is significant. The memory buffer of the online DRL agent is initialized with the transitions obtained from the RBC operation (i.e., imitation learning). This procedure is detailed in Section 4.4.3. Moreover, a number of gradient steps higher compared to the offline DRL agent is adopted to ease the exploration process and speed up the learning process after the first week of online DRL implementation [7]. However, a large number of gradient steps could involve the risk that the control agent converges to an optimal but deterministic control policy as the training process proceeds. To mitigate this issue, the value of the time step in which the learning process takes place is increased compared to the offline DRL strategy. A graphical representation of the online DRL strategy is shown in Fig. 5(b) and further details are given in Section 5.6. 4.4.3 Online transfer learning strategy The strategy adopted for OTL emulates a real-time implementation as in the online DRL, but in this case the agent was pre-trained on the source building. Then, the agent was further fine-tuned on the new environmental conditions. The whole process is developed over a single episode. A representation of the implemented OTL strategy is shown in Fig. 5(a). In detail, the knowledge reuse approach is organized into two phases: imitation learning and transfer learning with weight-initialization. The agent pre-trained on the source building is transferred to initialize the target controller, but it does not operate during the imitation learning phase, performed during the first week of the analyzed period (i.e., from 1 June to 7 June), as the RBC is implemented. During this phase, the memory buffer of the OTL agent is initialized with transitions from RBC logical strategy described in Section 5.3. Transitions are stored in the memory buffer during each control time step. This process was found effective for enhancing the OTL agent in learning during the first days of deployment the relation between the chosen action, states (i.e., the evolution of the environment to be controlled), and reward function (i.e., the electricity cost associated with the operation of energy system and the temperature violations). As defined in Section 2.1, this knowledge reuse process is known as imitation learning. After this first phase, weights of the neural networks that approximate the DRL control policy are initialized with those of the source agent. Therefore, the controller is fine-tuned over the cooling season with a strategy guaranteeing that the target agent updates its control policy without completely overriding the pre-acquired knowledge in the source building that could be useful to the target controller. Thus, the value of the learning rate is decreased by half and the learning procedure is modified compared to the training phase of the source DRL controller. As shown in Fig. 5(a), the training period is alternated by steps in which the learning of the agent occurs or not. Concretely, a learning step is defined every n days, starting from the end of the warm-up period. Moreover, to avoid performance degradation for the controller, it is adopted a number of gradient steps higher compared to the offline DRL control strategy. The number of gradient steps denotes the number of batches extracted randomly from the buffer memory on which the gradient is updated at each control time step [7]. Detailed information about the values chosen for the typical parameters adopted for the OTL strategy is given in Section 5.6. 5 Implementation This section discusses the implementation details for the source building and configurations of target buildings. Moreover, it provides a detailed description of the developed simulation environment and the control strategies implemented on source and target buildings. 5.1 Implementation details on source and target building configurations This section provides details about the source and target building configurations in which RBC, offline DRL, online DRL and OTL controllers are implemented. As specified in Section 3, source and target buildings are characterized by the same geometrical features and cooling system. In particular, these buildings consist of three spaces, two 10-person office rooms and one 3-person control room, plus a technical room not served by the air-conditioning system, with a total floor area of 196m2 and a net conditioned area of 97m2. The office rooms and control room are occupied at maximum capacity during the whole occupancy period. The average transmittance values of the opaque and transparent envelope components for the source buildings are 0.16 and 0.55 W/m2K respectively, with a window-to-wall ratio of 7%. The source building is occupied from Monday to Friday between 8:00 and 18:00. The price of electricity supplied from the grid to enable the operation of the energy system in the source building is defined according to a time-based tariff structure (i.e., Time-Of-Use (TOU)) commonly applied in Italy, derived from the 0.143 \u20ac/kWh electricity average price for the period June\u2013September 2021 indicated by Italian grid regulating authority [66] (i.e., ARERA). In detail, three price bands were defined: low price, with a rate of 0.071 \u20ac/kWh (i.e., equal to one-half of the medium electricity price defined by ARERA); medium price, with an electricity rate of 0.143 \u20ac/kWh (i.e., assuming a value equal to the medium electricity price defined by ARERA); high price, with a rate of 0.214 \u20ac/kWh (i.e., equal to 1.5 times the medium electricity price defined by ARERA). The chiller capacity and the power delivered to each thermal zone are determined from the ideal case where the building demand is considered as an external disturbance of the system. According to the ideal-load EnergyPlus calculation, the design cooling power to maintain an indoor temperature of 26°C and a relative humidity of 55% during the occupancy period is 1.8 kW per each office zone and 1 kW for the control room. Moreover, the chiller has a 4.7 kW design capacity Q c a p . These design values are derived for the source building from the sizing process when implementing the reference weather file available in EnergyPlus for Turin, Italy (ITA-TORINO-CASELLE-IGDG.epw). Furthermore, the TES is sized considering 3 times the maximum ideal hourly cooling demand of the building. Therefore, according to the ideal-load calculation, the TES size for the source building is 3m3. The same approach was adopted to find these design features for each target building, according to each weather condition. Other specifications for the TES and chiller are the same for source and target buildings. In detail, the thermal energy storage operates between a minimum temperature T s , m i n of 10°C and a maximum temperature T s , m a x of 18°C, which match respectively the maximum ( S O C T E S = 1) and minimum ( S O C T E S = 0) state of charge. The design water mass flow rate during the charging phase is 0.2 kg/s while for discharging phase corresponds to the sum of the design mass flow rates of the office rooms and control room, equal to 0.35 kg/s. The chiller supply water temperature at the outlet is 7°C, while reference leaving and entering fluid temperatures are respectively 6.7°C and 35°C. These two features are employed by the EnergyPlus chiller model to provide the reference Coefficient of Performance (COP) value, equal to 2.7. Source and target buildings differ in terms of weather conditions, electricity price schedules, occupancy schedules and building thermophysical properties, but have the same geometry and energy system. As a result, the effect of each variable on the performance of TL can be independently quantified. The target buildings were evaluated in four different cities (i.e., weather conditions) and employed different electricity price schedules and occupancy schedules to explore the capabilities of the transferred agent in adjusting the pre-trained control policy from source building considering these changes. The examined configurations of target buildings are shown in Fig. 6. Each target building configuration is denoted by the code T w x y z , where w [0, 3] refers to the climatic conditions investigated, x [0, 1] and y [0, 1] to the price and occupancy schedules and z [0, 4] to the building envelope features considered. The impact of climate on the control policy transfer process was evaluated considering the same (i.e., Turin) or similar (i.e., Paris) climatic conditions as those of the source building, as well as very different conditions, in warmer (i.e., Palermo) or colder (i.e., Helsinki) locations. These localities were chosen according to the classification established by the European Commission based on Cooling and Heating Degree Days. Each city represents a particular climate type (e.g., mediterranean climate for Palermo) according to the weather classification described in [67,68]. Furthermore, the target building configurations are distinguished by the electricity price and occupancy schedules employed. In detail, two price schedules were considered, as shown in Fig. 7: the first schedule (i.e., 0) is based on TOU as in the source building, while the other (i.e., 1) is an on\u2013off peak price scheme based on the Austin (Texas) electricity tariffs [69]. Specifically, an off-peak rate (0.029 \u20ac/kWh) during the period 20:00\u20137:00 and an on-peak rate (0.063 \u20ac/kWh) during the daytime period 7:00 - 20:00 were assumed. Two occupancy schedules were implemented in the target buildings and these differ in terms of weekdays and occupancy hours. The first occupancy schedule (i.e., 0) assumes that the building is occupied during the period Monday\u2013Friday 8:00\u201318:00, while the other schedule (i.e., 1) evaluates the presence of occupants during the period Monday\u2013Sunday 7:00\u201319:00. Eventually, different combinations of envelope efficiency were assessed for each target building, matching the thermophysical properties of the opaque envelope (i.e., opaque thermal transmittance U O P and internal heat capacity χ i ) and the transparent envelope (i.e., transparent thermal transmittance U T R and solar heat gain coefficient g ). The five envelope efficiency combinations employed for target buildings are shown in Table 1. The envelope efficiency configuration 0 refers to the source reference building, while the others are defined according to the building standards of each locality in which the requirements for the thermophysical features are specified. The thermophysical properties values of the reference buildings for Palermo (i.e., efficiency configuration 1), Paris (i.e., efficiency configuration 2), Helsinki (i.e., efficiency configuration 3) and Turin (i.e., efficiency configuration 4) were chosen according to [70\u201373]. 5.2 Simulation environment The experiments were conducted via a co-simulation environment integrating EnergyPlus [28] and a Python interface based on OpenAI Gym [74]. Fig. 8 shows the architecture of the developed co-simulation environment, modified from [16]. Building dynamics and energy system are modeled in EnergyPlus. At each simulation time step, the EnergyPlus building model receives in input the control actions from the Python side and information about weather conditions from the EnergyPlus reference weather file. The outputs of this model consist of information on the energy system (i.e., TES SOC), indoor conditions (i.e., indoor air temperature, occupancy status) and additional information (i.e., outdoor air temperature, weekday and hour of the day) included on the state\u2013space of the controller. Control systems are developed in Python. In detail, the outputs from the EnergyPlus side and information about the electricity price are provided as inputs to the Python side, while the outputs are the control actions (i.e., cooling system operation mode and cooling fraction to thermal zones). The RBC and DRL agent select the control action at each time step according to the state space information and the reward function. The two software are interfaced through the Building Control Virtual Test Bed (BCVTB), operating as a middleware [23], and the ExternalInterface function of EnergyPlus. The interaction between Python and EnergyPlus is dynamic and occurs during each simulation time step. However, it can occur that the agent does not perform a control action in each simulation time step. In this case, time steps are distinguished between control and simulation. The simulation time step was set to 15 min since it ensures an optimal convergence of numerical results during the EnergyPlus simulation. However, in this work the control time step was not set equal to the simulation time step since it is not optimal to perform an action every 15 min in an energy system including a TES. As a result, the control time step was set to 30 min to adequately take into account for the thermal inertia of the TES. In this framework, each control action (performed every 30 min) was applied to every two simulation time steps. A similar approach was adopted in [13,23]. 5.3 Rule-based control strategy The RBC is made of two agents that decide to provide cooling energy to the environment ( R B C C F ) and the operation mode of the energy system ( R B C O M ). The R B C C F control logic consists of two parts, a pre and post first switch ON phase, where the agent starts to supply cooling energy to the building according to specific indoor temperature conditions and the time of the day during working days. The R B C C F starts to supply cooling energy to the thermal zone according to the requirements shown in Table 2. The four combinations listed in Table 2 of start time window and indoor temperature conditions resulted from a sensitivity analysis where different thresholds were tested to minimize temperature violations during the early stages of the occupancy period. After the starting phase, cooling energy is supplied to the building until the indoor temperature falls below the lower threshold of the acceptability range T L O W (i.e., 25°C). Conversely, when the indoor temperature rises over the upper threshold of the acceptability range T U P P (i.e., 27°C), the R B C C F agent starts again to provide energy to the building. The cooling energy supply is interrupted when occupants leave the building (i.e., 18:00). The second agent ( R B C O M ) manages the cooling energy system to select its operation mode. In detail, when the electricity price is low and the S O C T E S is lower than 0.75 (i.e., corresponding to a TES temperature of 12°C), the R B C O M operates the cooling system in charging mode, until the electricity price rises above the minimum value or the S O C T E S reaches the maximum value (i.e., S O C T E S equal to 1 or TES temperature equal to 10°C). When the R B C C F decides to supply cooling energy to the building and the electricity price is not low, R B C O M operates the cooling system in discharging mode if S O C T E S is not zero and in chiller mode if the storage is empty. 5.4 Design of DRL control strategy In this section, details about the design of the reward function and state\u2013action spaces are provided. Furthermore, the setup of the agent during the training phase on source building is discussed. 5.4.1 Design of state\u2013space The choice of observations to be included within the state\u2013space is fundamental for the DRL controller as it enables the agent to understand the effect of the selected action on the controlled environment. Furthermore, the state\u2013space should be constituted by variables easily measurable and that speed up the control problem optimization, increasing the chances of its real-world implementation. The state variables used in this work are shown in Table 3, with a detail relative to the reference time step (i.e., specified as a function of the actual control timestep t). Incorporating the Outdoor air temperature in the state space was required to evaluate its impact on building energy consumption. Indoor temperature conditions were evaluated employing the difference between the indoor setpoint temperature and the actual indoor air temperature. This variable was observed at the current time step and at the two previous timesteps (i.e., t-1 and t-2), to assess the temperature evolution in the building over time and account for the thermal dynamics effect of the building and energy system [16]. For the same reason, the state of charge of the cooling thermal storage (TES SOC) was evaluated during the same timesteps. Introducing the SOC within the state space was required to provide the agent with adequate information for better managing the cooling system. Since it is required that the DRL agent reduces the cost of electricity withdrawn from the grid, the Electricity price was included within the state space. In addition, its perfect predictions over the next 12 h were accounted for enabling the agent to optimally choose the operation mode of the cooling system. The information regarding the presence of occupants in the building was provided to the agent through the (0, 1) binary variable Occupants\u2019 presence status, included in the state\u2013space for the current time step and the following 12 h. To conclude, the occupancy schedule could be recognized by the controller by combining this feature with the last two variables contained in the state\u2013space, Day of the week and Time of the day. 5.4.2 Design of action-space Considering that the discrete version of SAC was used as control algorithm, the action space is of the same type. The action space could be expressed as: (1) A = O M x C F = ( o m , c f ) : o m ∈ O M , c f ∈ C F In detail, the DRL controller chose an action in the range [0, 4], each one corresponding to a combination of Operation Mode (OM) and Cooling Fraction (CF). Therefore five possible actions could be selected by the agent according to Table 4 to choose: \u2022 the operation mode of the cooling system (i.e, discharging mode [−1], cooling mode [0], charging mode [1]); \u2022 whether [−1] or not [0] to supply cooling energy to the thermal zone. Moreover, safety constraints were introduced to avoid that the system operated in charging mode when the storage was fully charged (i.e., S O C T E S = 1) and in discharging mode when the storage was empty (i.e., S O C T E S = 0). In these circumstances, the system operated in chiller mode. 5.4.3 Design of reward function The reward function must be defined according to the objectives of the control problem. Therefore, in this case study the reward function was made of two terms, an electricity cost-related term and a temperature-related term, since the DRL controller was designed to reduce electricity cost of the cooling system while improving indoor temperature conditions. Furthermore, the two reward terms were weighted by introducing coefficients δ and β to adjust their importance. The general expression of the reward was defined as follows: (2) R = − ( δ ∗ R E + β ∗ R T ) The electricity cost-related term refers to costs associated with the energy withdrawn from the grid to feed chiller and circulation systems, and it was expressed in the following way: (3) R E = c E ∗ ( E C H I L L E R + E P U M P ) where c E [\u20ac/kWh] is the electricity price for buying defined according to the implemented price schedule, while E C H I L L E R and E P U M P corresponds to chiller and pumping system energy consumption, evaluated in kWh. The temperature related-term was defined according to the presence of the occupant and the indoor temperature conditions. When the building is not occupied, the temperature-term is: (4) R T = 0 During working hours, the temperature-term could have different expressions: \u2022 if T I N T < T L O W − 2 : (5) R T = 50 \u2022 if T L O W − 2 ≤ T I N T < T L O W : (6) R T = ( S P i n t − T I N T ) 3 \u2022 if T L O W ≤ T I N T ≤ T U P P : (7) R T = 0 \u2022 if T U P P < T I N T ≤ T U P P + 2 : (8) R T = ( T I N T − S P i n t ) 3 \u2022 if T I N T > T U P P + 2 : (9) R T = 50 The reward had a fixed value if the temperature was below 23°C or above 29°C to avoid convergence problems for the algorithm related to the high magnitude of the reward, as in the SAC the learning process was influenced by the definition of the Boltzmann temperature coefficient α as a function of the reward magnitude. 5.5 Training setup of DRL agent on source building The DRL agent learns the optimal control policy on the source building before sharing it with the target buildings in the case of OTL. During the training phase the controller is trained in offline manner, whose implementation details are reported in Section 4.4.1. The performance of DRL algorithms depends significantly on the choice of several hyperparameters which have to be chosen accurately. To this end, an automated procedure was adopted to extract the optimal set of hyperparameters using the open-source Python library Optuna [61]. The hyperparameters optimization was performed only during the training phase of the DRL agent on source building. In particular, Optuna minimizes or maximizes an objective function by performing the optimization of the set of hyperparameters provided as input with the corresponding acceptability range. The hyperparameters and the corresponding range values involved in the optimization process are listed in Table 5. In this work, the Tree-structured Parzen Estimator (TPE) was chosen among Optuna sampling algorithms [75] to optimize a multi-objective function, since the DRL agent should minimize the electricity cost while reducing the indoor temperature violations compared to the RBC strategy in the source building. In this case an optimal Pareto-front solution set exists [76], so it was employed the criterion of the minimum Euclidean distance from the ideal point [77] (i.e., the non-real point whose coordinates have the lowest values when separately considering the objectives in the target function). During the automated hyperparameter optimization procedure, twenty agents trained for 30 episodes were considered, and a coordinate point [ E c o s t , T v i o l ] indicating its performance was retrieved for each agent. E c o s t represents the total electricity cost, calculated as the product of the electricity cost withdrawn from the grid and the sum of the energy consumption of the chiller and auxiliaries in kWh, and it is expressed as follows: (10) E c o s t = c E ∗ ( E C H I L L E R + E P U M P ) T v i o l stands as the cumulated sum of the temperature violations during the whole cooling season, as indicated in the following equation: (11) T v i o l = ∑ t = 0 t e n d T v i o l , i A temperature violation T v i o l , i is computed as the absolute temperature difference between indoor temperature and the upper or lower limit of the temperature acceptability range [25, 27] °C, when the indoor temperature falls outside this range during the occupancy period. T v i o l , i could have different expressions according to the indoor temperature value T I N T : \u2022 if T I N T < T L O W : (12) T v i o l , i = T L O W − T I N T \u2022 if T L O W ≤ T I N T ≤ T U P P : (13) T v i o l , i = 0 \u2022 if T I N T > T U P P : (14) T v i o l , i = T I N T − T U P P Therefore, the Euclidean distance between the performance at the end of the training phase of each source DRL controller and the ideal point was calculated. Therefore, the solution with the lowest distance and the best performance compared to the RBC in terms of total electricity cost and cumulated sum of temperature violations was chosen as the best. A training episode includes 90 days, from 1 June to 29 August. Each episode took on average 35 min to be simulated on a machine with an 8th Generation Intel@CoreTMi7-8550U @ 4.0 GHz processor and 16.0 GB RAM. The simulation of an episode (which corresponds to one cooling season) includes both EnergyPlus simulation and Python control process. During the simulation, the exchange of information between EnergyPlus and Python was handled through BCVTB. Thus, 35 min per episode refer to the time required to complete the simulation of one episode for the DRL controller on the source building. However, that time is not relevant in the framework of OTL. In fact, the OTL strategy proposed in the present work was conceived to emulate the real-world implementation of a DRL agent pre-trained on the source building to different target buildings without any further modeling effort. 5.6 Implementation details on online transfer learning and DRL learning strategies This section provides insights about the implementation of the OTL and DRL offline and online learning strategies. The automated optimization of hyperparameters, also including reward weights δ and β and Boltzmann temperature coefficient α , was carried out only for the DRL controller trained on the source building. The hyperparameters were not re-optimized in target buildings since their optimization process should be performed over several episodes. However, in building applications one episode usually represents an entire cooling/heating season, hence a re-optimization appears inconsistent with the online strategy implemented in this work. Therefore, the hyperparameters δ , β and α are the same as those optimized in the source DRL controller for all controllers implemented in target buildings. However, the weight factor δ of the reward electricity cost-term was doubled, after a sensitivity analysis, compared to that of the source DRL agent when the price schedule implemented in the target building was of the on\u2013off peak type. This procedure was necessary to balance the two reward function terms since the TOU price tariff implemented in the source DRL agent is represented by a higher average weekly electricity price than in the on\u2013off peak tariff case. The three advanced control strategies implemented in the target buildings differ in terms of the value of some hyperparameters (i.e., batch size, learning rate, learning step and gradient steps), as indicated in Table 6. Compared to the controller trained on the source building, the period of analysis is the same (i.e., June\u2013August) and the complete set of hyperparameters remains unaltered only for the offline DRL. In the offline DRL setting, a training episode was repeated 30 times before obtaining an optimal solution, with a control time step of 30 min and a batch size of 128. Conversely, the online DRL and OTL strategies were implemented for a single episode aiming to represent the direct application in the real system. Moreover, the batch size value was reduced to 32 since a smaller data volume is available to train the control policy. This results in a faster convergence speed towards a near-optimal solution [78], a prerequisite for a DRL agent directly implemented on the system without offline pre-training. Furthermore, the online DRL and OTL employ larger values of training steps (i.e., every 3 days) and gradient steps (i.e., 20) when compared to the offline DRL strategy. In the case of the OTL, this prevents that the pre-trained control strategy on the source building is not entirely overwritten (i.e., also using a reduced learning rate value), while guaranteeing that the control policy can be optimized according to the different boundary conditions in the target building to be controlled. This approach avoids the over-exploration of the action space since this might result in a deviation from the optimal control policy that the agent may learn at the beginning of the training phase. In the case of online DRL, the use of a gradient step equal to 20 is effective in accelerating the training process during the first weeks of implementation, since the agent in the online DRL configuration has limited amount of available data for training due to the limited experience stored in the buffer at the beginning of the process. Furthermore, using a training step of three days results in performance degradation in the early stages of training but ensures that the control agent acquires a larger number of transitions before performing the next learning stage. Thus, the performance level of the agent improves moving forward in the training period. 6 Results This section outlines the results achieved by implementing the methodological framework described in Section 4. The result of the training on the source building of the proposed DRL controller are presented in the first part of the section. The second part describes the results of the proposed OTL strategy and the relative comparison with RBC and DRL approaches. 6.1 Training of DRL agent on source building As introduced in Section 5.5, the values of the hyperparameters characterizing the DRL controller were optimized through a procedure implemented in the python library Optuna [61]. Twenty different configurations of hyperparameters were analyzed and listed in Table 7. All configurations were trained on 30 episodes with a Boltzmann temperature coefficient α of 0.1. The best configuration was selected according to the criterion of the minimum distance from the ideal point [77] since the optimization process involves two different objectives (i.e., minimization of total electrical cost and minimization of cumulated sum of temperature violations). According to this criterion the 9th configuration, highlighted in yellow in Table 7, was the best among the twenty analyzed configurations. Compared to the RBC, the DRL controller achieved an electricity cost saving of 19.6% ( E c o s t , D R L = 56 . 6 \u20acvs E c o s t , R B C = 70 . 4 \u20ac), as well as a significant enhancement in indoor temperature conditions, due to a 69% reduction in the value of cumulated sum of temperature violations over an entire cooling season ( T v i o l , D R L = 54 . 7 °C vs T v i o l , R B C = 176 . 2 °C). Figs. 9 and 10 provide details about the comparison between the DRL and RBC controllers implemented on the source building. Fig. 9 shows the indoor temperature profiles obtained with the RBC and DRL agent during 15 days of the analyzed period, while Fig. 10 provides insights about the chiller consumption (on the top panel) and SOC evolution (on the bottom panel) for both controllers and during the same period evaluated in Fig. 9, with a detail on the electricity price tariff. The DRL controller achieved better performance in terms of indoor temperature control and reduction of electricity cost with respect to the RBC through a more accurate management of the energy system. As shown in Fig. 9, the DRL agent scheduled in advance the supply of cooling energy to the thermal zones compared to RBC. Moreover, the DRL controller maintained a better control over the indoor temperature values during the day limiting the number of times in which indoor temperature raised above 27°C. Although the building was not occupied, the implementation of DRL controller results in some temperature drops during weekends. This pattern is linked to the formulation of the reward function, since the temperature drops are associated with the supply of cooling energy to thermal zones by means of the activation of the chiller during low-cost hours, as indicated in Fig. 10. As a result, the indoor environment is pre-cooled to ensure that the temperature is maintained as close as possible to the temperature acceptability range during the early stages of occupancy period on Monday. Moreover, this behavior allows to delay the operation of TES compared to RBC. From Fig. 10 can be observed how the DRL controller managed the cooling system by operating the chiller in charging mode during low-price periods (in light gray) to supply energy to the environment and to charge the TES. In detail, the TES was fully charged at the end of the low price period to operate the system in discharging mode during the medium or high electricity price periods to save energy related to the operation of chiller by maximizing the utilization of the TES. Contrarily, the RBC usually operated the system in chiller mode during high-price periods (dark gray) since TES was discharged before the end of the occupancy period when the building required cooling energy to meet indoor temperature requirements. Furthermore, during weekends (i.e., 07/03\u201307/04 and 07/10\u201307/11) RBC and DRL controllers managed differently the cooling system. In particular, RBC charged the TES during the early stages of weekend, while DRL controller charged the TES by the end of the weekend. As a result, DRL minimized TES losses as well as holding the maximum SOC at the beginning of the occupancy period in contrast to RBC. 6.2 Performance benchmarking of online transfer learning with RBC and DRL control strategies on target buildings This section presents the results derived from the implementation for a cooling season lasting 90 days (i.e., from 1 June to 29 August) of the OTL strategy on the target buildings analyzed, providing a benchmark with the performance achieved with the RBC and the offline and online DRL controllers developed. OTL and online DRL were implemented over a single episode as specified in Section 5.6. Conversely, offline DRL involved a training phase performed for 30 episodes followed by a testing phase performed for 1 episode in which the agent was statically deployed to evaluate the performance of the learned control policy. Table 8 summarizes the performance achieved from the implementation of RBC, Offline DRL (Off-DRL), Online DRL (On-DRL) and OTL (highlighted in yellow) in all target buildings in terms of electricity cost and temperature violations. Each target building is denoted by the code T w x y z , where indices refer respectively to the considered climatic condition (w), price schedule (x), occupancy schedule (y) and envelope efficiency (z). Overall, it can be noticed that the OTL agent performed better in terms of both total electricity cost and cumulated sum of temperature violations with respect to RBC and online DRL. However, the OTL agent was outperformed by offline DRL solution. This pattern was expected since offline DRL controllers had at their disposal several episodes (i.e., 30 episodes) for each target building to converge to the optimal control policy. Conversely online DRL and OTL strategies relied on a single simulation episode to emulate the direct implementation of these controllers in physical systems. In detail, OTL implementation led to an electrical cost higher between 1% and 13% as well as worse performance in terms of indoor temperature control compared to offline DRL (e.g., cumulated sum of temperature violations are twice or three times higher than those obtained from the offline DRL implementation for target buildings located in Palermo and Helsinki). However, it should be mentioned that the offline DRL agent training process involves the definition of a model that emulates the behavior of the building, contrarily to the case of the OTL. Conversely, the OTL control strategy performed better than the online DRL controller since it had at its disposal information about the control policy pre-trained on the source building. In particular, the OTL was capable to achieve better performance than the online DRL since the boundary conditions between source and target buildings were similar. Thus, part of the knowledge of the control policy trained on the source building was effectively exploited to reduce learning time in the target buildings by the OTL agent. As a result, OTL agent achieved cost savings ranging between 16% (target building T 2110 ) and 41% (building T 2112 ) as well as ensured an average reduction of the cumulated sum of temperature violations over all experiments up to 82% compared to online DRL agent. Furthermore, OTL controller achieved savings up to 20% (building T 1000 ) in terms of electricity cost and a reduction between 30% and 60% in temperature violations compared to RBC. A more detailed overview of the results achieved by RBC, offline DRL and OTL is provided in Figs. 11 and 12. In these figures the target buildings were grouped according to the implemented price tariff. These figures show the target buildings arranged in ascending order with respect to the degree of change in weather conditions from the source building. Therefore, from left to right the buildings located in locations with equal (Turin), similar (Paris), colder (Helsinki), and warmer (Palermo) climates are clustered. Thereby, these figures show how the performance of the OTL agent varied as a function of the differences between the source and target buildings. The performance of the online DRL agent has not been reported in these figures since it was worse compared to the other implemented controllers. The scatter plot in Fig. 11 displays the results in terms of total electricity cost and cumulated sum of temperature violations for the 7 target building configurations in which the TOU price schedule was implemented. Conversely, Fig. 12 provides the same details for the 12 target buildings in which on\u2013off price schedule was applied. Figs. 11 and 12 show that the performance of the control policy transfer on target buildings matched or was slightly worse than those obtained using offline DRL for the buildings located in Turin or Paris. Differences in total electricity cost and cumulated sum of temperature violations increased between the OTL and offline DRL controllers as weather differences (colder and warmer climates than the source building climate) increase. Moreover, considering the same climate but different occupancy schedules led to an increased gap between OTL and offline DRL as well, but to a smaller extent than modifying the weather conditions. Following these considerations, weather differences between source and targets were identified as the most important influencing factor of the performance of the proposed transfer learning methodology. Since the analyzed case study focuses on thermal energy management, the importance of weather differences on the effectiveness of the proposed OTL methodology can be explained by the influence of the climatic conditions on the patterns of the thermal loads of the considered building. However, if the price schedule implemented was of the on\u2013off peak type and the climate was similar to the source building (i.e., Turin or Paris), the trend was reversed, as the change of the building occupancy pattern from schedule 0 to 1 causes a reduction in the performance gap between OTL and offline DRL in terms of cost and temperature violations (e.g., T 0100 vs T 0110 ). In the case of target buildings implementing the occupancy schedule 1 (i.e., building for which the y-value in the code T w x y z is equal to 1) the OTL strategy achieved a greater improvement of the performance with respect to the RBC strategy than in the case in which the occupancy schedule 0 was selected considering the same weather conditions. Figs. 13 and 14 compare the cumulative curves of the daily total electricity cost and sum of temperature violations over the considered cooling season for the online and offline DRL (at the 30th episode) controllers and OTL controller. In particular, this outcome is shown for 4 particular target buildings, each related to investigated weather conditions (Turin and Paris in Fig. 13 and Palermo and Helsinki in Fig. 14) along with all possible differences compared to source building: electricity price schedule, occupancy pattern and envelope efficiency. For all target buildings examined, the costs linked to the implementation of the online DRL controller were higher than those obtained via the offline DRL agent and the controller trained with OTL strategy as expected, since it needed to explore behaviors in the early period that could be harmful, lacking a priori knowledge of the system. Moreover, the cumulative cost curves for OTL and offline DRL were similar over the cooling season. The cumulative curve of daily temperature violations exhibited a similar trend in all buildings analyzed in Figs. 13 and 14 for the online DRL agent. In detail, after the first week of RBC implementation, the slope of the cumulative curve for online DRL agent (in blue) tended to infinity for approximately 10\u201315 days, due to the need for the controller to gain experience from interacting with the environment. Thereafter, the slope of the cumulative curve decreased since the control policy starts to converge to a near-optimal solution. For the OTL and offline DRL agents, the difference between the daily cumulative violation value for these controllers reached a maximum in the first 20 days of June, and then kept on the same deviation over the entire period for buildings located in Helsinki and Palermo as represented in Fig. 14 or decreased for those located in Turin or Paris as indicated in Fig. 13. In summary, the implementation of the proposed OTL strategy achieved acceptable performance during early stages of deployment in all target buildings. The OTL performed better in terms of both electricity cost and temperature violations than the online DRL control strategy. This comparison between OTL and online DRL is fair since for both strategies the offline pre-training phase was not performed for the target buildings and highlights the benefits of exploiting transfer learning techniques for DRL controllers. The offline DRL performed better in terms of both electricity cost and temperature violations than OTL. However, this comparison is not fair since the offline DRL controller had the possibility to refine its control policy over multiple episodes for the target buildings. Eventually, the results obtained showed that differences in climatic conditions have the greatest impact on the performance of the OTL, as the performance gap with the offline DRL increases with the differences in climatic conditions between the source and the target buildings. 7 Discussion This paper focused on the development of an effective transfer learning methodology to share a DRL-based control policy for the management of a TES-based cooling system in office buildings. A DRL agent was pre-trained offline on a source building to minimize electricity cost associated to chiller operation while maintaining indoor air temperature conditions. To this purpose, the controller can decide the operating mode of the cooling system and whether or not to supply cooling energy to the thermal zone. The best pre-trained control policy was transferred to several target buildings characterized by the same geometry and the same energy system of the source but with different weather conditions, electricity price schedules, occupancy patterns and building envelope levels of efficiency. The proposed transfer learning methodology was designed to enhance the scalability and deployment of DRL-based control strategies for the built environment that minimized utility costs while providing occupant comfort. The innovative aspect of the proposed methodology with respect to the current scientific literature relies in the approach adopted to transfer a pre-trained control policy. Conventional applications of transfer learning reported in literature evaluate the performance of the controller by applying a fine-tuning process performed over several episodes. However, it is the authors\u2019 opinion that this approach is unable to fully demonstrate the applicability of transfer learning in improving the scalability of a DRL agent in a real-world context. If multiple episodes (i.e., a cooling season in the present study) are required for the transferred control policy to reach acceptable performance, then, in a real-world context, it would possibly take years of deployment. To overcome this limitation, an OTL controller was conceived to effectively assess if a transferred control policy is capable to guarantee acceptable performance within a reasonable amount of time thus enhancing the scalability of DRL control strategies in real-world context. The proposed OTL control strategy was benchmarked against an RBC, an offline DRL controller (i.e., which corresponds to the same control strategy employed during the training phase of the source DRL agent) and online DRL controller. The proposed solution showed excellent performance on the nineteen considered target buildings. The OTL controllers were capable to reduce electricity cost and to enhance indoor air temperature control with respect to RBC and online DRL controllers. The online DRL control strategy was conceived to emulate the implementation of a control agent with no prior training in a real building. Considering that the development of a simulation model of the controlled environment to perform pre-training is not required, this strategy represents the fairest comparison for the proposed OTL approach to highlight the benefits of applying transfer learning. Several KPIs were introduced in literature reviews on transfer learning regardless of the application domain and type of transfer model. However, for building control systems many Key Performance Indicators (KPIs) are problem-dependent, so cannot be used directly and should be readjusted according to each case study. The KPI Performance with fixed number of epochs defined in [65] can be employed to compare the performance of OTL and online DRL controllers. Therefore, in this paper this metric was adopted to assess the performance comparison between the online DRL agent and OTL at the end of the single episode in which they were evaluated. This KPI can be computed per each target building (and separately in terms of total electricity cost and cumulated sum of temperature violations) as the relative percentage difference between the performance of the OTL and online DRL controllers reported in Table 8. The offline DRL controller was developed to emulate the performance of an agent pre-trained offline for several episodes. This strategy performs better than the OTL strategy since it could learn a mapping between states and actions that is more effective considering the opportunity of interacting with the building for a longer time. However, the modeling effort required to produce a model representing the environment to be controlled for the offline DRL controller constitutes a drawback that prohibits its deployment in real buildings even though it performs better than the other controllers. The development of a building surrogate model demands an amount of time not compatible with the real-time implementation of DRL controller, as well as in-depth domain expertise. In the present work, the offline DRL agent was trained from scratch and requires several training episodes (i.e., several cooling season) to achieve the same performance level that the online TL achieves in one cooling season. To this end, Fig. 15 shows a lollipop chart specifying for each target building the number of episodes needed for the offline DRL to perform as well as the OTL. Target buildings were arranged in ascending order with respect to the degree of change in weather conditions from the source building (i.e., Turin, Paris, Helsinki, Palermo). This analysis suggests that OTL implementation was more effective as the number of episodes required for offline DRL to achieve the same performance as OTL increases. It can be observed that offline DRL requires a higher number of training episodes to achieve similar performance to OTL when the climatic conditions of the target buildings are more similar to those of the source building. In fact, the offline DRL controller was pre-trained for almost 30 episodes to achieve performance in terms of electricity costs and indoor temperature conditions equal to OTL for target buildings located in Turin and Paris (indicated in Fig. 15 respectively in dark green and light green). Conversely, the implementation of OTL was less effective as weather condition differences increase, as in the case of target buildings located in Helsinki (in blue) or Palermo (in orange), since the offline DRL control strategy was trained for a lower number of episodes to achieve OTL performance. As a result, climatic differences have the greatest impact on the quality of the transfer learning process considering that they impact on the magnitude and distribution of cooling load [46]. Therefore, the definition of building archetypes by climate type could be useful to transfer the control policy between buildings of the same group, improving the performance of the developed knowledge sharing methodology. Eventually, the results obtained demonstrate that the proposed OTL methodology can lead to DRL agents performing better that their RBC counterparts while considerably reducing the implementation effort compared to the offline DRL training framework. 8 Conclusion The present paper proposes an OTL strategy for enhancing the generalizability and scalability of DRL controllers in buildings. The proposed methodology was employed to test the performance of homogeneous transductive transfer learning, considering that DRL controllers operate in office buildings with the same geometry and energy system but different weather conditions, electricity price schedules, occupancy patterns and building thermophysical properties. This application exploits a simulation environment in which the BCVTB operates as middleware between EnergyPlus and Python. First of all, a DRL agent based on discrete Soft Actor\u2013Critic was developed for the control of a cooling system consisting of an electric chiller and a cold thermal storage in a source building. The objective of the proposed controller is to maintain adequate indoor temperature conditions during occupancy hours while reducing electricity cost with respect to a RBC, through the management of the operation mode of the cooling system and deciding whether to supply cooling energy to the building. An automated procedure was performed during the training phase of the DRL controller in source building to identify the best configuration of hyperparameters. As required in the OTL framework, the best pre-trained agent was transferred to several target buildings, initializing the weights of the networks that approximate the control policy for advanced controller. Then, the agent is further fine-tuned to update the control policy in relation to the boundary conditions of each building. The performances of the OTL agent were benchmarked with those of the RBC and two DRL control strategies, offline DRL and online DRL. The best-trained agent on source building was more effective than the RBC and provides a 20% reduction in total electricity cost while enhancing the indoor temperature conditions through a 69% reduction in cumulated sum of temperature violations compared to the RBC. Therefore, the implementation of the OTL strategy on the considered target buildings led to an average reduction in cumulated sum of temperature violations of 50% and total electrical cost of 10% compared to a RBC, as well as being more efficient than the online DRL controller with electricity cost savings of between 20% and 40% and an average reduction in the cumulated sum of temperature violations of more than 80%. Conversely, the OTL agent performed worse than the offline DRL controller, with total electricity cost and cumulated temperature violations during occupancy hours higher than those of the offline DRL agent with an average of 4% and 80%. However, this comparison is not fair, as the performance of the OTL agent is evaluated on a single episode, while that of the offline DRL controller is the result of an offline pre-training process on 30 episodes of an agent trained from scratch on each target building. This feature constitutes the major advantage of using transfer learning since the definition of a surrogate model of the environment to pre-train the control policy is not required as in the case of the offline DRL agent. Future works will focus on the following aspects: \u2022 Evaluation of OTL on more complex case studies, including the generation from renewable energy sources and batteries and extending the analyzed period to the heating season. \u2022 Comparison of the OTL with an advanced model-based optimization method that ensures optimal solution instead of the rule-based controller. In that case, the TL process for a model-based controller would also involve the transfer of a model of system dynamics. \u2022 Exploitation of the proposed methodology for heterogeneous and/or inductive transfer learning, assessing the performance of the knowledge sharing process when control policy is transferred between DRL controllers operating in different domains (e.g., different energy systems or different buildings) or having different objective functions. \u2022 Definition of robust metrics and KPIs to benchmark transfer learning performance and to quantify the similarity between source and target buildings to avoid negative transfer learning, possibly leading to worse performance in advanced controllers than in the case without transfer. \u2022 Development of an accurate simulation environment, modeling the energy system through Modelica and integrating it with the building model developed in EnergyPlus and the control system developed in Python through the use of Spawn of EnergyPlus [79]. The use of Spawn enhances the accuracy of the simulation results by providing semi-realistic performance, since the energy system in Modelica and the physical model of the building in EnergyPlus are represented in detail. \u2022 Implementation of the proposed transfer learning strategy in a real-world testbed. In this case the development of an infrastructure to enable the implementation of DRL controllers, as well as their further transfer process, will be required. CRediT authorship contribution statement Davide Coraci: Conceptualization, Methodology, Software, Investigation, Formal analysis, Data curation, Writing \u2013 original draft, Visualization. Silvio Brandi: Conceptualization, Methodology, Investigation, Writing \u2013 review & editing. Tianzhen Hong: Methodology, Validation, Writing \u2013 review & editing. Alfonso Capozzoli: Conceptualization, Methodology, Validation, Writing \u2013 review & editing, Supervision. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. References [1] Yang L. Nagy Z. Goffin P. Schlueter A. Reinforcement learning for optimal control of low exergy buildings Appl Energy 156 2015 577 586 10.1016/j.apenergy.2015.07.050 L. Yang, Z. Nagy, P. Goffin, A. Schlueter, Reinforcement learning for optimal control of low exergy buildings, Applied Energy 156 (2015) 577 \u2013 586. doi:https://doi.org/10.1016/j.apenergy.2015.07.050. [2] Martinopoulos G. Papakostas K.T. Papadopoulos A.M. A comparative review of heating systems in EU countries, based on efficiency and fuel cost Renew Sustain Energy Rev 90 2018 687 699 10.1016/j.rser.2018.03.060 G. Martinopoulos, K. T. Papakostas, A. M. Papadopoulos, A comparative review of heating systems in eu countries, based on efficiency and fuel cost, Renewable and Sustainable Energy Reviews 90 (2018) 687 \u2013 699. doi:https://doi.org/10.1016/j.rser.2018.03.060. [3] Baniasadi A. Habibi D. Al-Saedi W. Masoum M.A. Das C.K. Mousavi N. Optimal sizing design and operation of electrical and thermal energy storage systems in smart buildings J Energy Storage 28 2020 101186 10.1016/j.est.2019.101186 A. Baniasadi, D. Habibi, W. Al-Saedi, M. A. Masoum, C. K. Das, N. Mousavi, Optimal sizing design and operation of electrical and thermal energy storage systems in smart buildings, Journal of Energy Storage 28 (2020) 101186. doi:https://doi.org/10.1016/j.est.2019.101186. [4] Wang Z. Hong T. Reinforcement learning for building controls: The opportunities and challenges Appl Energy 269 2020 115036 10.1016/j.apenergy.2020.115036 Z. Wang, T. Hong, Reinforcement learning for building controls: The opportunities and challenges, Applied Energy 269 (2020) 115036. doi:https://doi.org/10.1016/j.apenergy.2020.115036. [5] Geng G. Geary G.M. On performance and tuning of PID controllers in HVAC systems Proceedings of IEEE international conference on control and applications vol. 2 1993 819 824 10.1109/CCA.1993.348229 Guang Geng, G. M. Geary, On performance and tuning of pid controllers in hvac systems, in: Proceedings of IEEE International Conference on Control and Applications, 1993, pp. 819\u2013824 vol.2. doi:10.1109/CCA.1993.348229. [6] Salsbury T.I. A survey of control technologies in the building automation industry IFAC Proc Vol 38 1 2005 90 100 10.3182/20050703-6-CZ-1902.01397 16th IFAC World Congress T. I. Salsbury, A survey of control technologies in the building automation industry, IFAC Proceedings Volumes 38 (1) (2005) 90\u2013100, 16th IFAC World Congress. doi:https://doi.org/10.3182/20050703-6-CZ-1902.01397. [7] Brandi S. Fiorentini M. Capozzoli A. Comparison of online and offline deep reinforcement learning with model predictive control for thermal energy management Autom Constr 135 2022 104128 10.1016/j.autcon.2022.104128 S. Brandi, M. Fiorentini, A. Capozzoli, Comparison of online and offline deep reinforcement learning with model predictive control for thermal energy management, Automation in Construction 135 (2022) 104128. doi:https://doi.org/10.1016/j.autcon.2022.104128. [8] Serale G. Fiorentini M. Capozzoli A. Bernardini D. Bemporad A. Model predictive control (MPC) for enhancing building and HVAC system energy efficiency: problem formulation, applications and opportunities Energies 11 3 2018 10.3390/en11030631 G. Serale, M. Fiorentini, A. Capozzoli, D. Bernardini, A. Bemporad, Model predictive control (mpc) for enhancing building and hvac system energy efficiency: Problem formulation, applications and opportunities, Energies 11 (3) (2018). doi:10.3390/en11030631. [9] Serale G. Fiorentini M. Capozzoli A. Cooper P. Perino M. Formulation of a model predictive control algorithm to enhance the performance of a latent heat solar thermal system Energy Convers Manage 173 2018 438 449 10.1016/j.enconman.2018.07.099 G. Serale, M. Fiorentini, A. Capozzoli, P. Cooper, M. Perino, Formulation of a model predictive control algorithm to enhance the performance of a latent heat solar thermal system, Energy Conversion and Management 173 (2018) 438\u2013449. doi:https://doi.org/10.1016/j.enconman.2018.07.099. [10] Drgoňa J. Arroyo J. Cupeiro Figueroa I. Blum D. Arendt K. Kim D. All you need to know about model predictive control for buildings Annu Rev Control 2020 10.1016/j.arcontrol.2020.09.001 J. Drgoňa, J. Arroyo, I. Cupeiro Figueroa, D. Blum, K. Arendt, D. Kim, E. P. Ollé, J. Oravec, M. Wetter, D. L. Vrabie, L. Helsen, All you need to know about model predictive control for buildings, Annual Reviews in Control (2020). doi:https://doi.org/10.1016/j.arcontrol.2020.09.001. [11] Oldewurtel F. Parisio A. Jones C.N. Gyalistras D. Gwerder M. Stauch V. Use of model predictive control and weather forecasts for energy efficient building climate control Energy Build 45 2012 15 27 10.1016/j.enbuild.2011.09.022 F. Oldewurtel, A. Parisio, C. N. Jones, D. Gyalistras, M. Gwerder, V. Stauch, B. Lehmann, M. Morari, Use of model predictive control and weather forecasts for energy efficient building climate control, Energy and Buildings 45 (2012) 15 \u2013 27. doi:https://doi.org/10.1016/j.enbuild.2011.09.022. [12] Henze G.P. Dodier R.H. Krarti M. Development of a predictive optimal controller for thermal energy storage systems HVAC R Res 3 3 1997 233 264 10.1080/10789669.1997.10391376 G. P. Henze, R. H. Dodier, M. Krarti, Development of a predictive optimal controller for thermal energy storage systems, HVAC&R Research 3 (3) (1997) 233\u2013264. arXiv:https://www.tandfonline.com/doi/pdf/10.1080/10789669.1997.10391376, doi:10.1080/10789669.1997.10391376. [13] Coraci D. Brandi S. Piscitelli M.S. Capozzoli A. Online implementation of a soft actor-critic agent to enhance indoor temperature control and energy efficiency in buildings Energies 14 4 2021 10.3390/en14040997 D. Coraci, S. Brandi, M. S. Piscitelli, A. Capozzoli, Online implementation of a soft actor-critic agent to enhance indoor temperature control and energy efficiency in buildings, Energies 14 (4) (2021). doi:10.3390/en14040997. [14] Kontes G.D. Giannakis G.I. Sánchez V. De Agustin-Camacho P. Romero-Amorrortu A. Panagiotidou N. Simulation-based evaluation and optimization of control strategies in buildings Energies 11 12 2018 10.3390/en11123376 G. D. Kontes, G. I. Giannakis, V. Sánchez, P. De Agustin-Camacho, A. Romero-Amorrortu, N. Panagiotidou, D. V. Rovas, S. Steiger, C. Mutschler, G. Gruen, Simulation-based evaluation and optimization of control strategies in buildings, Energies 11 (12) (2018). doi:10.3390/en11123376. [15] Sutton R.S. Barto A.G. Reinforcement learning: an introduction 2nd ed. 2018 The MIT Press URL http://incompleteideas.net/book/the-book-2nd.html R. S. Sutton, A. G. Barto, Reinforcement Learning: An Introduction, 2nd Edition, The MIT Press, 2018. http://incompleteideas.net/book/the-book-2nd.html [16] Brandi S. Gallo A. Capozzoli A. A predictive and adaptive control strategy to optimize the management of integrated energy systems in buildings Energy Rep 8 2022 1550 1567 10.1016/j.egyr.2021.12.058 S. Brandi, A. Gallo, A. Capozzoli, A predictive and adaptive control strategy to optimize the management of integrated energy systems in buildings, Energy Reports 8 (2022) 1550\u20131567. doi:https://doi.org/10.1016/j.egyr.2021.12.058. [17] Mnih V. Kavukcuoglu K. Silver D. Rusu A.A. Veness J. Bellemare M.G. Human-level control through deep reinforcement learning Nature 518 7540 2015 529 533 URL http://dx.doi.org/10.1038/nature14236 V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, D. Hassabis, Human-level control through deep reinforcement learning, Nature 518 (7540) (2015) 529\u2013533. http://dx.doi.org/10.1038/nature14236 [18] Valladares W. Galindo M. Gutiérrez J. Wu W.-C. Liao K.-K. Liao J.-C. Energy optimization associated with thermal comfort and indoor air control via a deep reinforcement learning algorithm Build Environ 155 2019 105 117 10.1016/j.buildenv.2019.03.038 W. Valladares, M. Galindo, J. Gutiérrez, W.-C. Wu, K.-K. Liao, J.-C. Liao, K.-C. Lu, C.-C. Wang, Energy optimization associated with thermal comfort and indoor air control via a deep reinforcement learning algorithm, Building and Environment 155 (2019) 105\u2013117. doi:https://doi.org/10.1016/j.buildenv.2019.03.038. [19] Zou Z. Yu X. Ergan S. Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network Build Environ 168 2020 106535 10.1016/j.buildenv.2019.106535 Z. Zou, X. Yu, S. Ergan, Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network, Building and Environment 168 (2020) 106535. doi:https://doi.org/10.1016/j.buildenv.2019.106535. [20] Du Y. Zandi H. Kotevska O. Kurte K. Munk J. Amasyali K. Mckee E. Li F. Intelligent multi-zone residential HVAC control strategy based on deep reinforcement learning Appl Energy 281 2021 116117 10.1016/j.apenergy.2020.116117 Y. Du, H. Zandi, O. Kotevska, K. Kurte, J. Munk, K. Amasyali, E. Mckee, F. Li, Intelligent multi-zone residential hvac control strategy based on deep reinforcement learning, Applied Energy 281 (2021) 116117. doi:https://doi.org/10.1016/j.apenergy.2020.116117. [21] Wang Y. Velswamy K. Huang B. A long-short term memory recurrent neural network based reinforcement learning controller for office heating ventilation and air conditioning systems Processes 5 3 2017 10.3390/pr5030046 Y. Wang, K. Velswamy, B. Huang, A long-short term memory recurrent neural network based reinforcement learning controller for office heating ventilation and air conditioning systems, Processes 5 (3) (2017). doi:10.3390/pr5030046. [22] Zhang Z. Chong A. Pan Y. Zhang C. Lam K.P. Whole building energy model for HVAC optimal control: A practical framework based on deep reinforcement learning Energy Build 199 2019 472 490 10.1016/j.enbuild.2019.07.029 Z. Zhang, A. Chong, Y. Pan, C. Zhang, K. P. Lam, Whole building energy model for hvac optimal control: A practical framework based on deep reinforcement learning, Energy and Buildings 199 (2019) 472 \u2013 490. doi:https://doi.org/10.1016/j.enbuild.2019.07.029. [23] Brandi S. Piscitelli M.S. Martellacci M. Capozzoli A. Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings Energy Build 224 2020 110225 10.1016/j.enbuild.2020.110225 S. Brandi, M. S. Piscitelli, M. Martellacci, A. Capozzoli, Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings, Energy and Buildings 224 (2020) 110225. doi:https://doi.org/10.1016/j.enbuild.2020.110225. [24] Vázquez-Canteli J.R. Ulyanin S. Kämpf J. Nagy Z. Fusing TensorFlow with building energy simulation for intelligent energy management in smart cities Sustainable Cities Soc 45 2019 243 257 10.1016/j.scs.2018.11.021 J. R. Vázquez-Canteli, S. Ulyanin, J. Kämpf, Z. Nagy, Fusing tensorflow with building energy simulation for intelligent energy management in smart cities, Sustainable Cities and Society 45 (2019) 243 \u2013 257. doi:https://doi.org/10.1016/j.scs.2018.11.021. [25] Pinto G. Piscitelli M.S. Vázquez-Canteli J.R. Nagy Z. Capozzoli A. Coordinated energy management for a cluster of buildings through deep reinforcement learning Energy 229 2021 120725 10.1016/j.energy.2021.120725 G. Pinto, M. S. Piscitelli, J. R. Vázquez-Canteli, Z. Nagy, A. Capozzoli, Coordinated energy management for a cluster of buildings through deep reinforcement learning, Energy 229 (2021) 120725. doi:https://doi.org/10.1016/j.energy.2021.120725. [26] Pinto G. Deltetto D. Capozzoli A. Data-driven district energy management with surrogate models and deep reinforcement learning Appl Energy 304 2021 117642 10.1016/j.apenergy.2021.117642 G. Pinto, D. Deltetto, A. Capozzoli, Data-driven district energy management with surrogate models and deep reinforcement learning, Applied Energy 304 (2021) 117642. doi:https://doi.org/10.1016/j.apenergy.2021.117642. [27] Association M. Modelica® - A unified object-oriented language for physical systems modeling. Tutorial 1.4 ed. 2000 Modelica Association URL http://www.modelica.org/documents/ModelicaTutorial14.pdf Modelica Association, Modelica® - a unified object-oriented language for physical systems modeling. Tutorial (Dec. 2000). http://www.modelica.org/documents/ModelicaTutorial14.pdf [28] Crawley D.B. Lawrie L.K. Winkelmann F.C. Buhl W. Huang Y. Pedersen C.O. Strand R.K. Liesen R.J. Fisher D.E. Witte M.J. Glazer J. Energyplus: creating a new-generation building energy simulation program Energy Build 33 2001 319 331 10.1016/S0378-7788(00)00114-6 D. B. Crawley, L. K. Lawrie, C. O. Pedersen, F. C. Winkelmann, Energy plus: energy simulation program, ASHRAE journal 42 (4) (2000) 49\u201356. [29] Pinto G. Wang Z. Roy A. Hong T. Capozzoli A. Transfer learning for smart buildings: A critical review of algorithms, applications, and future perspectives Adv Appl Energy 5 2022 100084 10.1016/j.adapen.2022.100084 G. Pinto, Z. Wang, A. Roy, T. Hong, A. Capozzoli, Transfer learning for smart buildings: A critical review of algorithms, applications, and future perspectives, Advances in Applied Energy 5 (2022) 100084. doi:https://doi.org/10.1016/j.adapen.2022.100084. [30] Pan S.J. Yang Q. A survey on transfer learning IEEE Trans Knowl Data Eng 22 10 2010 1345 1359 10.1109/TKDE.2009.191 S. J. Pan, Q. Yang, A survey on transfer learning, IEEE Trans. on Knowl. and Data Eng. 22 (10) (2010) 1345\u20131359. doi:10.1109/TKDE.2009.191. [31] Da Silva F.L. Costa A.H.R. A survey on transfer learning for multiagent reinforcement learning systems J Artif Int Res 64 1 2019 645 703 10.1613/jair.1.11396 F. L. Da Silva, A. H. R. Costa, A survey on transfer learning for multiagent reinforcement learning systems, J. Artif. Int. Res. 64 (1) (2019) 645\u2013703. doi:10.1613/jair.1.11396. [32] Peirelinck T. Kazmi H. Mbuwir B.V. Hermans C. Spiessens F. Suykens J. Deconinck G. Transfer learning in demand response: A review of algorithms for data-efficient modelling and control Energy AI 7 2022 100126 10.1016/j.egyai.2021.100126 T. Peirelinck, H. Kazmi, B. V. Mbuwir, C. Hermans, F. Spiessens, J. Suykens, G. Deconinck, Transfer learning in demand response: A review of algorithms for data-efficient modelling and control, Energy and AI 7 (2022) 100126. doi:https://doi.org/10.1016/j.egyai.2021.100126. [33] Himeur Y. Elnour M. Fadli F. Meskin N. Petri I. Rezgui Y. Next-generation energy systems for sustainable smart cities: Roles of transfer learning Sustainable Cities Soc 85 2022 104059 10.1016/j.scs.2022.104059 Y. Himeur, M. Elnour, F. Fadli, N. Meskin, I. Petri, Y. Rezgui, F. Bensaali, A. Amira, Next-generation energy systems for sustainable smart cities: Roles of transfer learning, Sustainable Cities and Society 85 (2022) 104059. doi:https://doi.org/10.1016/j.scs.2022.104059. [34] Leon-Malpartida J. Farfan-Escobedo J.D. Cutipa-Arapa G.E. A new method of classification with rejection applied to building images recognition based on transfer learning 2018 IEEE XXV international conference on electronics, electrical engineering and computing 2018 1 4 10.1109/INTERCON.2018.8526392 J. Leon-Malpartida, J. D. Farfan-Escobedo, G. E. Cutipa-Arapa, A new method of classification with rejection applied to building images recognition based on transfer learning, in: 2018 IEEE XXV International Conference on Electronics, Electrical Engineering and Computing (INTERCON), 2018, pp. 1\u20134. doi:10.1109/INTERCON.2018.8526392. [35] Simonyan K. Zisserman A. Very deep convolutional networks for large-scale image recognition 2014 10.48550/ARXIV.1409.1556 K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image recognition (2014). doi:10.48550/ARXIV.1409.1556. [36] Silver D. Huang A. Maddison C.J. Guez A. Sifre L. Mastering the game of Go with deep neural networks and tree search Nature 529 7587 2016 484 489 10.1016/j.energy.2021.122775 D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al., Mastering the game of go with deep neural networks and tree search, nature 529 (7587) (2016) 484\u2013489. [37] Singh S. Kearns M. Litman D. Walker M. Reinforcement learning for spoken dialogue systems Adv Neural Inf Process Syst 12 1999 https://proceedings.neurips.cc/paper/1999/file/36d7534290610d9b7e9abed244dd2f28-Paper.pdf S. Singh, M. Kearns, D. Litman, M. Walker, Reinforcement learning for spoken dialogue systems, Advances in neural information processing systems 12 (1999). [38] Ren Z, Wang X, Zhang N, Lv X, Li L-J. Deep reinforcement learning-based image captioning with embedding reward. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, p. 290\u20138, https://doi.org/10.48550/arXiv.1704.03899. Z. Ren, X. Wang, N. Zhang, X. Lv, L.-J. Li, Deep reinforcement learning-based image captioning with embedding reward, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 290\u2013298. [39] Fan C. Lei Y. Sun Y. Piscitelli M.S. Chiosa R. Capozzoli A. Data-centric or algorithm-centric: Exploiting the performance of transfer learning for improving building energy predictions in data-scarce context Energy 240 2022 122775 10.1016/j.energy.2021.122775 C. Fan, Y. Lei, Y. Sun, M. S. Piscitelli, R. Chiosa, A. Capozzoli, Data-centric or algorithm-centric: Exploiting the performance of transfer learning for improving building energy predictions in data-scarce context, Energy 240 (2022) 122775. doi:https://doi.org/10.1016/j.energy.2021.122775. [40] Oliveira P. Fernandes B. Analide C. Novais P. Forecasting energy consumption of wastewater treatment plants with a transfer learning approach for sustainable cities Electronics 10 10 2021 10.3390/electronics10101149 P. Oliveira, B. Fernandes, C. Analide, P. Novais, Forecasting energy consumption of wastewater treatment plants with a transfer learning approach for sustainable cities, Electronics 10 (10) (2021). doi:10.3390/electronics10101149. [41] Fang X. Gong G. Li G. Chun L. Li W. Peng P. A hybrid deep transfer learning strategy for short term cross-building energy prediction Energy 215 2021 119208 10.1016/j.energy.2020.119208 X. Fang, G. Gong, G. Li, L. Chun, W. Li, P. Peng, A hybrid deep transfer learning strategy for short term cross-building energy prediction, Energy 215 (2021) 119208. doi:https://doi.org/10.1016/j.energy.2020.119208. [42] Chen W.-H. Cho P.-C. Jiang Y.-L. Activity recognition using transfer learning Sens Mater 29 7 2017 897 904 10.18494/SAM.2017.1546 W.-H. Chen, P.-C. Cho, Y.-L. Jiang, Activity recognition using transfer learning, Sens. Mater 29 (7) (2017) 897\u2013904. [43] Pardamean B. Muljo H.H. Cenggoro T.W. Chandra B.J. Rahutomo R. Using transfer learning for smart building management system J Big Data 6 1 2019 1 12 10.1186/s40537-019-0272-6 B. Pardamean, H. H. Muljo, T. W. Cenggoro, B. J. Chandra, R. Rahutomo, Using transfer learning for smart building management system, Journal of Big Data 6 (1) (2019) 1\u201312. [44] Chen Y. Tong Z. Zheng Y. Samuelson H. Norford L. Transfer learning with deep neural networks for model predictive control of HVAC and natural ventilation in smart buildings J Clean Prod 254 2020 119866 10.1016/j.jclepro.2019.119866 Y. Chen, Z. Tong, Y. Zheng, H. Samuelson, L. Norford, Transfer learning with deep neural networks for model predictive control of hvac and natural ventilation in smart buildings, Journal of Cleaner Production 254 (2020) 119866. doi:https://doi.org/10.1016/j.jclepro.2019.119866. [45] Demianenko M. De Gaetani C.I. A procedure for automating energy analyses in the BIM context exploiting artificial neural networks and transfer learning technique Energies 14 10 2021 10.3390/en14102956 M. Demianenko, C. I. De Gaetani, A procedure for automating energy analyses in the bim context exploiting artificial neural networks and transfer learning technique, Energies 14 (10) (2021). doi:10.3390/en14102956. [46] Pinto G. Messina R. Li H. Hong T. Piscitelli M.S. Capozzoli A. Sharing Is Caring: An extensive analysis of parameter-based transfer learning for the prediction of building thermal dynamics Energy Build 2022 112530 10.1016/j.enbuild.2022.112530 G. Pinto, R. Messina, H. Li, T. Hong, M. S. Piscitelli, A. Capozzoli, Sharing is caring: An extensive analysis of parameter-based transfer learning for the prediction of building thermal dynamics, Energy and Buildings (2022) 112530, doi:https://doi.org/10.1016/j.enbuild.2022.112530. [47] Fan L. Zhang J. He Y. Liu Y. Hu T. Zhang H. Optimal scheduling of microgrid based on deep deterministic policy gradient and transfer learning Energies 14 3 2021 10.3390/en14030584 L. Fan, J. Zhang, Y. He, Y. Liu, T. Hu, H. Zhang, Optimal scheduling of microgrid based on deep deterministic policy gradient and transfer learning, Energies 14 (3) (2021). doi:10.3390/en14030584. [48] Lissa P. Schukat M. Keane M. Barrett E. Transfer learning applied to DRL-Based heat pump control to leverage microgrid energy efficiency Smart Energy 3 2021 100044 10.1016/j.segy.2021.100044 P. Lissa, M. Schukat, M. Keane, E. Barrett, Transfer learning applied to drl-based heat pump control to leverage microgrid energy efficiency, Smart Energy 3 (2021) 100044. doi:https://doi.org/10.1016/j.segy.2021.100044. [49] Argall B.D. Chernova S. Veloso M. Browning B. A survey of robot learning from demonstration Robot Auton Syst 57 5 2009 469 483 10.1016/j.robot.2008.10.024 B. D. Argall, S. Chernova, M. Veloso, B. Browning, A survey of robot learning from demonstration, Robotics and Autonomous Systems 57 (5) (2009) 469\u2013483. doi:https://doi.org/10.1016/j.robot.2008.10.024. [50] Xu J. Li Z. Du G. Liu Q. Gao L. Zhao Y. A transferable energy management strategy for hybrid electric vehicles via dueling deep deterministic policy gradient Green Energy Intell Transp 2022 100018. 10.1016/j.geits.2022.100018 J. Xu, Z. Li, G. Du, Q. Liu, L. Gao, Y. Zhao, A transferable energy management strategy for hybrid electric vehicles via dueling deep deterministic policy gradient, Green Energy and Intelligent Transportation (2022) 100018. doi:https://doi.org/10.1016/j.geits.2022.100018. [51] Lissa P. Schukat M. Barrett E. Transfer learning applied to reinforcement learning-based hvac control SN Comput Sci 1 3 2020 1 12 10.1007/s42979-020-00146-7 P. Lissa, M. Schukat, E. Barrett, Transfer learning applied to reinforcement learning-based hvac control, SN Computer Science 1 (3) (2020) 1\u201312. [52] Fang X. Gong G. Li G. Chun L. Peng P. Li W. Cross temporal-spatial transferability investigation of deep reinforcement learning control strategy in the building HVAC system level Energy 2022 125679. 10.1016/j.energy.2022.125679 X. Fang, G. Gong, G. Li, L. Chun, P. Peng, W. Li, X. Shi, Cross temporal-spatial transferability investigation of deep reinforcement learning control strategy in the building hvac system level, Energy (2022) 125679. doi:https://doi.org/10.1016/j.energy.2022.125679. [53] Xu S. Wang Y. Wang Y. O\u2019Neill Z. Zhu Q. One for many: transfer learning for building HVAC control Proceedings of the 7th ACM international conference on systems for energy-efficient buildings, cities, and transportation 2020 Association for Computing Machinery New York, NY, USA 230 239 10.1145/3408308.3427617 S. Xu, Y. Wang, Y. Wang, Z. O\u2019Neill, Q. Zhu, One for many: Transfer learning for building hvac control, in: Proceedings of the 7th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, BuildSys \u201920, Association for Computing Machinery, New York, NY, USA, 2020, p. 230\u2013239. doi:10.1145/3408308.3427617. [54] Zhang T. Aakash Krishna G.S. Afshari M. Musilek P. Taylor M.E. Ardakanian O. Diversity for transfer in learning-based control of buildings Proceedings of the thirteenth ACM international conference on future energy systems 2022 Association for Computing Machinery New York, NY, USA 556 564 10.1145/3538637.3539615 T. Zhang, A. K. G. S, M. Afshari, P. Musilek, M. E. Taylor, O. Ardakanian, Diversity for transfer in learning-based control of buildings, in: Proceedings of the Thirteenth ACM International Conference on Future Energy Systems, e-Energy \u201922, Association for Computing Machinery, New York, NY, USA, 2022, p. 556\u2013564. doi:10.1145/3538637.3539615. [55] Tsang N. Cao C. Wu S. Yan Z. Yousefi A. Fred-Ojala A. Autonomous household energy management using deep reinforcement learning 2019 IEEE international conference on engineering, technology and innovation 2019 1 7 10.1109/ICE.2019.8792636 N. Tsang, C. Cao, S. Wu, Z. Yan, A. Yousefi, A. Fred-Ojala, I. Sidhu, Autonomous household energy management using deep reinforcement learning, in: 2019 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC), 2019, pp. 1\u20137. doi:10.1109/ICE.2019.8792636. [56] Zhang X, Jin X, Tripp C, Biagioni DJ, Graf P, Jiang H. Transferable reinforcement learning for smart homes. In: Proceedings of the 1st international workshop on reinforcement learning for energy management in buildings & cities. 2020, p. 43\u20137, https://doi.org/10.1145/3427773.3427865. X. Zhang, X. Jin, C. Tripp, D. J. Biagioni, P. Graf, H. Jiang, Transferable reinforcement learning for smart homes, in: Proceedings of the 1st International Workshop on Reinforcement Learning for Energy Management in Buildings & Cities, 2020, pp. 43\u201347. [57] Mbuwir B.V. Paridari K. Spiessens F. Nordström L. Deconinck G. Transfer learning for operational planning of batteries in commercial buildings 2020 IEEE international conference on communications, control, and computing technologies for smart grids (SmartGridComm) 2020 1 6 10.1109/SmartGridComm47815.2020.9303016 B. V. Mbuwir, K. Paridari, F. Spiessens, L. Nordström, G. Deconinck, Transfer learning for operational planning of batteries in commercial buildings, in: 2020 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm), 2020, pp. 1\u20136. doi:10.1109/SmartGridComm47815.2020.9303016. [58] Zhao P. Hoi S.C. Wang J. Li B. Online transfer learning Artificial Intelligence 216 2014 76 102 10.1016/j.artint.2014.06.003 P. Zhao, S. C. Hoi, J. Wang, B. Li, Online transfer learning, Artificial Intelligence 216 (2014) 76\u2013102. doi:https://doi.org/10.1016/j.artint.2014.06.003. [59] Grubinger T. Chasparis G.C. Natschläger T. Generalized online transfer learning for climate control in residential buildings Energy Build 139 2017 63 71 10.1016/j.enbuild.2016.12.074 T. Grubinger, G. C. Chasparis, T. Natschläger, Generalized online transfer learning for climate control in residential buildings, Energy and Buildings 139 (2017) 63\u201371. doi:https://doi.org/10.1016/j.enbuild.2016.12.074. [60] Christodoulou P. Soft actor-critic for discrete action settings 2019 CoRR abs/1910.07207, URL http://arxiv.org/abs/1910.07207, arXiv:1910.07207 P. Christodoulou, Soft actor-critic for discrete action settings, CoRR abs/1910.07207 (2019). arXiv:1910.07207. http://arxiv.org/abs/1910.07207 [61] Akiba T. Sano S. Yanase T. Ohta T. Koyama M. Optuna: a next-generation hyperparameter optimization framework Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining 2019 Association for Computing Machinery New York, NY, USA 2623 2631 10.1145/3292500.3330701 T. Akiba, S. Sano, T. Yanase, T. Ohta, M. Koyama, Optuna: A next-generation hyperparameter optimization framework, in: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD \u201919, Association for Computing Machinery, New York, NY, USA, 2019, p. 2623\u20132631. doi:10.1145/3292500.3330701. [62] Bellman R. Dynamic programming Science 153 3731 1966 34 37 10.1126/science.153.3731.34 arXiv:https://science.sciencemag.org/content/153/3731/34.full.pdf R. Bellman, Dynamic programming, Science 153 (3731) (1966) 34\u201337. arXiv:https://science.sciencemag.org/content/153/3731/34.full.pdf, doi:10.1126/science.153.3731.34. [63] Haarnoja T. Zhou A. Hartikainen K. Tucker G. Ha S. Tan J. Soft actor-critic algorithms and applications 2019 arXiv:1812.05905 T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta, P. Abbeel, S. Levine, Soft actor-critic algorithms and applications (2019). arXiv:1812.05905. [64] Taylor M.E. Stone P. Transfer learning for reinforcement learning domains: a survey J Mach Learn Res 10 56 2009 1633 1685 URL http://jmlr.org/papers/v10/taylor09a.html M. E. Taylor, P. Stone, Transfer learning for reinforcement learning domains: A survey, Journal of Machine Learning Research 10 (56) (2009) 1633\u20131685. http://jmlr.org/papers/v10/taylor09a.html [65] Zhu Z. Lin K. Jain A.K. Zhou J. Transfer learning in deep reinforcement learning: a survey 2020 10.48550/ARXIV.2009.07888 URL https://arxiv.org/abs/2009.07888 Z. Zhu, K. Lin, A. K. Jain, J. Zhou, Transfer learning in deep reinforcement learning: A survey (2020). doi:10.48550/ARXIV.2009.07888. https://arxiv.org/abs/2009.07888 [66] ARERA - Andamento del prezzo dell\u2019energia elettrica per il consumatore domestico tipo in maggior tutela 2022 https://www.arera.it/it/dati/eep35.htm. (Accessed: 23 August 2022) Arera - andamento del prezzo dell\u2019energia elettrica per il consumatore domestico tipo in maggior tutela, https://www.arera.it/it/dati/eep35.htm, accessed: 2022-08-23. [67] D2.2 European climate zones and bio-climatic design requirements - report: PVSITES-WP2-T21-D22-M03-BEAR-20160831-v01 2016 https://www.pvsites.eu/downloads/category/project-results?page=4. (Accessed: 23 August 2022) D2.2 european climate zones and bio-climatic design requirements - report: Pvsites-wp2-t21-d22-m03-bear-20160831-v01, https://www.pvsites.eu/downloads/category/project-results? page=4, accessed: 2022-08-23 (2016). [68] Tsikaloudaki K. Laskos K. Bikas D. On the establishment of climatic zones in europe with regard to the energy performance of buildings Energies 5 1 2012 32 44 10.3390/en5010032 K. Tsikaloudaki, K. Laskos, D. Bikas, On the establishment of climatic zones in europe with regard to the energy performance of buildings, Energies 5 (1) (2012) 32\u201344. doi:10.3390/en5010032. [69] Austin energy. Electricity tariff pilot programs 2022 https://austinenergy.com/ae/. (Accessed: 23 August 2022) Austin energy. electricity tariff pilot programs, https://austinenergy.com/ae/, accessed: 2022-08-23. [70] Ministry of Economic Development - Interministerial Decree of 26 June 2015: Application of energy performance calculation methodologies and definition of prescriptions and minimum requirements for buildings. Appendix A: general criteria and requirements for the energy performance of buildings 2015 https://www.mise.gov.it/index.php/it/normativa/decreti-interministeriali/decreto-interministeriale-26-giugno-2015-applicazione-delle-metodologie-di-calcolo-delle-prestazioni-energetiche-e-definizione-delle-prescrizioni-e-dei-requisiti-minimi-degli-edifici?cldee=ZW5lcmdpYS5kZW1hcmNvQGxpYmVyby5pdA%3D%3D&urlid=0?hitcount=0. (Accessed: 23 August 2022) Ministry of economic development - interministerial decree of 26 june 2015: Application of energy performance calculation methodologies and definition of prescriptions and minimum requirements for buildings. appendix a: General criteria and requirements for the energy performance of buildings, https://www.mise.gov.it/index.php/it/normativa/decreti-interministeriali/decreto-interministeriale-26-giugno-2015-applicazione-delle-metodologie-di-calcolo-delle-prestazioni-energetiche-e-definizione-delle-prescrizioni-e-dei-requisiti-minimi-degli-edifici?cldee=ZW5lcmdpYS5kZW1hcmNvQGxpYmVyby5pdA%3D% 3D&urlid=0?hitcount=0, accessed: 2022-08-23 (2015). [71] Ministry of economic development - interministerial decree of 26 june 2015: Application of energy performance calculation methodologies and definition of prescriptions and minimum requirements for buildings. Appendix b: specific requirements for existing buildings subject to energy rehabilitation 2015 https://www.mise.gov.it/index.php/it/normativa/decreti-interministeriali/decreto-interministeriale-26-giugno-2015-applicazione-delle-metodologie-di-calcolo-delle-prestazioni-energetiche-e-definizione-delle-prescrizioni-e-dei-requisiti-minimi-degli-edifici?cldee=ZW5lcmdpYS5kZW1hcmNvQGxpYmVyby5pdA%3D%3D&urlid=0?hitcount=0. (Accessed: 23 August 2022) Ministry of economic development - interministerial decree of 26 june 2015: Application of energy performance calculation methodologies and definition of prescriptions and minimum requirements for buildings. appendix b: Specific requirements for existing buildings subject to energy rehabilitation, https://www.mise.gov.it/index.php/it/normativa/decreti-interministeriali/decreto-interministeriale-26-giugno-2015-applicazione-delle-metodologie-di-calcolo-delle-prestazioni-energetiche-e-definizione-delle-prescrizioni-e-dei-requisiti-minimi-degli-edifici?cldee=ZW5lcmdpYS5kZW1hcmNvQGxpYmVyby5pdA%3D% 3D&urlid=0?hitcount=0, accessed: 2022-08-23 (2015). [72] Bienvenido-Huertas D. Oliveira M. Rubio-Bellido C. Marín D. A comparative analysis of the international regulation of thermal properties in building envelope Sustainability 11 20 2019 10.3390/su11205574 D. Bienvenido-Huertas, M. Oliveira, C. Rubio-Bellido, D. Marín, A comparative analysis of the international regulation of thermal properties in building envelope, Sustainability 11 (20) (2019). doi:10.3390/su11205574. [73] Huynh A. Dias Barkokebas R. Al-Hussein M. Cruz-Noguez C. Chen Y. Energy-efficiency requirements for residential building envelopes in cold-climate regions Atmosphere 12 3 2021 10.3390/atmos12030405 A. Huynh, R. Dias Barkokebas, M. Al-Hussein, C. Cruz-Noguez, Y. Chen, Energy-efficiency requirements for residential building envelopes in cold-climate regions, Atmosphere 12 (3) (2021). doi:10.3390/atmos12030405. [74] Brockman G. Cheung V. Pettersson L. Schneider J. Schulman J. Tang J. OpenAI Gym 2016 arXiv:1606.01540 G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, W. Zaremba, Openai gym (2016). arXiv:1606.01540. [75] Bergstra J. Bardenet R. Bengio Y. Kégl B. Algorithms for hyper-parameter optimization Proceedings of the 24th international conference on neural information processing systems 2011 Curran Associates Inc. Red Hook, NY, USA 2546 2554 https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf J. Bergstra, R. Bardenet, Y. Bengio, B. Kégl, Algorithms for hyper-parameter optimization, in: Proceedings of the 24th International Conference on Neural Information Processing Systems, NIPS\u201911, Curran Associates Inc., Red Hook, NY, USA, 2011, p. 2546\u20132554. [76] Xin Q. 3 - Optimization techniques in diesel engine system design Xin Q. Diesel engine system design 2013 Woodhead Publishing 203 296 10.1533/9780857090836.1.203 Q. Xin, 3 - optimization techniques in diesel engine system design, in: Q. Xin (Ed.), Diesel Engine System Design, Woodhead Publishing, 2013, pp. 203\u2013296. doi:https://doi.org/10.1533/9780857090836.1.203. [77] Zelany M. A concept of compromise solutions and the method of the displaced ideal Comput Oper Res 1 3 1974 479 496 10.1016/0305-0548(74)90064-1 M. Zelany, A concept of compromise solutions and the method of the displaced ideal, Computers & Operations Research 1 (3) (1974) 479\u2013496. doi:https://doi.org/10.1016/0305-0548(74)90064-1. [78] Smith S.L. Kindermans P.-J. Ying C. Le Q.V. Don\u2019t decay the learning rate, increase the batch size 2017 arXiv preprint arXiv:1711.00489 S. L. Smith, P.-J. Kindermans, C. Ying, Q. V. Le, Don\u2019t decay the learning rate, increase the batch size, arXiv preprint arXiv:1711.00489 (2017). [79] Wetter M. Benne K.S. Gautier A. Nouidui T.S. Ramle A. Roth A. Lifting the garage door on spawn, an open-source bem- controls engine 2020 building performance modeling conference and SimBuild Co-Organized by ASHRAE and IBPSA-USA 2020 https://simulationresearch.lbl.gov/wetter/download/2020-simBuild-spawn.pdf M. Wetter, K. S. Benne, A. Gautier, T. S. Nouidui, A. Ramle, A. Roth, H. Tummescheit, S. G. Mentzer, C. Winther, Lifting the garage door on spawn, an open-source bem- controls engine, in: 2020 Building Performance Modeling Conference and SimBuild co-organized by ASHRAE and IBPSA-USA, 2020.",
    "scopus-id": "85146093540",
    "coredata": {
        "eid": "1-s2.0-S0306261922018554",
        "dc:description": "In recent years, advanced control strategies based on Deep Reinforcement Learning (DRL) proved to be effective in optimizing the management of integrated energy systems in buildings, reducing energy costs and improving indoor comfort conditions when compared to traditional reactive controllers. However, the scalability and implementation of DRL controllers are still limited since they require a considerable amount of time before converging to a near-optimal solution. This issue is currently addressed in literature through the offline pre-training of the DRL agent. However this solution results in two main critical issues: (1) the need to develop a building surrogate model to perform the training task, and (2) the need to perform a fine-tuning process over several training episodes to obtain a near-optimal control policy. In this context, this paper introduces an Online Transfer Learning (OTL) strategy that exploits two knowledge-sharing techniques, weight-initialization and imitation learning, to transfer a DRL control policy from a source office building to various target buildings in a simulation environment coupling EnergyPlus and Python. A DRL controller based on discrete Soft Actor\u2013Critic (SAC) is trained on the source building to manage the operation of a cooling system consisting of a chiller and a thermal storage. Several target buildings are defined to benchmark the performance of the OTL strategy with that of a Rule-Based Controller (RBC) and two DRL-based control strategies, deployed in offline and online fashion. The strategy adopted for OTL emulates the real world implementation with a simulation process by implementing the transferred DRL agent for a single episode in the target buildings. Target buildings have the same geometrical features and are served by the same energy system as the source building, but differ in terms of weather conditions, electricity price schedules, occupancy patterns, and building envelope efficiency levels. The results show that the OTL strategy can reduce the cumulated sum of temperature violations on average by 50% and 80% respectively when compared to RBC and online DRL while enhancing the energy system operation with electricity cost savings ranging between 20% and 40%. The OTL agent performs slightly worse than the offline DRL controller but it does not require any modeling effort and can be implemented directly on target buildings emulating a real-world implementation.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2023-03-01",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S0306261922018554",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Coraci, Davide"
            },
            {
                "@_fa": "true",
                "$": "Brandi, Silvio"
            },
            {
                "@_fa": "true",
                "$": "Hong, Tianzhen"
            },
            {
                "@_fa": "true",
                "$": "Capozzoli, Alfonso"
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S0306261922018554"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S0306261922018554"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S0306-2619(22)01855-4",
        "prism:volume": "333",
        "articleNumber": "120598",
        "prism:publisher": "Elsevier Ltd.",
        "dc:title": "Online transfer learning strategy for enhancing the scalability and deployment of deep reinforcement learning control in smart buildings",
        "prism:copyright": "© 2023 Elsevier Ltd. All rights reserved.",
        "openaccess": "0",
        "prism:issn": "03062619",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "Online transfer learning"
            },
            {
                "@_fa": "true",
                "$": "Homogeneous transfer learning"
            },
            {
                "@_fa": "true",
                "$": "Intra-agent transfer learning"
            },
            {
                "@_fa": "true",
                "$": "Building adaptive control"
            },
            {
                "@_fa": "true",
                "$": "Deep reinforcement learning"
            },
            {
                "@_fa": "true",
                "$": "Energy efficiency"
            }
        ],
        "openaccessArticle": "false",
        "prism:publicationName": "Applied Energy",
        "openaccessSponsorType": null,
        "prism:pageRange": "120598",
        "pubType": "fla",
        "prism:coverDisplayDate": "1 March 2023",
        "prism:doi": "10.1016/j.apenergy.2022.120598",
        "prism:startingPage": "120598",
        "dc:identifier": "doi:10.1016/j.apenergy.2022.120598",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "122",
            "@width": "301",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-ga1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "41932",
            "@ref": "ga1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "715",
            "@width": "642",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr10.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "154972",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "420",
            "@width": "596",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "66101",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "173",
            "@width": "624",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "32832",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "349",
            "@width": "653",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "63142",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "235",
            "@width": "653",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "93179",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "664",
            "@width": "627",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "108834",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "604",
            "@width": "636",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "122827",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "257",
            "@width": "602",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "57916",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "388",
            "@width": "541",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-fx1002.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "107225",
            "@ref": "fx1002",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "435",
            "@width": "602",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr8.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "74055",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "733",
            "@width": "641",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr14.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "103417",
            "@ref": "gr14",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "367",
            "@width": "646",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-fx1001.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "90562",
            "@ref": "fx1001",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "260",
            "@width": "638",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr9.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "70082",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "733",
            "@width": "641",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr13.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "101276",
            "@ref": "gr13",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "801",
            "@width": "639",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr12.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "150881",
            "@ref": "gr12",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "804",
            "@width": "639",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr11.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "126970",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "362",
            "@width": "595",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr15.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "116416",
            "@ref": "gr15",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "89",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-ga1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "22249",
            "@ref": "ga1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "147",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr10.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "25729",
            "@ref": "gr10",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "154",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "17788",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "61",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "11675",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "117",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "14506",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "79",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "20700",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "155",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "17170",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "173",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "20447",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "93",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "15974",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "157",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-fx1002.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "24610",
            "@ref": "fx1002",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "158",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr8.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "18301",
            "@ref": "gr8",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "143",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr14.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "15157",
            "@ref": "gr14",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "124",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-fx1001.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "17212",
            "@ref": "fx1001",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "89",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr9.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "16803",
            "@ref": "gr9",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "143",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr13.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "15131",
            "@ref": "gr13",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "131",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr12.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "18457",
            "@ref": "gr12",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "130",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr11.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "17267",
            "@ref": "gr11",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "133",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr15.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "70900",
            "@ref": "gr15",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "539",
            "@width": "1333",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-ga1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "239732",
            "@ref": "ga1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "3170",
            "@width": "2845",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr10_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1214353",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1859",
            "@width": "2641",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "484274",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "766",
            "@width": "2765",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "168124",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1547",
            "@width": "2891",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "332696",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1041",
            "@width": "2891",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "516270",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2939",
            "@width": "2776",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "607097",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2672",
            "@width": "2815",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "692938",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1137",
            "@width": "2667",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "295777",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1718",
            "@width": "2395",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-fx1002_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "633671",
            "@ref": "fx1002",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1927",
            "@width": "2667",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr8_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "453314",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "3245",
            "@width": "2838",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr14_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "646583",
            "@ref": "gr14",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1623",
            "@width": "2860",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-fx1001_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "539640",
            "@ref": "fx1001",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1153",
            "@width": "2824",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr9_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "488337",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "3246",
            "@width": "2838",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr13_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "617151",
            "@ref": "gr13",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "3547",
            "@width": "2831",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr12_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "847443",
            "@ref": "gr12",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "3561",
            "@width": "2830",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr11_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "707436",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1603",
            "@width": "2637",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-gr15_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "372344",
            "@ref": "gr15",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si10.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2412",
            "@ref": "si10",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si104.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1434",
            "@ref": "si104",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si105.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6378",
            "@ref": "si105",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si106.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "14922",
            "@ref": "si106",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si11.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2483",
            "@ref": "si11",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si111.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9902",
            "@ref": "si111",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si112.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "17044",
            "@ref": "si112",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si116.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3552",
            "@ref": "si116",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si117.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7901",
            "@ref": "si117",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si118.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4633",
            "@ref": "si118",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si119.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8784",
            "@ref": "si119",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si12.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2093",
            "@ref": "si12",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si120.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "13331",
            "@ref": "si120",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si121.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9424",
            "@ref": "si121",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si123.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8650",
            "@ref": "si123",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si124.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "13332",
            "@ref": "si124",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si125.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7711",
            "@ref": "si125",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si13.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7873",
            "@ref": "si13",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si131.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "19013",
            "@ref": "si131",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si133.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "12183",
            "@ref": "si133",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si134.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5216",
            "@ref": "si134",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si137.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6786",
            "@ref": "si137",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si138.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11297",
            "@ref": "si138",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si14.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4550",
            "@ref": "si14",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si140.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6380",
            "@ref": "si140",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si141.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6457",
            "@ref": "si141",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si142.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "10972",
            "@ref": "si142",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si143.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1147",
            "@ref": "si143",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si144.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1365",
            "@ref": "si144",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si145.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1381",
            "@ref": "si145",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si146.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1686",
            "@ref": "si146",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si15.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5020",
            "@ref": "si15",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si156.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11048",
            "@ref": "si156",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si157.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "10945",
            "@ref": "si157",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si158.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "10861",
            "@ref": "si158",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si159.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11856",
            "@ref": "si159",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si16.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3033",
            "@ref": "si16",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si161.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3132",
            "@ref": "si161",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si162.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2545",
            "@ref": "si162",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si163.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2550",
            "@ref": "si163",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si164.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2550",
            "@ref": "si164",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si165.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2550",
            "@ref": "si165",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si18.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4499",
            "@ref": "si18",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si19.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2796",
            "@ref": "si19",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si20.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2425",
            "@ref": "si20",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si21.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1995",
            "@ref": "si21",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si23.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5511",
            "@ref": "si23",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si24.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3391",
            "@ref": "si24",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si25.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2390",
            "@ref": "si25",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si27.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5252",
            "@ref": "si27",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si28.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3217",
            "@ref": "si28",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si29.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3747",
            "@ref": "si29",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si3.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1391",
            "@ref": "si3",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si30.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3858",
            "@ref": "si30",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si31.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6448",
            "@ref": "si31",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si32.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6287",
            "@ref": "si32",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si33.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2415",
            "@ref": "si33",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si34.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2025",
            "@ref": "si34",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si35.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3541",
            "@ref": "si35",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si36.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4555",
            "@ref": "si36",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si37.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5691",
            "@ref": "si37",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si38.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3284",
            "@ref": "si38",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si39.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3532",
            "@ref": "si39",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si40.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3735",
            "@ref": "si40",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si42.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2700",
            "@ref": "si42",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si43.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2395",
            "@ref": "si43",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si44.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5076",
            "@ref": "si44",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si46.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3563",
            "@ref": "si46",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si47.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3258",
            "@ref": "si47",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si48.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5476",
            "@ref": "si48",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si49.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5171",
            "@ref": "si49",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si5.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2315",
            "@ref": "si5",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si51.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3173",
            "@ref": "si51",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si52.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2868",
            "@ref": "si52",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si53.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4654",
            "@ref": "si53",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si54.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4349",
            "@ref": "si54",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si70.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1626",
            "@ref": "si70",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si71.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1321",
            "@ref": "si71",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si76.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11798",
            "@ref": "si76",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si77.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11657",
            "@ref": "si77",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si78.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "12048",
            "@ref": "si78",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si79.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1783",
            "@ref": "si79",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si84.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5688",
            "@ref": "si84",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si85.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5331",
            "@ref": "si85",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si86.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "10101",
            "@ref": "si86",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si87.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5806",
            "@ref": "si87",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si88.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9616",
            "@ref": "si88",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si89.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5079",
            "@ref": "si89",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si9.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2072",
            "@ref": "si9",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si90.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9453",
            "@ref": "si90",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si91.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3514",
            "@ref": "si91",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-si92.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9572",
            "@ref": "si92",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922018554-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "10225480",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85146093540"
    }
}}