{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85141527430",
    "originalText": "serial JL 271434 291210 291731 291800 291881 31 Building and Environment BUILDINGENVIRONMENT 2022-11-01 2022-11-01 2022-11-08 2022-11-08 2023-03-20T15:02:08 1-s2.0-S0360132322009775 S0360-1323(22)00977-5 S0360132322009775 10.1016/j.buildenv.2022.109747 S300 S300.1 FULL-TEXT 1-s2.0-S0360132322X00164 2023-03-20T15:44:13.43059Z 0 0 20221201 20221231 2022 2022-11-01T01:38:52.585081Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantsponsor highlightsabst nomenclature orcid primabst ref 0360-1323 03601323 true 226 226 C Volume 226 11 109747 109747 109747 202212 December 2022 2022-12-01 2022-12-31 2022 Research Papers article fla © 2022 Elsevier Ltd. All rights reserved. TOWARDSSELFLEARNINGCONTROLHVACSYSTEMSCONSIDERATIONDYNAMICOCCUPANCYPATTERNSAPPLICATIONMODELFREEDEEPREINFORCEMENTLEARNING ESRAFILIANNAJAFABADI M Nomenclature 1 Introduction 2 Methodology 2.1 Reinforcement learning 2.1.1 Markov decision process 2.1.1.1 State-space formulation 2.1.1.2 Action-space formulation 2.1.1.3 Reward function 2.1.2 Q-learning algorithm 2.1.3 Deep Q-network 2.1.4 Double deep Q-network (DDQN) 2.1.5 Policy 2.1.6 Hyperparameter tuning 2.1.7 DDQN employment 2.2 Model-based predictive control (MPC) 2.2.1 Building and occupancy prediction models 2.2.2 Optimization algorithm 2.2.3 MPC employment 2.3 Building model 2.4 Performance evaluation metrics 3 Case study 3.1 Occupancy database preprocessing 3.2 Building testbed 4 Results and discussion 4.1 Building model selection 4.2 DDQN control system 4.2.1 Hyperparameter tuning 4.2.2 Training process and convergence 4.3 Test performance 4.3.1 Intra-day performance 4.3.2 Overall performance 5 Conclusion CRediT authorship contribution statement Acknowledgment References ERICKSON 2013 203 216 V IPSN2013PROC12THINTCONFINFPROCESSSENSNETWORKSPARTCPSWEEK2013 POEMPOWEREFFICIENTOCCUPANCYBASEDENERGYMANAGEMENTSYSTEM GLUCK 2017 J ASYSTEMATICAPPROACHFOREXPLORINGTRADEOFFSINPREDICTIVEHVACCONTROLSYSTEMSFORBUILDINGS NAGELE 2017 1254 1268 F BELTRAN 2013 1 8 A PROC5THACMWORKEMBEDSYSTENERGYEFFICIENTBUILDBUILDSYS13ASSOCIATIONFORCOMPUTINGMACHINERYACM THERMOSENSEOCCUPANCYTHERMALBASEDSENSINGFORHVACCONTROL ESRAFILIANNAJAFABADI 2021 111377 M ESRAFILIANNAJAFABADI 2021 107810 M YAO 2021 107952 Y OLDEWURTEL 2013 521 532 F SHI 2017 267 276 J TURLEY 2020 5396 C SALIMI 2020 106818 S GAO 2009 67 72 G BUILDSYS2009PROC1STACMWORKEMBEDSENSSYSTENERGYEFFICIENCYBUILDHELDCONJUNCTIONACMSENSYS2009 SELFPROGRAMMINGTHERMOSTATOPTIMIZINGSETBACKSCHEDULESBASEDHOMEOCCUPANCYPATTERNS KONTES 2018 3376 G GOYAL 2013 S LORK 2020 115426 C BARRETT 2015 3 19 E JTEURCONFMACHLEARNKNOWLDISCOVDATABASES AUTONOMOUSHVACCONTROLAREINFORCEMENTLEARNINGAPPROACH JAVED 2014 604 611 A 2014IEEEFOURTHINTCONFBIGDATACLOUDCOMPUT COMPARISONROBUSTNESSRNNMPCANNCONTROLLERFORRESIDENTIALHEATINGSYSTEM KURTE 2020 7727 K VAZQUEZCANTELI 2019 1072 1089 J WIERING 2012 729 M SUTTON 2018 R REINFORCEMENTLEARNINGINTRODUCTION YANG 2021 100003 B ESRAFILIANNAJAFABADI 2022 109152 M ZHANG 2018 279 294 Y BRANDI 2020 110225 S JIA 2019 6158 6163 R ZHANG 2018 22 23 Z 2018BUILDPERFORMANALCONFSIMBUILD ADEEPREINFORCEMENTLEARNINGAPPROACHUSINGWHOLEBUILDINGENERGYMODELFORHVACOPTIMALCONTROL 2010 STANDARD55THERMALENVIRONMENTALCONDITIONSFORHUMANOCCUPANCY 2017 ASHRAESTANDARD552017THERMALENVIRONMENTALCONDITIONSFORHUMANOCCUPANCY DOBBS 2014 675 684 J GUPTA 2021 101739 A DING 2019 326 335 X PROC6THACMINTCONFSYSTENERGYEFFICIENTBUILDCITIES OCTOPUSDEEPREINFORCEMENTLEARNINGFORHOLISTICSMARTBUILDINGCONTROL VALLADARES 2019 105 117 W WATKINS 1989 C LEARNINGDELAYEDREWARDS CHENG 2016 43 55 Z HASSELT 2010 2613 2621 H HUCHUK 2021 111201 B MNIH 2015 529 533 V KOBAYASHI 2021 63 71 T STOOKE 2019 A RLPYTARESEARCHCODEBASEFORDEEPREINFORCEMENTLEARNINGINPYTORCH FUJITA 2019 Y CHAINERRLADEEPREINFORCEMENTLEARNINGLIBRARY KINGMA 2014 D GOODFELLOW 2016 I DEEPLEARNING VANHASSELT 2016 H PROCAAAICONFARTIF DEEPREINFORCEMENTLEARNINGDOUBLEQLEARNING ZOU 2020 106535 Z WIERING 1999 M EXPLORATIONSINEFFICIENTREINFORCEMENTLEARNING CHEN 2018 195 205 Y ENERGYPLUS CHOLLET 2015 F KERAS FABIAN 2011 2825 2830 P PLAPPERT 2016 M KERASRLGITHUBREPOS BROCKMAN 2016 G OPENAIGYM HUCHUK 2019 106177 B ELKHOUKHI 2019 H PROC20197THINTRENEWSUSTAINENERGYCONFIRSEC2019 USEDEEPLEARNINGAPPROACHESFOROCCUPANCYPREDICTIONINENERGYEFFICIENTBUILDINGS KLEIMINGER 2014 493 505 W SANGOGBOYE 2018 105 115 F COMPUTSCIRESDEV PROMTPREDICTINGOCCUPANCYPRESENCEINMULTIPLERESOLUTIONTIMESHIFTAGNOSTICCLASSIFICATION ESRAFILIANNAJAFABADI 2022 111808 M CANDANEDO 2013 1032 1045 J WANG 2022 111708 C ESRAFILIAN 2019 20 37 M SHAMS 2021 6549 S REYNOLDS 2018 729 739 J SUN 2020 110022 Y PEDREGOSA 2011 2825 2830 F ABADI 2016 265 283 M 12THUSENIXSYMP TENSORFLOWASYSTEMFORLARGESCALEMACHINELEARNING AKBARI 2021 111303 S TRIMBLE S 2009 STANDARDFORDESIGNHIGHPERFORMANCEGREENBUILDINGS ESRAFILIANNAJAFABADIX2022X109747 ESRAFILIANNAJAFABADIX2022X109747XM 2024-11-08T00:00:00.000Z 2024-11-08T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2022 Elsevier Ltd. All rights reserved. 2022-11-12T00:49:29.364Z http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/aggregated-refined https://doi.org/10.15223/policy-017 https://doi.org/10.15223/policy-037 https://doi.org/10.15223/policy-012 https://doi.org/10.15223/policy-029 https://doi.org/10.15223/policy-004 item S0360-1323(22)00977-5 S0360132322009775 1-s2.0-S0360132322009775 10.1016/j.buildenv.2022.109747 271434 2023-03-20T15:44:13.43059Z 2022-12-01 2022-12-31 1-s2.0-S0360132322009775-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/MAIN/application/pdf/8bb511fbba5828b13807dca9f9692530/main.pdf main.pdf pdf true 7961178 MAIN 15 1-s2.0-S0360132322009775-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/PREVIEW/image/png/acc2114121dc33a25f379b09ccbd8229/main_1.png main_1.png png 58480 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0360132322009775-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr7/DOWNSAMPLED/image/jpeg/591e2575eca3acb58b76e124b620f8fa/gr7.jpg gr7 gr7.jpg jpg 109181 605 267 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322009775-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr6/DOWNSAMPLED/image/jpeg/ea3d00bd7ec1afb4765e6c396e1aea96/gr6.jpg gr6 gr6.jpg jpg 101785 582 335 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322009775-gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr10/DOWNSAMPLED/image/jpeg/789fa39e34d6ec7f706991c4ee1a92af/gr10.jpg gr10 gr10.jpg jpg 183464 456 780 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322009775-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr9/DOWNSAMPLED/image/jpeg/173201ed09f42b3ef3c2561eabc61c30/gr9.jpg gr9 gr9.jpg jpg 128244 589 313 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322009775-gr11.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr11/DOWNSAMPLED/image/jpeg/a6b391832ce63d457720f41631119da0/gr11.jpg gr11 gr11.jpg jpg 200470 498 780 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322009775-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr8/DOWNSAMPLED/image/jpeg/2d77f2f0bfde76c077d3283b86689e06/gr8.jpg gr8 gr8.jpg jpg 107863 600 268 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322009775-gr12.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr12/DOWNSAMPLED/image/jpeg/c884ccd998a3c61a5fda38f375b7550b/gr12.jpg gr12 gr12.jpg jpg 110820 548 313 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322009775-gr13.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr13/DOWNSAMPLED/image/jpeg/bdfea022b985eab86d2972513fa35c0a/gr13.jpg gr13 gr13.jpg jpg 99974 227 735 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322009775-gr14.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr14/DOWNSAMPLED/image/jpeg/c27b44501d2dbdebb0d340b627680793/gr14.jpg gr14 gr14.jpg jpg 107503 509 291 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322009775-gr15.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr15/DOWNSAMPLED/image/jpeg/71a5504140dee7cbcbddf0cdda822327/gr15.jpg gr15 gr15.jpg jpg 110784 582 268 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322009775-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr1/DOWNSAMPLED/image/jpeg/768ba61c2e938173ba8b8a76dbc980a3/gr1.jpg gr1 gr1.jpg jpg 123541 373 669 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322009775-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr3/DOWNSAMPLED/image/jpeg/f9d0c5be94d6e0e0425c38e4dd364517/gr3.jpg gr3 gr3.jpg jpg 115300 435 379 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322009775-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr2/DOWNSAMPLED/image/jpeg/a5ea54610bbb35c3f6a3704256ab9dfe/gr2.jpg gr2 gr2.jpg jpg 114734 378 669 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322009775-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr5/DOWNSAMPLED/image/jpeg/d3737289528e5ab9663f62ce814dda92/gr5.jpg gr5 gr5.jpg jpg 116477 501 380 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322009775-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr4/DOWNSAMPLED/image/jpeg/3417b3b5af094e5d3a0084adf604a989/gr4.jpg gr4 gr4.jpg jpg 131124 373 735 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322009775-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr7/THUMBNAIL/image/gif/cbe7409b756533cf93392042398dfdd7/gr7.sml gr7 gr7.sml sml 70353 163 72 IMAGE-THUMBNAIL 1-s2.0-S0360132322009775-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr6/THUMBNAIL/image/gif/39d653c898861223570569aa91dbe717/gr6.sml gr6 gr6.sml sml 70067 163 94 IMAGE-THUMBNAIL 1-s2.0-S0360132322009775-gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr10/THUMBNAIL/image/gif/c6ad8e8e421a98a4bd3a90bfc25395d8/gr10.sml gr10 gr10.sml sml 80033 128 219 IMAGE-THUMBNAIL 1-s2.0-S0360132322009775-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr9/THUMBNAIL/image/gif/d4a6d7348e885664dab1dd3c59a5a1ca/gr9.sml gr9 gr9.sml sml 72765 164 87 IMAGE-THUMBNAIL 1-s2.0-S0360132322009775-gr11.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr11/THUMBNAIL/image/gif/5327b200b6a2ba5cd1021d43bfc39b29/gr11.sml gr11 gr11.sml sml 82225 140 219 IMAGE-THUMBNAIL 1-s2.0-S0360132322009775-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr8/THUMBNAIL/image/gif/7a5a96f81759da92935205c6ce07be22/gr8.sml gr8 gr8.sml sml 70335 163 73 IMAGE-THUMBNAIL 1-s2.0-S0360132322009775-gr12.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr12/THUMBNAIL/image/gif/e4ac7a374041caedc777d5a00af22fc0/gr12.sml gr12 gr12.sml sml 72363 163 93 IMAGE-THUMBNAIL 1-s2.0-S0360132322009775-gr13.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr13/THUMBNAIL/image/gif/36a2ffd1c8d258182b902b8b8529158b/gr13.sml gr13 gr13.sml sml 69272 68 219 IMAGE-THUMBNAIL 1-s2.0-S0360132322009775-gr14.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr14/THUMBNAIL/image/gif/bf3448403b7ab306f623e8b240d5a8aa/gr14.sml gr14 gr14.sml sml 72422 163 93 IMAGE-THUMBNAIL 1-s2.0-S0360132322009775-gr15.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr15/THUMBNAIL/image/gif/54f020d0e1a25893779d48ef81f50192/gr15.sml gr15 gr15.sml sml 72098 163 75 IMAGE-THUMBNAIL 1-s2.0-S0360132322009775-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr1/THUMBNAIL/image/gif/0731c0ad80fb1e8b5788a6a5d4a8bc32/gr1.sml gr1 gr1.sml sml 74826 122 219 IMAGE-THUMBNAIL 1-s2.0-S0360132322009775-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr3/THUMBNAIL/image/gif/f8408b22cfe66227f2677a68478dad78/gr3.sml gr3 gr3.sml sml 77089 164 143 IMAGE-THUMBNAIL 1-s2.0-S0360132322009775-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr2/THUMBNAIL/image/gif/4704de520479815f4bd1f06addbae31b/gr2.sml gr2 gr2.sml sml 72522 124 219 IMAGE-THUMBNAIL 1-s2.0-S0360132322009775-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr5/THUMBNAIL/image/gif/9a77bdbd7e3690eaaf35a791d20f2f59/gr5.sml gr5 gr5.sml sml 74705 164 124 IMAGE-THUMBNAIL 1-s2.0-S0360132322009775-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/gr4/THUMBNAIL/image/gif/c29f99d155cbc673eff7052f0837575f/gr4.sml gr4 gr4.sml sml 74906 111 219 IMAGE-THUMBNAIL 1-s2.0-S0360132322009775-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/HIGHRES/image/jpeg/bb433b1f4fb34d1051ff0a897f335d1a/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 307057 2678 1182 IMAGE-HIGH-RES 1-s2.0-S0360132322009775-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/HIGHRES/image/jpeg/09804fbe07ce1c3abd5fe743459d4ab3/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 315475 2580 1484 IMAGE-HIGH-RES 1-s2.0-S0360132322009775-gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/HIGHRES/image/jpeg/274428f2564b516e2fe3d4ccc8e04030/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 895764 2019 3453 IMAGE-HIGH-RES 1-s2.0-S0360132322009775-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/HIGHRES/image/jpeg/dc7fd11c3f7decf4ee9c53d6e08bf87e/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 428208 2605 1385 IMAGE-HIGH-RES 1-s2.0-S0360132322009775-gr11_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/HIGHRES/image/jpeg/62c369af61e1a4506d23f1841b0204fe/gr11_lrg.jpg gr11 gr11_lrg.jpg jpg 985544 2203 3453 IMAGE-HIGH-RES 1-s2.0-S0360132322009775-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/HIGHRES/image/jpeg/d1f130eac17888d7941bb5c8faaf9c46/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 304737 2662 1190 IMAGE-HIGH-RES 1-s2.0-S0360132322009775-gr12_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/HIGHRES/image/jpeg/583b0c68a882d5252a5daf02db7ee2f4/gr12_lrg.jpg gr12 gr12_lrg.jpg jpg 324529 2425 1386 IMAGE-HIGH-RES 1-s2.0-S0360132322009775-gr13_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/HIGHRES/image/jpeg/ab84ebfe3f9600489056691ba8ed78c9/gr13_lrg.jpg gr13 gr13_lrg.jpg jpg 273539 1006 3256 IMAGE-HIGH-RES 1-s2.0-S0360132322009775-gr14_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/HIGHRES/image/jpeg/cb2f94b0a70febfb7103ced617492772/gr14_lrg.jpg gr14 gr14_lrg.jpg jpg 297699 2251 1288 IMAGE-HIGH-RES 1-s2.0-S0360132322009775-gr15_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/HIGHRES/image/jpeg/4dbba8984ba3a8b5802ec3372f989c1b/gr15_lrg.jpg gr15 gr15_lrg.jpg jpg 318502 2586 1190 IMAGE-HIGH-RES 1-s2.0-S0360132322009775-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/HIGHRES/image/jpeg/44d8ee5b5d2afd936ea75595789bf7c1/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 398035 1650 2962 IMAGE-HIGH-RES 1-s2.0-S0360132322009775-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/HIGHRES/image/jpeg/03bcc27cc6c751d4d521d38fab343ae6/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 361915 1929 1680 IMAGE-HIGH-RES 1-s2.0-S0360132322009775-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/HIGHRES/image/jpeg/c503a83e21a4569cbbdcdc6c5e26bf4b/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 358538 1673 2961 IMAGE-HIGH-RES 1-s2.0-S0360132322009775-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/HIGHRES/image/jpeg/e5c7a7e421e42f20ef267442fe6bc9c9/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 379442 2219 1682 IMAGE-HIGH-RES 1-s2.0-S0360132322009775-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/HIGHRES/image/jpeg/6255c42cecd6e1f16b26f527e84cb118/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 540033 1653 3256 IMAGE-HIGH-RES 1-s2.0-S0360132322009775-si33.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/f158536213a001a4267d3555f582b880/si33.svg si33 si33.svg svg 14255 ALTIMG 1-s2.0-S0360132322009775-si35.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/b6b5a5d63b218d66a729e5904013f857/si35.svg si35 si35.svg svg 76457 ALTIMG 1-s2.0-S0360132322009775-si23.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/3dc11a669f61b35d1cbce80f1617c094/si23.svg si23 si23.svg svg 85183 ALTIMG 1-s2.0-S0360132322009775-si37.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/10b5235ad078e0dca6fc24dfa4aac2c4/si37.svg si37 si37.svg svg 124455 ALTIMG 1-s2.0-S0360132322009775-si39.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/978a6627593791b26a53e0135f2d67c3/si39.svg si39 si39.svg svg 16668 ALTIMG 1-s2.0-S0360132322009775-si25.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/b7d2b1d0fd5504f8a9482451ba9900d5/si25.svg si25 si25.svg svg 4589 ALTIMG 1-s2.0-S0360132322009775-si19.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/b4f96b97388099a5a318170fa3d8d41b/si19.svg si19 si19.svg svg 7288 ALTIMG 1-s2.0-S0360132322009775-si34.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/7324545c324fa2c2cd3f127b943441e8/si34.svg si34 si34.svg svg 85947 ALTIMG 1-s2.0-S0360132322009775-si14.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/d955063039c79c25e197465c686aaeae/si14.svg si14 si14.svg svg 53829 ALTIMG 1-s2.0-S0360132322009775-si9.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/65dba5edee97836a60016e5d3a312ec5/si9.svg si9 si9.svg svg 85401 ALTIMG 1-s2.0-S0360132322009775-si28.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/a8d3e0327fc0e42ecfae393d9563cab4/si28.svg si28 si28.svg svg 52675 ALTIMG 1-s2.0-S0360132322009775-si36.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/e552615e7454e1a7360611caf435be7d/si36.svg si36 si36.svg svg 25430 ALTIMG 1-s2.0-S0360132322009775-si16.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/ffd57363a76cf5ca973dfb39273e0f57/si16.svg si16 si16.svg svg 3911 ALTIMG 1-s2.0-S0360132322009775-si7.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/35d99dda713582b8b63529907f94960a/si7.svg si7 si7.svg svg 16193 ALTIMG 1-s2.0-S0360132322009775-si15.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/82c534683d63609e1f23789d1a808560/si15.svg si15 si15.svg svg 19937 ALTIMG 1-s2.0-S0360132322009775-si17.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/010436902e85d10bf1033ee397000600/si17.svg si17 si17.svg svg 70233 ALTIMG 1-s2.0-S0360132322009775-si2.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/ef06b0cc2613a99e73112dad1ba0d733/si2.svg si2 si2.svg svg 29103 ALTIMG 1-s2.0-S0360132322009775-si21.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/22c81a4eacfd8bd03e4bf3c13cef4c09/si21.svg si21 si21.svg svg 62176 ALTIMG 1-s2.0-S0360132322009775-si29.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/4f290271f65922399bfa0bd4f84ab27c/si29.svg si29 si29.svg svg 19281 ALTIMG 1-s2.0-S0360132322009775-si32.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/b9b49e1d7676629912d9780201df8b75/si32.svg si32 si32.svg svg 71928 ALTIMG 1-s2.0-S0360132322009775-si22.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/50cf5b5740b1eb030081e63a7047aa04/si22.svg si22 si22.svg svg 23804 ALTIMG 1-s2.0-S0360132322009775-si30.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/50a3399db8a0a42f92d97e5b736d5ef1/si30.svg si30 si30.svg svg 19912 ALTIMG 1-s2.0-S0360132322009775-si31.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/4f96ac2544e3702a8ad21d998a083ab8/si31.svg si31 si31.svg svg 100298 ALTIMG 1-s2.0-S0360132322009775-si12.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/32d7aff9501ba84a4a97121c54292254/si12.svg si12 si12.svg svg 112479 ALTIMG 1-s2.0-S0360132322009775-si24.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/d48273a9ebeec66f3ad5eaa89bad49c3/si24.svg si24 si24.svg svg 101616 ALTIMG 1-s2.0-S0360132322009775-si6.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/5bd672623ef79d07af4cc970bea060b8/si6.svg si6 si6.svg svg 29496 ALTIMG 1-s2.0-S0360132322009775-si38.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/dcf3417921d0f60a5e5e42bc5fc4d9e7/si38.svg si38 si38.svg svg 44080 ALTIMG 1-s2.0-S0360132322009775-si18.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/fa34194ab8ea33aa481774e570a301d6/si18.svg si18 si18.svg svg 6936 ALTIMG 1-s2.0-S0360132322009775-si26.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/94e42776b102019be4023a317009f910/si26.svg si26 si26.svg svg 7286 ALTIMG 1-s2.0-S0360132322009775-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/c0419ba86c7b9726f74f30a5d14d4ea5/si3.svg si3 si3.svg svg 20419 ALTIMG 1-s2.0-S0360132322009775-si20.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/5989623c095e031372be732f31a82ec6/si20.svg si20 si20.svg svg 4198 ALTIMG 1-s2.0-S0360132322009775-si4.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/5b41035b650c0d5335a74b467d36cbe7/si4.svg si4 si4.svg svg 26864 ALTIMG 1-s2.0-S0360132322009775-si27.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/d745e2aa3f1527ebe919c5e91ef1d834/si27.svg si27 si27.svg svg 76094 ALTIMG 1-s2.0-S0360132322009775-si1.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/59ade9dbdcdd3a12f9532f47043cba2d/si1.svg si1 si1.svg svg 21676 ALTIMG 1-s2.0-S0360132322009775-si5.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/32bd576c2d00f9de8a85ab00a0dd7d88/si5.svg si5 si5.svg svg 5186 ALTIMG 1-s2.0-S0360132322009775-si13.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/88365eea7aa65d086afbc4ac614a473e/si13.svg si13 si13.svg svg 38529 ALTIMG 1-s2.0-S0360132322009775-si8.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/0bcd3771ce3a3c6678e5600d518c88b4/si8.svg si8 si8.svg svg 17932 ALTIMG 1-s2.0-S0360132322009775-si11.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/62596ccd6f0c87d78cd9c0cd937484dd/si11.svg si11 si11.svg svg 5421 ALTIMG 1-s2.0-S0360132322009775-si10.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322009775/image/svg+xml/3528ee9b0addf6e1a36ff2f76d27fd19/si10.svg si10 si10.svg svg 12802 ALTIMG 1-s2.0-S0360132322009775-am.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:10SVWH6G6JP/MAIN/application/pdf/3541e49c42b3544acad39aabf7e24146/am.pdf am am.pdf pdf false 1780111 AAM-PDF BAE 109747 109747 S0360-1323(22)00977-5 10.1016/j.buildenv.2022.109747 Elsevier Ltd Fig. 1 The working diagram of the implemented DDQN algorithm. Fig. 1 Fig. 2 The general overview of the interactions between libraries and software packages utilized to develop the DDQN controller. Fig. 2 Fig. 3 The overview of the MPC workflow. Fig. 3 Fig. 4 The schematic diagram of the MPC algorithm implemented in this study. Fig. 4 Fig. 5 The performance of the building models in terms of prediction performance and computational time. Fig. 5 Fig. 6 The episodic rewards achieved by the DDQN agent during the training process. Fig. 6 Fig. 7 Average temperature deviation caused by the DDQN controller during 10 training episodes. Fig. 7 Fig. 8 Average deviation period caused by the DDQN controller during 10 training episodes. Fig. 8 Fig. 9 The energy consumption per timestep during the training period of the DDQN controller. Fig. 9 Fig. 10 The intra-day operative temperature and energy consumption profiles based on perfect occupancy prediction for a sample occupancy profile on February 1st. Fig. 10 Fig. 11 The intra-day operative temperature and energy consumption based on actual occupancy prediction for a sample occupancy profile on February 1st. Fig. 11 Fig. 12 The distribution of the average temperature deviation associated with the control systems based on perfect and actual occupancy predictions. Fig. 12 Fig. 13 The frequency of occurences of different temperature deviations based on a sample occupancy pattern for a) the MPC with an average temperature deviation of 0.21 °C and for b) the DDQN with an average temperature deviation of 0.08 °C in this case. Fig. 13 Fig. 14 The distribution of the deviation period associated with the control systems based on perfect and actual occupancy predictions. Fig. 14 Fig. 15 The distribution of energy saving associated with the control systems based on perfect and actual occupancy models. Fig. 15 Table 1 The features utilized to represent the state space. Table 1 Feature Unit HVAC characteristics Outdoor air temperature --°C Indoor operative temperature --°C Occupancy schedules Occupancy states during the last 2 h \u2013 Time of day Hour Actual future occupancy states \u2013 Table 2 The structural parameters of the function approximator. Table 2 Parameter Value Number of hidden layers 3 Number of neurons (64, 32, 64) Optimizer Adam [42] Activation function in the hidden layers Rectified Linear Unit (ReLU) [43] Table 3 Hyperparameters of the DDQN algorithm and the associated search ranges for the tuning process. Table 3 Hyperparameters Ranges Buffer size [0,0\u20132,2\u201320] Target model update [10−5, 10−4, 10−3, 10−2, 10−1, 1, 10, 100, 1000] Learning rate [10−5, 10−4, 10−3, 10−2] Discount factor [0.1\u20130.9] Policy [Epsilon-greedy, Boltzmann] Decay duration (episodes) [5\u201310] Minimum search coefficient [0.05, 0.1, 0.15, 0.2] Table 4 The parameters utilized for developing the GA. Table 4 Parameters MPC optimizer Population size 30 Mutation rate 0.05 Selection method Tournament Stopping criteria When no improvements are obtained in the solution Table 5 Hyperparameters of the ML models employed as black-box building models. Table 5 Hyperparameters Values RF Number of trees 10 kNN Number of neighbors 10 ANN Number of hidden layers 2 Number of neurons in the hidden layers 1st layer: 642nd layer: 128 Learning rate 0.005 Optimizer Adam Activation function Hidden layers: ReLUOutput layer: Linear function Method Early stopping with a patience size of 2000 Number of epochs 20,000 Batch size 5000 Table 6 The characteristics of the virtual building and the conditions under which the simulation is performed. Table 6 Parameter Value Net conditioned area 135 m2 Number of floors 1 Type Detached house Number of bedrooms 3 Season Winter (Heating season) Weather data Very cold climate [48] Heating equipment Electric baseboards Efficiency of the heating device 100% Building material ASHRAE standard [70] Design software SketchUp and Openstudio [68,69] Simulation engine EnergyPlus [71] Simulation timestep 10 min Table 7 The hyperparameters of the DDQN algorithm. Table 7 Hyperparameters Ranges Reward function Type Eq. 2 N 2 β 0.7 DDQN algorithm Median buffer size 12,875 Median target model update 0.1 Median learning rate 0.001 Median discount factor 0.8 Median decay duration 7 episodes Mode policy Boltzmann Median minimum search coefficient 0.10 Towards self-learning control of HVAC systems with the consideration of dynamic occupancy patterns: Application of model-free deep reinforcement learning Mohammad Esrafilian-Najafabadi Writing \u2013 review & editing Writing \u2013 original draft Validation Methodology Data curation Conceptualization Fariborz Haghighat Writing \u2013 review & editing Supervision Resources Project administration Methodology Investigation Funding acquisition ∗ Energy and Environment Group, Department of Building, Civil and Environmental Engineering, Concordia University, Montreal, Canada Energy and Environment Group Department of Building Civil and Environmental Engineering Concordia University Montreal Canada Energy and Environment Group, Department of Building, Civil and Environmental Engineering, Concordia University, Montreal, Canada ∗ Corresponding author. This study proposes a self-learning control system that aims to learn occupancy profiles, building energy consumption patterns, and lag-time of the heating, ventilation, and air-conditioning (HVAC) systems. The control system learns by interacting with the environment with no need to develop building models and occupancy prediction models. The controller is developed based on a double deep Q-networks (DDQN) algorithm, as a model-free reinforcement learning method. The system's performance is evaluated and compared with that of a model predictive control (MPC) system under two scenarios of perfect and actual occupancy predictions based on occupancy data collected from 20 residential units. The MPC is assisted by a genetic algorithm and supervised learning models for predicting future occupancy patterns, indoor operative temperature, and building energy consumption. The results show that in the case of using perfect occupancy prediction, the self-learning controller operates almost as well as the MPC while not requiring any models. When occupancy prediction uncertainty is added to the problem, the proposed method outperforms the MPC in terms of thermal comfort by increasing the average temperature deviation and deviation period by 0.24 °C and 7.87%, respectively. However, the DDQN agent causes significant thermal comfort violations during the initial training period. The system causes up to a 2.8% longer deviation period and a 0.32 °C higher average temperature deviation, compared with the performance of the fully-trained system. Keywords Double deep Q-networks Model predictive control (MPC) Self-learning control Occupancy patterns Energy efficiency Data availability Data will be made available on request. Nomenclature Abbreviations ANN Artificial neural networks CO2 Carbon dioxide DDQN Double deep Q-network DNN Deep neural network DQN Deep Q-network EC Energy consumption ES Energy saving, % GA Genetic algorithm HVAC Heating, ventilation, and air conditioning kNN k-nearest neighbors LR Linear regression MAE Mean absolute error MDP Markov decision process ML Machine learning MLP Multi-layer perceptron MPC Model predictive control ReLU Rectified linear unit RF Random forest RL Reinforcement learning SR Solar radiation Symbols A Action space a\u2032 Next action A t Selected action at timestep t a t Action at each timestep C dev Temperature deviation coefficient c Controller D Number of total timesteps in each episode E Expected value H Number of timesteps during the prediction horizon h Hour of day L Loss function N Population size n Temperature deviation weight Occ Occupancy states P dev Deviation period, % Q State-action values Q heat Heating energy, kWh Q heat,max Maximum heating energy, kWh r Instant reward R t Reward at timestep t S State s\u2032 Next state s t State at timestep t T \u203e d e v Average temperature deviation, °C T Temperature parameter t Timestep T d e s i r e d Desired temperature, °C T d e v Temperature deviation, °C T op Operative temperature, °C T out Outdoor temperature, °C T sp Setpoint temperature, °C Y Target value α Learning rate β Reward function weight Δ T max Maximum temperature deviation, °C γ Discount factor θ Q-network weights θ − Target network weights 1 Introduction Advanced heating, ventilation, and air-conditioning (HVAC) control systems have attracted growing attention because of their promising performance in terms of energy saving, cost reduction, carbon dioxide (CO2) footprint decrease, and thermal comfort improvement [1\u20134]. Among such control systems, occupancy-based HVAC control systems have demonstrated a great potential to save energy and costs by changing indoor air temperature in response to dynamic changes in occupancy patterns [5]. These systems aim to decrease energy consumption by minimizing unnecessary HVAC operation during vacancy periods while providing thermal comfort for occupants. Such control systems can adjust HVAC operation to forecasted future arrival and departure times of occupants. This predictive strategy can boost energy saving while improving thermal comfort. Earlier studies proposed the use of model predictive control (MPC) as one of the most advanced occupancy-based control systems. By employing a building model and an occupancy model, MPC can optimize indoor air temperature based on probabilities of future occupancy states [6,7]. Oldewurtel et al. [8] studied the performance of an MPC algorithm that controlled lighting levels and ventilation rates in terms of energy saving with the consideration of dynamic occupancy profiles. They reported up to 34% energy saving, compared with a standard scheduled thermostat as the benchmark. Shi et al. [9] reported the development of an MPC algorithm assisted with a logistic regression model to forecast future occupancy states. The controller allowed users to make a trade-off between thermal comfort and electricity saving via defining a penalty factor. The controller demonstrated superior performance by saving 8% more energy in comparison with a traditional MPC (an MPC with no occupancy forecasting). In a similar study conducted by Turley et al. [10], occupancy-based MPC yielded an energy saving of up to 13.3% based on six residential case studies. Salimi and Hammad [11] proposed an optimal control strategy that took the probabilities of future occupancy states as a control input to adjust the air temperature. The simulation results showed a 50% improvement in thermal comfort while saving 2% energy, compared with a standard scheduled thermostat. Gao and Whitehouse [12] reported up to a 15% reduction in cooling and heating demands by implementing an optimal control method, compared with a standard scheduled thermostat. Despite the promising performance of model-based control systems, they provide poor generalization potentials as they require building models and occupancy prediction models. The need for implementing developing such models also increases the complexities associated with the development of the control systems. These limitations have been an obstacle, preventing such systems from being widely applied to the building sector [13,14]. Model-free control systems have been proposed as a potential solution in the literature. These algorithms can construct self-learning control frameworks that learn directly through trial and error. Such techniques have been successfully implemented for building energy improvement [15\u201318] and demand response management [19]. Despite the successful applications of model-free techniques, earlier studies have neglected the essential role of online occupancy patterns in developing the control systems. It has not been investigated to what extent these algorithms can learn occupancy patterns and adjust setpoint temperature when no occupancy models are employed. This control problem is challenging due to the highly stochastic nature of occupant behavior; any wrong temperature settings can substantially damage occupants\u2019 thermal comfort. Moreover, earlier research has ignored the possible influences that the trial-and-error-based learning process might have on thermal comfort. During the learning period of the controllers, too many wrong decisions might be made, causing significant thermal discomfort for occupants. These limitations can be a major barrier for such controllers, preventing them from being applicable in practical cases. As a vital step towards implementing self-learning controllers, there is a need to investigate the system performance when considering dynamic changes in occupancy patterns from the energy-saving and thermal comfort viewpoints. This study reports the development of a self-learning occupancy-based HVAC control system that eliminates the need for developing building and occupancy models while defining optimal temperature schedules in residential buildings. The control system employs double deep Q-networks (DDQN), as a reinforcement learning (RL) method, to determine the indoor operative temperature in each control timestep to maximize energy saving and thermal comfort. It directly interacts with the environment to learn optimal control decisions by observing occupancy profiles, weather data, and temperature feedback from buildings. Its performance is assessed and compared with that of an MPC algorithm using the occupancy data collected from 20 apartments. The MPC algorithm is assisted with a perfect building model to estimate the energy demand, an occupancy model to forecast future patterns, and a genetic algorithm (GA) to solve the optimization problem. Five machine learning (ML) algorithms, namely k-nearest neighbors (kNN), random forests (RF), linear regression (LR), and multi-layer perceptron (MLP), are utilized as the candidate building models; the algorithm with the superior performance is selected for use in the control framework. The rest of the present study is structured as follows: Section 2 discusses the proposed methodology, including the development of the DDQN and MPC. Section 3 describes the testbed and occupancy data utilized to evaluate the performance. The results are discussed in Section 4 and the conclusions are made in Section 5. 2 Methodology 2.1 Reinforcement learning 2.1.1 Markov decision process To develop the RL algorithm, the problem is, first, formulated as a Markov decision process (MDP). The MDP framework consists of three key components: states, actions, and rewards. Each state is a representation of the current situation of the environment [20]. In every state, the RL agent interacts with the environment by taking certain actions (i.e., control decisions). This interaction leads to a transition from the current state to a different state and an instant reward. The reward signal is utilized to define the goal of the RL algorithm because the agent always tries to choose the actions that maximize the amount of the cumulative reward [21]. In the following sections, the state space, the action space, and the reward signal are defined in detail. 2.1.1.1 State-space formulation Five attributes are employed in the MDP framework to define the states. As summarized in Table 1 , based on the type of information these features provide for the agent, they are classified into two categories: HVAC characteristics and occupancy schedules. The first category helps the agent to learn HVAC lag time and energy consumption patterns. This category involves outdoor temperature and indoor operative temperature because of the dependency of HVAC operation on these variables. For example, it takes more time to warm up a building when the difference between indoor and outdoor temperature is larger. Regarding the second category, the agent can utilize the features to learn the occupancy patterns and adjust the setpoint temperature to the probabilities of future occupancy states. Various attributes, such as the day of the week, the time of day, weekends, seasons, and CO2 concentration, might be useful for this purpose [22]. However, it was shown in Ref. [23] that occupancy states during the last 2 h and the hour of the day can provide acceptable accuracy in most cases and, as a result, they are employed in this category. Due to the complex nature of occupant behavior, high uncertainty exists in estimating future occupancy patterns [24]. The uncertainty brings about prediction errors, which might negatively impact control performance [2]. To evaluate such effects, a scenario with perfect occupancy prediction (i.e., occupancy prediction with no errors) is also considered in this study. In this way, the agent accesses the actual occupancy data in the following 2 h at every timestep with no need to receive other occupancy-related features (i.e., previous occupancy states and the time of day) 2.1.1.2 Action-space formulation The task of the control system is to determine a setpoint temperature at every timestep. Although temperature can be defined as a continuous variable, the action space is discretized for the sake of avoiding complexities. Given that the controller is assumed to operate during the heating seasons in this study, three indoor temperature settings of deep setback at 15 °C, conservative setback at 19 °C, and setpoint temperature at 22 °C are utilized to define the action set: 1 A = [ 15 , 19 , 22 ] ° C where A denotes the action space. It should be noted that the thermal inertia of the building could make it possible to reach intermediate temperature values via switching between these settings. Thus, introducing more actions does not necessarily lead to performance improvement [25]. 2.1.1.3 Reward function The control agent always tries to find the actions that can maximize the received rewards. Therefore, the reward function should be defined in a way that reflects the control system's objectives, which are to minimize energy consumption and thermal discomfort [26]. The former is defined as a function of heating energy supplied to the building, Q heat [27]. The latter can be quantified based on the difference between indoor operative temperature, T o p , and desired setpoint, T s p , during the periods when occupants are present. The operative temperature can be estimated by calculating the average of mean radiant temperature and indoor air temperature [28]. This parameter considers both radiant and convective heat transfers from human bodies, which are two factors affecting thermal comfort [29]. The following reward function can be defined based on both energy consumption and thermal comfort terms [25,30]: 2 R t = − β ( Q h e a t ( t ) ) − ( 1 − β ) . O c c ( t ) . ( T o p ( t ) − T s p ) n where R t denotes the reward at timestep t, β weights each term, and n determines the importance of higher values of temperature deviation. Occ(t) indicates the binary occupancy state at timestep t (0 and 1 showing the absence and presence of occupants, respectively). As suggested in Refs. [31,32], normalizing the reward function to make the energy and thermal comfort terms have the same scale might further improve the learning process and, therefore, is also considered in this study: 3 R t = − β ( Q h e a t ( t ) Q h e a t , max ) − ( 1 − β ) . O c c ( t ) . ( T o p ( t ) − T s p Δ T max ) n in which Q h e a t , max and Δ T max represent the maximum heating energy demand and temperature difference, respectively. The parameters of the reward function, namely n and β , and the type of the reward function (i.e., the normalized or original reward function) are carefully selected through trial and error process [33]. 2.1.2 Q-learning algorithm Q-learning, a popular RL algorithm proposed by Watkins [34], can be utilized to find optimal solutions to MDP problems [35]. It is a model-free off-policy learning algorithm, in which the agent learns directly from its experience achieved through trial and error. The agent tries to find the actions that lead to the highest Q-value in each state. The Q-value, also called the state-action value, is defined as the expected reward achieved for taking a specific action a given the state s [21]: 4 Q ( s , a ) = E ( r t + γ r t + 1 + γ 2 r t + 2 + \u2026 | s t = s , a t = a ) where Q ( s , a ) , E, r, t, and γ respectively represent the Q-value, expected return, instant reward, control timestep, and discount factor. The Q-values are randomly initiated and updated at each timestep based on the agent's new experience [36]: 5 Q ( s , a ) = Q ( s , a ) + α ( r + γ max a \u2032 Q ( s \u2032 , a \u2032 ) − Q ( s , a ) ) in which α, s \u2032 , and a \u2032 respectively indicate the learning rate, next state, and next action. The learning rate and discount factor are considered hyperparameters and determined via trial and error. 2.1.3 Deep Q-network Q-learning techniques traditionally employ the so-called Q-tables, where the Q-values for every action and state are recorded [37]. This method becomes infeasible when a large number of actions or states are implemented [31]. Deep Q-network (DQN) algorithms were proposed in Ref. [38] to overcome this issue by replacing conventional Q-tables with a deep neural network (DNN) as a well-practiced function approximator. In the DNN, called Q-network, the numbers of neurons in the input layer and output layer are equal to the size of the state space and action space, respectively. The weights of the Q-network, θ , are updated via optimizing the following loss function, L [38]: 6 L ( θ ) = [ Y D Q N − Q ( s , a ; θ ) ] 2 where Y D Q N is the target value that can be defined as: 7 Y D Q N = r + γ max a \u2032 Q ( s \u2032 , a \u2032 ; θ − ) in which θ − represents the weights of the target network. The target network is a separate DNN, with the same structure as the Q-network, that is utilized to prevent possible divergences in the learning process [31]. It was initially suggested to update the target networks' weights cyclically as a copy of the Q-network's weights, which is known as the hard update method [38,39]. However, the soft update method was later suggested, in which the weights are updated via an interpolation process with a fixed ratio in each step [40,41]. The update method is selected in the hyperparameter tuning process, which will be discussed in detail in Section 2.1.6. Table 2 summarizes the structural parameter of the implemented DNN networks. 2.1.4 Double deep Q-network (DDQN) In practice, DQN is prone to overestimating the Q-values, which might negatively affect the learning performance [36]. Hasalt et al. [44] stated that utilizing the same Q-network for both the selection and the evaluation of an action is the main reason behind this issue and proposed the use of the DDQN as a potential solution. The DDQN decouples the processes of action evaluation and selection by redefining the target value as follows: 8 Y D D Q N = r + γ Q ( s \u2032 , a r g m a x a \u2032 Q ( s \u2032 , a \u2032 ; θ ) , θ − ) Based on this relationship, first, the DDQN algorithm selects the optimal action based on the Q-network. Then, instead of using the same network to evaluate the Q-value, the target network is employed. 2.1.5 Policy A policy is defined as the mapping between the states and the probability of selecting different actions [25]. The policy provides a trade-off between exploration and exploitation so as to enable the agent to find the optimal solution. For example, if the agent always exploits with no exploration, the controller might never try all the possible actions in different states, increasing the likelihood of converging to a suboptimal solution. A balance between exploration and exploitation can be obtained using the epsilon-greedy policy, in which the agent selects a random action with a probability of ε while exploiting with the probability of 1 − ε [45]. However, selecting a random action in the exploration phase might cause substantial thermal discomfort. For example, the agent might randomly select a deep setback temperature although the occupants are present at home, which clearly causes thermal discomfort. To address this limitation, the Boltzmann exploration policy is also considered as an alternative approach. In this method, each action is assigned with a selection probability based on their Q-values using the following equation [46]: 9 P ( a | s ) = exp ( Q ( s , a ) T ) ∑ i exp ( Q ( s , i ) T ) where T is the temperature parameter, used to change the exploration rate; its higher values increases the search rate. Based on this equation, the higher the Q-values of a certain action, the more probable that the action is selected by the agent. This policy tends to become greedy when there are large differences between the probabilities while exploring more when the probability distribution is almost uniform. In the initial steps of the learning process, the estimated Q-values are often unreliable, and consequently, more explorations are required in both epsilon-greedy and Boltzmann policies. However, as the learning process continues and more valid Q-values are obtained, the agent can exploit more to minimize the risk of thermal discomfort and energy waste. To consider this factor in the learning process, the temperature parameter and epsilon are initiated with the value of 1 (i.e., higher explorations) and linearly decrease to a minimum value. This minimum value is used to ensure that the agent continues to improve its actions and adapts to new changes. In this study, the policy, the decay duration (i.e., the number of episodes in which the search coefficient are linearly decreasing), and the minimum values of the epsilon and temperature parameter are considered hyperparameters that need to be tuned. 2.1.6 Hyperparameter tuning The hyperparameters of the DDQN algorithm are tuned using trial-and-error based on the assigned search ranges, as summarized in Table 3 ; those parameters leading to the highest accumulative rewards are selected to develop the final control system. Buffer size indicates the maximum number of the recent data elements that are stored in the memory to be utilized for training the agent. Target model update shows how frequently and based on what update method the target model is updated. The values greater than or equal to 1 indicate the number of steps used in the hard update method while the smaller values represent the interpolation coefficient in the soft update method. The Learning rate controls how fast the gradient descent algorithm adjusts the weights of the Q-network to the target values. While a too large learning rate can cause divergence in the learning process, small values can increase the training time [31]. The Discount factor determines the weights that the agent gives to the future rewards, compared with the immediate reward. A discount factor equal to 0 results in a controller that only considers the immediate rewards, ignoring the effects of the current action on the following steps. In contrast, a discount factor equal to 1 leads the agent to give equal weights to the instant and future rewards when it selects the control actions [47]. As discussed earlier, the policy, decay duration, and the minimum search coefficient are also considered hyperparameters and determined in the tuning process. 2.1.7 DDQN employment The operational diagram of the DDQN algorithm is demonstrated in Fig. 1 . The learning process begins with the environment sending the current state at timestep t, S(t), to the control system (step 1). The current state is represented to the agent using occupancy states over the last four timesteps, Occ [t-4, t], the hour of the day, h(t), the outdoor temperature, T out (t), and the operative temperature, T op (t). The Q-network utilizes this information to estimate the Q-values, based on which the policy selects the action at timestep t, A(t), and applies it to the environment (step 2). The action (i.e., the setpoint temperature at the following timestep, T sp (t + 1)), T op (t), solar radiation, SR(t + 1), and the outdoor temperature at the next timestep, T out (t + 1), are passed into the building model to calculate the associated energy consumption, EC(t + 1) and the operative temperature, T op (t + 1). Step 3 calculates the instant reward, R(t), as a function of the operative temperature, occupancy state, and energy consumption. Then, the feedback that consists of the state, action, and reward is passed into the control system (step 4). The feedback is stored in the reply buffer, and then, a batch of the recent experiences is given to the networks (step 5). The aim is to calculate the loss function in step 6 and update the weights of the Q-networks via the gradient descent method in step 7. Depending on the update approach (i.e., soft or hard updates) the weights of the target network are regularly updated (step 8). The learning and control processes are continuously repeated until the end of the episode. A heating season starting from the first of November to the end of March is utilized for training the DDQN algorithm, defined as a training episode. The agent is trained repeatedly on the same heating season; after 20 episodes of training, the agent is applied to a heating season with an unseen occupancy pattern to evaluate its performance using a greedy policy. More details about the occupancy databases are provided in Section 3. The interactions between different software and Python libraries utilized to develop the DDQN agent are demonstrated in Fig. 2 . In the first step, a virtual testbed is constructed by developing a white-box model using the EnergyPlus [48] engine as described in Section 3.2. It should be noted that in real-world applications, the agent directly interacts with an actual building environment, and consequently, there is no need for developing such models. In this study, the virtual building is used to provide an interactive environment for the control system, which is later utilized to evaluate the system performance. As will be discussed in Section 2.3, the simulation process is accelerated by developing a black-box building model that replicates the behavior of the virtual building. The building model is developed utilizing Keras [49] and Scikit-learn [50] libraries in Python. The Keras library is also utilized to develop the function approximator of the DDQN. The DDQN algorithm is developed via the Keras-RL library [51]. Interactions between the building environment and the control agent are enabled through the use of OpenAi Gym toolkit in Python [52], which includes a Python class with three main functions: Initialization, Step, and Reset. In the Initialization function, the action space, observation space, the initial state, and the length of each episode are defined. At each timestep, the control agent calls the Step function with a selected action as the input and receives the instant reward and the next state as the output. The reward is calculated using the occupancy and weather data, stored in a MySQL database [53]. This process continues and the Q-values are updated until the first episode ends. At this point, the Reset function is utilized to reinitialize the process so as to begin the learning process from the initial state again. 2.2 Model-based predictive control (MPC) The working diagram of the MPC is demonstrated in Fig. 3 . This control framework consists of two main parts: Prediction models and an optimization algorithm. Prediction models are developed based on historical data to forecast future changes in occupancy profiles, estimate building energy consumption, and predict the changes in operative temperature, which will be discussed in Section 2.2.1. The purpose of the optimization algorithm is to take advantage of the prediction models to make optimal control decisions. The algorithm aims to fulfill the energy-saving and thermal-comfort objectives, which will be discussed in Section 2.2.2. The feedback from the building should be continuously measured and stored in a database so that it can be later utilized to improve the quality of the building model and occupancy model. 2.2.1 Building and occupancy prediction models The MPC algorithm needs an accurate occupancy model to adjust the operative temperature to future occupancy patterns. Many studies have made attempts to predict future building occupancy states using different models from simple Naïve prediction models to complex deep learning algorithms [12,54\u201357]. Among the candidate models, it was reported in Ref. [58] that a kNN model can outperform other alternatives in most cases. Thus, a kNN model with a k value of 15, as suggested in Ref. [58], is developed and integrated with the MPC. The occupancy model is developed based on the previous occupancy states and time of day as discussed in Section 2.1.1.1. Separate kNN models are developed to predict the occupancy states at future control timesteps. Given a control timestep of 30 min and the need for future occupancy patterns in the next 2 h, four separate kNN models are developed. In addition to developing the actual occupancy models, a second scenario is considered in which the MPC accesses a perfect occupancy prediction model to evaluate the ideal performance of the MPC. In this case, the MPC receives the actual occupancy states 2 h in advance. The MPC algorithm also requires an accurate building model to evaluate the impacts of the control decisions on the system performance over its prediction horizon [59]. This study assumes a perfect building model (i.e., a building model with no prediction errors) for use in the MPC algorithm by utilizing the same model for both decision making and performance evaluation. Hence, the performance of the MPC can be overestimated in this study. The building modeling procedure and the description of the case study building are provided in Section 2.3 and Section 3, respectively. 2.2.2 Optimization algorithm The MPC makes the control decisions by solving an optimization problem at every timestep [60]. Since it seeks the same goal as the DDQN agent (i.e., to maximize thermal comfort while minimizing energy consumption), the DDQN's reward function, described in Section 2.1.1.3, is employed to define the MPC objective. In this study, the MPC aims to find the optimal setpoint temperature that can maximize the cumulative rewards over its prediction horizon, H, as follows: 10 max ∑ h = 1 H R t + h where R t + h represents the instant reward at each future timestep. Although the setpoint temperature is optimized over the prediction horizon, only the first optimal variable (i.e., the setpoint temperature at the next timestep) is applied to the HVAC system. At the next sampled timestep, the whole optimization process is repeated to define the succeeding operative temperature. 2.2.3 MPC employment The operational diagram of the MPC is demonstrated in Fig. 4 . A GA is selected as the optimization method because of its potential to reach global optimum and its promising performance in different applications [61,62]. The hyperparameters of this algorithm are summarized in Table 4 . In step 1, the GA starts the process by defining the first population, which consists of N chromosomes (also called individuals). Every chromosome indicates a prospective solution, which contains H genes that describe the setpoint temperatures over the prediction horizon. It is assumed that 2 h are adequate for the HVAC system to provide the desired temperature from a prior setback temperature in most cases, and as a result, the prediction horizon of the MPC is considered four timesteps. In step 2, the fitness of the chromosomes needs to be evaluated based on the objective function. To this end, each solution (T sp (t + 1), \u2026, T sp (t + H)) along with weather data and indoor operative temperature over the prediction horizon are required to predict future energy consumption and operative temperature using the building model. It should be noted that as well as the current value of the operative temperature recorded by the sensors in the building, its future values (i.e., T op (t + 1), \u2026, T op (t + H-1)) are also required as the inputs of the building model. Since the actual future values are not available at timestep t, the estimated operative temperature by the building model is utilized as the input in the following timesteps. It means that T op (t) is, first, utilized to estimate the operative temperature at the subsequent timestep, T op (t + 1), which is then used as an input for forecasting T op (t + 2). This process continues until the operative temperature in the last timestep is calculated. In addition, for the sake of not over-complicating the control process, it is assumed that the changes in the weather condition are negligible over the course of the prediction horizon, and therefore, no weather forecasting is implemented. It is also essential to forecast future occupancy states to estimate the future values of Occ as an input for the reward function. The occupancy model receives the recent occupancy states (i.e., Occ(t-4), \u2026, Occ(t)) and the time of day as input and forecasts future occupancy states over the prediction horizon. The same occupancy database utilized to train the DDQN network is also employed to develop the kNN models for occupancy prediction. In step 4, the reward function is calculated, as defined in Eq. 10, to obtain the fitness of each individual. In step 5, the chromosomes are ranked based on the fitness, and the best half of them is selected to proceed to the next step. A crossover operation is performed on the selected individuals to repopulate the whole set via producing the children. The children are randomly undergone mutation to add more diversity to the solution set to avoid being stuck in a local optimum. This process is repeated to improve the population until the stopping criteria are met (step 6). In the last iteration at timestep t, T sp (t + 1) is selected and applied to the HVAC system in the building (step 7). The process is repeated to make the control decisions for future timesteps. 2.3 Building model The building model is a key component of the control systems. It is utilized to estimate the impact of making different control decisions on the building energy consumption and occupants\u2019 thermal comfort. Although white-box building models often yield high accuracy by solving detailed mathematical equations, they are computationally inefficient for use in such control applications [63]. It is essential to select a fast building model because the MPC and DDQN are iterative algorithms, which are computationally expensive. To this end, this study utilizes black-box models to replicate the behavior of the virtual building, described in Section 3.2. Various ML models can be employed for this purpose [64]. Some of the most popular models, namely RF, LR, kNN, and artificial neural networks (ANN) are developed, and their performance is compared in terms of computational time and mean absolute error (MAE). The hyperparameters of the algorithms, determined through trial and error, are outlined in Table 5 . RF, LR, and kNN models are developed using the Scikit-learn [65] library and the ANN model is developed using Keras [49] with TensorFlow backend [66] in the Python environment. To generate the required database for developing the black-box models, 10 separate simulations are performed using the white-box model described in Section 3.2. Each simulation is completed using a different setpoint schedule, created by assigning a random operative temperature in the range of 15\u201323 °C to each hour of the day. The simulations are performed in 10-min time intervals, with each simulation providing 21,745 data records for the heating season. The granularity of the created database is decreased to 30-min intervals to be consistent with the length of the control timesteps. The entire database is split into two subsets of train and test with a ratio of 80:20 using random sampling. The models are trained on the train set, and their performance is measured on the test set in terms of MAE and the computational time (i.e., the time that the model takes to predict future occupancy states for the entire test dataset). 2.4 Performance evaluation metrics The thermal comfort of occupants is measured using two different criteria: Average temperature deviation, T \u203e d e v , and deviation period, P d e v . The former criterion is defined as the average difference between the indoor operative temperature and the desired setpoint when occupants are present at home and is formulated as a function of temperature deviation, T d e v , as follows: 11 T d e v ( t ) = { | T o p ( t ) − T d e s i r e d | O c c ( t ) = 1 0 O t h e r w i s e 12 T \u203e d e v = ∑ t = 1 D T d e v ( t ) D where O c c ( t ) shows the occupancy state at timestep t and D indicates the number of total timesteps in each heating season. The deviation period is defined as the proportion of time when occupants encounter a temperature deviation of more than 0.5 °C. This parameter can be calculated using the following equation: 13 C d e v ( t ) = { 1 T d e v ( t ) > 0.5 ° C 0 O t h e r w i s e 14 P d e v = ∑ t = 1 D C d e v ( t ) D × 100 in which C d e v ( t ) is the temperature deviation coefficient. The control performance is also assessed in terms of saving energy by comparing the energy consumption of MPC and DDQN with that of an always-on controller as the baseline. The always-on controller maintains the temperature at the desired setpoint regardless of the occupancy states. The energy saving, ES, of the RL and MPC is estimated using the following equation: 15 E S = E C c − E C b a s e l i n e E C b a s e l i n e × 100 , c = R L and M P C where E C b a s e l i n e and E C c are respectively the energy consumption of the baseline and the controllers. 3 Case study 3.1 Occupancy database preprocessing This study employs an occupancy database gathered from 20 residential units in an apartment block from 2015 to 2017, described in detail in Refs. [23,58,67]. In the preprocessing step, the occupancy data is aggregated from 1-min to 30-min intervals, which is consistent with the duration of the control timestep. Such data granularity reduction can accelerate the simulation process and reduce the impact of occupancy detection errors [5]. Each interval takes a binary value in the preprocessed database; 1 indicates that at least one motion was detected over the time interval, and 0 indicates that no motions were recorded. Due to sensor failure or network maintenance, there are missing values in the occupancy database, which approximately constitute 4% of the whole database. During such periods when there is no reliable information about occupancy states, these missing values are replaced with 1 to ensure that thermal comfort is not sacrificed. It should be noted that in cases when missing data records exist during a long vacancy period, the utilized replacement method might cause energy waste. In such cases, other techniques, such as replacement with the mode of the neighboring values and the imputation using a kNN method, might address the issue. However, in this database, the majority of the missing values occur continuously over subsequent days and there are rarely missing values that are surrounded by the periods of available data. Hence, the mentioned issue is neglected based on the utilized occupancy database and the proposed replacement method is considered a reasonable choice with minimum negative impacts. As discussed in Sections 2.2.1, two types of predictor variables, namely the hour of day and occupancy states from four previous timesteps (i.e., 2 h), are implemented to predict future occupancy patterns. The hour-of-day attribute takes integer values in the range of 0\u201323 to indicate each hour of the day. In order to utilize this feature in the ML models, it is converted to 23 separate dummy variables. 3.2 Building testbed To assess the performance of the control algorithms, a virtual detached house, also implemented in earlier studies [5,58], is employed as a testbed. The characteristics of the building and the conditions used for the building simulation are summarized in Table 6 . The simulation consists of a five-month period covering the whole heating season from the beginning of November to the end of March based on electric baseboards as heating equipment. Given that the efficiency of electric baseboards is assumed as 100%, the amount of electricity consumption equals the amount of heating energy demand associated with each controller. The building envelope and the construction material are defined in SketchUp software [68] via the Openstudio plug-in Ref. [69]. Then, the developed model is imported into EnergyPlus to complete the simulation. 4 Results and discussion 4.1 Building model selection Before investigating the performance of the controllers, the best building model needs to be identified. Fig. 5 demonstrates the performance of the ML algorithms in terms of MAE and computational time. The ANN model provides the most accurate energy and temperature predictions with MAE of 0.18 kWh and 0.21 °C, respectively. However, the high accuracy of the ANN is obtained at the expense of computational time, taking 0.23 s to perform the prediction task. The RF model closely follows the performance of the ANN model by providing MAE of 0.19 kWh and 0.22 °C for the energy and temperature predictions, respectively. On the other hand, the RF model leads to much faster computation with 0.05 s of execution time. Since the MPC and DDQN are iterative control algorithms, the execution time, although small, can accumulate and bring about too slow computation, especially in the hyperparameter tuning process. Hence, a tradeoff between prediction performance and speed becomes necessary in such applications. In this study, the RF model is selected as the building model because it is almost four times faster than the ANN model while providing acceptable performance. Although the LR model provides much faster execution at 0.002 s, it is not selected because of its poor prediction performance, providing MAE of 0.59 kWh and 0.69 °C for energy and temperature predictions, respectively. 4.2 DDQN control system 4.2.1 Hyperparameter tuning The type of the reward function and its coefficients need to be determined before the hyperparameters of the DDQN algorithm are tuned. For this purpose, different values of n and β are tried in both Eqs. 2 and 3, and the agent's performance is closely monitored. Those parameters of the reward function that enable the agent to learn the following behavior are selected to construct the final reward function: 1) not sacrificing thermal comfort to save energy and 2) pre-heating the building before the occupants arrive. The latter criterion evaluates whether the agent could estimate future occupancy patterns and adjust the indoor operative temperature to the forecasted states. As discussed in Ref. [6], using such proactive control approach based on pre-heating the indoor environment can considerably enhance thermal comfort. Furthermore, regarding the virtual building implemented in this study, it was shown that due to the thermal mass of the building, pre-heating can play an essential role in maintaining thermal comfort of occupants [5]. Table 7 summarizes the parameters of the reward function and the hyperparameters of the control system. The original form of the reward function presented in Eq. 2 provides superior performance and is selected as the final reward function. The selected reward function gives a higher weight to the thermal comfort term using a β equal to 0.7. The hyperparameters are determined via trial-and-error using a random search technique. In this method, those parameters leading to the highest accumulated reward are selected to develop the ultimate control system. It should be noted that a separate DDQN algorithm is developed for every residential unit, and consequently, the hyperparameters are determined separately for each algorithm. As observed in Table 7, a median buffer size of 12,875 is selected in the tuning process. Given that an episode consists of almost 7248 timesteps, this value demonstrates that in most cases, the agent uses the experience from more than one episode. The median coefficient of the target model update is selected as 0.1, showing that a soft update method is utilized in most cases. Additionally, the Boltzmann policy is selected with a median minimum search coefficient of 0.10. 4.2.2 Training process and convergence Fig. 6 shows the episodic rewards (i.e., the accumulated rewards in each episode) obtained by the DDQN algorithm over 40 episodes of training for a sample occupancy profile. Each episode is defined as the entire heating season, starting from the first of November to the end of March. It should be noted that in real-world applications, the control problem should be addressed as a continuous learning process, in which the number of episodes is not limited. In this way, the agent always tries to learn new occupancy patterns. A limited number of episodes (i.e., 40 heating seasons) is considered in this study to illustrate the accumulated rewards and the trends during the control process. It is seen that the performance sharply increases from less than −8500 to almost −5100 in the first two episodes, which is an approximately 41% improvement. Although the speed of the improvement decreases, the agent receives higher rewards at almost −4700 after eight training episodes. However, the agent cannot find a better policy during the rest of the training phase. The sharp increase during the initial training period demonstrates that the agent's actions might cause considerable thermal discomfort or energy waste before learning the optimal policy. Fig. 7 depicts the average T \u203e d e v caused by the DDQN algorithm during the first 10 episodes of training for all the residential units. During the first two episodes, T \u203e d e v decreases from 0.44 °C to 0.28 °C, an almost 36% improvement in thermal comfort. After 10 episodes, T \u203e d e v reaches 0.10 °C, more than 75% enhancement in the training performance. Fig. 8 demonstrates average P d e v over the same duration. Occupants encounter thermal discomfort during 4% of the length of the first episode, approximately. This value further decreases and finally reaches 1.2% in the last episode. The average energy consumption per control timestep over all the residential units is demonstrated in Fig. 9 . Despite the improvements in thermal comfort, the energy consumption does not change much during the training period. It increases from 2.04 kWh in the first episode to 2.06 kWh in the second episode and gradually decreases to 2.03 kWh in the last one. 4.3 Test performance This section discusses the performance of a trained DDQN agent on the test dataset, which is then compared with that of the MPC. The intra-day and annual performance of the controllers is discussed in the following sections. 4.3.1 Intra-day performance Fig. 10 illustrates the operative temperature profile and the energy consumption of the DDQN and MPC controllers for a sample occupancy profile based on perfect occupancy prediction. It is observed that the controllers provide similar temperature settings, which leads to almost the same energy consumption profile. The controllers define a slight setback temperature during the nighttime vacancy periods to save energy and provide an acceptable level of thermal comfort as soon as the occupancy states are reported. During the daytime long vacancy period, starting from 11:30, both controllers select a deep setback of 15 °C to maximize energy saving. They avoid causing thermal discomfort by pre-heating the indoor environment one timestep before the arrival time. Fig. 11 demonstrates the control decisions based on the actual occupancy prediction for the same occupancy profile as presented in Fig. 10. Although the MPC and DDQN make similar control decisions when perfect occupancy prediction is concerned, they respond quite differently to the occupancy prediction uncertainty. During the long vacancy period between 11:30 and 20:30, the DDQN agent follows a conservative temperature setting strategy as it maintains the temperature near the setpoint level and starts preheating the building long before the arrival time of occupants. On the other hand, the MPC algorithm chooses a deep setback and allows the temperature to drop to as low as 17 °C right before the occupants\u2019 arrival, which provides a higher energy saving at the cost of thermal comfort. It is worth mentioning that given the poor occupancy detection performance of motion sensors when occupants are mostly stationary, the uncertainty and randomness are expected to increase during the nighttime when people are often sleeping. The DDQN agent shows an ability to recognize such uncertainty and, as a result, maintains the thermal comfort level during the night regardless of the reported vacancy states. In contrast, the MPC reacts to the nighttime changes in the occupancy pattern and reduces the temperature to save energy, which leads to thermal discomfort. 4.3.2 Overall performance The distributions of the average temperature deviation, deviation period, and energy saving based on perfect and actual occupancy predictions over an entire heating season are investigated in this section. Fig. 12 depicts the distribution of the average temperature deviation for all the residential units. Regarding the perfect occupancy prediction, both control systems result in a similar deviation of less than 0.1 °C in most cases. When occupancy prediction uncertainty is introduced into the control problem, the MPC causes poor thermal comfort with a median T \u203e d e v of 0.32 °C. The DDQN agent still keeps T \u203e d e v below 0.1 °C in most cases despite the higher uncertainty. Fig. 13 illustrates how frequently occupants encounter a temperature deviation during an episode for a sample occupancy profile based on actual occupancy prediction. It is seen that large temperature deviations occur more frequently when the MPC method is employed. More specifically, occupants encounter temperature deviations of more than 0.5 °C during 371 time intervals in case of using the MPC, which is almost three times higher than that of the DDQN. A similar pattern is demonstrated for P d e v in Fig. 14 . The MPC slightly outperforms the DDQN agent when perfect prediction is available with a median P d e v of 2.23%, which is 1.22% lower than that of the DDQN. However, when the occupancy prediction uncertainty is considered, the DDQN improves thermal comfort by providing a median P d e v of as low as 1.17%. In contrast, the MPC causes significant thermal discomfort at a median P d e v of 9.04% in the same situation. The higher thermal comfort provided by the DDQN for the actual occupancy prediction can be justified by its conservative intra-day decisions described in Section 4.3.1. With the consideration of the uncertainty that exists in future occupancy prediction, the DDQN agent acts conservatively to avoid thermal discomfort by maintaining the temperature close to the desired setpoint. On the other hand, the MPC adjusts the indoor temperature to the predicted occupancy patterns to save energy, which ultimately causes thermal discomfort for occupants. The distribution of the energy saving is shown in Fig. 15 for the perfect and actual occupancy predictions. Regarding the perfect prediction, the MPC algorithm yields slightly higher energy saving at a median value of 27.97%, which is almost 0.5% higher than that of the DDQN. The reason behind MPC's higher energy saving and thermal comfort performance can be linked with the use of a perfect building model in the optimization algorithm, which can lead to an over-estimation of the MPC performance (discussed in Section 2.2.1). In contrast, the DDQN needs to learn the building energy demands and operative temperature changes via trial-and-error, which might affect the performance. Concerning the actual occupancy prediction, the energy performance of the DDQN algorithm significantly drops to a median value of less than 10%. This drop in the performance is consistent with its conservative strategy of temperature setting, which naturally requires higher energy consumption. In contrast, the MPC keeps the energy saving level at around that of the perfect prediction case with a median saving of 27.60%. 5 Conclusion This study proposes a self-learning control method based on a DDQN algorithm, aiming to find optimal temperature settings based on learning dynamic occupancy patterns, building energy consumption, and the lag time of the HVAC system. The effectiveness of the proposed controller is evaluated by comparing its performance with that of an MPC algorithm, as a well-practiced predictive control method. In contrast to the MPC, the proposed control system is developed with no need for occupancy models and building models. This study also assesses the impact of occupancy prediction uncertainty on the control system performance by considering two scenarios of perfect and actual occupancy predictions. When perfect occupancy prediction is concerned, the DDQN and MPC provide similar energy saving and thermal comfort performance. Nonetheless, they respond differently to the occupancy prediction uncertainty when the actual occupancy prediction is considered. The DDQN algorithm shows the ability to consider such uncertainty in making control decisions. Such understanding leads the agent to adopt a conservative approach to the temperature setting, which significantly reduces thermal discomfort, compared with that of the MPC. The DDQN leads to a deviation period of 1.17%, which is 7.87% lower than that of the MPC. However, the higher thermal comfort is achieved at the cost of energy saving as the DDQN algorithm causes almost three times higher energy consumption than that of the MPC. The DDQN algorithm also shows the ability to reduce the negative impacts of occupancy detection errors due to the use of motion sensors. During the periods when the possibility of false vacancy states is high, such as during the nighttime when the occupants are mostly stationary, the DDQN controller maintains a setpoint temperature regardless of the occupancy state. This control policy can further decrease thermal discomfort in such cases. Despite its promising test performance, the DDQN causes severe thermal discomfort during the training period. Its interactions with the environment in the learning phase cause an almost 0.43 °C average temperature deviation and a 3.9% deviation period in most cases. This limitation of the DDQN algorithms can be a barrier to the wide adoption of such smart controllers in real-world cases. More research is required to focus on the training process to avoid thermal discomfort for the occupants. CRediT authorship contribution statement Mohammad Esrafilian-Najafabadi: Writing \u2013 review & editing, Writing \u2013 original draft, Validation, Methodology, Data curation, Conceptualization. Fariborz Haghighat: Writing \u2013 review & editing, Supervision, Resources, Project administration, Methodology, Investigation, Funding acquisition. Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgment The authors would like to thank Mr. Moein Dadollahi Sarab for his invaluable comments and suggestions regarding the modeling and development of the MPC. The authors would also like to express their gratitude to Concordia University\u2013Canada for the support through the Concordia Research Chair\u2013Energy & Environment. References [1] V.L. Erickson S. Achleitner A.E. Cerpa POEM: power-efficient occupancy-based energy management system IPSN 2013 - Proc. 12th Int. Conf. Inf. Process. Sens. Networks, Part CPSWeek 2013 2013 ACM Press New York, New York, USA 203 216 10.1145/2461381.2461407 V.L. Erickson, S. Achleitner, A.E. Cerpa, POEM: Power-efficient occupancy-based energy management system, in: IPSN 2013 - Proc. 12th Int. Conf. Inf. Process. Sens. Networks, Part CPSWeek 2013, ACM Press, New York, New York, USA, 2013: pp. 203-216. https://doi.org/10.1145/2461381.2461407. [2] J. Gluck C. Koehler J. Mankoff A. Dey Y. Agarwal A Systematic Approach for Exploring Tradeoffs in Predictive HVAC Control Systems for Buildings 2017 ArXiv Prepr. ArXiv1705.02058 https://arxiv.org/abs/1705.02058v1 J. Gluck, C. Koehler, J. Mankoff, A. Dey, Y. Agarwal, A Systematic Approach for Exploring Tradeoffs in Predictive HVAC Control Systems for Buildings, ArXiv Prepr. ArXiv1705.02058. (2017). https://arxiv.org/abs/1705.02058v1. [3] F. Nägele T. Kasper B. Girod Turning up the heat on obsolete thermostats: a simulation-based comparison of intelligent control approaches for residential heating systems Renew. Sustain. Energy Rev. 75 2017 1254 1268 10.1016/j.rser.2016.11.112 F. Nagele, T. Kasper, B. Girod, Turning up the heat on obsolete thermostats: A simulation-based comparison of intelligent control approaches for residential heating systems, Renew. Sustain. Energy Rev. 75 (2017) 1254-1268. https://doi.org/10.1016/j.rser.2016.11.112. [4] A. Beltran V.L. Erickson A.E. Cerpa ThermoSense: occupancy thermal based sensing for HVAC control Proc. 5th ACM Work. Embed. Syst. Energy-Efficient Build. - BuildSys'13, Association for Computing Machinery (ACM) 2013 1 8 10.1145/2528282.2528301 New York, New York, USA A. Beltran, V.L. Erickson, A.E. Cerpa, ThermoSense: Occupancy Thermal Based Sensing for HVAC Control, in: Proc. 5th ACM Work. Embed. Syst. Energy-Efficient Build. - BuildSys\u201913, Association for Computing Machinery (ACM), New York, New York, USA, 2013: pp. 1-8. https://doi.org/10.1145/2528282.2528301. [5] M. Esrafilian-Najafabadi F. Haghighat Occupancy-based HVAC control using deep learning algorithms for estimating online preconditioning time in residential buildings Energy Build. 252 2021 111377 10.1016/j.enbuild.2021.111377 M. Esrafilian-Najafabadi, F. Haghighat, Occupancy-based HVAC control using deep learning algorithms for estimating online preconditioning time in residential buildings, Energy Build. 252 (2021) 111377. https://doi.org/10.1016/j.enbuild.2021.111377. [6] M. Esrafilian-Najafabadi F. Haghighat Occupancy-based HVAC control systems in buildings: a state-of-the-art review Build. Environ. 197 2021 107810 10.1016/j.buildenv.2021.107810 M. Esrafilian-Najafabadi, F. Haghighat, Occupancy-based HVAC control systems in buildings: A state-of-the-art review, Build. Environ. 197 (2021) 107810. https://doi.org/10.1016/j.buildenv.2021.107810. [7] Y. Yao D.K. Shekhar State of the art review on model predictive control (MPC) in Heating Ventilation and Air-conditioning (HVAC) field Build. Environ. 200 2021 107952 Y. Yao, D.K. Shekhar, State of the art review on model predictive control (MPC) in Heating Ventilation and Air-conditioning (HVAC) field, Build. Environ. 200 (2021) 107952. [8] F. Oldewurtel D. Sturzenegger M. Morari Importance of occupancy information for building climate control Appl. Energy 101 2013 521 532 10.1016/j.apenergy.2012.06.014 F. Oldewurtel, D. Sturzenegger, M. Morari, Importance of occupancy information for building climate control, Appl. Energy. 101 (2013) 521-532. https://doi.org/10.1016/j.apenergy.2012.06.014. [9] J. Shi N. Yu W. Yao Energy efficient building HVAC control algorithm with real-time occupancy prediction Energy Proc. 111 2017 267 276 10.1016/j.egypro.2017.03.028 J. Shi, N. Yu, W. Yao, Energy Efficient Building HVAC Control Algorithm with Real-time Occupancy Prediction, Energy Procedia. 111 (2017) 267-276. https://doi.org/10.1016/j.egypro.2017.03.028. [10] C. Turley M. Jacoby G. Pavlak G. Henze Development and evaluation of occupancy-aware HVAC control for residential building energy efficiency and occupant comfort Energies 13 2020 5396 10.3390/en13205396 C. Turley, M. Jacoby, G. Pavlak, G. Henze, Development and Evaluation of Occupancy-Aware HVAC Control for Residential Building Energy Efficiency and Occupant Comfort, Energies. 13 (2020) 5396. https://doi.org/10.3390/en13205396. [11] S. Salimi A. Hammad Optimizing energy consumption and occupants comfort in open-plan offices using local control based on occupancy dynamic data Build. Environ. 176 2020 106818 10.1016/j.buildenv.2020.106818 S. Salimi, A. Hammad, Optimizing energy consumption and occupants comfort in open-plan offices using local control based on occupancy dynamic data, Build. Environ. 176 (2020) 106818. https://doi.org/10.1016/j.buildenv.2020.106818. [12] G. Gao K. Whitehouse The self-programming thermostat: optimizing setback schedules based on home occupancy patterns BUILDSYS 2009 - Proc. 1st ACM Work. Embed. Sens. Syst. Energy-Efficiency Build. Held Conjunction with ACM SenSys 2009 2009 ACM Press New York, New York, USA 67 72 10.1145/1810279.1810294 G. Gao, K. Whitehouse, The self-programming thermostat: Optimizing setback schedules based on home occupancy patterns, in: BUILDSYS 2009 - Proc. 1st ACM Work. Embed. Sens. Syst. Energy-Efficiency Build. Held Conjunction with ACM SenSys 2009, ACM Press, New York, New York, USA, 2009: pp. 67-72. https://doi.org/10.1145/1810279.1810294. [13] G. Kontes G. Giannakis V. Sánchez P. de Agustin-Camacho A. Romero-Amorrortu N. Panagiotidou D. Rovas S. Steiger C. Mutschler G. Gruen Simulation-based evaluation and optimization of control strategies in buildings Energies 11 2018 3376 10.3390/en11123376 G. Kontes, G. Giannakis, V. Sanchez, P. de Agustin-Camacho, A. Romero-Amorrortu, N. Panagiotidou, D. Rovas, S. Steiger, C. Mutschler, G. Gruen, Simulation-Based Evaluation and Optimization of Control Strategies in Buildings, Energies. 11 (2018) 3376. https://doi.org/10.3390/en11123376. [14] S. Goyal H.A. Ingley P. Barooah Occupancy-based zone-climate control for energy-efficient buildings: complexity vs. performance Appl. Energy 2013 10.1016/j.apenergy.2013.01.039 S. Goyal, H.A. Ingley, P. Barooah, Occupancy-based zone-climate control for energy-efficient buildings: Complexity vs. performance, Appl. Energy. (2013). https://doi.org/10.1016/j.apenergy.2013.01.039. [15] C. Lork W.-T. Li Y. Qin Y. Zhou C. Yuen W. Tushar T.K. Saha An uncertainty-aware deep reinforcement learning framework for residential air conditioning energy management Appl. Energy 276 2020 115426 C. Lork, W.-T. Li, Y. Qin, Y. Zhou, C. Yuen, W. Tushar, T.K. Saha, An uncertainty-aware deep reinforcement learning framework for residential air conditioning energy management, Appl. Energy. 276 (2020) 115426. [16] E. Barrett S. Linder Autonomous hvac control, a reinforcement learning approach Jt. Eur. Conf. Mach. Learn. Knowl. Discov. Databases 2015 Springer 3 19 E. Barrett, S. Linder, Autonomous hvac control, a reinforcement learning approach, in: Jt. Eur. Conf. Mach. Learn. Knowl. Discov. Databases, Springer, 2015: pp. 3-19. [17] A. Javed H. Larijani A. Ahmadinia R. Emmanuel Comparison of the robustness of RNN, MPC and ANN controller for residential heating system 2014 IEEE Fourth Int. Conf. Big Data Cloud Comput. 2014 IEEE 604 611 A. Javed, H. Larijani, A. Ahmadinia, R. Emmanuel, Comparison of the robustness of RNN, MPC and ANN controller for residential heating system, in: 2014 IEEE Fourth Int. Conf. Big Data Cloud Comput., IEEE, 2014: pp. 604-611. [18] K. Kurte J. Munk O. Kotevska K. Amasyali R. Smith E. McKee Y. Du B. Cui T. Kuruganti H. Zandi Evaluating the adaptability of reinforcement learning based HVAC control for residential houses Sustainability 12 2020 7727 K. Kurte, J. Munk, O. Kotevska, K. Amasyali, R. Smith, E. McKee, Y. Du, B. Cui, T. Kuruganti, H. Zandi, Evaluating the adaptability of reinforcement learning based HVAC control for residential houses, Sustainability. 12 (2020) 7727. [19] J.R. Vázquez-Canteli Z. Nagy Reinforcement learning for demand response: a review of algorithms and modeling techniques Appl. Energy 235 2019 1072 1089 J.R. Vazquez-Canteli, Z. Nagy, Reinforcement learning for demand response: A review of algorithms and modeling techniques, Appl. Energy. 235 (2019) 1072-1089. [20] M.A. Wiering M. Van Otterlo Reinforcement learning, adapt. Learn Optim 12 2012 729 M.A. Wiering, M. Van Otterlo, Reinforcement learning, Adapt. Learn. Optim. 12 (2012) 729. [21] R.S. Sutton A.G. Barto Reinforcement Learning: an Introduction 2018 MIT press R.S. Sutton, A.G. Barto, Reinforcement learning: An introduction, MIT press, 2018. [22] B. Yang F. Haghighat B.C.M. Fung K. Panchabikesan Season-based occupancy prediction in residential buildings using machine learning models E-Prime 2021 100003 10.1016/J.PRIME.2021.100003 B. Yang, F. Haghighat, B.C.M. Fung, K. Panchabikesan, Season-Based Occupancy Prediction in Residential Buildings Using Machine Learning Models, E-Prime. (2021) 100003. https://doi.org/10.1016/J.PRIME.2021.100003. [23] M. Esrafilian-Najafabadi F. Haghighat Impact of predictor variables on the performance of future occupancy prediction: feature selection using genetic algorithms and machine learning Build. Environ. 2022 109152 10.1016/J.BUILDENV.2022.109152 M. Esrafilian-Najafabadi, F. Haghighat, Impact of predictor variables on the performance of future occupancy prediction: Feature selection using genetic algorithms and machine learning, Build. Environ. (2022) 109152. https://doi.org/10.1016/J.BUILDENV.2022.109152. [24] Y. Zhang X. Bai F.P. Mills J.C.V. Pezzey Rethinking the role of occupant behavior in building energy performance: a review Energy Build. 172 2018 279 294 10.1016/J.ENBUILD.2018.05.017 Y. Zhang, X. Bai, F.P. Mills, J.C.V. Pezzey, Rethinking the role of occupant behavior in building energy performance: A review, Energy Build. 172 (2018) 279-294. https://doi.org/10.1016/J.ENBUILD.2018.05.017. [25] S. Brandi M.S. Piscitelli M. Martellacci A. Capozzoli Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings Energy Build. 224 2020 110225 10.1016/j.enbuild.2020.110225 S. Brandi, M.S. Piscitelli, M. Martellacci, A. Capozzoli, Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings, Energy Build. 224 (2020) 110225. https://doi.org/10.1016/j.enbuild.2020.110225. [26] R. Jia M. Jin K. Sun T. Hong C. Spanos Advanced building control via deep reinforcement learning Energy Proc. 158 2019 6158 6163 10.1016/J.EGYPRO.2019.01.494 R. Jia, M. Jin, K. Sun, T. Hong, C. Spanos, Advanced Building Control via Deep Reinforcement Learning, Energy Procedia. 158 (2019) 6158-6163. https://doi.org/10.1016/J.EGYPRO.2019.01.494. [27] Z. Zhang A. Chong Y. Pan C. Zhang S. Lu K.P. Lam A deep reinforcement learning approach to using whole building energy model for hvac optimal control 2018 Build. Perform. Anal. Conf. SimBuild 2018 22 23 Z. Zhang, A. Chong, Y. Pan, C. Zhang, S. Lu, K.P. Lam, A deep reinforcement learning approach to using whole building energy model for hvac optimal control, in: 2018 Build. Perform. Anal. Conf. SimBuild, 2018: pp. 22-23. [28] American Society of Heating Refrigerating and Air-Conditioning Engineers (ASHRAE) Standard 55 - Thermal Environmental Conditions for Human Occupancy 2010 American Society of Heating Refrigerating and Air-Conditioning Engineers (ASHRAE), Standard 55 - Thermal environmental conditions for human occupancy, 2010. [29] ASHRAE ASHRAE Standard 55-2017: Thermal Environmental Conditions for Human Occupancy 2017 https://www.ashrae.org/technical-resources/bookstore/standard-55-thermal-environmental-conditions-for-human-occupancy ASHRAE, ASHRAE Standard 55-2017: Thermal environmental conditions for human occupancy, 2017. https://www.ashrae.org/technical-resources/bookstore/standard-55-thermal-environmental-conditions-for-human-occupancy. [30] J.R. Dobbs B.M. Hencey Model predictive HVAC control with online occupancy model Energy Build. 82 2014 675 684 10.1016/j.enbuild.2014.07.051 J.R. Dobbs, B.M. Hencey, Model predictive HVAC control with online occupancy model, Energy Build. 82 (2014) 675-684. https://doi.org/10.1016/j.enbuild.2014.07.051. [31] A. Gupta Y. Badr A. Negahban R.G. Qiu Energy-efficient heating control for smart buildings with deep reinforcement learning J. Build. Eng. 34 2021 101739 A. Gupta, Y. Badr, A. Negahban, R.G. Qiu, Energy-efficient heating control for smart buildings with deep reinforcement learning, J. Build. Eng. 34 (2021) 101739. [32] X. Ding W. Du A. Cerpa Octopus: deep reinforcement learning for holistic smart building control Proc. 6th ACM Int. Conf. Syst. Energy-Efficient Build. Cities 2019 Transp. 326 335 X. Ding, W. Du, A. Cerpa, Octopus: Deep reinforcement learning for holistic smart building control, in: Proc. 6th ACM Int. Conf. Syst. Energy-Efficient Build. Cities, Transp., 2019: pp. 326-335. [33] W. Valladares M. Galindo J. Gutiérrez W.-C. Wu K.-K. Liao J.-C. Liao K.-C. Lu C.-C. Wang Energy optimization associated with thermal comfort and indoor air control via a deep reinforcement learning algorithm Build. Environ. 155 2019 105 117 W. Valladares, M. Galindo, J. Gutierrez, W.-C. Wu, K.-K. Liao, J.-C. Liao, K.-C. Lu, C.-C. Wang, Energy optimization associated with thermal comfort and indoor air control via a deep reinforcement learning algorithm, Build. Environ. 155 (2019) 105-117. [34] C.J.C.H. Watkins Learning from Delayed Rewards 1989 C.J.C.H. Watkins, Learning from delayed rewards, (1989). [35] Z. Cheng Q. Zhao F. Wang Y. Jiang L. Xia J. Ding Satisfaction based Q-learning for integrated lighting and blind control Energy Build. 127 2016 43 55 10.1016/j.enbuild.2016.05.067 Z. Cheng, Q. Zhao, F. Wang, Y. Jiang, L. Xia, J. Ding, Satisfaction based Q-learning for integrated lighting and blind control, Energy Build. 127 (2016) 43-55. https://doi.org/10.1016/j.enbuild.2016.05.067. [36] H. Hasselt Double Q-learning Adv. Neural Inf. Process. Syst. 23 2010 2613 2621 H. Hasselt, Double Q-learning, Adv. Neural Inf. Process. Syst. 23 (2010) 2613-2621. [37] B. Huchuk S. Sanner W. O'Brien Development and evaluation of data-driven controls for residential smart thermostats Energy Build. 249 2021 111201 B. Huchuk, S. Sanner, W. O\u2019Brien, Development and evaluation of data-driven controls for residential smart thermostats, Energy Build. 249 (2021) 111201. [38] V. Mnih K. Kavukcuoglu D. Silver A.A. Rusu J. Veness M.G. Bellemare A. Graves M. Riedmiller A.K. Fidjeland G. Ostrovski Human-level control through deep reinforcement learning Nature 518 2015 529 533 V. Mnih, K. Kavukcuoglu, D. Silver, A.A. Rusu, J. Veness, M.G. Bellemare, A. Graves, M. Riedmiller, A.K. Fidjeland, G. Ostrovski, Human-level control through deep reinforcement learning, Nature. 518 (2015) 529-533. [39] T. Kobayashi W.E.L. Ilboudo T-soft update of target network for deep reinforcement learning Neural Network. 136 2021 63 71 T. Kobayashi, W.E.L. Ilboudo, T-soft update of target network for deep reinforcement learning, Neural Networks. 136 (2021) 63-71. [40] A. Stooke P. Abbeel Rlpyt: A Research Code Base for Deep Reinforcement Learning in Pytorch 2019 ArXiv Prepr. ArXiv1909.01500 A. Stooke, P. Abbeel, rlpyt: A research code base for deep reinforcement learning in pytorch, ArXiv Prepr. ArXiv1909.01500. (2019). [41] Y. Fujita P. Nagarajan T. Kataoka T. Ishikawa Chainerrl: A Deep Reinforcement Learning Library 2019 ArXiv Prepr. ArXiv1912.03905 Y. Fujita, P. Nagarajan, T. Kataoka, T. Ishikawa, Chainerrl: A deep reinforcement learning library, ArXiv Prepr. ArXiv1912.03905. (2019). [42] D.P. Kingma J. Ba Adam A method for stochastic optimization ArXiv Prepr. ArXiv1412. 6980 2014 D.P. Kingma, J. Ba, Adam: A method for stochastic optimization, ArXiv Prepr. ArXiv1412.6980. (2014). [43] I. Goodfellow Y. Bengio A. Courville Deep Learning 2016 MIT Press https://mitpress.mit.edu/books/deep-learning I. Goodfellow, Y. Bengio, A. Courville, Deep Learning, MIT Press, 2016. https://mitpress.mit.edu/books/deep-learning. [44] H. Van Hasselt A. Guez D. Silver Deep reinforcement learning with double q-learning Proc. AAAI Conf. Artif 2016 . Intell. H. Van Hasselt, A. Guez, D. Silver, Deep reinforcement learning with double q-learning, in: Proc. AAAI Conf. Artif. Intell., 2016. [45] Z. Zou X. Yu S. Ergan Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network Build. Environ. 168 2020 106535 Z. Zou, X. Yu, S. Ergan, Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network, Build. Environ. 168 (2020) 106535. [46] M.A. Wiering Explorations in Efficient Reinforcement Learning 1999 M.A. Wiering, Explorations in efficient reinforcement learning, (1999). [47] Y. Chen L.K. Norford H.W. Samuelson A. Malkawi Optimal control of HVAC and window systems for natural ventilation through reinforcement learning Energy Build. 169 2018 195 205 10.1016/J.ENBUILD.2018.03.051 Y. Chen, L.K. Norford, H.W. Samuelson, A. Malkawi, Optimal control of HVAC and window systems for natural ventilation through reinforcement learning, Energy Build. 169 (2018) 195-205. https://doi.org/10.1016/J.ENBUILD.2018.03.051. [48] EnergyPlus Weather Data by Location, (n.d.) https://energyplus.net/weather-location/north_and_central_america_wmo_region_4/CAN/PQ/CAN_PQ_Montreal.Intl.AP.716270_CWEC EnergyPlus, Weather Data by Location, (n.d.). https://energyplus.net/weather-location/north_and_central_america_wmo_region_4/CAN/PQ/CAN_PQ_Montreal.Intl.AP.716270_CWEC. [49] F. Chollet Keras 2015 https://github.com/fchollet/keras F. Chollet, Keras, (2015). https://github.com/fchollet/keras. [50] P. Fabian G. Varoquaux A. Gramfort V. Michel B. Thirion O. Grisel M. Blondel P. Prettenhofer R. Weiss V. Dubourg J. Vanderplas A. Passos D. Cournapeau M. Brucher M. Perrot E. Duchesnay Scikit-learn: machine learning in Python J. Mach. Learn. Res. 12 2011 2825 2830 10.1145/2786984.2786995 P. Fabian, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, E. Duchesnay, Scikit-learn: Machine Learning in Python, J. Mach. Learn. Res. 12 (2011) 2825-2830. https://doi.org/10.1145/2786984.2786995. [51] M. Plappert Keras-rl, GitHub Repos 2016 https://github.com/keras-rl/keras-rl M. Plappert, Keras-rl, GitHub Repos. (2016). https://github.com/keras-rl/keras-rl. [52] G. Brockman V. Cheung L. Pettersson J. Schneider J. Schulman J. Tang W. Zaremba Openai Gym 2016 ArXiv Prepr. ArXiv1606.01540 G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, W. Zaremba, Openai gym, ArXiv Prepr. ArXiv1606.01540. (2016). [53] Oracle Corporation, MySQL,(n.d.). https://dev.mysql.com/doc/. [54] B. Huchuk S. Sanner W. O'Brien Comparison of machine learning models for occupancy prediction in residential buildings using connected thermostat data Build. Environ. 160 2019 106177 10.1016/j.buildenv.2019.106177 B. Huchuk, S. Sanner, W. O\u2019Brien, Comparison of machine learning models for occupancy prediction in residential buildings using connected thermostat data, Build. Environ. 160 (2019) 106177. https://doi.org/10.1016/j.buildenv.2019.106177. [55] H. Elkhoukhi M. Bakhouya M. Hanifi D. El Ouadghiri On the use of deep learning approaches for occupancy prediction in energy efficient buildings Proc. 2019 7th Int. Renew. Sustain. Energy Conf. IRSEC 2019 2019 Institute of Electrical and Electronics Engineers Inc. 10.1109/IRSEC48032.2019.9078164 H. Elkhoukhi, M. Bakhouya, M. Hanifi, D. El Ouadghiri, On the use of Deep Learning Approaches for Occupancy prediction in Energy Efficient Buildings, in: Proc. 2019 7th Int. Renew. Sustain. Energy Conf. IRSEC 2019, Institute of Electrical and Electronics Engineers Inc., 2019. https://doi.org/10.1109/IRSEC48032.2019.9078164. [56] W. Kleiminger F. Mattern S. Santini Predicting household occupancy for smart heating control: a comparative performance analysis of state-of-the-art approaches Energy Build. 85 2014 493 505 10.1016/j.enbuild.2014.09.046 W. Kleiminger, F. Mattern, S. Santini, Predicting household occupancy for smart heating control: A comparative performance analysis of state-of-the-art approaches, Energy Build. 85 (2014) 493-505. https://doi.org/10.1016/j.enbuild.2014.09.046. [57] F.C. Sangogboye M.B. Kjærgaard PROMT: predicting occupancy presence in multiple resolution with time-shift agnostic classification Comput. Sci. - Res. Dev. 2018 Springer Verlag 105 115 10.1007/s00450-017-0351-x F.C. Sangogboye, M.B. Kjaergaard, PROMT: predicting occupancy presence in multiple resolution with time-shift agnostic classification, in: Comput. Sci. - Res. Dev., Springer Verlag, 2018: pp. 105-115. https://doi.org/10.1007/s00450-017-0351-x. [58] M. Esrafilian-Najafabadi F. Haghighat Impact of occupancy prediction models on building HVAC control system performance: application of machine learning techniques Energy Build. 257 2022 111808 10.1016/j.enbuild.2021.111808 M. Esrafilian-Najafabadi, F. Haghighat, Impact of occupancy prediction models on building HVAC control system performance: Application of machine learning techniques, Energy Build. 257 (2022) 111808. https://doi.org/https://doi.org/10.1016/j.enbuild.2021.111808. [59] J.A. Candanedo V.R. Dehkordi M. Stylianou Model-based predictive control of an ice storage device in a building cooling system Appl. Energy 111 2013 1032 1045 10.1016/j.apenergy.2013.05.081 J.A. Candanedo, V.R. Dehkordi, M. Stylianou, Model-based predictive control of an ice storage device in a building cooling system, Appl. Energy. 111 (2013) 1032-1045. https://doi.org/10.1016/j.apenergy.2013.05.081. [60] C. Wang B. Wang M. Cui F. Wei Cooling seasonal performance of inverter air conditioner using model prediction control for demand response Energy Build. 256 2022 111708 10.1016/J.ENBUILD.2021.111708 C. Wang, B. Wang, M. Cui, F. Wei, Cooling seasonal performance of inverter air conditioner using model prediction control for demand response, Energy Build. 256 (2022) 111708. https://doi.org/10.1016/J.ENBUILD.2021.111708. [61] M. Esrafilian R. Ahmadi Energy, environmental and economic assessment of a polygeneration system of local desalination and CCHP Desalination 454 2019 20 37 10.1016/j.desal.2018.12.004 M. Esrafilian, R. Ahmadi, Energy, environmental and economic assessment of a polygeneration system of local desalination and CCHP, Desalination. 454 (2019) 20-37. https://doi.org/10.1016/j.desal.2018.12.004. [62] S.A. Shams R. Ahmadi Dynamic optimization of solar-wind hybrid system connected to electrical battery or hydrogen as an energy storage system Int. J. Energy Res. 2021 6549 10.1002/er.6549 S.A. Shams, R. Ahmadi, Dynamic optimization of solar-wind hybrid system connected to electrical battery or hydrogen as an energy storage system, Int. J. Energy Res. (2021) er.6549. https://doi.org/10.1002/er.6549. [63] J. Reynolds Y. Rezgui A. Kwan S. Piriou A zone-level, building energy optimisation combining an artificial neural network, a genetic algorithm, and model predictive control Energy 151 2018 729 739 10.1016/j.energy.2018.03.113 J. Reynolds, Y. Rezgui, A. Kwan, S. Piriou, A zone-level, building energy optimisation combining an artificial neural network, a genetic algorithm, and model predictive control, Energy. 151 (2018) 729-739. https://doi.org/10.1016/j.energy.2018.03.113. [64] Y. Sun F. Haghighat B.C.M. Fung A review of the-state-of-the-art in data-driven approaches for building energy prediction Energy Build. 221 2020 110022 10.1016/j.enbuild.2020.110022 Y. Sun, F. Haghighat, B.C.M. Fung, A review of the-state-of-the-art in data-driven approaches for building energy prediction, Energy Build. 221 (2020) 110022. https://doi.org/10.1016/j.enbuild.2020.110022. [65] F. Pedregosa G. Varoquaux A. Gramfort V. Michel B. Thirion O. Grisel M. Blondel P. Prettenhofer R. Weiss V. Dubourg Scikit-learn: machine learning in Python J. Mach. Learn. Res. 12 2011 2825 2830 https://www.jmlr.org/papers/v12/pedregosa11a.html F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, Scikit-learn: Machine learning in Python, J. Mach. Learn. Res. 12 (2011) 2825-2830. https://www.jmlr.org/papers/v12/pedregosa11a.html. [66] M. Abadi P. Barham J. Chen Z. Chen A. Davis J. Dean M. Devin S. Ghemawat G. Irving M. Isard TensorFlow: a system for large-scale machine learning 12th USENIX Symp 2016 Oper. Syst. Des. Implement. 265 283 M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, TensorFlow: A System for Large-Scale Machine Learning, in: 12th USENIX Symp. Oper. Syst. Des. Implement., 2016: pp. 265-283. [67] S. Akbari F. Haghighat Occupancy and occupant activity drivers of energy consumption in residential buildings Energy Build. 250 2021 111303 10.1016/J.ENBUILD.2021.111303 S. Akbari, F. Haghighat, Occupancy and occupant activity drivers of energy consumption in residential buildings, Energy Build. 250 (2021) 111303. https://doi.org/10.1016/J.ENBUILD.2021.111303. [68] SketchUp Trimble https://www.sketchup.com/ 2020 Trimble, SketchUp, (2020). https://www.sketchup.com/. [69] National Renewable Energy Laboratory, OpenStudio,(n.d.). http://nrel.github.io/OpenStudio-user-documentation/. [70] ASHRAE Standard for the Design of High-Performance Green Buildings 2009 ASHRAE ASHRAE, Standard for the Design of High-Performance Green Buildings, ASHRAE, 2009. [71] U.S. Department of Energy (DOE), National Renewable Energy Laboratory (NREL), EnergyPlus, (n.d.).https://energyplus.net/documentation.",
    "scopus-id": "85141527430",
    "coredata": {
        "eid": "1-s2.0-S0360132322009775",
        "dc:description": "This study proposes a self-learning control system that aims to learn occupancy profiles, building energy consumption patterns, and lag-time of the heating, ventilation, and air-conditioning (HVAC) systems. The control system learns by interacting with the environment with no need to develop building models and occupancy prediction models. The controller is developed based on a double deep Q-networks (DDQN) algorithm, as a model-free reinforcement learning method. The system's performance is evaluated and compared with that of a model predictive control (MPC) system under two scenarios of perfect and actual occupancy predictions based on occupancy data collected from 20 residential units. The MPC is assisted by a genetic algorithm and supervised learning models for predicting future occupancy patterns, indoor operative temperature, and building energy consumption. The results show that in the case of using perfect occupancy prediction, the self-learning controller operates almost as well as the MPC while not requiring any models. When occupancy prediction uncertainty is added to the problem, the proposed method outperforms the MPC in terms of thermal comfort by increasing the average temperature deviation and deviation period by 0.24 °C and 7.87%, respectively. However, the DDQN agent causes significant thermal comfort violations during the initial training period. The system causes up to a 2.8% longer deviation period and a 0.32 °C higher average temperature deviation, compared with the performance of the fully-trained system.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2022-12-31",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S0360132322009775",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Esrafilian-Najafabadi, Mohammad"
            },
            {
                "@_fa": "true",
                "$": "Haghighat, Fariborz"
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S0360132322009775"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S0360132322009775"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S0360-1323(22)00977-5",
        "prism:volume": "226",
        "articleNumber": "109747",
        "prism:publisher": "Elsevier Ltd.",
        "dc:title": "Towards self-learning control of HVAC systems with the consideration of dynamic occupancy patterns: Application of model-free deep reinforcement learning",
        "prism:copyright": "© 2022 Elsevier Ltd. All rights reserved.",
        "openaccess": "0",
        "prism:issn": "03601323",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "Double deep Q-networks"
            },
            {
                "@_fa": "true",
                "$": "Model predictive control (MPC)"
            },
            {
                "@_fa": "true",
                "$": "Self-learning control"
            },
            {
                "@_fa": "true",
                "$": "Occupancy patterns"
            },
            {
                "@_fa": "true",
                "$": "Energy efficiency"
            }
        ],
        "openaccessArticle": "false",
        "prism:publicationName": "Building and Environment",
        "openaccessSponsorType": null,
        "prism:pageRange": "109747",
        "pubType": "fla",
        "prism:coverDisplayDate": "December 2022",
        "prism:doi": "10.1016/j.buildenv.2022.109747",
        "prism:startingPage": "109747",
        "dc:identifier": "doi:10.1016/j.buildenv.2022.109747",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "605",
            "@width": "267",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "109181",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "582",
            "@width": "335",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "101785",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "456",
            "@width": "780",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr10.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "183464",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "589",
            "@width": "313",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr9.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "128244",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "498",
            "@width": "780",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr11.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "200470",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "600",
            "@width": "268",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr8.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "107863",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "548",
            "@width": "313",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr12.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "110820",
            "@ref": "gr12",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "227",
            "@width": "735",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr13.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "99974",
            "@ref": "gr13",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "509",
            "@width": "291",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr14.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "107503",
            "@ref": "gr14",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "582",
            "@width": "268",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr15.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "110784",
            "@ref": "gr15",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "373",
            "@width": "669",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "123541",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "435",
            "@width": "379",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "115300",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "378",
            "@width": "669",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "114734",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "501",
            "@width": "380",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "116477",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "373",
            "@width": "735",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "131124",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "72",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "70353",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "94",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "70067",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "128",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr10.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "80033",
            "@ref": "gr10",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "87",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr9.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "72765",
            "@ref": "gr9",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "140",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr11.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "82225",
            "@ref": "gr11",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "73",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr8.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "70335",
            "@ref": "gr8",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "93",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr12.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "72363",
            "@ref": "gr12",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "68",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr13.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "69272",
            "@ref": "gr13",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "93",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr14.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "72422",
            "@ref": "gr14",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "75",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr15.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "72098",
            "@ref": "gr15",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "122",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "74826",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "143",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "77089",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "124",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "72522",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "124",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "74705",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "111",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "74906",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "2678",
            "@width": "1182",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "307057",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2580",
            "@width": "1484",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "315475",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2019",
            "@width": "3453",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr10_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "895764",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2605",
            "@width": "1385",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr9_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "428208",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2203",
            "@width": "3453",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr11_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "985544",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2662",
            "@width": "1190",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr8_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "304737",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2425",
            "@width": "1386",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr12_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "324529",
            "@ref": "gr12",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1006",
            "@width": "3256",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr13_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "273539",
            "@ref": "gr13",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2251",
            "@width": "1288",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr14_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "297699",
            "@ref": "gr14",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2586",
            "@width": "1190",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr15_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "318502",
            "@ref": "gr15",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1650",
            "@width": "2962",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "398035",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1929",
            "@width": "1680",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "361915",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1673",
            "@width": "2961",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "358538",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2219",
            "@width": "1682",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "379442",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1653",
            "@width": "3256",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "540033",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si33.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "14255",
            "@ref": "si33",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si35.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "76457",
            "@ref": "si35",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si23.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "85183",
            "@ref": "si23",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si37.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "124455",
            "@ref": "si37",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si39.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "16668",
            "@ref": "si39",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si25.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4589",
            "@ref": "si25",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si19.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7288",
            "@ref": "si19",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si34.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "85947",
            "@ref": "si34",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si14.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "53829",
            "@ref": "si14",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si9.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "85401",
            "@ref": "si9",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si28.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "52675",
            "@ref": "si28",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si36.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "25430",
            "@ref": "si36",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si16.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3911",
            "@ref": "si16",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si7.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "16193",
            "@ref": "si7",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si15.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "19937",
            "@ref": "si15",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si17.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "70233",
            "@ref": "si17",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si2.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "29103",
            "@ref": "si2",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si21.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "62176",
            "@ref": "si21",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si29.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "19281",
            "@ref": "si29",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si32.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "71928",
            "@ref": "si32",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si22.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "23804",
            "@ref": "si22",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si30.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "19912",
            "@ref": "si30",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si31.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "100298",
            "@ref": "si31",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si12.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "112479",
            "@ref": "si12",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si24.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "101616",
            "@ref": "si24",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si6.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "29496",
            "@ref": "si6",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si38.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "44080",
            "@ref": "si38",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si18.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6936",
            "@ref": "si18",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si26.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7286",
            "@ref": "si26",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si3.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "20419",
            "@ref": "si3",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si20.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4198",
            "@ref": "si20",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si4.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "26864",
            "@ref": "si4",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si27.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "76094",
            "@ref": "si27",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si1.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "21676",
            "@ref": "si1",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si5.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5186",
            "@ref": "si5",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si13.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "38529",
            "@ref": "si13",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si8.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "17932",
            "@ref": "si8",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si11.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5421",
            "@ref": "si11",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-si10.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "12802",
            "@ref": "si10",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322009775-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "1780111",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85141527430"
    }
}}