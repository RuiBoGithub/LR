{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85122507811",
    "originalText": "serial JL 271427 291210 291817 291881 291883 31 Automation in Construction AUTOMATIONINCONSTRUCTION 2022-01-10 2022-01-10 2022-01-10 2022-01-10 2022-02-02T22:57:16 1-s2.0-S0926580522000012 S0926-5805(22)00001-2 S0926580522000012 10.1016/j.autcon.2022.104128 S300 S300.1 FULL-TEXT 1-s2.0-S0926580521X00131 2022-12-12T20:09:02.538094Z 0 0 20220301 20220331 2022 2022-01-11T00:55:26.999574Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast highlightsabst nomenclature primabst ref 0926-5805 09265805 true 135 135 C Volume 135 17 104128 104128 104128 202203 March 2022 2022-03-01 2022-03-31 2022 article fla © 2022 Elsevier B.V. All rights reserved. COMPARISONONLINEOFFLINEDEEPREINFORCEMENTLEARNINGMODELPREDICTIVECONTROLFORTHERMALENERGYMANAGEMENT BRANDI S Nomenclature 1 Introduction 1.1 Research gap and contributions 2 Case study and control problem 3 Methodology 3.1 MPC 3.1.1 Model 3.1.2 Constraints 3.1.3 Cost function 3.2 Deep reinforcement learning 3.2.1 Design of the action-space 3.2.2 Safety constraints 3.2.3 Reward function 3.2.4 Design of the state-space 3.2.5 DRL with offline training 3.2.6 DRL with online training 3.3 Modelled predictions of forcing variables 4 Implementation 4.1 Implementation of rule-based control strategies 4.2 Implementation of model predictive control strategy 4.3 Implementation of deep reinforcement learning control strategies 4.4 Implementation details of the DRL agent with offline training 4.5 Implementation details of the DRL agent with online training 4.6 Implementation of modelled predictions of disturbances 5 Results 6 Discussion 7 Conclusion and future work References MARTINOPOULOS 2018 687 699 G KATHIRGAMANATHAN 2021 110120 A MAY 2019 R REINFORCEMENTLEARNINGMETHODAFEASIBLESUSTAINABLECONTROLSTRATEGYFOREFFICIENTOCCUPANTCENTREDBUILDINGOPERATIONINSMARTCITIES SALSBURY 2005 90 100 T MOLINASOLANA 2017 598 609 M CAPOZZOLI 2018 336 352 A MILLER 2015 1 17 C SERALE 2018 631 G MA 2012 92 100 J CHEN 2018 250 265 J HENZE 1997 233 264 G CHO 2003 1333 1342 S SERALE 2018 438 449 G FIORENTINI 2017 465 479 M SEAL 2020 118456 S ZHAO 2015 415 426 Y VASALLO 2017 1165 1177 M FRANKE 1997 171 180 R RASTEGARPOUR 2020 104209 S KONTES 2018 3376 G SUTTON 2018 R REINFORCEMENTLEARNINGINTRODUCTION WANG 2020 115036 Z HAN 2019 101748 M MNIH 2015 529 533 V BRANDI 2020 110225 S ZHANG 2019 472 490 Z WANG 2016 77 86 Y SCHREIBER 2020 110490 T VAZQUEZCANTELI 2019 243 257 J DU 2021 116117 Y WANG 2017 46 Y GAO 2019 G CHEN 2020 174 B RUELENS 2015 F CRAWLEY 2001 319 331 D ZOU 2020 106535 Z BARRETT 2015 3 19 E RAMAN 2020 2326 2332 N BIAGIONI 2020 29 33 D PROCEEDINGS1STINTERNATIONALWORKSHOPREINFORCEMENTLEARNINGFORENERGYMANAGEMENTINBUILDINGSCITIESRLEM20 ACOMPARISONMODELFREEMODELPREDICTIVECONTROLFORPRICERESPONSIVEWATERHEATERS CEUSTERS 2021 117634 G HAARNOJA 2019 T HILL A MATHWORKS HERCEG 2013 502 510 M 2013EUROPEANCONTROLCONFERENCEECC MULTIPARAMETRICTOOLBOX30 GUROBI BEMPORAD 2006 6678 6683 A PROCEEDINGS45THIEEECONFERENCEDECISIONCONTROL MODELPREDICTIVECONTROLDESIGNNEWTRENDSTOOLS SMITH S BRANDIX2022X104128 BRANDIX2022X104128XS 2024-01-10T00:00:00.000Z 2024-01-10T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2022 Elsevier B.V. All rights reserved. 0 https://doi.org/10.15223/policy-017 https://doi.org/10.15223/policy-037 https://doi.org/10.15223/policy-012 https://doi.org/10.15223/policy-029 https://doi.org/10.15223/policy-004 item S0926-5805(22)00001-2 S0926580522000012 1-s2.0-S0926580522000012 10.1016/j.autcon.2022.104128 271427 2022-04-16T04:52:30.985197Z 2022-03-01 2022-03-31 1-s2.0-S0926580522000012-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/MAIN/application/pdf/2bb98deb7aa992aa379cd40b2458de1c/main.pdf main.pdf pdf true 10150180 MAIN 15 1-s2.0-S0926580522000012-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/PREVIEW/image/png/85e5cd23b6af0a8b1cd5079bf467f442/main_1.png main_1.png png 60353 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0926580522000012-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr8/DOWNSAMPLED/image/jpeg/7c89401b0fe0d5de76caf4337d54f35b/gr8.jpg gr8 gr8.jpg jpg 103391 526 711 IMAGE-DOWNSAMPLED 1-s2.0-S0926580522000012-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr9/DOWNSAMPLED/image/jpeg/276475a1621fd3c672ee829ba6ab0cba/gr9.jpg gr9 gr9.jpg jpg 106411 526 711 IMAGE-DOWNSAMPLED 1-s2.0-S0926580522000012-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr4/DOWNSAMPLED/image/jpeg/4b663eeb057d1e1073c85852ea7aca2f/gr4.jpg gr4 gr4.jpg jpg 23069 336 367 IMAGE-DOWNSAMPLED 1-s2.0-S0926580522000012-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr5/DOWNSAMPLED/image/jpeg/8a130eeebb762d4acf82590ced549496/gr5.jpg gr5 gr5.jpg jpg 58153 385 800 IMAGE-DOWNSAMPLED 1-s2.0-S0926580522000012-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr6/DOWNSAMPLED/image/jpeg/91603ce44fca8fc2d554f464c535de8b/gr6.jpg gr6 gr6.jpg jpg 71770 460 800 IMAGE-DOWNSAMPLED 1-s2.0-S0926580522000012-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr7/DOWNSAMPLED/image/jpeg/34803904d6cf58460f630749ca7ebfa1/gr7.jpg gr7 gr7.jpg jpg 196388 697 711 IMAGE-DOWNSAMPLED 1-s2.0-S0926580522000012-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr1/DOWNSAMPLED/image/jpeg/398ad5f4e1a66f97c6a9cf4016077967/gr1.jpg gr1 gr1.jpg jpg 46482 399 622 IMAGE-DOWNSAMPLED 1-s2.0-S0926580522000012-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr2/DOWNSAMPLED/image/jpeg/4f2a4f000f2645f95e255b4d211b5a54/gr2.jpg gr2 gr2.jpg jpg 70940 362 711 IMAGE-DOWNSAMPLED 1-s2.0-S0926580522000012-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr3/DOWNSAMPLED/image/jpeg/d89d78d0d03b2e00a30b82cebe373692/gr3.jpg gr3 gr3.jpg jpg 36137 193 711 IMAGE-DOWNSAMPLED 1-s2.0-S0926580522000012-gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr10/DOWNSAMPLED/image/jpeg/414db28835d5fcad3ac820a80d677b6a/gr10.jpg gr10 gr10.jpg jpg 107794 526 711 IMAGE-DOWNSAMPLED 1-s2.0-S0926580522000012-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr8/THUMBNAIL/image/gif/13d6b201b6513187c051725cb1921677/gr8.sml gr8 gr8.sml sml 12402 162 219 IMAGE-THUMBNAIL 1-s2.0-S0926580522000012-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr9/THUMBNAIL/image/gif/227a2f848b18c381c86eb12e81284b87/gr9.sml gr9 gr9.sml sml 12519 162 219 IMAGE-THUMBNAIL 1-s2.0-S0926580522000012-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr4/THUMBNAIL/image/gif/4b5d9da53b6a1efbd8c20111dd63f80d/gr4.sml gr4 gr4.sml sml 6903 164 179 IMAGE-THUMBNAIL 1-s2.0-S0926580522000012-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr5/THUMBNAIL/image/gif/62df839f1d128015e6c984b2070c2d61/gr5.sml gr5 gr5.sml sml 5907 105 219 IMAGE-THUMBNAIL 1-s2.0-S0926580522000012-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr6/THUMBNAIL/image/gif/57755f1d24cf9d466616448c96776add/gr6.sml gr6 gr6.sml sml 5923 126 219 IMAGE-THUMBNAIL 1-s2.0-S0926580522000012-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr7/THUMBNAIL/image/gif/b63c9dbcc06b6ff291270d2afa9221f8/gr7.sml gr7 gr7.sml sml 11760 164 167 IMAGE-THUMBNAIL 1-s2.0-S0926580522000012-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr1/THUMBNAIL/image/gif/5f6522144d19523f0f335e1a88f00cb5/gr1.sml gr1 gr1.sml sml 6716 141 219 IMAGE-THUMBNAIL 1-s2.0-S0926580522000012-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr2/THUMBNAIL/image/gif/a6f4839c8cc66545145dad060e085085/gr2.sml gr2 gr2.sml sml 6781 112 219 IMAGE-THUMBNAIL 1-s2.0-S0926580522000012-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr3/THUMBNAIL/image/gif/7ee9d071ac75cbd47d86541bdf662394/gr3.sml gr3 gr3.sml sml 4780 60 219 IMAGE-THUMBNAIL 1-s2.0-S0926580522000012-gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr10/THUMBNAIL/image/gif/aa7283b599ae262f1c95b53927db6bad/gr10.sml gr10 gr10.sml sml 12611 162 219 IMAGE-THUMBNAIL 1-s2.0-S0926580522000012-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr8/HIGHRES/image/jpeg/085b55eb2982a3d86ba3f60a5cf336ae/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 882111 2329 3150 IMAGE-HIGH-RES 1-s2.0-S0926580522000012-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr9/HIGHRES/image/jpeg/81e37730427e517053687b4d15e4b6f6/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 899749 2329 3150 IMAGE-HIGH-RES 1-s2.0-S0926580522000012-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr4/HIGHRES/image/jpeg/3e86c19d4b8a5df20f60f10eb3d2ea57/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 169414 1486 1624 IMAGE-HIGH-RES 1-s2.0-S0926580522000012-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr5/HIGHRES/image/jpeg/02c77b1c5d11d2f990b0c7ca99146690/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 458535 1703 3543 IMAGE-HIGH-RES 1-s2.0-S0926580522000012-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr6/HIGHRES/image/jpeg/b0da5a4e0deabdf6f2256eb40138925a/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 556946 2037 3543 IMAGE-HIGH-RES 1-s2.0-S0926580522000012-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr7/HIGHRES/image/jpeg/2c934cd52f2a6a70ca7d5413a9bb6615/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 1827741 3090 3150 IMAGE-HIGH-RES 1-s2.0-S0926580522000012-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr1/HIGHRES/image/jpeg/0a36be3023f1c0c22291b575965e71ec/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 338559 1770 2756 IMAGE-HIGH-RES 1-s2.0-S0926580522000012-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr2/HIGHRES/image/jpeg/df7ce20a018aff97188546cec3f16032/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 473399 1604 3150 IMAGE-HIGH-RES 1-s2.0-S0926580522000012-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr3/HIGHRES/image/jpeg/81cf7aae2e677478a34d06db4a108c40/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 123618 514 1890 IMAGE-HIGH-RES 1-s2.0-S0926580522000012-gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/gr10/HIGHRES/image/jpeg/254331821798d27ff231c6ad736764a3/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 912144 2329 3150 IMAGE-HIGH-RES 1-s2.0-S0926580522000012-si1.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/STRIPIN/image/svg+xml/3335e6fdaa3325b37425bf6dbd859778/si1.svg si1 si1.svg svg 1577 ALTIMG 1-s2.0-S0926580522000012-si10.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/STRIPIN/image/svg+xml/731c4469c085cedabc981507d2ea2cd8/si10.svg si10 si10.svg svg 14021 ALTIMG 1-s2.0-S0926580522000012-si2.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/STRIPIN/image/svg+xml/97b297a492d8ae67a77efb0833509a6c/si2.svg si2 si2.svg svg 22630 ALTIMG 1-s2.0-S0926580522000012-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/STRIPIN/image/svg+xml/4bb0c84e3cd0ce43375ba68ec656cb30/si3.svg si3 si3.svg svg 7856 ALTIMG 1-s2.0-S0926580522000012-si4.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/STRIPIN/image/svg+xml/158a2b44297587e2d58b394b813f90fc/si4.svg si4 si4.svg svg 7960 ALTIMG 1-s2.0-S0926580522000012-si5.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/STRIPIN/image/svg+xml/5e1e607a21036dd5851e0628b863d2b5/si5.svg si5 si5.svg svg 11168 ALTIMG 1-s2.0-S0926580522000012-si6.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/STRIPIN/image/svg+xml/b5ac3c5edfc4c0dc05593e53fcf56e63/si6.svg si6 si6.svg svg 10740 ALTIMG 1-s2.0-S0926580522000012-si7.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/STRIPIN/image/svg+xml/451b6f218eacb862f51bc53359aa0f42/si7.svg si7 si7.svg svg 7159 ALTIMG 1-s2.0-S0926580522000012-si8.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/STRIPIN/image/svg+xml/f9afbb7a2548c1d03cde013c1d6a5fa6/si8.svg si8 si8.svg svg 11845 ALTIMG 1-s2.0-S0926580522000012-si9.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0926580522000012/STRIPIN/image/svg+xml/528b41639f07bfbc6d2fcf4fb351db85/si9.svg si9 si9.svg svg 11837 ALTIMG 1-s2.0-S0926580522000012-am.pdf am am.pdf pdf 9097221 AAM-PDF https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:10MDXQ2FG2K/MAIN/application/pdf/c5b202cbc985e067b532396b568a1ba4/am.pdf AUTCON 104128 104128 S0926-5805(22)00001-2 10.1016/j.autcon.2022.104128 Elsevier B.V. Fig. 1 Schematics of the cooling system analyzed. Fig. 1 Fig. 2 Methodological framework of the proposed study. Fig. 2 Fig. 3 Detail of the electricity prices used in the work. Fig. 3 Fig. 4 Summary of the variables included in the state-space, action-space and employed to evaluate the reward. Fig. 4 Fig. 5 a) Evolution of the learning rate and of b) Number of the gradient steps during the simulation of Online trained DRL agent. Fig. 5 Fig. 6 Total electricity cost obtained by the different control strategies in the period July\u2013August in the case of a)perfect prediction and b)modelled predictions of external forcing variables. Fig. 6 Fig. 7 Comparison of storage tank temperature profiles for the different control strategies (perfect predictions), June\u2013August. Fig. 7 Fig. 8 Storage cooling load and temperature patterns for MPC control strategy in the period 21/08\u201327/08. Fig. 8 Fig. 9 Storage cooling load and temperature patterns for DRL with Offline Training control strategy in the period 21/08\u201327/08. Fig. 9 Fig. 10 Storage cooling load and temperature patterns for DRL with Online Training control strategy in the period 21/08\u201327/08. Fig. 10 Table 1 Parameters used in the MPC controller. Table 1 Parameter Value Units ε 0.005 \u20ac/K ε max 3 K Δt s 1 h N 48 \u2013 COP 2.67 \u2013 C s 11.62 kWh/K UA 0.012 kW/K Table 2 Hyperparameters of the reward function. Table 2 Variable Value Weight factor (β) 100 Penalty cost (P) 1 Storage temperature tolerance (τ) 1 °C Table 3 Variables included in the state-space. Table 3 Variable Min value Max value Unit Timestep State of charge (SOC) 0 1 \u2013 k-4, \u2026, k-1, k Electricity Price (R) 0.03 0.3 \u20ac/kWh k, k + 1, \u2026, k + 24 Building Cooling Demand (Q d ) 0 20 kW k, k + 1, \u2026, k + 24 Ambient Air Temperature (T a ) 13 30 °C k Outdoor Air Temperature (T o ) 7.5 40 °C k Table 4 Hyperparameters of the DRL Agents. Table 4 Hyperparameter Value Discount factor (γ) 0.99 Number of hidden layers 2 Number of Neurons per Hidden Layer 256 Activation Function ReLu Optimizer Adam Entropy regularization coefficient (α) 0.2 Memory size 2160 Table 5 Total operating cost and electricity consumption comparison of the system using the different control strategies. The Pred column refers to the type of predictions of external disturbances used (perfect P or M modelled predictions). Table 5 Strategy Pred June July August \u20ac kWh \u20ac kWh \u20ac kWh RBC 1 \u2013 13.1 272 21.9 344 18.9 310 RBC 2 \u2013 8.40 280 10.8 361 9.78 326 MPC P 8.16 272 10.26 343 9.24 309 DRL Offline P 8.28 277 10.5 351 9.6 320 DRL Online P 21.24 269 13.1 357 10.14 326 MPC M 8.16 272 10.26 342 9.3 310 DRL Offline M 8.34 278 10.5 351 9.6 319 DRL Online M 24.3 270 12.78 365 10.62 314 Table 6 Thermal energy exchanged by the storage tank during charging (Ch) and discharging (Disch) phases and the fraction of cooling demand satisfied (Dem) by the different control strategies. Table 6 Strategy Pred June July August Ch kWh Disch kWh Dem % Ch kWh Disch kWh Dem % Ch kWh Disch kWh Dem % RBC 1 \u2013 845 886 95 1014 963 89 923 877 90 RBC 2 \u2013 924 936 100 1189 1080 100 1077 976 100 MPC P 897 936 100 1132 1080 100 1022 976 100 DRL Off. P 906 936 100 1142 1080 100 1042 976 100 DRL On. P 743 797 85 1154 1068 99 1066 976 100 MPC M 896 936 100 1131 1080 100 1024 976 100 DRL Off. M 906 936 100 1142 1080 100 1043 976 100 DRL On. M 729 786 84 1185 1076 99 1025 976 100 Comparison of online and offline deep reinforcement learning with model predictive control for thermal energy management Silvio Brandi a Massimo Fiorentini b Alfonso Capozzoli a \u204e a Dipartimento Energia \u201cGalileo Ferraris\u201d, Politecnico di Torino, TEBE Research Group, BAEDA Lab, Corso Duca degli Abruzzi 24, 10129 Torino, Italy Dipartimento Energia \u201cGalileo Ferraris\u201d Politecnico di Torino TEBE Research Group BAEDA Lab Corso Duca degli Abruzzi 24 Torino 10129 Italy Dipartimento Energia \u201cGalileo Ferraris\u201d, Politecnico di Torino, TEBE Research Group, BAEDA lab, Corso Duca degli Abruzzi 24, 10129 Torino, Italy b Urban Energy Systems Laboratory, Empa - Swiss Federal Laboratories for Materials Science and Technology, 8600 Dübendorf, Switzerland Urban Energy Systems Laboratory, Empa - Swiss Federal Laboratories for Materials Science and Technology Dübendorf 8600 Switzerland Empa - Swiss Federal Laboratories for Materials Science and Technology, 8600 Dübendorf, Switzerland \u204e Corresponding author. This paper proposes a comparison between an online and offline Deep Reinforcement Learning (DRL) formulation with a Model Predictive Control (MPC) architecture for energy management of a cold-water buffer tank linking an office building and a chiller subject to time-varying energy prices, with the objective of minimizing operating costs. The intrinsic model-free approach of DRL is generally lost in common implementations for energy management, as they are usually pre-trained offline and require a surrogate model for this purpose. Simulation results showed that the online-trained DRL agent, while requiring an initial 4 weeks adjustment period achieving a relatively poor performance (160% higher cost), it converged to a control policy almost as effective as the model-based strategies (3.6% higher cost in the last month). This suggests that the DRL agent trained online may represent a promising solution to overcome the barrier represented by the modelling requirements of MPC and offline-trained DRL approaches. Keywords Deep reinforcement learning Model predictive control HVAC control Building energy consumption Energy savings Building energy management Nomenclature Δt s Control sampling time (h) m ̇ Water mass flow rate (m 3/s) C s Total thermal capacitance of the water storage (kWh/K) c p Water specific heat capacity (kJ/sK) N Length of prediction horizon Q cap Capacity of chiller (kW) Q ch, max Maximum heat transfer to the storage in charging (kW) Q ch Heat transfer to the storage in charging (kW) Q d Building heat demand (kW) R Energy cost (/kWh) T a Air temperature of the space in which the storage is located (°C) T s Storage temperature (°C) T ch Supply temperature of the chiller (°C) T s, max Storage temperature upper boundary (°C) T s, min Storage temperature lower boundary (°C) UA Total UA-value of the storage tank (kW/K) Acronyms DR Demand Response AI Artificial Intelligence HVAC Heating, Ventilation and Air Conditioning RBC Rule-Based Control PID Proportional-Integrative-Derivative MPC Model Predictive Control RL Reinforcement Learning DRL Deep Reinforcement Learning SAC Soft-Actor-Critic MDP Markov Decision Process POMDP Partially Observable Markov Decision Process DNN Deep Neural Networks MILP Mixed Integer Linear Programming LSTM Long-Short Term Memory COP Coefficient Of Performance RMSE Root Mean Squared Error 1 Introduction Heating, Ventilation and Air Conditioning (HVAC) systems account for approximately 50% of the energy demand in buildings. The recent widespread adoption of on-site Renewable Energy Sources (RES) and energy storage systems [1] raised several challenges for cost effective controllers for HVAC and energy systems in buildings. The intrinsic variability of RES production could jeopardize grid stability if not correctly balanced by the supply side through demand side management strategies [2]. The optimal operation of buildings and their energy-related services is influenced by exogenous factors such as climate conditions, electricity prices and Demand Response (DR) programs, which all vary in time and can be affected by different degrees of uncertainty. Advanced building controllers are able, differently from classical controllers, to consider trade-offs between multiple and often contrasting objectives such as indoor thermal comfort, energy consumption and grid requirements, and should autonomously learn to adapt to a changing environment, boundary conditions and constraints [3]. The classical control strategies usually implemented in the various building systems, such as rule-based or Proportional-Integrative-Derivative (PID) controllers, are robust and easily implementable, but are generally only reactive and cannot deal with conflicting objectives [4]. To overcome these limitations researchers explored control techniques based on online predictions, optimization processes and Artificial Intelligence (AI), enabled also by the widespread adoption of advanced metering infrastructure in buildings. This information has the potential to provide useful insights into the current and future behaviour of buildings and their energy systems [5\u20137]. Model Predictive Control (MPC) is a well-established method for controlling complex interacting dynamical systems. Firstly developed in the process industry, it has recently been receiving wide attention from the building industry as it is capable of considering the physical behaviour and dynamics of the controlled systems, its constraints, and a prediction of the future disturbances to minimize a cost function solved with different optimization methods [8]. In the energy and buildings field, MPC has been successfully applied to several applications from low-level to supervisory control, such as zone temperature control in multi-zone buildings [9,10], charging and discharging of ice-storage systems [11] and management of radiant heating systems [12]. MPC has proven to be particularly effective in managing renewable sources and energy storage systems, because of its ability to use predictions of future intermittent renewable generation [13,14], as well as managing bi-directional energy exchange with the grid and variable energy tariffs [15]. The complexity of the model chosen affects the type of optimization method that has to be used to formulate the MPC controller, as well as the required computational time. In the case of thermal short-term storage for example, the complexity varies from a single capacity and constant losses [16,17], to models that describe the stratification in the storage tanks [18,19]. One of the drawbacks of Model Predictive Control implementations is also the labour-intensive process necessary to build the model of the controlled system. This is particularly relevant for buildings, as the required control-oriented modelling of their envelope and energy systems is challenging, since each building is a quite unique entity and the model built for one would most likely not fit another one directly. As a consequence, despite its robustness and advantages, MPC is still not widely adopted in the building industry [20]. An alternative approach to model-based control strategies is Reinforcement Learning (RL). This control approach consists of a control agent that learns the optimal control policy by interacting with the controlled environment through a delayed reward mechanism [21], and has recently gained popularity in the scientific literature related to building energy management [22,23]. A specific family of algorithms identified as Deep Reinforcement Learning (DRL), which employ Deep Neural Networks (DNN) as function approximators of the control policy, has been recently developed and applied to solve extremely complex control problems with nearly-human performances [24]. In the context of building energy management, DRL control was applied to supply water temperature control for heating systems in office buildings [25,26], thermal storage charging and discharging [27], control of the operational parameters of a compression chiller [28], control of the temperature setpoint of a thermal storage unit [29], control of indoor temperature or humidity setpoint [30\u201332], regulation of heat fluxes provided to zones in commercial buildings [33], control of domestic water heaters [34]. In [29] the authors investigated the application of Q-learning for managing thermal storage in the cooling season achieving an energy saving between 4 and 10% considering different scenarios. Brandi et al. [25] applied Deep Q-learning to control supply water temperature setpoint for a gas boiler achieving an energy saving between 5% and 12% compared to rule-based control. The authors highlighted the importance of hyperparameters and variables selection in the DRL problem formulation. In [22] the application of DRL for building control was investigated. In the discussion, the authors identified significant findings related to the utilization of prediction of external disturbances, the definition of the control action and the limited adoption in actual buildings. Eventually, this work highlights how the application of DRL in the building sector is still in its infancy and needs to be furtherly investigated. In ideal conditions, a model-free DRL agent should be directly employed in the controlled environment to gradually learn the optimal control policy. However, this process may take a considerable amount of time leading to poor control performance in its first implementation period. A common approach explored in the scientific literature to overcome this problem is to pre-train offline the DRL agent using a surrogate model of the environment. The surrogate model could be a physics-based model [25,26,30] developed through dynamic energy simulation software like EnergyPlus [35], a data-driven model built on monitored data [36] or a simplified first-order model [37]. Besides the necessity of pre-training the control agent, DRL algorithms are characterized by a vast amount of hyperparameters that require careful tuning in order to achieve good performance. As a consequence, despite being successful this approach requires a considerable effort in developing the surrogate model of the controlled environment undermining the complete model-free nature of the DRL approach. 1.1 Research gap and contributions Based on the reasoning presented above the main contribution of this paper is to present a robust comparison between an MPC and a DRL strategy applied to building energy management, benchmarking them against baseline classical rule-based controllers. Moreover, besides the approach typically followed in the literature in which DRL agents are commonly pre-trained offline on surrogate models of the environment, this paper present an online implementation of a DRL controller which maintains the model-free nature of the algorithm. In the current scientific literature, DRL algorithms are commonly compared to rule-based control strategies and only a few studies implemented model-based benchmarks. Raman et al. [38] implemented a DRL control strategy pre-trained offline without developing an online counterpart. A similar approach was adopted by Biagioni et al. [39] for price responsive water heaters. Outside building energy management applications, Ceusters et al. [40] compared MPC and DRL in dynamically simulated multi-energy systems where the complexity of the proposed case studies made it difficult to discern the differences between the two controllers. For this reason, in the present paper, the comparison between MPC and DRL is proposed for a simple case study in order to better analyze the performance of the implemented control strategies. The controllers have to manage the charging and discharging operations of a cold-water storage tank within an HVAC system of an office building while minimizing the cost associated with the operation of an electric chiller. The comparison was performed in a simulation environment that employs EnergyPlus [35] as dynamic simulation software. The paper is organized as follows: Section 2 introduces the proposed case study, Section 3 describes the control methodology proposed in this study, Section 4 reports the implementation details of the different control strategies. Section 5 presents the results obtained. The last two sections include a discussion and the conclusions of the present work. 2 Case study and control problem The case study selected for the performance benchmarking of the different control approaches presented in this paper consists of a cold thermal storage tank that acts as a buffer between the demand of an office building and the generation of cold water via an air-to-water chiller. The system can operate in two modes, i) charging mode, where the cold water can be fed to the building and to the storage tank at the same time and ii) discharging mode, where the demand of the building is met only through the storage. The controller in the charging and discharging phases can modulate the amount of heat transfer to the storage tank by adjusting the fraction of the nominal flow rate to/from the storage as shown in Fig. 1 . In charging mode (left-hand side of Fig. 1), if the building cooling demand (Q d ) is not zero, the chiller provides cooling to the building and the capacity of the chiller limits the amount of energy transferable to the storage. The supply water temperature (T ch ) is considered to be constant. The storage can be operated within a defined temperature range (between T s, min and T s, max ), however, these boundaries are not as considered hard constraints and may be slightly exceeded in particular situations. In discharging mode (right-hand side of Fig. 1) the chiller is by-passed and the building is cooled only via the cold thermal storage, with a constant design water mass flow rate m D . The control problem aims at optimizing the total electricity cost (R) of the energy used by the chiller by managing i) the scheduling of the two operating modes and ii) the charging/discharging power at each time-step. For simplicity, the building's thermostatic control is not considered and its cooling demand is considered as an external disturbance along with the price of the electricity and the temperature of the zone in which the storage is located. 3 Methodology This section introduces the methodological framework of this study. As shown in Fig. 2 the case study introduced in Section 2 was used as a testbed to benchmark the performance of three different control strategies, i) an MPC controller, ii) a DRL with offline training and iii) a DRL with online training, against two classical Rule-Based Control (RBC) controllers. 3.1 MPC As the system is allowed to either discharge the storage to provide the cooling required by the building, or to supply cooling via the chiller to the building and/or to the storage, this results in two distinct operating modes leading to a Mixed Integer Linear Programming (MILP) problem to be solved. 3.1.1 Model Defining a Boolean variable δ that is true when the system is in discharge mode, the storage dynamical behaviour is described in Eq. (1): (1) Δ T s k Δ t s = Q d k − UA T s k − T a k C s ifδ = 1 Δ T s k Δ t s = − Q ch k − UA T s k − T a k C s ifδ = 0 where Q d and T a , the building demand and the ambient temperature where the storage is located are measured disturbances; T s , the storage temperature, is the system state; Q ch , the charging power to the storage, and δ, the selection of the operating mode, are the controlled inputs; UA and C s are the storage UA value and capacitance respectively. 3.1.2 Constraints At each time step the controlled input Q ch is limited by the possibility of the chiller to charge the storage. The chiller capacity, Q cap , constrains the maximum thermal energy delivered to the building and the storage, as in Eq. (2). (2) Q ch k + Q d k ≤ Q cap The storage temperature, since the supply temperature and maximum flow rate to the storage are fixed, limits the maximum heat transfer rate Q ch, max , as in Eq. (3). (3) Q ch k ≤ Q ch , max k The maximum heat transfer rate Q ch, max is calculated as in Eq. (4). (4) Q ch , max k = m ̇ c p T s k − T ch The storage operation should be maintained within a reasonable temperature range. This constraint, also to help to ensure the feasibility of the optimization when the controller is deployed, was formulated as a soft constraint, as in Eq. (5). (5) T s , min k − ε ≤ T s k ≤ T s , max k + ε where T s, max and T s, min are the upper and lower temperature boundaries for the operation of the storage, and ε is the slacking variable. This latter variable is also subject to a constraint that limits the allowable temperature excess as in Eq. (6). (6) ε ≤ T s k ≤ ε max 3.1.3 Cost function In this economic MPC formulation, the total cost to be minimized is the actual energy cost over the prediction horizon N, as in Eq. (7): (7) J = ∑ k = 1 N R k Q ch k + Q d k COP where R is the cost of the electrical energy per kWh, and the Coefficient Of Performance (COP) of the chiller is considered to be constant and equal to the equipment nominal value. The problem is solved at each time step k, with a control time step Δt s equal to 1 h and the length of the horizon N equal to 48 h. The control actions of the MPC are the value of δ, which determines the operating mode, and the charging power Q ch , from which the mass flow rate is determined (by dividing Q ch by the known temperature difference between storage and charging flow, water density and specific heat capacity). 3.2 Deep reinforcement learning In this work, a Soft-Actor-Critic (SAC), an off-policy RL algorithm, was implemented [41]. SAC methods are useful as they are capable of handling continuous action spaces. The Actor-Critic architecture employs two function approximators. The Actor has the aim to determine the optimal action for a given specific state of the controlled environment (policy-based), while the Critic evaluates the decisions made by the actor (value-based). This framework is generally coupled with an off-policy implementation, enabling the re-utilization of the previous experience collected by the agent in order to improve the control policy. The Soft-Actor-Critic algorithm originally implemented in the library Stable-Baselines [42] was employed in this work. In the following sub-sections, the design of the action-space and of the reward function are presented along with the different training strategies employed. 3.2.1 Design of the action-space At each time step, the action has to control the scheduling of the charging and discharging modes, as well as the fraction of design water mass flow rate (m D ) that should circulate through the storage. These aspects were encoded through an action space defined in the interval between −1 and 1 from which the DRL agent can select control actions. Following this approach, if the agent selects an action strictly less than 0 the system operates in discharging mode. Conversely, if the agent selects an action greater or equal than 0 the system operates in charging/chiller cooling mode and the water mass flow rate circulated to the storage is set proportional to the modulo control action value. 3.2.2 Safety constraints A safety constraint was introduced in order to guarantee that the cooling demand of the building is always met and to maintain the temperature of the storage within the prescribed range. In particular, the constraint was introduced in the discharging mode (i.e. control action < 0). If the temperature of the storage tank rises above a certain value and the building cooling demand is not zero the system automatically switches to charging/chiller cooling mode in order to meet the demand regardless of the negative control action selected by the agent. This value was set equal to the upper-temperature boundary (T s, max ) plus a defined tolerance value τ. The violation of the upper-temperature boundary is penalized by associating a cost to τ, as explained in the following subsection. 3.2.3 Reward function The reward function obtained by the agent after selecting an action at each time step measures its control performance. Since the building's thermostatic control was not considered in this application, the objective of the controller is to minimize the cost of the electricity consumed by the chiller unit. The reward depends from this value as described in Eq. (8): (8) r t = − β ∗ R k ∗ Q ch , elec k − P k where R(k) is the electricity cost at each time step k and Q ch, elec (k) is the chiller electricity demand in the time interval between k − 1 and k. β is a weight factor introduced to regulate the magnitude of the reward. Moreover, the term P(k) is a cost term introduced to penalize the agent of a quantity P if the temperature of the storage rises over the upper-temperature boundary (T s, max ) as introduced in the previous paragraph. In any other case (i.e. the temperature of the storage lower than upper-temperature boundary) the cost term P(k) is equal to zero. 3.2.4 Design of the state-space The state or observation space includes all the variables which describe the environment at each time step as it is seen by the DRL agent. In this study, the state-space does not include only information about the current time step but also information about the recent past and the future disturbances. Historical values were added to the state-space in order to account for the effect of thermal dynamics which characterize the present control problem as suggested in [22]. All the variables included in the state-space are physical quantities directly extracted from the simulation output with the exception of the State of Charge (SOC) of the storage tank that was calculated according to Eq. (9): (9) SOC k = 1 − T s k − T s , min T s , max + τ − T s , min More detailed information on the variables included within the state-space is provided in Section 4. 3.2.5 DRL with offline training According to offline strategy, a DRL control agent was first trained using a calibrated model of the energy system over a training period and successively statically deployed on the same model over the deployment period. The training period, also identified as training episodes, was repeated multiple times, allowing the agent to explore different control policies in order to identify the optimal control strategy. Once the training phase was completed, the agent was statically deployed meaning that the parameters of the control policy were not updated during the process. The advantages of such an approach are the limited computational cost and the relative stability provided by a static control policy. The disadvantage is that the agent is unable to automatically adapt in the case key features of the controlled system change (e.g. revamping intervention) and may need to be retrained. 3.2.6 DRL with online training According to the online strategy, a DRL control agent was directly deployed on the calibrated model of the energy system. The control agent has no prior knowledge of the dynamics of the controlled environment. Thus, it was forced to learn the parameters of the optimal control policy while actively controlling the system. This strategy completely emulates a model-free controller in the sense that no previously built model was employed to pre-train the control agent. A particular configuration of two hyperparameters, learning rate and number of gradient steps, was adopted to directly deploy the DRL agent on the controlled system. The learning rate of the optimizer implemented in DNN is an hyperparameter that controls the degree of change of the network in response to the estimated error each time the weights are updated. The number of gradient steps is an hyperparameter that regulates the number of batches randomly drawn from memory buffer on which gradient update is performed at each control time step. In the offline approach was implemented a constant value of learning rate and number of gradient steps since the agent has at disposal a large amount of experience in the replay buffer, generated through multiple episodes. In the online approach the values of learning rate and the number of gradient steps vary over time according to two step functions. In particular, high values of the learning rate and number gradient steps were employed during the first period to encourage faster learning of neural networks weights. This approach is motivated by the fact that at the beginning of the deployment period the online agent has no prior knowledge of the problem and limited experience is available in the memory buffer. The learning rate and the number of gradient steps were then gradually reduced as long as learning progress to limit the risk of converging to near-optimal control policies. The step functions adopted for both the learning rate and number of the gradient steps during the simulation of online trained DRL agent are reported in Section 4.5. 3.3 Modelled predictions of forcing variables Since in this application both MPC and DRL implements predictions of the forcing variables to identify the optimal control policy, the performance of the three strategies was evaluated employing both perfect and modelled predictions of external disturbances. The implementation of perfect predictions represents an ideal test scenario in which the performance of the controllers is benchmarked knowing exactly the evolution of the disturbances. The implementation of modelled predictions represents a test scenario closer to reality in which the evolution of the disturbances cannot be exactly known but can be estimated using data-driven methods. Through this approach was possible to evaluate the effect of the accuracy of the predictions on the different control strategies. The modelled prediction was obtained by developing an Long-Short Term Memory (LSTM) Network model for each disturbance. In particular, the disturbances predicted through this approach were the building cooling demand (Q d ) and the air temperature of the space in which the storage is located (T a ). The prediction of the price of electricity was always supposed to be perfectly known. The predictions models were developed on an hourly basis with a prediction horizon of 48 h. The inputs sequences to the two LSTM models are formed by the following common variables: day of the week, hour of the day, outdoor air temperature and outdoor solar radiation. Along with these variables, the sequences were completed with building cooling demand or air temperature of the space in which the storage is located depending on which was the target output. The sequences were provided to the LSTM models up to 48 h in the past. 4 Implementation The case study described in Fig. 1 consists of an office module that is currently under construction at Politecnico di Torino, Italy. The module has an overall surface of 95 m 2 and consists of two 10-persons office rooms, one control room and a 3-persons technical room. The technical room is not served by the air-conditioning system and the storage tank is placed within it. The average transmittance value of the opaque and transparent envelope components are 0.15 and 0.6 W/m 2 K respectively. The reference capacity of the chiller (Q cap ) is 12 kW and the reference COP is 2.67. The chiller can provide cooling energy to the building or to cold water storage which has a volume of 10 m 3. The storage was sized considering 1.5-times the maximum daily cooling demand of the building. The design water mass flow rate during charging phase is 0.2 kg/s while during discharging phase is 0.35 kg/s. This latter value corresponds to the sum of the design mass flow rates of the three air-conditioned zones. The supply water temperature at the outlet of the chiller was set equal to 7 °C. The operating range of temperature of the storage tank ranged between 10 °C(T s, min ) and 17 °C(T s, max ). The cooling demand was considered as an external disturbance of the system and was calculated within EnergyPlus in order to maintain an indoor temperature of 26 °C and a relative humidity of 55 % between 08:30 and 18:00 from Monday to Friday. In this time interval, the zones were supposed to be occupied at their maximum capacity. The price of the electric energy drawn from the grid to operate the chiller unit is a further external disturbance of the analyzed system. A summary of the electricity prices used in this work is presented in Fig. 3 , which is based on the tariff structure commonly implemented in Italy. Three different tariff levels were considered: i) a \u201cHigh Price\u201d level, with an electricity rate of 0.3 \u20ac/kWh, ii) a \u201cMedium Price\u201d level, with a rate of 0.165 \u20ac/kWh and iii) a \u201cLow Price\u201d level, with a rate of 0.03 \u20ac/kWh. The price of the electricity was assumed to be relatively different from each other, to discriminate the values for the optimization application. Specifically, the low and medium price values were chosen to be respectively 1/10 and 1/2 of the higher one. The system was simulated using EnergyPlus. DRL control agents were designed and implemented in Python, the MPC controller in Matlab [43]. The interaction between EnergyPlus and the controllers in Python [25] and Matlab was achieved using the Building Control Virtual Test Bed (BCVTB) as a middleware. The weather file used in this work is the reference weather file (ITA_TORINO-CASELLE_IGDG.epw) available in EnergyPlus for Torino, Italy. The cooling season was defined to last between June and August. The control time step was set equal to 1 h whereas the simulation time step was set equal to 5 min in order to improve the precision of the simulation. As a result, a control action is defined every 12 simulation steps for which the same control action is repeated. 4.1 Implementation of rule-based control strategies Two different RBC controllers were implemented in this work. The first one, named RBC 1, was designed in order to charge the storage during mornings (i.e. between 0 a.m. and 7 a.m) of working days when the price of electricity is low and if the temperature of the storage (T s ) is greater than 12 °C. In this mode, the controller charges the storage at the maximum flow rate. The storage is discharged during peak-cost hours until the storage temperature reaches 17 °C or until the building cooling demand (Q d ) is null. The second one, identified as RBC 2, charges the storage whenever the price of electricity is low (i.e. between 11 p.m. and 7 a.m. during Mondays and Saturdays and between 0 a.m. and 24 p.m. during Sundays) and the temperature of the storage is greater than 12 °C. In this phase, the storage is charged until its temperature reaches the lower limit of the temperature range or the price of electricity rises. The storage is switched to discharging mode whenever the building demand is not zero until this value return to zero or the temperature of the storage is greater than 17 °C. RBC 1 was initially conceived as the only benchmark solution. However, as showed in the next sections, despite it can be considered a reasonable control law, its performance resulted extremely poor leading to the design of RBC 2. Nevertheless, RBC 1 was still considered as a baseline together with RBC 2, to show how an expert-based design of the control strategy may require different trials before converging to the optimal setup. 4.2 Implementation of model predictive control strategy The MPC strategy was implemented using Matlab R2019b, the Multi Parametric Toolbox and Hysdel for the problem formulation [44], and Gurobi [45] as a solver for the MILP problem. As Hysdel was used to describe the Mixed Logical Dynamical (MLD) system, the future measured disturbances were taken into account by augmenting the prediction model with an additional linear model and treating the vector of the future references and measured disturbances as additional states [46]. For this reason in the implementation phase, the three tariffs described in the previous section were converted into discrete variables to enable switching to linear systems with a fixed energy cost equal to the rate of that time window. The parameters used for the implementation of the MPC strategy, including the system design variables (the storage UA and C s , and the chiller COP) and MPC formulation (the control time step Δt s , the horizon length N, and weight and maximum value of the slacking variable ε) are reported in Table 1 . 4.3 Implementation of deep reinforcement learning control strategies As introduced in Section 3 the reinforcement learning control problem is defined by action-space, by the reward function and the state-space that are summarized in Fig. 4 . The figure shows the variables included in the state-space highlighting the disturbance for which a prediction was provided (i.e. electricity price and building cooling demand). Moreover, Fig. 4 summarizes the control action (i.e. charging/discharging mode and water mass flow rate fraction circulated from/to the storage) and the variables employed to evaluate the reward. These features are the same for both DRL trained through offline approach and DRL trained through online approach. Table 2 shows the hyperparameters of the reward function introduced in Section 3.2.3. These values were found to be the most effective after conducting a sensitivity analysis as implemented in [25]. Table 3 furtherly describes the variables included in the state-space along with their maximum and minimum values that were employed to re-scale the state space through a min-max normalization before providing the variables as inputs to the DNNs. The storage tank State Of Charge (SOC) at the timestep k was introduced to provide to the agent information about the amount of energy actually stored. Moreover, past values of this variable were introduced to provide information about the evolution of the temperature caused by the charging/discharging of the system up to 4 h before the actual control time step. These values were included to provide to the agent information about the inertia of the system evaluated at the time step k. The electricity price is a key-information for the agent in order to correctly plan the operations of the system. Actual value is provided along with the exact values for the 24 h ahead. As introduced in Section 3.3, the electricity price patterns were supposed to be always known. The building cooling demand together with the price of electricity is a fundamental information to optimally manage the controlled system. Also the values of building cooling demand from time step k to time step k + 24 were provided to the agent. The predictions of building cooling demand were assumed to be perfectly known or estimated by means of a neural network model. Eventually, information about the air temperature of the space in which the storage is located along with information about outdoor air temperature were included. The first variable provides knowledge about the heat losses from the storage while the latter affects the COP of the chiller unit. Besides the formulation of the reward function and of the state-space, the reinforcement learning frameworks require of a series of hyperparameters to be set as the discount factor for future rewards (γ) and the structure of the neural networks employed as function approximators. The values of the hyperparameters selected for this application that are the same for the two control strategies based on DRL are summarized in Table 4 . In the next following subsections the different implementations of DRL agent trained in offline and online modes are described. 4.4 Implementation details of the DRL agent with offline training In this case the DRL control agent was pre-trained offline using the model of the system as introduced in Section 3.2.5. The pre-training unfolds by repeating the same training episode in order to let the agent converge to the optimal control policy. The selected training episode includes the month of June of the weather file implemented in this work. The training episode was repeated 15 times before obtaining an acceptable solution. The trained agent was successively deployed statically on the system for the whole cooling season (June\u2013August) in order to assess its performance. The deployment process was performed considering also the training period to assess the stability of the learned control policy. The optimizer learning rate was set equal to 0.001 while the batch size equal to 256. The agent was trained using only the perfect predictions of the external disturbances while the deployment was performed considering both perfect and modelled predictions. 4.5 Implementation details of the DRL agent with online training This DRL agent was directly implemented on the simulated case study as if it was the real system. The behaviour of two hyperparameters, the learning rate of the DNN optimizer and the number of gradient steps, was defined according to the functions depicted in Fig. 5 . The batch size was set equal to 32 differently from the DRL agent trained offline due to the lower amount of data available to the agent to train the control policy in the online training fashion. Employing smaller batch size can provide a faster convergence to near-optimal solution [47] which is an extremely desirable feature for a DRL agent directly deployed on the controlled system. The figure shows how the two hyperparameters, learning rate figure (a) and number of gradient steps (b), were reduced during the simulation. This approach was found to be effective in speeding up the training process in the first weeks of deployment given the limited amount of data available to learn from. 4.6 Implementation of modelled predictions of disturbances As introduced in Section 3 modelled predictions of building cooling demand and air temperature of the zone in which the storage is located were obtained by means of two LSTM models. These models were characterized by two hidden layers with 48 neurons, one LSTM layer and one dense layer with relu activation function. The predictor attributes employed are described in Section 3.3. The models were trained for 300 epochs with a batch size equal to 64. The training dataset was generated with the same simulation model considering a different weather file obtained from real world measures collected for Torino (Italy) during the year 2019. The trained LSTM models were used to predict the values of the building cooling demand and air temperature of the zone in which the storage is located considering the same weather file employed to analyze the different control strategies (i.e. reference weather file ITA_TORINO-CASELLE_IGDG.epw). The performance of the two LSTM models were evaluated in terms of Root Mean Squared Error (RMSE). The models developed to predict building cooling load and the air temperature of the zone achieved in testing conditions an RMSE of 229.6 W and 0.84 °C respectively. 5 Results As introduced in Section 3 the main objective of this work is to compare the performance of model predictive and deep reinforcement learning control strategies. Considering that both control techniques benefit from predictions of future disturbances values for the evaluation of the optimal control sequence, a comparison of their performance considering both perfect and modelled predictions was undertaken. The two rule-based control strategies described in Section 5.1 were used as a benchmark. The different control strategies were simulated during the cooling season in the period ranging from June to August. Table 5 reports the total cost and consumption of electricity obtained by implementing the different control strategies. The results in Table 5 show that the MPC achieves the best performance in terms of total cost in both the implementations with perfect and modelled predictions, followed by the DRL strategy with offline pre-training. It can be noticed that RBC 1 obtained the worst performance among the employed control strategies, which was the main reason for the development of the RBC 2 controller. This latter RBC strategy, better suited for the system, performs only 4.7% worse in terms of operating cost compared to the MPC. However, it led to the highest amount of electrical energy over the simulation period, 4.6% higher than the best performing MPC. It is also interesting to notice that the DRL trained offline and the MPC obtained very similar results, showing that both solutions are most likely near-optimal. The MPC and the DRL with offline training were only mildly affected by inaccurate disturbances predictions, whether the DRL controller trained online was more affected, with a reduction in performance compared to the case with perfect predictions of 7.2%. The negative effect of modelled predictions was mitigated in the case of DRL with off-line training since this agent utilised more experience to converge to an optimal solution, relying less on the goodness of the prediction. As expected, the DRL controller trained online did not perform well if the entire 3-months period is considered, since the agent was deployed without prior knowledge of the controlled system. However, it can be noticed how the total cost achieved through this controller gradually decreased over time. This can be seen from the monthly results in Table 5, and in Fig. 6 , which reports the cumulative cost (starting from July) of each strategy. Table 5 shows that the performance difference between the MPC approach and the online DRL in terms of total cost decreases from 160% in the first month, to 21% in the second, to only 3.6% during the last month. This is also clear in Fig. 6, which shows that the cumulative cost of trained online DRL (green line) becomes parallel to the curves of the best performing solutions, indicating that these control solutions became more and more similar over time. The higher costs of the DRL with online training were expected as, differently from the one trained offline, needed to explore behaviors in the inital period that, without having a-priori knowledge of the system, could be detrimental. In addition, the agent trained online might not react to sudden changes (e.g. the increase in the cooling demand from June to July). The importance of flexibility of energy sources can be furtherly highlighted by reporting the costs related to an identical system without the introduction of an active thermal storage which are 86.7 \u20ac, 101.2 \u20ac and 90.4 \u20ac during June, July and August respectively. Table 6 reports the use of the storage tank in terms of thermal energy charged and discharged, as well as the fraction of cooling demand satisfied, achieved by the different control strategies over the three simulated months. The results show that the MPC, DRL trained offline and RBC 2 strategies used the storage to satisfy always the full building cooling demand. However, RBC 2 employed an higher amount of energy to charge the system resulting in an increased storage heat losses to the ambient. It can also be seen that RBC 1 never fully met the building cooling demand through the storage, resulting in a higher cost. RBC 1 was never capable to charge the storage enough to cover the demand during off-peak time slots reaching, during discharging phase, the upper limit of storage temperature range before the demand was completely satisfied and incurring in a higher energy cost. As far as the DRL trained online is concerned, a gradual improvement of the control policy can be seen, with an increase of the percentage of cooling demand satisfied through the storage during the second and third months. Fig. 7 shows the temperature profile of the storage tank achieved by the different control strategies. For simplicity, only the MPC and DRL results with perfect predictions were reported. In this figure it can be observed that RBC 1 charged the storage tank only during morning of working days, which is insufficient to cover the building demand. As a results the storage temperature gradually increases during the first weeks to later remain stable between 13 °C and 17 °C. RBC 2 control strategy charged the storage whenever the price of electricity was low and whenever the temperature of the storage was higher than 12 °C. Consequently, during weekends the storage was cooled to the lower temperature limit (i.e. 10 °C), resulting in a better cost performances compared to RBC 1, but in a higher energy losses to the ambient. The MPC controller, which achieved the best performance, tried to maintain the temperature of the storage as close as possible to the upper limit, to provide sufficient cooling energy to satisfy the daily demand while minimizing the thermal losses. During some Fridays the MPC controller violated the soft constraint on the upper limit of storage temperature range of approximately 0.5 °C, as enabled by the constraint formulation in Eq. (5). This could be due to a compensation of the mismatch between the model and the system closing the loop, or the willingness of the controller to incur the cost of violating the upper temperature constraint to access charging energy at a lower price a few hours later. The offline DRL controller achieved a very similar performance as the MPC, but with a slightly different sequence of inputs, as it charged the storage more during weekends. This additional energy was gradually released during the week allowing the controller to violate the upper constraint of storage tank temperature for shorter periods of times compared to the implemented MPC approach. The online DRL controller clearly struggles during the first weeks to manage the storage, charging it only intermittently resulting in a higher cost due to a more extensive use of the chiller during high-price periods. However, it can be observed how the control policy gradually improves to finally converge to a control pattern very similar to the behaviour described for the DRL controller pre-trained offline. Figs. 8, 9 and 10 provide more details on the behaviour of the MPC and DRL control strategies during a week included between 21/08 and 2/08. The patterns observed during this week were not necessarily observed in all other weeks, however it were deemed sufficiently representative to be presented and discussed. The top plot in these figures shows the amount of heat transfer to the storage tank, where a negative value means charging and a positive one discharging of the storage. The background color shows the electricity price, where a green background corresponds to a low price time period, a yellow one to a medium price and a red one to a high price period. The second subplot shows the temperature profile of the storage tank together with the upper temperature limit, marked with a red line at 17 °C. Fig. 8 is related to the operation of the MPC strategy. As expected, the controller charged the storage only during low-price time slots in order to match the cooling demand of the building. At the same time, the controller managed to maintain the temperature of the tank as close as possible to the upper temperature limit. During the last day of the week, (Friday 27/07 in this figure), the soft constraint on this limit was relaxed by the controller to satisfy immediately the cooling demand and return below the temperature boundary after a few hours, when the energy price lowered again. Fig. 9 shows the behaviour of the DRL controller pre-trained offline. Differently from the MPC, it is interesting to observe that this controller decided to pre-charge more the tank over the weekend in preparation for the coming week. In this region the control policy was relatively noisy, alternating between charging and free-floating. However, the storage tank was cooled at a lower temperature at the beginning of the week respect to the MPC strategy, and the upper temperature constraint was never violated during this week. Fig. 10 shows the control policy of the DRL controller trained online. This controller shows similar behaviour to DRL agent pre-trained offline after six weeks of training. During Sunday-Monday morning the control policy was uncertain resulting in a more pronounced alternation between charging and free-floating. It is interesting to observe from the bottom plot how this controller gradually released the cooling energy stored in the tank during the week. When comparing the results of the three control approaches it can be seen that, while they all charge during off-peak time slots between the working days, during the weekend they follow different policies. The MPC controller waits as long as possible before charging the storage on Monday morning, as it can be seen in Fig. 8. As Figs. 9 and 10 show, the two DRL controllers, charge the storage more and over a longer period of time, showing a more intermittent behaviour. This pattern suggests that the control policies learned by the DRL agent are still uncertain and might be slightly less efficient. However, the DRL control policies were able to capture requirements of the system beyond the 24 h prediction horizon. More specifically, DRL agents were capable to extract a recurrent pattern (i.e. the alternation between weekend and weekdays) which spawns for a longer period compared to the horizon of the predictions included in the state space. 6 Discussion The results from the comparison of a MPC, a DRL and a RBC applied to a case study featuring a cold water storage tank and an air-to-water chiller showed interesting similarities and differences between these approaches. The different control strategies, to minimize the operational cost, had to correctly manage the storage to shift the demand to the lower energy cost time slots, ensure that the storage was sufficiently charged to satisfy the demand each day of the week, and minimize the thermal losses to the ambient. The MPC approach achieved the best performance, managing the system in order to charge the storage tank with enough cooling energy during off-peak price periods in proximity of on-peak time slots. Thanks to this approach the controller was able to minimize thermal losses to the ambient and guarantee the building cooling demand satisfaction with the storage. In particular circumstances (e.g. at the end of each week) the MPC controller decided to soften the upper temperature boundary constraint, possibly due to the limited length of the prediction horizon (48 h) that did not allow the controller to use the weekend to further cool in advance the storage. The DRL agent pre-trained offline achieved a similar performance to the MPC approach, with a similar control pattern during the weekdays, but during weekends and early Monday mornings the DRL controller attempted to provide more cooling to the storage, in order to gradually release it during the week. This allowed the controller to violate less the upper limit on storage tank temperature. This difference can be explained by the fact that the DRL controller based its decisions on patterns such as alternation between weekend and weekdays that went beyond the 24 h prediction horizon that it had available in the state-space. A similar behaviour was observed for some weeks also for the DRL agent trained online. This pattern represents an advantage with respect to MPC provided by the off-policy evaluation method employed by DRL algorithms. This method leverages previous experience to learn an effective mapping between states and actions capable to identify complex patterns thanks to DNN capabilities. However, DRL controllers charged the storage intermittently and over a longer period of time, alternating it with no-charging periods. As a result, the heat losses to the ambient were greater compared to the MPC case, possibly showing intrinsic instabilities affecting a DRL control approach. RBC 2 achieved a similar performance in terms of cost of electricity compared to the MPC ad DRL trained offline approaches. However, it used a significantly higher amount of energy due to heat losses since it was designed to charge the storage to its minimum temperature whenever the price of the electricity was low. On the other hand, RBC 1 failed at providing the storage with enough cooling energy and was forced to use the chiller to satisfy the remainder of the building cooling demand at higher electricity costs as expected. The DRL agent trained online demonstrated to be capable to rapidly adapt to the controlled environment reaching, after one month, comparable results to the best performing solutions and outperforming the RBC 1 controller. On the other hand, during the first month, the poor performance of this agent led to high operating costs. In this application both MPC and DRL made use of predictions of external disturbances (electricity price, the temperature of the ambient of the storage and cooling demand) to derive the optimal control policy. They were considered as either perfectly predicted or modelled through a deep neural network to make a future 48 h forecast, to replicate an implementation in real-world conditions. The inputs to the disturbances models were outdoor air temperature and solar radiation for the unconditioned space, with the addition of the occupancy schedule for the cooling demand. Alongside the predictions of the external disturbances, the MPC required a model of the controlled system and an optimizer. The definition of the model is usually addressed as one of the most time-consuming tasks in MPC development. In this work, given the relatively simple nature of the problem considered, a straightforward model of the controlled system was derived. In this work MPC demonstrated a good performance, with an implementable solution that worked well with both perfect and modelled predictions of the external disturbances. A DRL agent pre-trained offline, despite not using a model directly in its formulation, still requires the development of a model of the controlled environment for training, as it needs to see the same simulation period several times to learn an efficient control policy comparable to the one obtained by an MPC approach. The DRL agent directly deployed on the controlled environment and trained online, despite an initial lower performance, was capable to converge to an acceptable though not optimal solution, demonstrating to be capable of improving the performance of the system controlled by a rule-based system without using a model of the system or supervision. The RBC controllers developed showed the limitations of classical approaches. The first RBC controller, RBC 1, despite having a reasonable control law, resulted in an under-performing solution, which lead to the design of RBC 2 controller for benchmarking. This solution obtained satisfying performance in terms of electricity cost but led to the greatest amount of energy losses to the ambient. The results obtained in this study highlight that, despite the simplicity of the control problem, an expert-based design of the control strategy may require time to identify the optimal control setup that is only applicable to a specific system. In this sense, rule-based controllers are not capable to adapt to the evolution of the controlled environment over time. For example modification to the patterns of building cooling demand or price of electricity may lead to a poor performance that advanced control strategies would not suffer, as they are more effective in adapting to known and changing boundary conditions. 7 Conclusion and future work The aim of this paper was to evaluate different control strategies for thermal energy management in buildings. Three cutting-edge solutions, model predictive control and deep reinforcement learning with offline and online training were tested and analyzed on a simple case study system and benchmarked against a classical rule-based control approach. The objective of the controllers was to satisfy the cooling demand of a small office building while minimizing the cost of electricity drawn from the grid to operate the chiller, making the best use of a thermal energy storage tank. The controllers could manage the amount of energy charged and discharged to/from a cold water storage by adjusting the water mass flow rate circulated to it. MPC is a model-based solution that employs a simplified model of the controlled system to perform an optimization process over a receding horizon, using predictions of external disturbances. Similarly, DRL employs predictions of external disturbances to learn a near-optimal control policy. However, despite the model-free nature of the control algorithm, as this control approach requires a certain amount of time to converge to an acceptable solution, a common approach consists in pre-training the DRL agent offline with a simulated model of the controlled system, losing the intrinsic model-free nature of the algorithm. Conversely, a DRL controller directly deployed in the controlled environment learning the control policy online may achieve a sub-optimal performance in the first period of deployment, as shown in this study, but can converge to a near-optimal strategy in an acceptable amount of time (in the order of a few weeks as shown in the results). This approach, differently from the DRL with offline training, is model-free in the entire deployment process. These considerations open several research questions on the development of DRL algorithms. If DRL control strategies are implemented with offline training, they require a model of the system, removing this theoretical advantage in comparison to an MPC approach. DRL has the advantage of not relying on a numerical optimization process which generally requires linearized models and and a convex problem to be formalized. This also leads to lower computational times compared to an MPC approach. On the other hand, MPC demonstrated to be a more robust and stable control approach. The flexibility shown by DRL agents is associated with the possibility of temporary poor control performance. This is particularly evident when employing a DRL agent trained online, but this represents nevertheless a promising truly model-free approach. The DRL agent trained online presented in this study proved to be able to improve its control performance over time, approaching the behaviour of a near-optimal MPC strategy or the similar one of a DRL pre-trained offline. However, the possibility to really deploy such a controller in a plug-and-play fashion is still to be assessed, as the hyperparameters and reward function, which play a key role in determining the performance of this category of controllers, can require different settings depending on the system on which they are implemented. Future work is therefore expected to cover the following aspects: \u2022 Continuing the development of online-trained DRL approaches, by identifying optimal hyperparamenters and reward function configurations that guarantee a fast convergence to a stable control policy, and by including domain expertise to guide the initial exploration phase. \u2022 Exploring the implications of implementing such advanced control strategies on more complex case studies, benchmarking and critically discussing the performance of different control approaches. \u2022 Analyzing the capability of DRL and MPC control approaches to adapt to changing environments without the need of external support from a technician. \u2022 Implementing a similar benchmarking approach of these control approaches on a experimental setup, providing a more realistic evaluation required for an industrial implementation. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgement The work of Silvio Brandi was made in the context of a Ph.D. scholarship at Politecnico di Torino funded by Enerbrain s.r.l. References [1] G. Martinopoulos K.T. Papakostas A.M. Papadopoulos A comparative review of heating systems in EU countries, based on efficiency and fuel cost Renew. Sust. Energ. Rev. 90 2018 687 699 10.1016/j.rser.2018.03.060 G. Martinopoulos, K. T. Papakostas, A. M. Papadopoulos, A comparative review of heating systems in eu countries, based on efficiency and fuel cost, Renewable and Sustainable Energy Reviews 90 (2018) pp. 687\u2013699. doi:10.1016/j.rser.2018.03.060. [2] A. Kathirgamanathan M. De Rosa E. Mangina D.P. Finn Data-driven predictive control for unlocking building energy flexibility: a review Renew. Sust. Energ. Rev. 135 2021 110120 10.1016/j.rser.2020.110120 A. Kathirgamanathan, M. De Rosa, E. Mangina, D. P. Finn, Data-driven predictive control for unlocking building energy flexibility: A review, Renewable and Sustainable Energy Reviews 135 (2021) 110120. doi:10.1016/j.rser.2020.110120. [3] R. May The Reinforcement Learning Method: A Feasible and Sustainable Control Strategy for Efficient Occupant-Centred Building Operation in Smart Cities 2019 Dalarna University URL https://www.diva-portal.org/smash/get/diva2:1358130/FULLTEXT02 (accessed October 14, 2021) R. May, The reinforcement learning method: A feasible and sustainable control strategy for efficient occupant-centred building operation in smart cities, Dalarna University (2019). URL: https://www.diva-portal.org/smash/get/diva2:1358130/FULLTEXT02, (accessed October 14, 2021). [4] T.I. Salsbury A survey of control technologies in the building automation industry IFAC Proc. Vol. 38 2005 90 100 10.3182/20050703-6-CZ-1902.01397 T. I. Salsbury, A SURVEY OF CONTROL TECHNOLOGIES IN THE BUILDING AUTOMATION INDUSTRY, IFAC Proceedings Volumes 38 (2005) pp. 90\u2013100. doi:10.3182/20050703-6-CZ-1902.01397. [5] M. Molina-Solana M. Ros M.D. Ruiz J. Gãmez-Romero M. Martin-Bautista ATA science for building energy management: a review Renew. Sust. Energ. Rev. 70 2017 598 609 10.1016/j.rser.2016.11.132 M. Molina-Solana, M. Ros, M. D. Ruiz, J. GÃ3mez-Romero, M. Martin-Bautista, ata science for building energy management: A review, Renewable and Sustainable Energy Reviews 70 (2017) pp. 598\u2013609. doi:10.1016/j.rser.2016.11.132. [6] A. Capozzoli M.S. Piscitelli S. Brandi D. Grassi G. Chicco Automated load pattern learning and anomaly detection for enhancing energy management in smart buildings Energy 157 2018 336 352 10.1016/j.energy.2018.05.127 A. Capozzoli, M. S. Piscitelli, S. Brandi, D. Grassi, G. Chicco, Automated load pattern learning and anomaly detection for enhancing energy management in smart buildings, Energy 157 (2018) pp. 336\u2013352. doi:10.1016/j.energy.2018.05.127. [7] C. Miller Z. Nagy A. Schlueter Automated daily pattern filtering of measured building performance data Autom. Constr. 49 2015 1 17 10.1016/j.autcon.2014.09.004 C. Miller, Z. Nagy, A. Schlueter, Automated daily pattern filtering of measured building performance data, Automation in Construction 49 (2015) pp. 1\u201317. doi:10.1016/j.autcon.2014.09.004. [8] G. Serale M. Fiorentini A. Capozzoli D. Bernardini A. Bemporad Model predictive control (MPC) for enhancing building and HVAC system energy efficiency: problem formulation Appl. Opport. Energ. 11 2018 631 10.3390/en11030631 G. Serale, M. Fiorentini, A. Capozzoli, D. Bernardini, A. Bemporad, Model Predictive Control (MPC) for Enhancing Building and HVAC System Energy Efficiency: Problem Formulation, Applications and Opportunities, Energies 11 (2018) 631. doi:10.3390/en11030631. [9] J. Ma J. Qin T. Salsbury P. Xu Demand reduction in building energy systems based on economic model predictive control Chem. Eng. Sci. 67 2012 92 100 10.1016/j.ces.2011.07.052 J. Ma, J. Qin, T. Salsbury, P. Xu, Demand reduction in building energy systems based on economic model predictive control, Chemical Engineering Science 67 (2012) pp. 92\u2013100. doi:10.1016/j.ces.2011.07.052. [10] J. Chen G. Augenbroe X. Song Lighted-weighted model predictive control for hybrid ventilation operation based on clusters of neural network models Autom. Constr. 89 2018 250 265 10.1016/j.autcon.2018.02.014 J. Chen, G. Augenbroe, X. Song, Lighted-weighted model predictive control for hybrid ventilation operation based on clusters of neural network models, Automation in Construction 89 (2018) pp. 250\u2013265. doi:10.1016/j.autcon.2018.02.014. [11] G.P. Henze R.H. Dodier M. Krarti Development of a predictive optimal controller for thermal energy storage systems HVAC&R Res. 3 1997 233 264 10.1080/10789669.1997.10391376 G. P. Henze, R. H. Dodier, M. Krarti, Development of a Predictive Optimal Controller for Thermal Energy Storage Systems, HVAC&R Research 3 (1997) pp. 233\u2013264. doi:10.1080/10789669.1997.10391376. [12] S. Cho M. Zaheer-uddin Predictive control of intermittently operated radiant floor heating systems Energy Convers. Manag. 44 2003 1333 1342 10.1016/S0196-8904(02)00116-4 S. Cho, M. Zaheer-uddin, Predictive control of intermittently operated radiant floor heating systems, Energy Conversion and Management 44 (2003) pp. 1333\u20131342. doi:10.1016/S0196-8904(02)00116-4. [13] G. Serale M. Fiorentini A. Capozzoli P. Cooper M. Perino Formulation of a model predictive control algorithm to enhance the performance of a latent heat solar thermal system Energy Convers. Manag. 173 2018 438 449 10.1016/J.ENCONMAN.2018.07.099 G. Serale, M. Fiorentini, A. Capozzoli, P. Cooper, M. Perino, Formulation of a model predictive control algorithm to enhance the performance of a latent heat solar thermal system, Energy Conversion and Management 173 (2018) pp. 438\u2013449. doi:10.1016/J.ENCONMAN.2018.07.099. [14] M. Fiorentini J. Wall Z. Ma J.H. Braslavsky P. Cooper Hybrid model predictive control of a residential HVAC system with on-site thermal energy generation and storage Appl. Energy 187 2017 465 479 M. Fiorentini, J. Wall, Z. Ma, J. H. Braslavsky, P. Cooper, Hybrid model predictive control of a residential HVAC system with on-site thermal energy generation and storage, Applied Energy 187 (2017) pp. 465\u2013479. [15] S. Seal B. Boulet V.R. Dehkordi Centralized model predictive control strategy for thermal comfort and residential energy management Energy 212 2020 118456 10.1016/j.energy.2020.118456 S. Seal, B. Boulet, V. R. Dehkordi, Centralized model predictive control strategy for thermal comfort and residential energy management, Energy 212 (2020) 118456. doi:10.1016/j.energy.2020.118456. [16] Y. Zhao Y. Lu C. Yan S. Wang MPC-based optimal scheduling of grid-connected low energy buildings with thermal energy storages Energy Build. 86 2015 415 426 10.1016/j.enbuild.2014.10.019 Y. Zhao, Y. Lu, C. Yan, S. Wang, MPC-based optimal scheduling of grid-connected low energy buildings with thermal energy storages, Energy and Buildings 86 (2015) pp. 415\u2013426. doi:10.1016/j.enbuild.2014.10.019. [17] M.J. Vasallo J.M. Bravo E.G. Cojocaru M.E. Gegundez Calculating the profits of an economic MPC applied to CSP plants with thermal storage system Sol. Energy 155 2017 1165 1177 10.1016/j.solener.2017.07.033 M. J. Vasallo, J. M. Bravo, E. G. Cojocaru, M. E. GegÃ°ndez, Calculating the profits of an economic MPC applied to CSP plants with thermal storage system, Solar Energy 155 (2017) pp. 1165\u20131177. doi:10.1016/j.solener.2017.07.033. [18] R. Franke Object-oriented modeling of solar heating systems Sol. Energy 60 1997 171 180 10.1016/S0038-092X(96)00156-9 R. Franke, Object-oriented modeling of solar heating systems, Solar Energy 60 (1997) pp. 171\u2013180. doi:10.1016/S0038-092X(96)00156-9. [19] S. Rastegarpour S. Gros L. Ferrarini MPC approaches for modulating air-to-water heat pumps in radiant-floor buildings Control. Eng. Pract. 95 2020 104209 10.1016/j.conengprac.2019.104209 S. Rastegarpour, S. Gros, L. Ferrarini, MPC approaches for modulating air-to-water heat pumps in radiant-floor buildings, Control Engineering Practice 95 (2020) 104209. doi:10.1016/j.conengprac.2019.104209. [20] G.D. Kontes G.I. Giannakis V. Sanchez P. De Agustin-Camacho A. Romero-Amorrortu N. Panagiotidou D.V. Rovas S. Steiger C. Mutschler G. Gruen Simulation-based evaluation and optimization of control strategies in buildings Energies 11 2018 3376 10.3390/en11123376 G. D. Kontes, G. I. Giannakis, V. SÃ¡nchez, P. De Agustin-Camacho, A. Romero-Amorrortu, N. Panagiotidou, D. V. Rovas, S. Steiger, C. Mutschler, G. Gruen, Simulation-Based Evaluation and Optimization of Control Strategies in Buildings, Energies 11 (2018) 3376. doi:10.3390/en11123376. [21] R.S. Sutton A.G. Barto Reinforcement Learning: An Introduction 2018 ISBN: 0262039249 R. S. Sutton, A. G. Barto, Reinforcement Learning: An Introduction (2018). ISBN: 0262039249. [22] Z. Wang T. Hong Reinforcement learning for building controls: the opportunities and challenges Appl. Energy 269 2020 115036 10.1016/j.apenergy.2020.115036 Z. Wang, T. Hong, Reinforcement learning for building controls: The opportunities and challenges, Applied Energy 269 (2020) 115036. doi:10.1016/j.apenergy.2020.115036. [23] M. Han R. May X. Zhang X. Wang S. Pan D. Yan Y. Jin L. Xu A review of reinforcement learning methodologies for controlling occupant comfort in buildings Sustain. Cities Soc. 51 2019 101748 10.1016/j.scs.2019.101748 M. Han, R. May, X. Zhang, X. Wang, S. Pan, D. Yan, Y. Jin, L. Xu, A review of reinforcement learning methodologies for controlling occupant comfort in buildings, Sustainable Cities and Society 51 (2019) 101748. doi:10.1016/j.scs.2019.101748. [24] V. Mnih K. Kavukcuoglu D. Silver A.A. Rusu J. Veness M.G. Bellemare A. Graves M. Riedmiller A.K. Fidjeland G. Ostrovski S. Petersen C. Beattie A. Sadik I. Antonoglou H. King D. Kumaran D. Wierstra S. Legg D. Hassabis Human-level control through deep reinforcement learning Nature 518 2015 529 533 10.1038/nature14236 V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, D. Hassabis, Human-level control through deep reinforcement learning, Nature 518 (2015) pp. 529\u2013533. doi:10.1038/nature14236. [25] S. Brandi M.S. Piscitelli M. Martellacci A. Capozzoli Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings Energy Build. 224 2020 110225 10.1016/j.enbuild.2020.110225 S. Brandi, M. S. Piscitelli, M. Martellacci, A. Capozzoli, Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings, Energy and Buildings 224 (2020) 110225. doi:10.1016/j.enbuild.2020.110225. [26] Z. Zhang A. Chong Y. Pan C. Zhang K.P. Lam Whole building energy model for hvac optimal control: a practical framework based on deep reinforcement learning Energy Build. 199 2019 472 490 10.1016/j.enbuild.2019.07.029 Z. Zhang, A. Chong, Y. Pan, C. Zhang, K. P. Lam, Whole building energy model for hvac optimal control: A practical framework based on deep reinforcement learning, Energy and Buildings 199 (2019) pp. 472\u2013490. doi:10.1016/j.enbuild.2019.07.029. [27] Y. Wang X. Lin M. Pedram A near-optimal model-based control algorithm for households equipped with residential photovoltaic power generation and energy storage systems IEEE Trans. Sust. Energy 7 2016 77 86 10.1109/TSTE.2015.2467190 Y. Wang, X. Lin, M. Pedram, A Near-Optimal Model-Based Control Algorithm for Households Equipped With Residential Photovoltaic Power Generation and Energy Storage Systems, IEEE Transactions on Sustainable Energy 7 (2016) pp. 77\u201386. doi:10.1109/TSTE.2015.2467190. [28] T. Schreiber S. Eschweiler M. Baranski D. Müller Application of two promising reinforcement learning algorithms for load shifting in a cooling supply system Energy Build. 229 2020 110490 10.1016/j.enbuild.2020.110490 T. Schreiber, S. Eschweiler, M. Baranski, D. Müller, Application of two promising Reinforcement Learning algorithms for load shifting in a cooling supply system, Energy and Buildings 229 (2020) 110490. doi:10.1016/j.enbuild.2020.110490. [29] J.R. Vázquez-Canteli S. Ulyanin J. Kämpf Z. Nagy Fusing TensorFlow with building energy simulation for intelligent energy management in smart cities Sustain. Cities Soc. 45 2019 243 257 10.1016/j.scs.2018.11.021 J. R. Vázquez-Canteli, S. Ulyanin, J. Kämpf, Z. Nagy, Fusing TensorFlow with building energy simulation for intelligent energy management in smart cities, Sustainable Cities and Society 45 (2019) pp. 243\u2013257. doi:10.1016/j.scs.2018.11.021. [30] Y. Du H. Zandi O. Kotevska K. Kurte J. Munk K. Amasyali E. Mckee F. Li Intelligent multi-zone residential hvac control strategy based on deep reinforcement learning Appl. Energy 281 2021 116117 10.1016/j.apenergy.2020.116117 Y. Du, H. Zandi, O. Kotevska, K. Kurte, J. Munk, K. Amasyali, E. Mckee, F. Li, Intelligent multi-zone residential hvac control strategy based on deep reinforcement learning, Applied Energy 281 (2021) 116117. doi:10.1016/j.apenergy.2020.116117. [31] Y. Wang K. Velswamy B. Huang A long-short term memory recurrent neural network based reinforcement learning controller for office heating ventilation and air conditioning systems Processes 5 2017 46 10.3390/pr5030046 Y. Wang, K. Velswamy, B. Huang, A long-short term memory recurrent neural network based reinforcement learning controller for office heating ventilation and air conditioning systems, Processes 5 (2017) 46. doi:10.3390/pr5030046. [32] G. Gao J. Li Y. Wen Energy-efficient thermal comfort control in smart buildings via deep reinforcement learning Comp. Res. Reposit. 2019 URL https://arxiv.org/abs/1901.04693 G. Gao, J. Li, Y. Wen, Energy-efficient thermal comfort control in smart buildings via deep reinforcement learning, Computing Research Repository (2019). URL: https://arxiv.org/abs/1901.04693, (accessed October 14, 2021). [33] B. Chen Z. Cai M. Bergés Gnu-RL: a practical and scalable reinforcement learning solution for building HVAC control using a differentiable MPC policy Front. Built Environ. 6 2020 174 10.3389/fbuil.2020.562239 B. Chen, Z. Cai, M. Bergés, Gnu-RL: A Practical and Scalable Reinforcement Learning Solution for Building HVAC Control Using a Differentiable MPC Policy, Frontiers in Built Environment 6 (2020) 174. doi:10.3389/fbuil.2020.562239. [34] F. Ruelens B. Claessens S. Quaiyum B.D. Schutter R. Babuska R. Belmans Reinforcement learning applied to an electric water heater: from theory to practice Comp. Res. Reposit. 2015 URL https://arxiv.org/abs/1512.00408 F. Ruelens, B. Claessens, S. Quaiyum, B. D. Schutter, R. Babuska, R. Belmans, Reinforcement learning applied to an electric water heater: From theory to practice, Computing Research Repository (2015). URL: https://arxiv.org/abs/1512.00408, (accessed October 14, 2021). [35] D. Crawley L. Lawrie F. Winkelmann W. Buhl Y. Huang C. Pedersen R. Strand R. Liesen D. Fisher M. Witte J. Glazer EnergyPlus: creating a new-generation building energy simulation program Energy Build. 33 2001 319 331 10.1016/S0378-7788(00)00114-6 D. Crawley, L. Lawrie, F. Winkelmann, W. Buhl, Y. Huang, C. Pedersen, R. Strand, R. Liesen, D. Fisher, M. Witte, J. Glazer, EnergyPlus: Creating a New-Generation Building Energy Simulation Program, Energy and Buildings 33 (2001) pp. 319\u2013331. doi:10.1016/S0378-7788(00)00114-6. [36] Z. Zou X. Yu S. Ergan Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network Build. Environ. 168 2020 106535 10.1016/j.buildenv.2019.106535 Z. Zou, X. Yu, S. Ergan, Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network, Building and Environment 168 (2020) 106535. doi:10.1016/j.buildenv.2019.106535. [37] E. Barrett S. Linder Autonomous hvac control, a reinforcement learning approach Mach. Learn. Knowledge Discov. Databas. 2015 3 19 10.1007/978-3-319-23461-8_1 E. Barrett, S. Linder, Autonomous hvac control, a reinforcement learning approach, Machine Learning and Knowledge Discovery in Databases (2015) pp. 3\u201319. doi:10.1007/978-3-319-23461-8_1. [38] N.S. Raman A.M. Devraj P. Barooah S.P. Meyn Reinforcement learning for control of building hvac systems 2020 Am. Contr. Conf. (ACC) 2020 2326 2332 10.23919/ACC45564.2020.9147629 N. S. Raman, A. M. Devraj, P. Barooah, S. P. Meyn, Reinforcement learning for control of building hvac systems, 2020 American Control Conference (ACC) (2020) pp. 2326\u20132332. doi:10.23919/ACC45564.2020.9147629. [39] D.J. Biagioni X. Zhang P. Graf D. Sigler W. Jones A comparison of model-free and model predictive control for price responsive water heaters Proceedings of the 1st International Workshop on Reinforcement Learning for Energy Management in Buildings & Cities (RLEM\u201920) 2020 29 33 10.1145/3427773.3427872 D. J. Biagioni, X. Zhang, P. Graf, D. Sigler, W. Jones, A Comparison of Model-Free and Model Predictive Control for Price Responsive Water Heaters, Proceedings of the 1st International Workshop on Reinforcement Learning for Energy Management in Buildings & Cities (RLEM\u201920) (2020) pp. 29\u201333. doi:10.1145/3427773.3427872. [40] G. Ceusters R.C. Rodríguez A.B. García R. Franke G. Deconinck L. Helsen A. Nowé M. Messagie L.R. Camargo Model-predictive control and reinforcement learning in multi-energy system case studies Appl. Energy 303 2021 117634 10.1016/j.apenergy.2021.117634 G. Ceusters, R. C. Rodríguez, A. B. García, R. Franke, G. Deconinck, L. Helsen, A. Nowé, M. Messagie, L. R. Camargo, Model-predictive control and reinforcement learning in multi-energy system case studies, Applied Energy 303 (2021) 117634. doi:10.1016/j.apenergy.2021.117634. [41] T. Haarnoja A. Zhou K. Hartikainen G. Tucker S. Ha J. Tan V. Kumar H. Zhu A. Gupta P. Abbeel S. Levine Soft actor-critic algorithms and applications Comp. Res. Reposit. 2019 URL https://arxiv.org/abs/1812.05905 T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta, P. Abbeel, S. Levine, Soft Actor-Critic Algorithms and Applications, Computing Research Repository (2019). URL: https://arxiv.org/abs/1812.05905, (accessed October 14, 2021). [42] A. Hill A. Raffin M. Ernestus A. Gleave A. Kanervisto R. Traore P. Dhariwal C. Hesse O. Klimov A. Nichol M. Plappert A. Radford J. Schulman S. Sidor Y. Wu Stable Baselines, GitHub Repository URL https://github.com/hill-a/stable-baselines 2018 (accessed October 14, 2021) A. Hill, A. Raffin, M. Ernestus, A. Gleave, A. Kanervisto, R. Traore, P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, Y. Wu, Stable baselines, GitHub repository (2018). URL: https://github.com/hill-a/stable-baselines, (accessed October 14, 2021). [43] MathWorks MATLAB Software URL https://es.mathworks.com/ 2019 (accessed November 11, 2021) MathWorks, MATLAB Software (2019). URL: https://es.mathworks.com/, (accessed November 11, 2021). [44] M. Herceg M. Kvasnica C.N. Jones M. Morari Multi-parametric toolbox 3.0 2013 European Control Conference (ECC) 2013 502 510 10.23919/ECC.2013.6669862 M. Herceg, M. Kvasnica, C. N. Jones, M. Morari, Multi-parametric toolbox 3.0, 2013 European Control Conference (ECC) (2013) pp. 502\u2013510. doi:10.23919/ECC.2013.6669862. [45] Gurobi Gurobi Solver URL https://www.gurobi.com/products/gurobi-optimizer/ 2020 (accessed October 14, 2021) Gurobi, Gurobi Solver (2020). URL: https://www.gurobi.com/products/gurobi-optimizer/, (accessed October 14, 2021). [46] A. Bemporad Model predictive control design: New trends and tools Proceedings of the 45th IEEE Conference on Decision and Control 1\u201314 2006 6678 6683 10.1109/CDC.2006.377490 A. Bemporad, Model predictive control design: New trends and tools, Proceedings of the 45th IEEE Conference on Decision and Control, Vols 1-14 (2006) pp. 6678\u20136683. doi:10.1109/CDC.2006.377490. [47] S.L. Smith P.-J. Kindermans C. Ying Q.V. Le Don't Decay the Learning Rate, Increase the Batch Size URL https://arxiv.org/abs/1711.00489 2018 S. L. Smith, P.-J. Kindermans, C. Ying, Q. V. Le, Don'\u2019t decay the learning rate, increase the batch size (2018). URL: https://arxiv.org/abs/1711.00489, (accessed October 14, 2021).",
    "scopus-id": "85122507811",
    "coredata": {
        "eid": "1-s2.0-S0926580522000012",
        "dc:description": "This paper proposes a comparison between an online and offline Deep Reinforcement Learning (DRL) formulation with a Model Predictive Control (MPC) architecture for energy management of a cold-water buffer tank linking an office building and a chiller subject to time-varying energy prices, with the objective of minimizing operating costs. The intrinsic model-free approach of DRL is generally lost in common implementations for energy management, as they are usually pre-trained offline and require a surrogate model for this purpose. Simulation results showed that the online-trained DRL agent, while requiring an initial 4 weeks adjustment period achieving a relatively poor performance (160% higher cost), it converged to a control policy almost as effective as the model-based strategies (3.6% higher cost in the last month). This suggests that the DRL agent trained online may represent a promising solution to overcome the barrier represented by the modelling requirements of MPC and offline-trained DRL approaches.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2022-03-31",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S0926580522000012",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Brandi, Silvio"
            },
            {
                "@_fa": "true",
                "$": "Fiorentini, Massimo"
            },
            {
                "@_fa": "true",
                "$": "Capozzoli, Alfonso"
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S0926580522000012"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S0926580522000012"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S0926-5805(22)00001-2",
        "prism:volume": "135",
        "articleNumber": "104128",
        "prism:publisher": "Elsevier B.V.",
        "dc:title": "Comparison of online and offline deep reinforcement learning with model predictive control for thermal energy management",
        "prism:copyright": "© 2022 Elsevier B.V. All rights reserved.",
        "openaccess": "0",
        "prism:issn": "09265805",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "Deep reinforcement learning"
            },
            {
                "@_fa": "true",
                "$": "Model predictive control"
            },
            {
                "@_fa": "true",
                "$": "HVAC control"
            },
            {
                "@_fa": "true",
                "$": "Building energy consumption"
            },
            {
                "@_fa": "true",
                "$": "Energy savings"
            },
            {
                "@_fa": "true",
                "$": "Building energy management"
            }
        ],
        "openaccessArticle": "false",
        "prism:publicationName": "Automation in Construction",
        "openaccessSponsorType": null,
        "prism:pageRange": "104128",
        "pubType": "fla",
        "prism:coverDisplayDate": "March 2022",
        "prism:doi": "10.1016/j.autcon.2022.104128",
        "prism:startingPage": "104128",
        "dc:identifier": "doi:10.1016/j.autcon.2022.104128",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "526",
            "@width": "711",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr8.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "103391",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "526",
            "@width": "711",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr9.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "106411",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "336",
            "@width": "367",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "23069",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "385",
            "@width": "800",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "58153",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "460",
            "@width": "800",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "71770",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "697",
            "@width": "711",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "196388",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "399",
            "@width": "622",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "46482",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "362",
            "@width": "711",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "70940",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "193",
            "@width": "711",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "36137",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "526",
            "@width": "711",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr10.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "107794",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "162",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr8.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "12402",
            "@ref": "gr8",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "162",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr9.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "12519",
            "@ref": "gr9",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "179",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "6903",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "105",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "5907",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "126",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "5923",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "167",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "11760",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "141",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "6716",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "112",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "6781",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "60",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "4780",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "162",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr10.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "12611",
            "@ref": "gr10",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "2329",
            "@width": "3150",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr8_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "882111",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2329",
            "@width": "3150",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr9_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "899749",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1486",
            "@width": "1624",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "169414",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1703",
            "@width": "3543",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "458535",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2037",
            "@width": "3543",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "556946",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "3090",
            "@width": "3150",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1827741",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1770",
            "@width": "2756",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "338559",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1604",
            "@width": "3150",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "473399",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "514",
            "@width": "1890",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "123618",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2329",
            "@width": "3150",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-gr10_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "912144",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-si1.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1577",
            "@ref": "si1",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-si10.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "14021",
            "@ref": "si10",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-si2.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "22630",
            "@ref": "si2",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-si3.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7856",
            "@ref": "si3",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-si4.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7960",
            "@ref": "si4",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-si5.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11168",
            "@ref": "si5",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-si6.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "10740",
            "@ref": "si6",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-si7.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7159",
            "@ref": "si7",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-si8.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11845",
            "@ref": "si8",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-si9.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11837",
            "@ref": "si9",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0926580522000012-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "9097221",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85122507811"
    }
}}