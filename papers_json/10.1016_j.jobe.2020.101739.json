{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85096546025",
    "originalText": "serial JL 312002 291210 291862 291881 31 Journal of Building Engineering JOURNALBUILDINGENGINEERING 2020-08-31 2020-08-31 2021-01-06 2021-01-06 2022-09-06T07:56:50 1-s2.0-S2352710220333726 S2352-7102(20)33372-6 S2352710220333726 10.1016/j.jobe.2020.101739 S300 S300.1 FULL-TEXT 1-s2.0-S2352710220X00064 2024-06-21T17:51:07.336013Z 0 0 20210201 20210228 2021 2020-08-31T15:12:39.723653Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast highlightsabst orcid primabst ref 2352-7102 23527102 true 34 34 C Volume 34 23 101739 101739 101739 202102 February 2021 2021-02-01 2021-02-28 2021 Research Articles article fla © 2020 Elsevier Ltd. All rights reserved. ENERGYEFFICIENTHEATINGCONTROLFORSMARTBUILDINGSDEEPREINFORCEMENTLEARNING GUPTA A 1 Introduction 1.1 Challenges and motivation 1.2 Research objectives 2 Related work 2.1 Statistical and machine learning methods requiring historical data for training 2.2 Reinforcement learning via real-time data 2.3 Contributions of this paper 3 Preliminaries: deep reinforcement learning 3.1 Elements of reinforcement learning 3.2 Deep Q-Networks 4 Methodology and implementation 4.1 Thermal model of a house 4.1.1 Setpoint temperature 4.1.2 Thermostat 4.1.3 Heater 4.1.4 Cost calculator 4.1.5 Average outdoor temperature and daily variation 4.1.6 House 4.2 The DRL-based heating controller 4.2.1 Actions 4.2.2 States 4.2.3 Reward function 4.3 Implementation of the DQN algorithm 5 Simulation experiments and results for a single building 5.1 Real-world outdoor temperature data 5.2 Hyperparameters of the DQN model 5.3 Statistical performance measures used for comparisons 5.4 Experimental results and comparisons 5.5 Reward analysis 5.6 Training results 5.7 Validation results 5.8 Testing results 6 Experiments and results for multiple buildings 6.1 Specification of the centralized controller 6.1.1 States, actions and reward for the centralized controller 6.1.2 The deep neural network for the centralized controller 6.2 Experimental results for the centralized controller 6.2.1 Case 1: same setpoints 6.2.2 Case 2: paired setpoints 6.2.3 Case 3: different setpoints 6.3 Specification of the decentralized (independent) controllers 6.3.1 Experimental results for decentralized controllers 6.3.2 Case 1: same setpoints 6.3.3 Case 2: paired setpoints 6.3.4 Case 3: different setpoints 7 Centralized vs decentralized control 7.1 Case 1: same setpoint 7.2 Case 2: paired setpoints 7.3 Case 3: different setpoints 7.4 Practical implications of the findings 8 Conclusions and future work CRediT author statement References 2014 INTERGOVERNMENTALPANELCLIMATECHANGEFIFTHASSESSMENTREPORTAR5 NEJAT 2015 843 862 P PEREZLOMBARD 2008 394 398 L YANG 2014 164 173 L LI 2012 103 112 D FU 2015 1016 1022 Y MARYREENA 2018 330 342 K AHN 2017 117 130 J CHAUDHURI 2017 72 77 T IEEEINTERNATIONALCONFERENCESMARTGRIDSMARTCITIESICSGSC MACHINELEARNINGBASEDPREDICTIONTHERMALCOMFORTINBUILDINGSEQUATORIALSINGAPORE POPA 2019 1317 1337 D LI 2019 5770 5780 Y VAZQUEZCANTELI 2019 1072 1089 J CHENG 2016 43 55 Z BRUSEY 2018 413 421 J CHEN 2018 195 205 Y FAZENDA 2014 675 690 P WEI 2017 1 6 T PROCEEDINGS54THANNUALDESIGNAUTOMATIONCONFERENCE DEEPREINFORCEMENTLEARNINGFORBUILDINGHVACCONTROL SUTTON 1998 R REINFORCEMENTLEARNINGINTRODUCTIONADAPTIVECOMPUTATIONMACHINELEARNING HASTIE 2009 T SPRINGERSERIESINSTATISTICS ELEMENTSSTATISTICALLEARNINGDATAMININGINFERENCEPREDICTION LAPALU 2013 503 510 J KAELBLING 1996 237 285 L MNIH 2015 529 533 V DURHAM 2004 R THERMALENVIRONMENTALCONDITIONSFORHUMANOCCUPANCY HORNIK 1991 251 257 K MOCANU 2019 3698 3708 E ZHANG 2019 472 490 Z GUPTAX2021X101739 GUPTAX2021X101739XA 2023-01-06T00:00:00.000Z 2023-01-06T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2020 Elsevier Ltd. All rights reserved. 2021-05-29T04:06:44.998Z http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/aggregated-refined 0 item S2352-7102(20)33372-6 S2352710220333726 1-s2.0-S2352710220333726 10.1016/j.jobe.2020.101739 312002 2022-09-06T07:20:27.815832Z 2021-02-01 2021-02-28 1-s2.0-S2352710220333726-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/MAIN/application/pdf/bb84a8e79dcf262de1dfa0e3f6f5734a/main.pdf main.pdf pdf true 6042069 MAIN 15 1-s2.0-S2352710220333726-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/PREVIEW/image/png/3638d897c56cfe04cee2c88f67fd6122/main_1.png main_1.png png 53198 849 656 IMAGE-WEB-PDF 1 1-s2.0-S2352710220333726-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr1/DOWNSAMPLED/image/jpeg/d8e2aaf78a8831203353fb71da884ee1/gr1.jpg gr1 gr1.jpg jpg 27141 204 357 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-fx1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/fx1/DOWNSAMPLED/image/jpeg/df5112a65bb196c47b3cd72f9ecff759/fx1.jpg fx1 fx1.jpg jpg 78158 320 498 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr7/DOWNSAMPLED/image/jpeg/776bfb6b8d57e851dfc1fdb993c93496/gr7.jpg gr7 gr7.jpg jpg 47514 328 624 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr6/DOWNSAMPLED/image/jpeg/cbd96ea958b8829f1063f4d41fe4ef8c/gr6.jpg gr6 gr6.jpg jpg 31492 200 357 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr10/DOWNSAMPLED/image/jpeg/ea312b365defcfe6b131bc749e91d11e/gr10.jpg gr10 gr10.jpg jpg 46464 261 357 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr9/DOWNSAMPLED/image/jpeg/5dc73d7dd7ac192233adae7ed46c0fea/gr9.jpg gr9 gr9.jpg jpg 36880 265 357 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr11.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr11/DOWNSAMPLED/image/jpeg/57561cc4f6fe3cebc4fa20fd54923836/gr11.jpg gr11 gr11.jpg jpg 89832 604 357 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr8/DOWNSAMPLED/image/jpeg/b743eeb57c0f1bd10ac11a228a7d8b82/gr8.jpg gr8 gr8.jpg jpg 55078 269 357 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr12.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr12/DOWNSAMPLED/image/jpeg/62ba536ed06b39885f3b108364175947/gr12.jpg gr12 gr12.jpg jpg 81527 606 357 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr3/DOWNSAMPLED/image/jpeg/c31ea9abbad3e00f9479b43e6654baa4/gr3.jpg gr3 gr3.jpg jpg 48363 262 713 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr13.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr13/DOWNSAMPLED/image/jpeg/9c1f67577c053b99f2801cd611043ec0/gr13.jpg gr13 gr13.jpg jpg 69055 532 379 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr2/DOWNSAMPLED/image/jpeg/b648e7ce93f2c54a55008137810cd5a1/gr2.jpg gr2 gr2.jpg jpg 33661 150 357 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr14.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr14/DOWNSAMPLED/image/jpeg/3a8e8aac1a7b6451adc4b4f28f1d563c/gr14.jpg gr14 gr14.jpg jpg 65858 162 802 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr5/DOWNSAMPLED/image/jpeg/ad2b48a480972e7abbbe679e6a26d352/gr5.jpg gr5 gr5.jpg jpg 28476 152 357 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr15.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr15/DOWNSAMPLED/image/jpeg/ab1dad1dca778a5f716d074817a32758/gr15.jpg gr15 gr15.jpg jpg 61624 163 802 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr4/DOWNSAMPLED/image/jpeg/e8958e3cc966c8eb980d5b5990ef33f5/gr4.jpg gr4 gr4.jpg jpg 20664 50 179 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr16.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr16/DOWNSAMPLED/image/jpeg/c662e2c3c4b1daec5d9b516223dd0a3e/gr16.jpg gr16 gr16.jpg jpg 48481 159 802 IMAGE-DOWNSAMPLED 1-s2.0-S2352710220333726-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr1/THUMBNAIL/image/gif/a79dafd626cdcc53f3461f3981a875b2/gr1.sml gr1 gr1.sml sml 8940 125 219 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-fx1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/fx1/THUMBNAIL/image/gif/0a7be7f4a368c16d6f3e43e4b747f376/fx1.sml fx1 fx1.sml sml 18585 141 219 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr7/THUMBNAIL/image/gif/dd7a5622ba0ff32a2b50720f52dd5ac7/gr7.sml gr7 gr7.sml sml 13144 115 219 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr6/THUMBNAIL/image/gif/34d30df8744e3ee80ed5d972e18ec011/gr6.sml gr6 gr6.sml sml 10935 123 219 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr10/THUMBNAIL/image/gif/3a637ca996ad1e57b9c42a553e4efd5d/gr10.sml gr10 gr10.sml sml 16144 160 219 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr9/THUMBNAIL/image/gif/11ed50d0deefca1cb8e0a3100c58bd3f/gr9.sml gr9 gr9.sml sml 11780 162 219 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr11.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr11/THUMBNAIL/image/gif/7082a951e4ef0e0f823aa46f7e5761a1/gr11.sml gr11 gr11.sml sml 13725 164 97 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr8/THUMBNAIL/image/gif/b7925434e6140dd73b446eacf11504c2/gr8.sml gr8 gr8.sml sml 20239 164 217 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr12.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr12/THUMBNAIL/image/gif/5cf129f197e98d1a87f23feccd3950a1/gr12.sml gr12 gr12.sml sml 12795 163 96 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr3/THUMBNAIL/image/gif/930f8ad99af4739e52590223b7b0160c/gr3.sml gr3 gr3.sml sml 11315 80 219 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr13.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr13/THUMBNAIL/image/gif/b30151f32f106c486d57847a56f7a04e/gr13.sml gr13 gr13.sml sml 13744 163 116 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr2/THUMBNAIL/image/gif/5fb71c5ea27ba49af1a45a208249c4b2/gr2.sml gr2 gr2.sml sml 12120 92 219 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr14.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr14/THUMBNAIL/image/gif/e27faee969eddf1c37f31b015257907d/gr14.sml gr14 gr14.sml sml 12460 44 219 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr5/THUMBNAIL/image/gif/3bf637adfaf24abffa91365a3f115c22/gr5.sml gr5 gr5.sml sml 9318 93 219 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr15.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr15/THUMBNAIL/image/gif/4886cafdf5d6d958c5fb72de449e9aa8/gr15.sml gr15 gr15.sml sml 12892 44 219 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr4/THUMBNAIL/image/gif/2bb67112b7f3a6e6bb2dc8104e0241c9/gr4.sml gr4 gr4.sml sml 8740 61 219 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr16.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr16/THUMBNAIL/image/gif/101a41a60d861bca9dd03d0565e28872/gr16.sml gr16 gr16.sml sml 9293 43 219 IMAGE-THUMBNAIL 1-s2.0-S2352710220333726-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr1/HIGHRES/image/jpeg/e3e5001629112d9f79f8c27fb4f8a2d2/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 85004 903 1580 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-fx1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/fx1/HIGHRES/image/jpeg/03368e5057c31fa527082ec8cce738bf/fx1_lrg.jpg fx1 fx1_lrg.jpg jpg 502861 1419 2205 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr7/HIGHRES/image/jpeg/4e4d32d093e35402522f6af40b81db3e/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 222733 1452 2764 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr6/HIGHRES/image/jpeg/fe0d010210fa4f8c9c319143e75895f3/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 118592 885 1582 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr10/HIGHRES/image/jpeg/9fafda2890cbcaf8e293a2aab78d634c/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 164474 1159 1583 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr9/HIGHRES/image/jpeg/6969cf8060b5217f3143da3c493f3594/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 132006 1174 1583 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-gr11_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr11/HIGHRES/image/jpeg/b0a8dc43fbd3ee8eba85344f6a542978/gr11_lrg.jpg gr11 gr11_lrg.jpg jpg 486695 2677 1583 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr8/HIGHRES/image/jpeg/4f3b02e2fb4100d3c0fa7093a6abbe73/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 209703 1191 1580 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-gr12_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr12/HIGHRES/image/jpeg/5e1d44b4404ae561b1dcbde063622440/gr12_lrg.jpg gr12 gr12_lrg.jpg jpg 481478 2685 1583 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr3/HIGHRES/image/jpeg/1e2626f275eb7fddae627707dc717f13/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 259994 1160 3158 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-gr13_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr13/HIGHRES/image/jpeg/b644e7fff113aa79a9b5a4e6b26c51a0/gr13_lrg.jpg gr13 gr13_lrg.jpg jpg 380398 2356 1678 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr2/HIGHRES/image/jpeg/43da8a80419b2dca38b7cb8ace4cae07/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 129358 666 1583 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-gr14_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr14/HIGHRES/image/jpeg/a178d8bb035feeecc25010167f0b244e/gr14_lrg.jpg gr14 gr14_lrg.jpg jpg 369002 719 3551 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr5/HIGHRES/image/jpeg/999a1559d75fdff96b91fa518f5a6ddd/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 91127 675 1583 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-gr15_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr15/HIGHRES/image/jpeg/d52c50b872f6a1fa354ed2d9d1a6b2c3/gr15_lrg.jpg gr15 gr15_lrg.jpg jpg 324313 721 3551 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr4/HIGHRES/image/jpeg/aeaa5a85444f22b6dd80c0bfe3e780ae/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 36234 220 795 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-gr16_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/gr16/HIGHRES/image/jpeg/8f3131826638f8617e2b586a8182db4c/gr16_lrg.jpg gr16 gr16_lrg.jpg jpg 298030 705 3551 IMAGE-HIGH-RES 1-s2.0-S2352710220333726-si33.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/23ce31b4e0b1d8961a2edd09c329e620/si33.svg si33 si33.svg svg 46289 ALTIMG 1-s2.0-S2352710220333726-si35.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/e45bdd142fc5cb1cb718cceabc9024d2/si35.svg si35 si35.svg svg 80997 ALTIMG 1-s2.0-S2352710220333726-si23.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/5a08a47fcff68615cd80fd5c90b80a3e/si23.svg si23 si23.svg svg 15033 ALTIMG 1-s2.0-S2352710220333726-si37.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/6b7b16d872043a8210f8b91a215613ea/si37.svg si37 si37.svg svg 107647 ALTIMG 1-s2.0-S2352710220333726-si25.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/28b08700948ecc2c2663381496175bc8/si25.svg si25 si25.svg svg 29772 ALTIMG 1-s2.0-S2352710220333726-si19.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/e2708bf2bb6d4838d4532acfb2b150ac/si19.svg si19 si19.svg svg 64285 ALTIMG 1-s2.0-S2352710220333726-si34.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/d7ecd034a53737892dedab73add8034c/si34.svg si34 si34.svg svg 14027 ALTIMG 1-s2.0-S2352710220333726-si14.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/6bf0ef72bbe714fed63bc15d322bbb6b/si14.svg si14 si14.svg svg 83769 ALTIMG 1-s2.0-S2352710220333726-si9.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/da969104479dd9d28cd22056fe051ffb/si9.svg si9 si9.svg svg 7141 ALTIMG 1-s2.0-S2352710220333726-si28.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/1da13f49c94d9bf01fcd2c5b0f209774/si28.svg si28 si28.svg svg 107038 ALTIMG 1-s2.0-S2352710220333726-si36.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/4e9be58c607e7471f57fcde086979f0b/si36.svg si36 si36.svg svg 11316 ALTIMG 1-s2.0-S2352710220333726-si16.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/4ebc46f7413a1990280ad1327fdd1083/si16.svg si16 si16.svg svg 21923 ALTIMG 1-s2.0-S2352710220333726-si7.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/990e5c2a031f6c55d4176a6e9bc839e6/si7.svg si7 si7.svg svg 6232 ALTIMG 1-s2.0-S2352710220333726-si15.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/b6da36e291f957526f94788796e7ded7/si15.svg si15 si15.svg svg 89998 ALTIMG 1-s2.0-S2352710220333726-si17.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/2f732af38442c30aac6c05632dd6d095/si17.svg si17 si17.svg svg 16976 ALTIMG 1-s2.0-S2352710220333726-si2.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/99091b451b22f5966ac98614374c149e/si2.svg si2 si2.svg svg 12758 ALTIMG 1-s2.0-S2352710220333726-si21.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/ef9ab4c391b8237d12c4c4030daa16ef/si21.svg si21 si21.svg svg 21934 ALTIMG 1-s2.0-S2352710220333726-si29.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/104984c8472296c6d1647a215509144e/si29.svg si29 si29.svg svg 2606 ALTIMG 1-s2.0-S2352710220333726-si32.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/502cd1ca02e92ff2d42ac30dd7ebf09a/si32.svg si32 si32.svg svg 48450 ALTIMG 1-s2.0-S2352710220333726-si22.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/b6948766dae67a600305a8a91ee85e4e/si22.svg si22 si22.svg svg 88073 ALTIMG 1-s2.0-S2352710220333726-si30.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/3c4c43c14d1c55f96e7d21702ce76833/si30.svg si30 si30.svg svg 68126 ALTIMG 1-s2.0-S2352710220333726-si31.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/4d8a9f0c63a32a4e220a402af7aa1c9d/si31.svg si31 si31.svg svg 53629 ALTIMG 1-s2.0-S2352710220333726-si12.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/55a0f0990e0507d3c42895ef29d091db/si12.svg si12 si12.svg svg 20154 ALTIMG 1-s2.0-S2352710220333726-si24.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/6184db3c7e533843f6da210519351db5/si24.svg si24 si24.svg svg 41010 ALTIMG 1-s2.0-S2352710220333726-si6.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/e98b85ba831eea9e5389d2bfc2f6ff74/si6.svg si6 si6.svg svg 96929 ALTIMG 1-s2.0-S2352710220333726-si38.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/15ff9539cd79c1756e720342fe75cc66/si38.svg si38 si38.svg svg 54668 ALTIMG 1-s2.0-S2352710220333726-si18.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/47043c6b50f630e8f0e6519e17b97d86/si18.svg si18 si18.svg svg 62480 ALTIMG 1-s2.0-S2352710220333726-si26.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/5811c2c79d3e07be47064360389fde8d/si26.svg si26 si26.svg svg 16781 ALTIMG 1-s2.0-S2352710220333726-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/ac3b05324e51c07b01bfa40805628e22/si3.svg si3 si3.svg svg 10951 ALTIMG 1-s2.0-S2352710220333726-si20.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/0a3292777b5089b8106576c621772e6a/si20.svg si20 si20.svg svg 19396 ALTIMG 1-s2.0-S2352710220333726-si4.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/ce49a7297a07119a06ba7aa199c831eb/si4.svg si4 si4.svg svg 14545 ALTIMG 1-s2.0-S2352710220333726-si27.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/1016859cb1bb616cbb635c52642c42fe/si27.svg si27 si27.svg svg 89754 ALTIMG 1-s2.0-S2352710220333726-si1.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/537d32785f2f413b7d91d3b407a73b08/si1.svg si1 si1.svg svg 77623 ALTIMG 1-s2.0-S2352710220333726-si5.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/d78231b36c7793ee581e8b0b5d60f944/si5.svg si5 si5.svg svg 9955 ALTIMG 1-s2.0-S2352710220333726-si13.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/187abd26e3ef290ac7cbe6a437833f07/si13.svg si13 si13.svg svg 8689 ALTIMG 1-s2.0-S2352710220333726-si8.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/d7e9a498b73c1fda051434f1d788ae2a/si8.svg si8 si8.svg svg 22962 ALTIMG 1-s2.0-S2352710220333726-si11.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/573431cd604568121cb88eebd08ca975/si11.svg si11 si11.svg svg 82338 ALTIMG 1-s2.0-S2352710220333726-si10.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2352710220333726/STRIPIN/image/svg+xml/de0a5d2303146bd4125ef614afe03fc5/si10.svg si10 si10.svg svg 32034 ALTIMG 1-s2.0-S2352710220333726-am.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:103JN69BZGS/MAIN/application/pdf/537b2d712bbcb270579557844519b913/am.pdf am am.pdf pdf false 1154329 AAM-PDF JOBE 101739 101739 S2352-7102(20)33372-6 10.1016/j.jobe.2020.101739 Elsevier Ltd Fig. 1 A schematic representation of reinforcement learning. Fig. 1 Fig. 2 Interaction of Deep Q-Network with environment. Fig. 2 Fig. 3 The thermal model of a house representing the RL environment. Fig. 3 Fig. 4 The thermostat subsystem. Fig. 4 Fig. 5 Heater subsystem. Fig. 5 Fig. 6 Thermodynamic model of the house. Fig. 6 Fig. 7 Deep Q-Network architecture and steps. Fig. 7 Fig. 8 Hourly outside temperature data for March 2019. There are 31 days and 744 h in March. Fig. 8 Fig. 9 5-min outside temperature data for March 4 interpolated from real-world hourly data. There are 288 5-min intervals in any given day. Fig. 9 Fig. 10 Reward comparison of DQN-based controller with Baseline controller for the training dataset (22 randomly selected days of March 2019). Fig. 10 Fig. 11 Comparison of indoor temperature variations and cost of DQN-based controller with baseline controller on training data (22 days). Fig. 11 Fig. 12 Comparison of indoor temperature variations and cost of DQN-based controller with baseline controller on test data (3 days). Fig. 12 Fig. 13 Centralized vs decentralized control strategies for multiple buildings. Fig. 13 Fig. 14 Comparison of mean and total cost for training, validation and test experiments for centralized and independent control under case 1 (same setpoints). Fig. 14 Fig. 15 Comparison of mean and total cost for training, validation and test experiments for centralized and independent control under case 2 (paired setpoints). Fig. 15 Fig. 16 Comparison of mean and total cost for training, validation and test experiments for centralized and independent control under case 3 (different setpoints). Fig. 16 Table 1 Hyperparameters used for training of DQN based heating controllers. Table 1 Hyperparameter Value Learning Rate 0.001 Batch Size 32 Hidden Layers 2 Training Length 800 Activation Function ReLU Discount Factor 0.95 Buffer Size 10,000 Exploration Step 1 episode Exploration Final 0.01 Table 2 Comparison of DQN-based controller with Baseline controller on training dataset (22 days). Table 2 DQN-based Controller Baseline Controller Mean (°C) Std (°C) Cost ($) Mean (°C) Std (°C) Cost ($) 1.75 1.14 421.41 2.42 1.61 437.42 Table 3 Comparison of DQN-based controller with baseline controller on validation data (6 days). Table 3 Episode DQN-based Controller Baseline Controller Mean (°C) Std (°C) Cost ($) Mean (°C) Std (°C) Cost ($) 650 1.76 1.26 139.27 2.51 1.57 146.95 675 1.69 1.18 144.5 2.51 1.57 146.95 700 1.72 1.23 140.76 2.51 1.57 146.95 725 1.75 1.24 139.67 2.51 1.57 146.95 750 1.68 1.19 142.78 2.51 1.57 146.95 775 1.79 1.27 138.36 2.51 1.57 146.95 800 1.68 1.17 144.92 2.51 1.57 146.95 Table 4 Comparison of DQN model with the baseline model on the test dataset (3 days). Table 4 DQN-based Controller Baseline Controller Mean (°C) Std (°C) Cost ($) Mean (°C) Std (°C) Cost ($) 1.84 1.25 61.43 2.47 1.58 64.61 Table 5 Lower and upper bound (LCI and UCI) for the 95% confidence interval for the performance of the DQN-bsaed controller as well as the p-value for the one-sample t-tests when comparing to the performance of the Baseline controller. Table 5 Lower Confidence Interval Upper Confidence Interval p-value Mean (°C) 1.737 1.956 ≪ 0.05 Std (°C) 1.18 1.325 ≪ 0.05 Cost ($) 60.33 62.53 ≪ 0.05 Table 6 Experimental design for multiple buildings used under both centralized and decentralized control. Table 6 Building Case 1: Case 2: Case 3: Same Setpoints (°C) Paired Setpoints (°C) Different Setpoints (°C) Building 1 21 18 18 Building 2 21 18 19 Building 3 21 20 20 Building 4 21 20 21 Building 5 21 23 22 Building 6 21 23 23 Table 7 Improvements in total cost and mean by the centralized DQN-based controller over the baseline scenario for multiple buildings under case 1 (same setpoints). Table 7 Buildings Setpoint (°C) Improvements (%) Train Val Test B1_Mean (°C) 21 22.8 30.3 28.45 B2_Mean (°C) 21 22.8 30.3 28.62 B3_Mean (°C) 21 14.1 13.9 23.61 B4_Mean (°C) 21 22.4 29.9 28.21 B5_Mean (°C) 21 22.01 30.7 28.33 B6_Mean (°C) 21 22.01 30.3 28.44 Total_Cost ($) 7.6 1.6 3.45 Table 8 Confidence intervals and results of t-test on test dataset for multiple buildings under centralized control for case 1 (same setpoints). Table 8 Buildings Setpoint (°C) LCI(%) UCI(%) p-value B1_Mean (°C) 21 28.07 28.82 ≪ 0.05 B2_Mean (°C) 21 28.82 28.97 ≪ 0.05 B3_Mean (°C) 21 22.91 24.31 ≪ 0.05 B4_Mean (°C) 21 27.83 28.59 ≪ 0.05 B5_Mean (°C) 21 27.95 28.71 ≪ 0.05 B6_Mean (°C) 21 28.07 28.82 ≪ 0.05 Total_Cost ($) 3.29 3.61 ≪ 0.05 Table 9 Improvements in total cost and mean by the centralized DQN-based controller over baseline scenario under case 2 (paired setpoints). Table 9 Buildings Setpoint (°C) Improvements (%) Train Val Test B1_Mean (°C) 18 18.5 26.1 25.57 B2_Mean (°C) 18 18.5 26.1 25.57 B3_Mean (°C) 20 14.6 7.8 19.49 B4_Mean (°C) 20 14.6 7.8 19.49 B5_Mean (°C) 23 11.01 17.2 14.46 B6_Mean (°C) 23 11.04 17.6 14.46 Total_Cost ($) 4.6 4.6 3.64 Table 10 Confidence intervals and results of t-test on test dataset for multiple buildings under centralized control for case 2 (paired setpoints). Table 10 Buildings Setpoint (°C) LCI(%) UCI(%) p-value B1_Mean (°C) 18 24.35 26.78 ≪ 0.05 B2_Mean (°C) 18 24.35 26.78 ≪ 0.05 B3_Mean (°C) 20 18.41 20.58 ≪ 0.05 B4_Mean (°C) 20 18.41 20.58 ≪ 0.05 B5_Mean (°C) 23 13.24 15.67 ≪ 0.05 B6_Mean (°C) 23 13.24 15.67 ≪ 0.05 Total_Cost ($) 3.28 4.01 ≪ 0.05 Table 11 Improvements in total cost and mean by the centralized DQN-based controller over the baseline scenario for multiple buildings under case 3 (different setpoints). Table 11 Buildings Setpoint (°C) Improvements (%) Train Val Test B1_Mean (°C) 18 10.01 17.6 17.12 B2_Mean (°C) 19 21.7 24.4 25.31 B3_Mean (°C) 20 21.9 24.7 24.56 B4_Mean (°C) 21 9.1 7.6 7.68 B5_Mean (°C) 22 11.6 15.7 14.81 B6_Mean (°C) 23 4.8 6.4 13.81 Total_Cost ($) 4.5 4.1 4.27 Table 12 Confidence intervals and results of t-test on test dataset for multiple buildings under centralized control for case 3 (different setpoints). Table 12 Buildings Setpoint (°C) LCI(%) UCI(%) p-value B1_Mean (°C) 18 16.71 17.53 ≪ 0.05 B2_Mean (°C) 19 25.81 25.43 ≪ 0.05 B3_Mean (°C) 20 24.13 24.98 ≪ 0.05 B4_Mean (°C) 21 6.88 8.48 ≪ 0.05 B5_Mean (°C) 22 14.12 15.49 ≪ 0.05 B6_Mean (°C) 23 13.14 14.46 ≪ 0.05 Total_Cost ($) 4.05 4.49 ≪ 0.05 Table 13 Improvements in total cost and mean by decentralized DQN-based controllers compared to the baseline scenario under case 1 (same setpoints). Table 13 Buildings Setpoint (°C) Improvements (%) Train Val Test B1_Mean (°C) 21 27.01 32.7 29.55 B2_Mean (°C) 21 26.6 31.9 29.96 B3_Mean (°C) 21 26.6 31.9 30.04 B4_Mean (°C) 21 27.8 33.1 29.55 B5_Mean (°C) 21 27.4 31.5 29.96 B6_Mean (°C) 21 27.4 31.1 29.92 Total_Cost ($) 2.8 1.5 2.83 Table 14 Confidence intervals and results of t-test on test dataset for multiple buildings under decentralized control for case 1 (same setpoints). Table 14 Buildings Setpoint (°C) LCI(%) UCI(%) p-value B1_Mean (°C) 21 28.74 30.36 ≪ 0.05 B2_Mean (°C) 21 29.15 30.36 ≪ 0.05 B3_Mean (°C) 21 29.96 30.16 ≪ 0.05 B4_Mean (°C) 21 28.74 30.36 ≪ 0.05 B5_Mean (°C) 21 29.15 30.36 ≪ 0.05 B6_Mean (°C) 21 29.80 30.36 ≪ 0.05 Total_Cost ($) 2.42 3.24 ≪ 0.05 Table 15 Improvements in total cost and mean by decentralized DQN-based controllers compared to the baseline scenario under case 2 (paired setpoints). Table 15 Buildings Setpoint (°C) Improvements (%) Train Val Test B1_Mean (°C) 18 21.6 24.5 24.76 B2_Mean (°C) 18 22.4 26.1 25.73 B3_Mean (°C) 20 23.9 31.01 29.73 B4_Mean (°C) 20 25.5 31.01 29.88 B5_Mean (°C) 23 24.1 23.2 21.63 B6_Mean (°C) 23 25.9 24.5 29.40 Total_Cost ($) 4.01 3.01 4.52 Table 16 Confidence intervals and results of t-test on test dataset for multiple buildings under decentralized control for case 2 (paired setpoints). Table 16 Buildings Setpoint (°C) LCI(%) UCI(%) p-value B1_Mean (°C) 18 24.72 25.02 ≪ 0.05 B2_Mean (°C) 18 25.51 25.92 ≪ 0.05 B3_Mean (°C) 20 29.49 29.92 ≪ 0.05 B4_Mean (°C) 20 29.49 30.31 ≪ 0.05 B5_Mean (°C) 23 20.17 23.09 ≪ 0.05 B6_Mean (°C) 23 28.97 29.83 ≪ 0.05 Total_Cost ($) 4.33 4.71 ≪ 0.05 Table 17 Improvements in total cost and mean by decentralized DQN-based controllers compared to the baseline scenario under case 3 (different setpoints). Table 17 Buildings Setpoint (°C) Improvements (%) Train Val Test B1_Mean (°C) 18 22.01 26.4 23.97 B2_Mean (°C) 19 16.2 22.01 25.77 B3_Mean (°C) 20 49.01 31.4 30.53 B4_Mean (°C) 21 26.01 32.7 24.54 B5_Mean (°C) 22 7.3 16.9 21.79 B6_Mean (°C) 23 27.6 26.6 27.64 Total_Cost ($) 6.6 4.2 5.72 Table 18 Confidence intervals and results of t-test on test dataset for multiple buildings under decentralized control for case 3 (different setpoints). Table 18 Buildings Setpoint (°C) LCI(%) UCI(%) p-value B1_Mean (°C) 18 22.75 25.52 ≪ 0.05 B2_Mean (°C) 19 25.26 26.17 ≪ 0.05 B3_Mean (°C) 20 30.38 30.67 ≪ 0.05 B4_Mean (°C) 21 21.84 27.24 ≪ 0.05 B5_Mean (°C) 22 20.49 23.08 ≪ 0.05 B6_Mean (°C) 23 27.17 28.109 ≪ 0.05 Total_Cost ($) 5.33 6.12 ≪ 0.05 Energy-efficient heating control for smart buildings with deep reinforcement learning Anchal Gupta Youakim Badr ∗ Ashkan Negahban Robin G. Qiu School of Graduate Professional Studies, The Pennsylvania State University, Malvern, PA, 19355, USA School of Graduate Professional Studies The Pennsylvania State University Malvern PA 19355 USA School of Graduate Professional Studies, The Pennsylvania State University, Malvern, PA, 19355, USA ∗ Corresponding author. Buildings account for roughly 40% of the total energy consumption in the world, out of which heating, ventilation, and air conditioning are the major contributors. Traditional heating controllers are inefficient due to lack of adaptability to dynamic conditions such as changing user preferences and outside temperature patterns. Therefore, it is necessary to design energy-efficient controllers that can improvise occupant thermal comfort (deviation from setpoint temperature) while reducing energy consumption. This research presents a Deep Reinforcement Learning (DRL)-based heating controller to improve thermal comfort and minimize energy costs in smart buildings. We perform extensive simulation experiments using real-world outside temperature data. The results show that the DRL-based smart controller outperforms a traditional thermostat controller by improving thermal comfort between 15% and 30% and reducing energy costs between 5% and 12% in the simulated environment. A second set of experiments is then performed for the case of multiple buildings, each having its own heating equipment. The performance is compared when the buildings are controlled centrally (using a single DRL-based controller) versus decentralized control, where each heater is controlled independently and has its own DRL-based controller. We observe that as the number of buildings and differences in their setpoint temperatures increase, decentralized control performs better than a centralized controller. The results have practical implications for heating control, especially in areas with multiple buildings such as residential complexes with multiple houses. Keywords Deep reinforcement learning Simulation Occupant thermal comfort Heating controller HVAC 1 Introduction Emerging technologies such as Artificial Intelligence (AI) and Internet of Things (IoT) seem to be shaping the future of the world, a world of connected and smart things/agents that requires minimal human intervention. Integrating such advanced technologies into building automation systems (BAS) has given rise to the notion of smart buildings, suggesting ability to sense the environment and react accordingly. As a result, AI and machine learning (such as the deep reinforcement learning algorithm adopted in this paper) have the potential to improve occupant comfort and energy consumption in buildings and improve individuals\u2019 quality of life. 1.1 Challenges and motivation If the current rate of fossil fuel consumption continues, it is estimated that one trillion tons of industrial era carbon dioxide (CO2) will have emitted by about 2040 [1]. Climate models suggest that if this one-trillion-ton threshold is surpassed, the planet will pass a tipping point after which it will continue to warm, leading to an ever-deteriorating climate system. The biggest challenge and concern is to meet global energy needs efficiently while minimizing carbon emissions to mitigate irreversible effects of climate change. The building sector is key in reducing carbon emissions as buildings account for roughly 40% of the total energy consumption in the world [2]. Heating, ventilation, and air conditioning (HVAC) is one of the most extensively used and most energy-consuming systems in buildings. HVAC accounts for almost half of building energy consumption in the U.S. and between 10 and 20% of overall energy consumption in developed countries [3]. These high percentages can be attributed to users prioritizing thermal comfort over the reduction in energy consumption [4]. Moreover, there is a strong correlation between outside weather conditions and HVAC energy consumption [5]. Today, most buildings use very basic programmable thermostats to control heater on/off cycles based on a predefined threshold in terms of deviation from a setpoint temperature. Therefore, there seems to be a significant potential to improve energy consumption while maintaining occupant comfort. This is the main motivation behind this research. Advancement in building design and material, automation systems, and energy generation and storage technologies can lead to significant economic and environmental benefits. Smart buildings can also help in achieving these goals. Smart buildings are an integration of IoT with building components [6] and include sensors and actuators, smart control systems, networking and communication systems, and software platforms. The objective of a smart BAS is to leverage sensor data for efficient control of building operations. If leveraged properly, the considerable amount of data collected from sensors provides significant potential to reduce energy consumption, while maintaining occupant comfort. 1.2 Research objectives Supervised and unsupervised machine learning algorithms and big data analytics techniques can discover specific trends and patterns from the data that would not be immediately apparent to humans. However, these algorithms generally require a large volume of high-quality data for effective training since a small data set is susceptible to sampling errors and can produce biased results. Though data acquisition has become more accessible by installing sensors in buildings, processing and mining of the data require significant computational effort. Reinforcement Learning (RL) is an area of machine learning that does not require large training data sets with correct answers/labels as it acts and learns from its actions in real-time. RL often requires a simulated environment for training and testing. Based on the above, this paper aims to develop a novel heating controller that can benefit smart buildings by regulating indoor temperature to maintain thermal comfort (deviation from setpoint temperature) and save energy. More specifically, we propose a Deep Reinforcement Learning (DRL) approach for heating control to automate decision-making in real-time with minimal dependency on historical data. Simulation experiments for a single building indicate that the DRL-based smart heating controller improves thermal comfort between 15% and 30% and reduces energy consumption between 5% and 12% compared to a traditional thermostat. Simulation results for the case of multiple similar buildings also show considerable improvement in terms of both thermal comfort and overall energy consumption over a traditional thermostat. We also compare two scenarios where the buildings are controlled centrally (using a single DRL-based controller) versus decentralized control, where each building's heater is controlled independently and has its own DRL-based controller. Simulation results suggest that as the number of buildings and differences in their setpoint temperatures increase, decentralized control performs better than a centralized controller. Our findings have significant practical implications for heating control, especially in multiple independent buildings consisting of similar structure, such as a residential complex. To facilitate future extensions and technology transfer, all codes and models are made available via a Mendeley Data repository associated with this paper [7]. The remainder of the paper is organized as follows. Section 2 presents an overview of the related literature. Section 3 introduces deep reinforcement learning and its basic function. Section 4 details the methodology and implementation of the proposed DRL-based heating controller. Section 5 presents the experimental results for a single building. The results for centralized and decentralized control for multiple buildings are discussed in Sections 6 and 7. Finally, Section 8 concludes the paper and discusses future work. 2 Related work Buildings equipped with smart sensors and IoT technologies generate massive amounts of data that traditional approaches fail to handle effectively. Machine learning, on the other hand, provides powerful predictive and prescriptive models that can leverage big data produced by various sources. In this section, we provide a review of statistical and machine learning methods that use historical data for training (Section 2.1) and reinforcement learning methods that involve real-time learning (Section 2.2) as related to our objective of improving thermal comfort (deviation from setpoint temperature) and optimizing energy consumption in buildings. Based on our literature review, we will then summarize the contributions of this paper in Section 2.3. 2.1 Statistical and machine learning methods requiring historical data for training In [8], a cluster of 24 Support Vector Machine (SVM) models is proposed to predict the electricity load for the next hour in a public building in Shanghai. The proposed model outperforms other forecasting models, namely Auto-Regressive Integrated Moving Average Extended (ARIMAX), Artificial Neural Network (ANN), and Decision Trees. The authors show that forecasting of short-term electricity load plays an important role in the energy management and control system. Effectively managing the demand and supply of energy can save energy consumption and costs. In [9], a data-driven framework using Structure Equation Modelling (SEM) is proposed that uses historical data to develop a multivariate model to find the thermal coupling of adjacent zones in HVAC control in large commercial buildings. SEM helps to improve the prediction of temperature within a zone to build energy-efficient HVAC systems. Analyzing occupant behavior and their interaction with building energy equipment can also help in better meeting thermal comfort. In [10], an integrated energy control model (including energy supply model, thermal comfort model, and decision-making model) is proposed. The energy supply model controls supply energy to improve user's thermal comfort, whereas the thermal comfort model analyzes and determines the factors that lead to abnormal and abrupt indoor temperature conditions. In [11], a data-driven approach is proposed to predict thermal comfort level of occupants using environmental and human factors as input. Six supervised machine learning models: SVM, ANN, Logistic Regression (LR), Linear Discriminant Analysis (LDA), K-Nearest Neighbour (KNN) and Classification Trees (CT) are used to predict one of the three comfort level categories (cool-discomfort, comfortable, warm-discomfort). In [12], a modular platform using deep neural networks is designed for load monitoring and energy load forecasting to build smart home controllers. The platform uses cloud services to collect and aggregate data from smart environments. It helps detect energy variation and advise occupants on how to improve their way of life to save on energy costs. Also, a smart thermostat based on Bayesian inference is introduced in[13] to learn occupant preferences from limited data. We see that supervised machine learning algorithms and deep learning models make better predictions than traditional statistical or regression techniques like ARIMA. But for efficient performance, the training of such models requires a vast amount of good quality labeled data. Also, as the data collected and annotated is from history, in case of a change in equipment or users, this data becomes obsolete, and the performance of trained machine learning algorithms can decrease. 2.2 Reinforcement learning via real-time data Traditional machine learning and deep learning (DL) models are data-driven and mainly depend on historical patterns to provide future predictions. However, reinforcement learning is inspired by behaviorist psychology, where an agent takes actions in an environment, receives a reward for its actions, and strives to maximize long-term rewards. In [14], the authors provide a review of different RL techniques for demand response analysis in a grids. According to [14], only a few articles have applied RL algorithms in physical systems, and still, real-world experiments need to be conducted. Similarly, the survey in [3] studies the research and practice of DL and RL in smart grids. In In [15], a Q-learning controller is developed to jointly operate lights and blinds in a building for an energy-efficient luminous environment. In [16], an RL-based controller using State-Action-Reward-State-Action (SARSA) is introduced to control the climate inside a vehicle and keep passengers thermally comfortable. The SARSA algorithm outperforms traditional controllers based on fuzzy logic. In [17], a control strategy is proposed to make optimal decisions for HVAC operations and window systems using RL. In [18], a Q-learning model is developed to learn to when to turn on/off the heater by observing the occupant behavior. The model assumes that the outside temperature is lower than the occupants\u2019 preferred temperature, which impedes generalizability to real-world scenarios. In [19], an HVAC controller is proposed for buildings with multiple zones. The authors use different neural networks to estimate the values of actions taken by the controller agent. The framework performs less accurately as the number of zones increases. In [20], a Deep Deterministic Policy Gradient (DDPG) algorithm is applied to learn an optimal policy to maintain thermal comfort. In [21], authors introduce a strategy for optimal control of space heating in buildings by comparing model-based and model-free DRL algorithms. The indoor temperature is considered as an input to the network and changing the power level of the heat pump as the action. The experiments focus on scenarios including flat electricity pricing, dual pricing scheme, and real-time pricing but ignore essential features such as outside temperature. Recent advancements in machine learning and reinforcement learning open research opportunities to interact in real-time with the building environment. Based on state of the art, it is observed that deep reinforcement learning algorithms can handle large state space by using a deep neural network to associate the value estimates and connected state-action pairs, thereby performing better than conventional RL algorithms like Q-learning or SARSA. However, as the number of buildings increases, the state space also increases, making the learning process of a DRL algorithm difficult. Our research studies the centralized and decentralized heating control for such clusters with multiple buildings intending to maximize thermal comfort (deviation from setpoint temperature) and reducing energy costs. 2.3 Contributions of this paper Based on the above, our contributions of this paper can be summarized as follows: \u2022 From a methodological standpoint, unlike existing RL-based methods that have complex structures and require many different input parameters, our proposed DRL-based heating controller is parsimonious and takes only the outdoor and indoor temperature values as input yet, based on our results, can lead to significant improvement in both thermal comfort (deviation from setpoint temperature) and energy consumption. The relatively simple structure of our algorithm makes it less computationally expensive and more appealing to practitioners. \u2022 From a practical standpoint, the insights derived from our comparison of centralized versus decentralized control for the case of multiple buildings have important implications for heating control, especially in buildings consisting of similar buildings, such as residential complexes. 3 Preliminaries: deep reinforcement learning This section presents the basic concepts related to reinforcement learning to facilitate understanding of the methodology used in this paper. More comprehensive discussions on RL can be found in [22]. RL is a goal-oriented machine learning algorithm that interacts with an environment and makes real-time decisions. RL is fundamentally different from other supervised and unsupervised machine learning paradigms. Supervised learning requires labeled historical observations to extrapolate and learn the relationships between inputs and outputs [23]. Once trained, the model can be applied to an unseen dataset to provide predictions. Unsupervised learning models, on the other hand, are only provided with input data to discover hidden patterns [24]. Given a valid simulation model of the environment is already available, RL does not require historical data for training. Instead, the algorithm, often called agent, learns from the sequel of its actions and the reward or penalty received from the environment in response to these actions. 3.1 Elements of reinforcement learning The main components of RL are as follows: \u2022 Agent: The agent interacts with the environment in discrete time steps [25]. Based on observations from the environment, the agent performs some actions, receives rewards based on its actions, and tries to learn the optimal strategy from these interactions. \u2022 Environment: The environment can be a physical or simulated world in which the agent resides and acts as a decision-maker. \u2022 States: These are variables that can represent the different possible states that the environment can be in. \u2022 Reward: This is the feedback to the agent from the environment as a result of the agent's action. Fig. 1 illustrates these components and their basic function. At each decision point, the agent takes an action based on the current state of the environment. The agent then receives a reward corresponding to its action based on which, the agent learns. The agent's action may or may not change the state of the environment. At the next decision point, the environment may have a different state and this process repeats until a terminal state is achieved. 3.2 Deep Q-Networks For reinforcement learning algorithms such as Q-learning [15], the quality of a state-action pair is calculated via a Q-function as in Equation (1): (1) Q ( S t , A t ) = Q ( S t , A t ) + α [ R t + γ m a x a Q ( S t + 1 , a ) − Q ( S t , A t ) ] , where R t is the reward received for moving from state ( S t ) to a new state ( S t + 1 ), α is the learning rate, and γ is the discount factor that determines the importance of future rewards. In Q-learning, the state-action pairs along with their Q-values are stored in a lookup table also known as Q-Table. The Q-Table needs to be searched for state-action pair at each time step to update the corresponding Q-value. In case of a large number of state-action pairs or continuous states and actions, it becomes increasingly difficult to maintain and search the Q-table. Deep Q-Networks (DQN) [26] are used to overcome this problem by approximating the Q-value via a deep neural network due to its advancement in building up more abstract representations of the data with several layers of nodes. To avoid the instability, we add an experience replay buffer and target network for better convergence of Q-function. Algorithm 1 the basic logic in DQN. The main components of a DQN algorithm are as follows. \u2022 Q-Network: The Q-Netowrk is a deep neural network that can be of any type (e.g., feed forward, convolution, or recurrent). States are passed as inputs to the network, and the output of the deep neural network is the approximated Q-value for each action as shown in Fig. 2 . The output layer has neurons equivalent to the total number of possible actions that the agent can take. Image 1 \u2022 Experience Replay: A replay buffer or an experience replay [27] stores the transition information of an agent moving from current state ( S t ) to the next state ( S t + 1 ) by performing an action A t . This information is also known as the agent's experience. A fixed number of recent experiences are stored in a queue, and a batch of them are randomly sampled to train the Q-network. The key idea to use an experience replay buffer is to ensure the training of the deep Q-network with independent samples instead of training with last transitions. Sequential sampling would result in instabilities due to internal correlations between experiences. \u2022 Target Network: The squared difference between the predicted and the target value is used as the loss function given by: (2) L o s s = ( R t + γ max a Q ( S t + 1 , A t ; θ \u2032 ) − Q ( S t , A t ; θ ) ) 2 , where R t is the reward obtained on taking action A t in the state S t to move to state S t + 1 , θ and θ ' represent the weights and bias of the Q-network and the target Q-network, respectively. Two separate deep neural networks (i.e., Q-network and target Q-network) are used to calculate the target and predicted value to avoid divergence [28,29]. The target network is used to calculate the index of the best action and its Q-value. The model parameters (weights and bias) of the Q-network are updated using a gradient descent algorithm. After certain number of time steps, the weights and bias of the Q-network are copied to the weights and bias of the target Q-network. Freezing the target network's weights for certain number of steps and then updating it with the actual Q-network's weights stabilizes the training. The m a x a impacts the current action by the possible future reward. This way the future reward is allocated to current actions to help the agent select the highest return action at any given state. 4 Methodology and implementation To build and test our smart heating controller, a simulated environment is used to train the DRL agent and investigate its performance in terms of energy consumption/costs and thermal comfort. This section describes this simulation model and the design of the proposed DQN algorithm. 4.1 Thermal model of a house We use a simulation model of a heater in a house as the environment to compare the performance of the proposed DRL-based heating controller with a traditional thermostat-based controller. Fig. 3 shows the simulated environment implemented in MATLAB/Simulink [30]. The building modeled here consists of one floor and six windows with an area of 3229.17 square feet and a height of 4 m (m). Each wall of the house is insulated with 0.2 m thick glass wool. Since the model and its description is publicly available online, and for the sake of conciseness, here we only provide a description of its main components and refrain from discussing the details of the underlying code. 4.1.1 Setpoint temperature Setpoint is a constant value which specifies an ideal indoor temperature to be maintained. It is defaulted to 21 °C, as an ideal ambient temperature for occupants [31]. In our experiments with multiple buildings, we will consider the case where different occupants may use different setpoint temperatures. 4.1.2 Thermostat The thermostat subsystem consists of a relay block and takes the difference between indoor temperature and setpoint (Terr) as an input and allows a fluctuation of ± 3 °C a. k.a deadband. If the indoor temperature increases to 23.89 °C, the heater is turned off by the thermostat and will remain off until the indoor temperature drops below 18.33 °C at which point the heater is turned on and will remain on until the indoor temperature reaches 23.89 °C. The command of turning on/off the heater is sent to the heater subsystem using the Blower switch as shown in Fig. 4 . In Section 4.3, we describe how we replace this thermostat with the proposed DRL-based controller. 4.1.3 Heater The heater subsystem (Fig. 5 ) has a constant airflow rate. When the heater is turned on by the thermostat, it blows hot air at a constant temperature ( T H e a t e r ) of 50 °C and a constant flow rate of Mdot (1 kg/sec = 3600 kg/h). Fig. 5 represents the heater subsystem that calculates the heat flow from the heater into the room given by Equation (3). (3) d Q d t = ( T H e a t e r − T r o o m ) ⋅ M d o t ⋅ c , where c is the heat capacity of air at constant pressure with value 1005.4 J/kg-K and T r o o m is the indoor temperature. 4.1.4 Cost calculator The cost calculator (shown in Fig. 3) is a gain block which integrates the heat flow and multiplies it by the energy cost rate which is set to $0.09 per 3.6 e 6 J by default. 4.1.5 Average outdoor temperature and daily variation In Fig. 3, the average outdoor temperature is a constant block set to 10 °C. The default daily temperature variation block uses a sine wave to generate outdoor temperature variations. However, the simple, smooth, and regular shape of the sine function oversimplifies outdoor temperature fluctuations, making it easy for the DQN algorithm to learn the patterns. To overcome this limitation, we use real-world temperature data in our experiments as discussed in Section 5.1. 4.1.6 House House is a subsystem that considers the heat flow by the heater and the heat loss to the environment in order to calculate the variations in indoor temperatures as shown in Fig. 6 and given by Equations (4) and (5), respectively. (4) ( d Q d t ) l o s s e s = T r o o m − T o u t R e q (5) d T r o o m d t = 1 M a i r ⋅ c ⋅ ( d Q h e a t e r d t − d Q l o s s e s d t ) where M a i r is the mass of air inside the house and R e q is the equivalent thermal resistance of the house. It is important to note that our goal is not to develop an absolutely valid simulation model for a specific real building. While this simulation involves many simplifying assumptions, it still serves our purpose, which is to develop and assess a DRL-based heating control mechanism that is able to learn the interactions between outdoor and indoor temperature dynamics and react accordingly and appropriately. Our present model can be modified by tuning the hyperparameters of the deep reinforcement learning algorithm to handle non-linearity within the data and complexity of the simulation model. Being said that deep neural networks are universal approximators [32], the DRL-based algorithm theoretically is expected to be able to learn the dynamics of the environment, if modified properly. Also, the DRL-based heating controller relies on learning from the environment, so it might take a longer time to converge. 4.2 The DRL-based heating controller The following subsections describe different components of the proposed DRL-based controller. 4.2.1 Actions The thermostat turns on/off the heater system based on a lower and upper bound threshold for the indoor temperature. We replace the thermostat by a DRL-based controller that decides whether to switch on/off the heater at discrete decision points based on the observed state values. The action is a set of two values given by Equation (6). (6) A c t i o n s = { O N = 1 , O F F = 0 } , 4.2.2 States States are used to describe the environment at different points in time. In our model, a state is defined as a set of two values: outside temperature and the difference between the setpoint and indoor temperature (see Equation (7)). (7) S t a t e = { T o u t , T d i f f } \u2022 Outdoor temperature ( T o u t ): The behavior of outdoor temperature highly influences the temperature inside the building. If the outdoor temperature is much lower than the indoor temperature, the heater needs to produce more heat to maintain a comfortable ambient for occupants. The heating controller should be able to learn a policy that reacts appropriately to fluctuations in the outdoor conditions. Therefore, we chose to include this state variable in our representation of the system state. \u2022 Difference between the indoor and setpoint temperature ( T d i f f ): Setpoint is the temperature preferred by the occupant. The outdoor temperature by itself is not sufficient to represent the state of the system properly. The DRL-based algorithm also needs to consider the difference between setpoint and current indoor temperature, to decide whether to turn on or off the heater. Therefore, we include this difference in the representation of the system state. 4.2.3 Reward function The main goal of the DRL algorithm is that the agent should follow a behavior (policy) to maximize the reward received for the actions taken to move from one state to another. Here, we introduce an integrated cost and comfort-driven reward function. Running the heater consumes energy and needs to be optimized along with occupant thermal comfort. To overcome this, we formulate a reward function in the hope to maintain a balance between energy consumption (captured through the cost output in the simulation model) and thermal comfort (captured by a measure of difference between indoor and setpoint temperature). The integrated cost and comfort reward function is given by Equation (8). (8) R e w a r d = − β ( c o s t ) − ( 1 − β ) ( T d i f f ) 2 . Cost in Equation (8) is calculated by the simulated environment as discussed in Section 4.1.4. The β coefficient is introduced to give different weights to cost and the difference between indoor and setpoint temperatures ( T d i f f ). Also, c o s t and T d i f f have different scales and to avoid biased results, we normalize both values between [0,1] using min-max normalization given by givren by Equation (9). (9) z = x − m i n ( x ) m a x ( x ) − m i n ( x ) , where x represents the variable of interest that is being normalized. 4.3 Implementation of the DQN algorithm We implement the proposed DQN algorithm in Python using Tensorflow. It receives data using a TCP/IP connection every 5 min of simulation time. This ensures discrete decision points that are not too close to each other and to prevent the DQN controller from turning the heater on/off too frequently in short cycles, which is infeasible in reality and can damage the equipment. The state is sent to the DQN and the action is predicted using a fully connected deep neural network following the epsilon-greedy policy. The main steps are shown in Fig. 7 and all codes and data files are provided in a Mendeley Data repository associated with this paper [7]. The fully connected deep neural network has been constructed to predict the action and Q-values of the actions. The deep neural network takes T o u t and T d i f f as input, and the output layer has two neurons, each dedicated to predicting the Q-values for \u201con\u201d and \u201coff\u201d actions (step 1). The action selected with the maximum Q-value is then sent to the simulated environment to receive the next state (step 2). The information of current state, action, resulting reward, and next state (denoted by S t , A t , R t , S t + 1 , respectively) is then stored in a replay buffer (step 3). After a certain number of iterations, a batch of recent experiences is chosen from the replay buffer. These values are used to train the Q-Network to predict the Q-values for the actions (step 4 and 5). The target Q-Network is updated with weights of Q-Networks after every N steps (step 6). The goal is to minimize the DQN loss illustrated in Equation (2). 5 Simulation experiments and results for a single building In this section, we use real-world outdoor temperature data to test the DRL-based heating controller in the case of a single building and compare its performance with the thermostat controller. 5.1 Real-world outdoor temperature data Real-world weather data for the Philadelphia region are obtained from [33] and used to evaluate the performance of the heating controller in realistic scenarios in terms of outdoor temperature variations. We use hourly temperature data from March 2019. Fig. 8 illustrates the hourly data for the 31 days in that month. We observe high temperature variations that are not easy to learn, requiring a heating controller that can manage the heater settings efficiently. We use linear interpolation to estimate 5-min interval temperature data as shown in Fig. 9 for a representative day. 5.2 Hyperparameters of the DQN model Table 1 summarizes the hyperparameters used in our experiments, which are described below: \u2022 Learning Rate: This parameter decides the stepping of a gradient descent algorithm to reach the global minimum. A large value of learning rate can lead to divergence in learning, whereas a small value increases computational time. \u2022 Batch Size: Batch size defines the number of data samples to work through before updating the internal model parameters (weights and bias). \u2022 Hidden Layers: Hidden layers consist of artificial neurons between the input and output layers of an ANN. The neurons take the weighted set of inputs and produce output by applying an activation function. \u2022 Training length: Training length is the total number of times the data is passed through a deep neural network for the learning process. Each pass of the total dataset is called an episode. \u2022 Activation Function: Activation function is the non-linear transformation applied to neurons in a layer before sending the output to the following layer. \u2022 Discount factor: The value of the discount factor can be between 0 and 1. It decides the significance of future rewards. By setting its value to 0, the agent ignores the knowledge of prior experiences and reacts based on the current states only, while setting it to 1 makes the environment's history infinitely long. \u2022 Buffer size: The maximum number of recent experiences of an agent that can be stored into a replay buffer in deep reinforcement learning is defined as the buffer size. \u2022 Exploration step: Q-learning and Deep Q-Networks follow the epsilon-greedy policy to select an action. This is the rate set for an agent to explore its environment. It can either be set to a fixed value or a decreasing value over iterations. \u2022 Exploration final: If the exploration step is set to a decreasing value, the exploration final value specifies a lower bound for exploration step beyond which it cannot decay. In [34], the authors show that the training time of deep learning models is directly proportional to the number of trainable parameters associated with hidden layers, the number of neurons in each layer, activation function and how many times a network backpropagates to update these parameters. Relatively less number of parameters as listed in Table 1, makes our algorithm less computationally expensive. Also, it requires only 1 wt update and 65 wt updates per epoch for single and multiple buildings respectively. 5.3 Statistical performance measures used for comparisons An action is taken by the DRL-based heating controller every 5 min of simulation time. Each episode runs for 1 day (24 h) which consists of a total of 288 5-min interval data points. Each data point is equivalent to one iteration. In our experiments, we use the following three statistical performance measures: 1. Mean difference from setpoint: Every 5 min, the absolute difference between the setpoint temperature and the indoor temperature ( T d i f f ) is calculated. At the end of each episode (24 h), the average of these differences is calculated as in Equation (10), where N is the number of intervals in an episode. In this case, N is the number of 5-min intervals in a day and we have N = 288 . A high mean value indicates high deviations from the setpoint temperature (on average) and low thermal comfort. (10) M e a n = ∑ i = 1 N a b s ( T ( d i f f ) ( i ) ) N 2. Standard Deviation (Std) of difference from setpoint: Every 5 min, the absolute difference between the setpoint temperature and the indoor temperature is calculated. At the end of each episode (24 h), the standard deviation of these differences is calculated as in Equation (11), where N is the number of intervals in an episode. (11) S t d = ∑ i = 1 N ( a b s ( T d i f f ) i − M e a n ) 2 N − 1 While the mean provides a measure of the average difference, it does not provide any information on the magnitude of the fluctuations from the setpoint. For example, consider the following two scenarios: (1) a day during which the indoor temperature was always 2° higher (or lower) than the setpoint; and, (2) a day during which the indoor temperature keeps oscillating between setpoint ± 4° in a symmetric fashion. Both of these scenarios will have a mean difference of 2°, however, the second scenario is clearly less comfortable as the occupants constantly need to adjust to different indoor temperatures (up to 8° difference between the lowest and highest temperature). The standard deviation of the observed differences allows us to distinguish between these two scenarios (the first scenario has a standard deviation of 0). In general, the higher the standard deviation value, the lower the thermal comfort for occupants. 3. Total/cumulative cost: Interval energy cost is calculated and returned every 5 min by the simulation model (given by Equation (12)). We use the total cost at the end of each episode (day) as a measure of energy consumption. (12) C u m u l a t i v e C o s t = ∑ i = 1 N C o s t i 5.4 Experimental results and comparisons We split the March 2019 outdoor temperature data into 70%, 20%, and 10% segments for training, validation, and testing, respectively. The training data are used for initial training of the controller, the validation data are used for hyperparameter tuning, and the testing data are used to test the tuned controller on unseen data. The DQN-based controller is trained on data combined from 22 randomly selected days from the pool of 31 days in March 2019. The trained controller is validated on six days and tested on three days. We are essentially trying to train the DQN-based controller offline with available situations of the environment and then considering it to be deployed for online training. The online training will let the DQN-based controller train post-deployment as well. This is equivalent to letting the controller train offline in the first 22 days of the month and then evaluating its online performance for the rest of the month. 5.5 Reward analysis The rewards given to the DQN-based controller for taking actions increase with more training (Fig. 10 ). The curve starts to stabilize after around 650 episodes of training. The final reward for the DQN-based controller is about 140 units higher than the reward for the thermostat-based controller (which we treat as the baseline scenario hereafter). A higher reward value shows better performance for the DRL controller than the thermostat in terms of the integrated cost and comfort measure defined in Equation (8). 5.6 Training results After 800 episodes of training, the DQN controller is run for 1 episode to compute the performance. For the training dataset, the controller shows 27.6% and 29.2% decrease in mean and standard deviation, respectively, and a 3.7% decrease in total energy cost/consumption as summarized in Table 2 . Fig. 11 shows that the fluctuations of the blue region (indoor temperature variation when the DQN-based controller is used) is closer to the setpoint temperature (21 °C) than the orange region corresponding to the indoor temperature variation when the thermostat is used. 5.7 Validation results Validation experiments are performed to hypertune the training length of the DQN algorithm. Six days are chosen randomly from the 9 remaining days that were not included in the training set. The convergence of the reward for the DQN controller slows down after episode number 650 (as discussed in Section 5.5). This signifies the stability in the training of the controller. To choose the best value for training length, we run the DQN controller on the validation dataset for the case of 650, 675, 700, 725, 750, 775, and 800 episodes of training. Table 3 compares the mean, standard deviation, and cumulative cost of DQN-based controller with baseline controller for these cases. We observe the best energy cost after training for 775 episodes. Therefore, the controller trained for 775 episodes is considered best for testing (which is performed next). 5.8 Testing results The trained and validated DQN-based controller is tested on unseen data for the three remaining days in the month. The test outside temperature dataset (green line) is shown in Fig. 12 . The test results, illustrated in Table 4 , show a decrease of 4.92% in total energy cost/consumption and 25.5% and 20.8% improvement in mean and standard deviation, respectively. To show that the observed differences are in fact statistically significant, we run the DQN-based controller 30 times on the test dataset and calculate the 95% confidence interval for the performance measures as shown in Table 5 . We also performed one-sample t-tests at a 5% significance level, which confirm the observed difference between the baseline and DQN controller are statistically significant. The null hypothesis for the t-tests are m e a n ( D Q N ) ≥ 2.47 , s t d ( D Q N ) ≥ 1.58 , and c o s t ( D Q N ) ≥ 64.61 . In all cases, the p-value is much smaller than 0.05 as indicated in Table 5. The results show that the DQN-based controller outperforms the baseline controller and improves both thermal comfort and energy consumption. Based on our simulation results, the proposed controller shows significant potential for buildings equipped with a single heating unit such as small apartments and gas stations. 6 Experiments and results for multiple buildings Residential complexes, often have multiple similar buildings each with its own heating unit, but their occupants may have different preferences related to thermal comfort and setpoint temperature. This section investigates centralized and decentralized control that can be applicable to such cases. More specifically, we perform experiments for the case of multiple buildings with the goal to compare the performance of a centralized DQN algorithm that controls heating in all buildings versus decentralized DQN-based controllers, where each building is controlled independently using its own DQN controller. Fig. 13 provides a schematic presentation of the centralized and decentralized control schemes. A simulated environment is developed by replicating the building from Fig. 3. All buildings in our experiments have the same building envelope and heating equipment, making these experiments relevant to applications such as a residential complex that consist of multiple similar buildings. The experiments are performed under various scenarios related to the setpoint temperatures in the buildings. The following results pertain to the simulation model with six buildings, while space limitations preclude inclusion of the results for other number of buildings that we studied. In the following subsections, we first compare the performance of centralized against the thermostat controller (subsection 6.2). We then compare the decentralized control strategy against the case where all buildings use their own thermostat-based controller (subsection 6.3). We will directly compare the performance of the centralized and decentralized DQN controllers in Section 7. 6.1 Specification of the centralized controller A central DQN-based controller controls heating in all of the buildings in the model. The controller aims to maintain overall thermal comfort and save total energy cost/consumption across all six buildings. 6.1.1 States, actions and reward for the centralized controller States in the centralized DQN controller are defined by the difference between indoor and setpoint temperature for each building (6 values) and the outdoor temperature (1 value, common for all buildings as all buildings are assumed to be located in the same area), given by Equation (13). (13) S t a t e = { T d i f f 1 , T d i f f 2 \u2026 T d i f f 6 , T o u t } A combinatorial action space [35] is defined for the centralized DQN controller, where the action taken at each decision point is a combination of multiple sub-actions. The number of subactions for m buildings is equivalent to 2 m as we have a binary action for each heater (turn on/off). The reward is a weighted function of the sum of the total costs and sum of differences in the setpoint and indoor temperature over all buildings given by Equation (14). (14) R e w a r d = β ∑ i = 1 m C o s t i + ( 1 − β ) ∑ i = 1 m ( T d i f f i ) 2 , where i is the index for each building (i = 1, 2, \u2026, 6 in this case) and β determines the weight given to overall cost and thermal comfort. 6.1.2 The deep neural network for the centralized controller The input layer of the deep neural network has m neurons for T d i f f values (one for each building) and one neuron for outdoor temperature ( T o u t ). The output layer has 2 m neurons, each representing the Q-value of the combinatorial action. In the case of 6 buildings, the deep neural network has 7 input and 64 output neurons. The number of neurons in the first and second hidden layers is 64 and 128, respectively. For one building, the deep neural network had 2 hidden layers with 24 neurons each. But due to increase in input neurons and the actions space, the same structure did not perform well. Therefore we hypertuned the number of neurons in the hidden layers and found 64 and 128 as the best suitable combination. We have two separate deep neural networks for the Q-network and target Q-network to avoid divergence in the predicted values. For the case of a single building presented in Section 5.2, training of the Q-network and updating of the target Q-network is done after every episode. Recall that exchange of states and actions at every 5-min interval between the simulation model and the DQN controller is referred to as an iteration whereas the total number of interchanges for the whole training dataset is referred to as an episode. In the case of 22 days of training dataset, 6336 iterations are equivalent to one episode. However, a divergence in the predicted values is observed as we increase the number of buildings, leading to a sharp decrease in the rewards. This shows that the Q-network requires more frequent training and the target Q-network requires frequent updating. Hence training of the Q-network is done every 100 iterations, and the target network is updated every 200 iterations. We made sure that reward function converges before using the modified architecture for further experiments. Our study is limited to cases with small clusters (3\u20136 buildings). Addition of more buildings may require changes/hyperparameter tuning of the algorithm for better convergence of reward function. 6.2 Experimental results for the centralized controller We evaluate the performance of the centralized controller under three cases summarized in Table 6 (the same cases where analyzed under decentralized control as well). The model is trained for 800 episodes on March 2019 outdoor temperature data. The data are divided into train, validate, and test with a ratio of 7:2:1. In each case, we statistically compare the performance of the centralized controller with a baseline scenario where all buildings use their own thermostat. In all cases, we run the centralized DQN-based heating controller 30 times on test dataset and provide 95% confidence intervals for the percentage improvement over the baseline scenario as well as the results of a one-sample t-test at a 5% significance level to verify statistically significant improvements. The null hypothesis for the t-tests are % i m p r o v e m e n t ≤ 0 . All p-values are much smaller than 0.05, suggesting statistically significant improvements. The results for hypothesis test are shown in Table 8 , Table 10 and Table 12 for different cases discussed in subsequent sections 6.2.1, 6.2.2 and 6.2.3 respectively. 6.2.1 Case 1: same setpoints In the first experiment, all six buildings have a setpoint of 21 °C. Table 7 compares the improvements in mean and total cost of a centralized DQN-based controller over the baseline scenario. The centralized controller improves thermal comfort between 13% and 30% and reduces energy costs between 1% and 8% over all six buildings. 6.2.2 Case 2: paired setpoints In the second experiment, the setpoint temperature for building 1 and 2 is 18 °C, for building 3 and 4 is 20 °C, and for building 5 and 6 is 23 °C. The results are summarized in Table 9. We observe that the DQN-based centralized controller improves thermal comfort between 7% and 26% and saves energy costs between 3% and 5% over all buildings . 6.2.3 Case 3: different setpoints In this case, the buildings have different setpoints. Table 11 shows that the DQN-based centralized controller improves thermal comfort between 4% and 26% and saves energy costs between 4% and 5% compared to the baseline scenario. 6.3 Specification of the decentralized (independent) controllers Under a decentralized control scheme, each building has its own DQN-based controller that is independent of other buildings. These controllers do not share a reward function, have independent actions, and aim to improve energy consumption and thermal comfort only for their corresponding building. The deep neural network architecture, states, actions, and reward function of each controller is the same as the configuration for the case of a single building (discussed in Section 4.2). 6.3.1 Experimental results for decentralized controllers The following subsections summarize the results for decentralized controllers under the three cases shown in Table 6. In all three cases, we run 30 simulation runs of the decentralized DQN-based heating control on the test dataset and provide 95% confidence intervals for the percentage improvement over the baseline scenario as well as the results of a one-sample t-test at a 5% significance level to verify statistically significant improvements. The null hypothesis for the t-tests are % i m p r o v e m e n t ≤ 0 . All p-values are much smaller than 0.05, suggesting statistically significant improvements. The results for hypothesis test are shown in Table 14 , Table 16 and Table 18 for different cases discussed in subsequent sections 6.3.2, 6.3.3 and 6.3.4 respectively. 6.3.2 Case 1: same setpoints In this case, all buildings have the same setpoint temperature at 21 °C. Table 13 shows that the independent controllers are able to outperform the baseline scenario by improving the deviation from setpoint temperature comfort between 26% and 33% and total energy cost over all buildings between 1% and 3%. 6.3.3 Case 2: paired setpoints In the second case, the setpoint for building 1 and 2 is set to 18 °C, building 3 and 4 is set to 20 °C, and building 5 and 6 is set to 21 °C. Table 15 shows that the independent controllers are able to improve thermal comfort between 21% and 31% and total energy costs over all buildings between 3% and 5%. 6.3.4 Case 3: different setpoints In this case, the DQN-based independent controllers are assigned to buildings having different setpoints ranging from 18 to 23. Table 17 shows that decentralized control improves thermal comfort between 7% and 49% and total cost over all buildings between 4% and 7%. 7 Centralized vs decentralized control Our simulation results show that both centralized and decentralized control outperform the baseline scenario in terms of thermal comfort and energy consumption. In this section, we directly compare the two control schemes to provide additional insight about potential applications where they may be appropriate. We use the three cases in Table 6 for the comparisons. 7.1 Case 1: same setpoint Under similar setpoints, the independent controllers are found to be more efficient in improving thermal comfort. However, the centralized controller performs better in terms of total cost across all buildings by about 1%\u20137% as shown in Fig. 14 . 7.2 Case 2: paired setpoints In this case, we observe that the independent control still performs better in terms of thermal comfort, while the centralized controller outperforms in terms of total cost across all buildings, saving between 4% and 5% over training and validation as shown in Fig. 15 . 7.3 Case 3: different setpoints In this case, independent control still outperforms in terms of thermal comfort over all buildings (except for Building 2). The two control schemes have comparable performance related to energy consumption over all buildings in the model (between 4% and 7% for decentralized controllers and 4%\u20135% for centralized control) as shown in Fig. 16 . 7.4 Practical implications of the findings Based on the above experiments, a trade-off is observed between the number of buildings, their settings (in terms of setpoint temperature), and performance of the two control schemes. As we move from the same setpoint for all buildings to different setpoints, decentralized controller performs better. Independent controllers have an objective to maximize thermal comfort (deviation from setpoint temperature) while saving energy costs within each building whereas the centralized controller acts to minimize the total cost over all the buildings. Due to this reason, we can observe a better performance for centralized controller in terms of optimizing total cost over all buildings. Our simulation results suggest that a centralized DQN controller can be an appropriate option for buildings with similar buildings that use similar setpoints. Independent/decentralized controllers, on the other hand, seem to be more appropriate for applications such as multiple houses within a residential complex, where the buildings have similar envelope but use different setpoints since the occupants (who may have different thermal comfort preferences) have full control over setting the setpoint temperature. 8 Conclusions and future work This paper provides a real-time decision-making algorithm for heating control for smart buildings using deep reinforcement learning. Through simulation experiments, we demonstrate that the DQN-based controller outperforms the traditional thermostat-based controller by improving both thermal comfort (deviation from setpoint temperature) and heating energy consumption, even though these are conflicting objectives, in general. Our results have practical implications for heating control in buildings consisting of similar buildings (in our experimental setting, the results show this is the case for up to 6 buildings) such as houses in a residential complex. Simulation results suggest that as the number of buildings and differences in their setpoint temperatures increase, decentralized control (with each heater having its own independent DRL-based controller) performs better than centralized control (a single DRL-based controller for all buildings). Our multi-buildings study can be extended to multi-zone scenarios by considering interactions such as heat transfer within the zones. This work presents an attempt at enhancing heating control via deep reinforcement learning and opens the gates to an abundance of future research opportunities. While our simulation results are promising, empirical evidence need to be collected by implementing the proposed algorithm in real-world settings. This is perhaps the main limitation of our work that future research needs to investigate. Also, our experiments uses a simple first-order linear simulation environment without considering noises (sensor noise, actuator noise, etc.), perturbations (human behaviors, infiltration, delayed behaviors of sensor, etc.) and non-continuous characteristics of heating equipment (non-linear heater's efficiency curve, internal control logics of heaters). Adding these features to the simulation model increases its complexity, and it becomes more challenging to learn the dynamics, and therefore the design of the controller needs to be modified. Future research could also replicate our methodology and experiments using more complex simulated environments that include interactions of occupants as well as other equipment and energy consumption sources such as lighting, ventilation, hot water boilers, and appliances [35]. Another interesting area for future research could involve designing new reinforcement learning algorithms for solving the peak smoothing problem during peak energy consumption hours for grids serving multiple buildings [35]. Considering dynamic pricing for such problems will require modifying the reward function and set of states to handle temporal dependencies. Future extensions could also study the effect of building envelope and other environmental factors such as humidity [36]. A systematic study on these factors will help select an appropriate design for the reinforcement learning algorithm based on the specific application at hand. HVAC systems also have dynamic characteristics, such as decreasing efficiency due to equipment aging and changing operational schedules (e.g., due to seasons or changing occupancy patterns [37]). Therefore, it is important to study how such controllers can be designed to effectively adapt to these changes. We hope that this paper and its future extensions along the above lines will help the building sector move beyond simplistic, traditional control approaches toward smarter and more energy-efficient methods enabled by artificial intelligence and machine learning. CRediT author statement Anchal Gupta: Investigation, Data curation, Visualization, Methodology, Software, Validation, Original draft preparation, Writing and Editing. Youakim Badr: Conceptualization, Methodology, Supervision, Resources, Original draft preparation, Validation, Reviewing and Editing. Robin Qiu: Conceptualization, Methodology, Validation, Reviewing and Editing. Ashkan Negahban: Conceptualization, Methodology, Validation, Reviewing and Editing. Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. References [1] Intergovernmental Panel on Climate Change, Fifth Assessment Report (AR5) 2014 Cambridge University Press Intergovernmental Panel on Climate Change, Fifth Assessment Report (AR5), Cambridge University Press, 2014. [2] P. Nejat F. Jomehzadeh M.M. Taheri M. Gohari M. Z. Abd Majid A global review of energy consumption, CO2 emissions and policy in the residential sector Renew. Sustain. Energy Rev. 43 C 2015 843 862 P. Nejat, F. Jomehzadeh, M. M. Taheri, M. Gohari, M. Z. Abd. Majid, A global review of energy consumption, CO2 emissions and policy in the residential sector, Renewable and Sustainable Energy Reviews 43 (C) (2015) 843-862. [3] L. Pérez-Lombard J. Ortiz C. Pout A review on buildings energy consumption information Energy Build. 40 3 2008 394 398 L. Perez-Lombard, J. Ortiz, C. Pout, A review on buildings energy consumption information, Energy and Buildings 40 (3) (2008) 394-398. [4] L. Yang H. Yan J.C. Lam Thermal comfort and building energy consumption implications \u2013 a review Appl. Energy 115 2014 164 173 L. Yang, H. Yan, J. C. Lam, Thermal comfort and building energy consumption implications - A review, Applied Energy 115 (2014) 164-173. [5] D.H.W. Li L. Yang J.C. Lam Impact of climate change on energy use in the built environment in different climate zones \u2013 a review Energy 42 1 2012 103 112 D. H. W. Li, L. Yang, J. C. Lam, Impact of climate change on energy use in the built environment in different climate zones - A review, Energy 42 (1) (2012) 103-112. [6] B. Qolomany, A. Al-Fuqaha, A. Gupta, D. Benhaddou, S. Alwajidi, J. Qadir, A. C. Fong, Leveraging Machine Learning and Big Data for Smart Buildings: A Comprehensive Survey, Computing Research Repository (CoRR). [7] A. Gupta, Y. Badr, A. Negahban, R. G. Qiu, Data for: Energy-Efficient Heating Control for Smart Buildings with Deep Reinforcement Learning, Mendeley Data, v1,doi:10.17632/v5c8v6bk8w.1. [8] Y. Fu Z. Li H. Zhang P. Xu Using support vector machine to predict next day electricity load of public buildings with sub-metering devices Proc. Eng. 121 2015 1016 1022 Y. Fu, Z. Li, H. Zhang, P. Xu, Using support vector machine to predict next day electricity load of public buildings with sub-metering devices, Procedia Engineering 121 (2015) 1016-1022. [9] K. Mary Reena A.T. Mathew L. Jacob A flexible control strategy for energy and comfort aware HVAC in large buildings Build. Environ. 145 2018 330 342 K. Mary Reena, A. T. Mathew, L. Jacob, A flexible control strategy for energy and comfort aware HVAC in large buildings, Building and Environment 145 (2018) 330-342. [10] J. Ahn S. Cho Anti-logic or common sense that can hinder machine's energy performance: energy and comfort control models based on artificial intelligence responding to abnormal indoor environments Appl. Energy 204 2017 117 130 J. Ahn, S. Cho, Anti-logic or common sense that can hinder machine\u2019s energy performance: Energy and comfort control models based on artificial intelligence responding to abnormal indoor environments, Applied Energy 204 (2017) 117-130. [11] T. Chaudhuri Y.C. Soh H. Li L. Xie Machine learning based prediction of thermal comfort in buildings of equatorial Singapore IEEE International Conference on Smart Grid and Smart Cities (ICSGSC) 2017 72 77 T. Chaudhuri, Y. C. Soh, H. Li, L. Xie, Machine learning based prediction of thermal comfort in buildings of equatorial Singapore, in: IEEE International Conference on Smart Grid and Smart Cities (ICSGSC), 2017, pp. 72-77. [12] D. Popa F. Pop C. Serbanescu A. Castiglione Deep learning model for home automation and energy reduction in a smart home environment platform Neural Comput. Appl. 31 5 2019 1317 1337 D. Popa, F. Pop, C. Serbanescu, A. Castiglione, Deep learning model for home automation and energy reduction in a smart home environment platform, Neural Computing and Applications 31 (5) (2019) 1317-1337. [13] Y. Li Z. Yan S. Chen X. Xu C. Kang Operation strategy of smart thermostats that self-learn user preferences IEEE Transac. Smart Grid 10 5 2019 5770 5780 Y. Li, Z. Yan, S. Chen, X. Xu, C. Kang, Operation strategy of smart thermostats that self-learn user preferences, IEEE Transactions on Smart Grid 10 (5) (2019) 5770-5780. [14] J.R. Vázquez-Canteli Z. Nagy Reinforcement learning for demand response: a review of algorithms and modeling techniques Appl. Energy 235 2019 1072 1089 J. R. Vazquez-Canteli, Z. Nagy, Reinforcement learning for demand response: A review of algorithms and modeling techniques, Applied Energy 235 (2019) 1072-1089. [15] Z. Cheng Q. Zhao F. Wang Y. Jiang L. Xia J. Ding Satisfaction based Q-learning for integrated lighting and blind control Energy Build. 127 2016 43 55 Z. Cheng, Q. Zhao, F. Wang, Y. Jiang, L. Xia, J. Ding, Satisfaction based Q-learning for integrated lighting and blind control, Energy and Buildings 127 (2016) 43-55. [16] J. Brusey D. Hintea E. Gaura N. Beloe Reinforcement learning-based thermal comfort control for vehicle cabins Mechatronics 50 2018 413 421 J. Brusey, D. Hintea, E. Gaura, N. Beloe, Reinforcement learning-based thermal comfort control for vehicle cabins, Mechatronics 50 (2018) 413-421. [17] Y. Chen L. Norford H. Samuelson A. Malkawi Optimal control of HVAC and window systems for natural ventilation through reinforcement learning Energy Build. 169 2018 195 205 Y. Chen, L. Norford, H. Samuelson, A. Malkawi, Optimal control of HVAC and window systems for natural ventilation through reinforcement learning, Energy and Buildings 169 (2018) 195-205. [18] P. Fazenda K. Veeramachaneni P. Lima U.-M. O'Reilly Using reinforcement learning to optimize occupant comfort and energy usage in HVAC systems J. Ambient Intell. Smart Environ. 6 6 2014 675 690 P. Fazenda, K. Veeramachaneni, P. Lima, U.-M. O\u2019Reilly, Using reinforcement learning to optimize occupant comfort and energy usage in HVAC systems, J. Ambient Intell. Smart Environ. 6 (6) (2014) 675-690. [19] T. Wei Y. Wang Q. Zhu Deep reinforcement learning for building HVAC control Proceedings of the 54th Annual Design Automation Conference 2017 ACM Press 1 6 T. Wei, Y. Wang, Q. Zhu, Deep reinforcement learning for building HVAC control, in: Proceedings of the 54th Annual Design Automation Conference, ACM Press, 2017, pp. 1-6. [20] G. Gao, J. Li, Y. Wen, Energy-efficient Thermal Comfort Control in Smart Buildings via Deep Reinforcement Learning, Computing Research Repository (CoRR). [21] A. Nagy, H. Kazmi, F. Cheaib, J. Driesen, Deep Reinforcement Learning for Optimal Control of Space Heating, Computing Research Repository (CoRR). [22] R.S. Sutton A.G. Barto Reinforcement Learning: an Introduction, Adaptive Computation and Machine Learning 1998 MIT Press Cambridge, Mass R. S. Sutton, A. G. Barto, Reinforcement Learning: An Introduction, Adaptive Computation and Machine Learning, MIT Press, Cambridge, Mass, 1998. [23] T. Hastie R. Tibshirani J. Friedman The Elements of Statistical Learning: Data Mining, Inference, and Prediction second ed. Springer Series in Statistics 2009 Springer-Verlag second ed. T. Hastie, R. Tibshirani, J. Friedman, The elements of statistical learning: Data mining, inference, and prediction, Second Edition, 2nd Edition, Springer Series in Statistics, Springer-Verlag, 2009. [24] J. Lapalu K. Bouchard A. Bouzouane B. Bouchard S. Giroux Unsupervised mining of activities for smart home prediction Proc. Comput. Sci. 19 2013 503 510 J. Lapalu, K. Bouchard, A. Bouzouane, B. Bouchard, S. Giroux, Unsupervised mining of activities for smart home prediction, Procedia Computer Science 19 (2013) 503-510. [25] L.P. Kaelbling M.L. Littman A.W. Moore Reinforcement learning: a survey J. Artif. Intell. Res. 4 1996 237 285 L. P. Kaelbling, M. L. Littman, A. W. Moore, Reinforcement learning: A survey, Journal of Artificial Intelligence Research 4 (1996) 237-285. [26] V. Mnih K. Kavukcuoglu D. Silver A.A. Rusu J. Veness M.G. Bellemare A. Graves M. Riedmiller A.K. Fidjeland G. Ostrovski S. Petersen C. Beattie A. Sadik I. Antonoglou H. King D. Kumaran D. Wierstra S. Legg D. Hassabis Human-level control through deep reinforcement learning Nature 518 7540 2015 529 533 10.1038/nature14236 http://www.nature.com/articles/nature14236 V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, D. Hassabis, Human-level control through deep reinforcement learning, Nature 518 (7540) (2015) 529-533. doi:10.1038/nature14236. URL http://www.nature.com/articles/nature14236 [27] D. Rolnick, A. Ahuja, J. Schwarz, T. P. Lillicrap, G. Wayne, Experience Replay for Continual Learning, Computing Research Repository (CoRR). [28] J. Fan, Z. Wang, Y. Xie, Z. Yang, A Theoretical Analysis of Deep Q-Learning, Computing Research Repository (CoRR). [29] M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar, D. Silver, Rainbow: Combining Improvements in Deep Reinforcement Learning, Computing Research Repository (CoRR). [30] Thermal model of a house - MATLAB & simulink Available at: https://www.mathworks.com/help/simulink/slref/thermal-model-of-a-house.html accessed: 2019-10-20 Thermal Model of a House - MATLAB & Simulink, Available at https://www.mathworks.com/help/simulink/slref/thermal-model-of-a-house.html, accessed: 2019-10-20. [31] R. Durham Thermal Environmental Conditions for Human Occupancy 2004 Ashrae R. Durham, Thermal Environmental Conditions for Human Occupancy., Ashrae, 2004. [32] K. Hornik Approximation capabilities of multilayer feedforward networks Neural Netw. 4 2 1991 251 257 10.1016/0893-6080(91)90009-T K. Hornik, Approximation capabilities of multilayer feedforward networks, Neural Networks 4 (2) (1991) 251-257. doi:10.1016/0893-6080(91)90009-T. [33] Pennsylvania state climatologist Available at: http://www.climate.psu.edu/ accessed: 2019-10-20 Pennsylvania State Climatologist, Available at http://www.climate.psu.edu/, accessed: 2019-10-20. [34] D. Justus, J. Brennan, S. Bonner, A. S. McGough, Predicting the Computational Cost of Deep Learning Models, Computing Research Repository (CoRR). [35] E. Mocanu D.C. Mocanu P.H. Nguyen A. Liotta M.E. Webber M. Gibescu J.G. Slootweg On-line building energy optimization using deep reinforcement learning IEEE Transac. Smart Grid 10 4 2019 3698 3708 E. Mocanu, D. C. Mocanu, P. H. Nguyen, A. Liotta, M. E. Webber, M. Gibescu, J. G. Slootweg, On-line building energy optimization using deep reinforcement learning, IEEE Transactions on Smart Grid 10 (4) (2019) 3698-3708. [36] Z. Zhang A. Chong Y. Pan C. Zhang K.P. Lam Whole building energy model for HVAC optimal control: a practical framework based on deep reinforcement learning Energy Build. 199 2019 472 490 Z. Zhang, A. Chong, Y. Pan, C. Zhang, K. P. Lam, Whole building energy model for HVAC optimal control: A practical framework based on deep reinforcement learning, Energy and Buildings 199 (2019) 472-490. [37] A. Pallikere, R. Qiu, P. Delgoshaei, A. Negahban, Incorporating occupancy data in scheduling building equipment: a simulation optimization framework, Energy Build.10.1016/j.enbuild.2019.109655.",
    "scopus-id": "85096546025",
    "coredata": {
        "eid": "1-s2.0-S2352710220333726",
        "dc:description": "Buildings account for roughly 40% of the total energy consumption in the world, out of which heating, ventilation, and air conditioning are the major contributors. Traditional heating controllers are inefficient due to lack of adaptability to dynamic conditions such as changing user preferences and outside temperature patterns. Therefore, it is necessary to design energy-efficient controllers that can improvise occupant thermal comfort (deviation from setpoint temperature) while reducing energy consumption. This research presents a Deep Reinforcement Learning (DRL)-based heating controller to improve thermal comfort and minimize energy costs in smart buildings. We perform extensive simulation experiments using real-world outside temperature data. The results show that the DRL-based smart controller outperforms a traditional thermostat controller by improving thermal comfort between 15% and 30% and reducing energy costs between 5% and 12% in the simulated environment. A second set of experiments is then performed for the case of multiple buildings, each having its own heating equipment. The performance is compared when the buildings are controlled centrally (using a single DRL-based controller) versus decentralized control, where each heater is controlled independently and has its own DRL-based controller. We observe that as the number of buildings and differences in their setpoint temperatures increase, decentralized control performs better than a centralized controller. The results have practical implications for heating control, especially in areas with multiple buildings such as residential complexes with multiple houses.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2021-02-28",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S2352710220333726",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Gupta, Anchal"
            },
            {
                "@_fa": "true",
                "$": "Badr, Youakim"
            },
            {
                "@_fa": "true",
                "$": "Negahban, Ashkan"
            },
            {
                "@_fa": "true",
                "$": "Qiu, Robin G."
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S2352710220333726"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S2352710220333726"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S2352-7102(20)33372-6",
        "prism:volume": "34",
        "articleNumber": "101739",
        "prism:publisher": "Elsevier Ltd.",
        "dc:title": "Energy-efficient heating control for smart buildings with deep reinforcement learning",
        "prism:copyright": "© 2020 Elsevier Ltd. All rights reserved.",
        "openaccess": "0",
        "prism:issn": "23527102",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "Deep reinforcement learning"
            },
            {
                "@_fa": "true",
                "$": "Simulation"
            },
            {
                "@_fa": "true",
                "$": "Occupant thermal comfort"
            },
            {
                "@_fa": "true",
                "$": "Heating controller"
            },
            {
                "@_fa": "true",
                "$": "HVAC"
            }
        ],
        "openaccessArticle": "false",
        "prism:publicationName": "Journal of Building Engineering",
        "openaccessSponsorType": null,
        "prism:pageRange": "101739",
        "pubType": "fla",
        "prism:coverDisplayDate": "February 2021",
        "prism:doi": "10.1016/j.jobe.2020.101739",
        "prism:startingPage": "101739",
        "dc:identifier": "doi:10.1016/j.jobe.2020.101739",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "204",
            "@width": "357",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "27141",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "320",
            "@width": "498",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-fx1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "78158",
            "@ref": "fx1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "328",
            "@width": "624",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "47514",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "200",
            "@width": "357",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "31492",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "261",
            "@width": "357",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr10.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "46464",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "265",
            "@width": "357",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr9.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "36880",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "604",
            "@width": "357",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr11.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "89832",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "269",
            "@width": "357",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr8.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "55078",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "606",
            "@width": "357",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr12.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "81527",
            "@ref": "gr12",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "262",
            "@width": "713",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "48363",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "532",
            "@width": "379",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr13.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "69055",
            "@ref": "gr13",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "150",
            "@width": "357",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "33661",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "162",
            "@width": "802",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr14.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "65858",
            "@ref": "gr14",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "152",
            "@width": "357",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "28476",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "163",
            "@width": "802",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr15.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "61624",
            "@ref": "gr15",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "50",
            "@width": "179",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "20664",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "159",
            "@width": "802",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr16.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "48481",
            "@ref": "gr16",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "125",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "8940",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "141",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-fx1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "18585",
            "@ref": "fx1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "115",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "13144",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "123",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "10935",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "160",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr10.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "16144",
            "@ref": "gr10",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "162",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr9.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "11780",
            "@ref": "gr9",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "97",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr11.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "13725",
            "@ref": "gr11",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "217",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr8.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "20239",
            "@ref": "gr8",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "96",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr12.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "12795",
            "@ref": "gr12",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "80",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "11315",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "116",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr13.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "13744",
            "@ref": "gr13",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "92",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "12120",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "44",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr14.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "12460",
            "@ref": "gr14",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "93",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "9318",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "44",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr15.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "12892",
            "@ref": "gr15",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "61",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "8740",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "43",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr16.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "9293",
            "@ref": "gr16",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "903",
            "@width": "1580",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "85004",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1419",
            "@width": "2205",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-fx1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "502861",
            "@ref": "fx1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1452",
            "@width": "2764",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "222733",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "885",
            "@width": "1582",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "118592",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1159",
            "@width": "1583",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr10_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "164474",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1174",
            "@width": "1583",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr9_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "132006",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2677",
            "@width": "1583",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr11_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "486695",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1191",
            "@width": "1580",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr8_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "209703",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2685",
            "@width": "1583",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr12_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "481478",
            "@ref": "gr12",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1160",
            "@width": "3158",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "259994",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2356",
            "@width": "1678",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr13_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "380398",
            "@ref": "gr13",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "666",
            "@width": "1583",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "129358",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "719",
            "@width": "3551",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr14_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "369002",
            "@ref": "gr14",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "675",
            "@width": "1583",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "91127",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "721",
            "@width": "3551",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr15_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "324313",
            "@ref": "gr15",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "220",
            "@width": "795",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "36234",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "705",
            "@width": "3551",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-gr16_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "298030",
            "@ref": "gr16",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si33.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "46289",
            "@ref": "si33",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si35.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "80997",
            "@ref": "si35",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si23.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "15033",
            "@ref": "si23",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si37.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "107647",
            "@ref": "si37",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si25.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "29772",
            "@ref": "si25",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si19.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "64285",
            "@ref": "si19",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si34.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "14027",
            "@ref": "si34",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si14.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "83769",
            "@ref": "si14",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si9.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7141",
            "@ref": "si9",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si28.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "107038",
            "@ref": "si28",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si36.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11316",
            "@ref": "si36",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si16.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "21923",
            "@ref": "si16",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si7.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6232",
            "@ref": "si7",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si15.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "89998",
            "@ref": "si15",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si17.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "16976",
            "@ref": "si17",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si2.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "12758",
            "@ref": "si2",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si21.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "21934",
            "@ref": "si21",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si29.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2606",
            "@ref": "si29",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si32.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "48450",
            "@ref": "si32",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si22.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "88073",
            "@ref": "si22",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si30.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "68126",
            "@ref": "si30",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si31.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "53629",
            "@ref": "si31",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si12.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "20154",
            "@ref": "si12",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si24.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "41010",
            "@ref": "si24",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si6.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "96929",
            "@ref": "si6",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si38.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "54668",
            "@ref": "si38",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si18.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "62480",
            "@ref": "si18",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si26.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "16781",
            "@ref": "si26",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si3.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "10951",
            "@ref": "si3",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si20.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "19396",
            "@ref": "si20",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si4.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "14545",
            "@ref": "si4",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si27.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "89754",
            "@ref": "si27",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si1.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "77623",
            "@ref": "si1",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si5.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9955",
            "@ref": "si5",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si13.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8689",
            "@ref": "si13",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si8.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "22962",
            "@ref": "si8",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si11.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "82338",
            "@ref": "si11",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-si10.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "32034",
            "@ref": "si10",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2352710220333726-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "1154329",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85096546025"
    }
}}