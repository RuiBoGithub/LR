{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85095440588",
    "originalText": "serial JL 271429 291210 291702 291731 291787 291877 291878 291881 31 Applied Energy APPLIEDENERGY 2020-11-05 2020-11-05 2020-11-05 2020-11-05 2020-12-22T10:55:28 1-s2.0-S030626192031535X S0306-2619(20)31535-X S030626192031535X 10.1016/j.apenergy.2020.116117 S300 S300.1 FULL-TEXT 1-s2.0-S0306261920X00213 2024-01-01T13:23:58.349315Z 0 0 20210101 2021 2020-11-05T23:10:10.267756Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast highlightsabst nomenclature primabst ref 0306-2619 03062619 true 281 281 C Volume 281 51 116117 116117 116117 20210101 1 January 2021 2021-01-01 2021 Research Papers article fla © 2020 Published by Elsevier Ltd. INTELLIGENTMULTIZONERESIDENTIALHVACCONTROLSTRATEGYBASEDDEEPREINFORCEMENTLEARNING DU Y Nomenclature 1 Introduction 2 Multi-zone residential HVAC system control problem formulation 2.1 A brief introduction of the multi-zone HVAC system control problem 2.2 Mapping HVAC control problem to Markov decision process (MDP) 3 DDPG-based control strategy for multi-zone HVAC system 3.1 A brief review of deep reinforcement learning methods 3.2 Understanding the basic principles behind typical deep RL methods 3.3 Realizing continuous control of HVAC system with the DDPG 4 Case study 4.1 Simulation environment 4.2 Design of the DNN structure in deep RL 4.3 Performance of the continuous HVAC control method 5 Conclusion CRediT authorship contribution statement Acknowledgement References PEREZLOMBARD 2008 394 398 L COSTA 2013 310 316 A KOU 2020 X MA 2016 605 614 K ERDINC 2016 362 372 O WU 2017 3844 3856 X LIN 2016 186 197 Y YU 2017 1646 1659 L HAO 2016 774 783 H SILVER 2016 484 489 D LI 2018 76 84 F DU 2019 3303 3305 Y DU 2020 1066 1076 Y HUANG 2017 1513 1524 T WU 2019 454 466 Y HAN 2019 113708 X HUA 2019 598 609 H ROCCHETTA 2019 291 301 R KOU 2020 114772 P WEI 2019 T CLAESSENS 2016 3259 3269 B MOCANU 2018 3698 3708 E WANG 2017 46 63 Y ZHANG 2019 472 490 Z AHN 2019 61 74 K ZOU 2020 1 15 Z LU 2012 1263 1270 N CUI 2019 B LOADFLEXIBILITYANALYSISRESIDENTIALHVACWATERHEATINGCOMMERCIALREFRIGERATION MNIH 2015 529 533 V DUX2021X116117 DUX2021X116117XY 2022-11-05T00:00:00.000Z http://www.elsevier.com/open-access/userlicense/1.0/ https://vtw.elsevier.com/content/oragreement/10131 CHU_DOE publishAcceptedManuscriptIndexable https://vtw.elsevier.com/content/oragreement/10144 CHU_NSF publishAcceptedManuscriptIndexable 2021-11-05T00:00:00Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2020 Published by Elsevier Ltd. 2020-11-10T00:05:41.181Z http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/nlp S030626192031535X ERC ERC European Research Council http://data.elsevier.com/vocabulary/SciValFunders/501100000781 http://sws.geonames.org/6695072/ NSF EEC-1041877 ECCS-1809458 NSF National Science Foundation http://data.elsevier.com/vocabulary/SciValFunders/100000001 http://sws.geonames.org/6252001/ CURENT U.S. Department of Energy USDOE U.S. Department of Energy http://data.elsevier.com/vocabulary/SciValFunders/100000015 http://sws.geonames.org/6252001/ U.S. National Science Foundation DOE USDOE U.S. Department of Energy http://data.elsevier.com/vocabulary/SciValFunders/100000015 http://sws.geonames.org/6252001/ Buildings Technologies Program Office of Energy Efficiency and Renewable Energy EERE Office of Energy Efficiency and Renewable Energy http://data.elsevier.com/vocabulary/SciValFunders/100006134 http://sws.geonames.org/6252001/ The authors would like to acknowledge the support in part by the U.S. Department of Energy (DOE), including Office of Energy Efficiency and Renewable Energy under the Buildings Technologies Program, in part by CURENT which is an Engineering Research Center (ERC) funded by the U.S. National Science Foundation (NSF) and DOE under the NSF award EEC-1041877, and in part by the U.S. NSF award ECCS-1809458. item S0306-2619(20)31535-X S030626192031535X 1-s2.0-S030626192031535X 10.1016/j.apenergy.2020.116117 271429 2024-01-01T13:23:58.349315Z 2021-01-01 1-s2.0-S030626192031535X-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/MAIN/application/pdf/9a860829f129a9ba4213a051a2ddc7ea/main.pdf main.pdf pdf true 5817192 MAIN 14 1-s2.0-S030626192031535X-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/PREVIEW/image/png/240a59feefdebdc9cb364ff6a88c39ce/main_1.png main_1.png png 47436 849 656 IMAGE-WEB-PDF 1 1-s2.0-S030626192031535X-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr8/DOWNSAMPLED/image/jpeg/fdfb416667c74a1f28d6b9ed5cfbb1b4/gr8.jpg gr8 gr8.jpg jpg 23423 209 373 IMAGE-DOWNSAMPLED 1-s2.0-S030626192031535X-gr11.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr11/DOWNSAMPLED/image/jpeg/7bc0c0961ceae94cbaac383727be082e/gr11.jpg gr11 gr11.jpg jpg 65811 613 542 IMAGE-DOWNSAMPLED 1-s2.0-S030626192031535X-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr9/DOWNSAMPLED/image/jpeg/5e907c1756ba40895599aaaa8d3f2bd2/gr9.jpg gr9 gr9.jpg jpg 74091 607 542 IMAGE-DOWNSAMPLED 1-s2.0-S030626192031535X-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr2/DOWNSAMPLED/image/jpeg/88b03d51565f505e1a0f9733ddb6a145/gr2.jpg gr2 gr2.jpg jpg 58497 294 622 IMAGE-DOWNSAMPLED 1-s2.0-S030626192031535X-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr3/DOWNSAMPLED/image/jpeg/7142e901208a20c59caae684af1f315a/gr3.jpg gr3 gr3.jpg jpg 13705 207 373 IMAGE-DOWNSAMPLED 1-s2.0-S030626192031535X-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr1/DOWNSAMPLED/image/jpeg/70293a990aa90ef1641aa072a9279d14/gr1.jpg gr1 gr1.jpg jpg 35523 209 542 IMAGE-DOWNSAMPLED 1-s2.0-S030626192031535X-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr6/DOWNSAMPLED/image/jpeg/3471987484b9a6eb25bc2bc949f69695/gr6.jpg gr6 gr6.jpg jpg 81685 609 542 IMAGE-DOWNSAMPLED 1-s2.0-S030626192031535X-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr7/DOWNSAMPLED/image/jpeg/7ebf245bfc66c1dcd89d23dd3debf725/gr7.jpg gr7 gr7.jpg jpg 76192 594 542 IMAGE-DOWNSAMPLED 1-s2.0-S030626192031535X-gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr10/DOWNSAMPLED/image/jpeg/93409f69fda5d6355a28f7fb6495c92f/gr10.jpg gr10 gr10.jpg jpg 63727 618 542 IMAGE-DOWNSAMPLED 1-s2.0-S030626192031535X-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr4/DOWNSAMPLED/image/jpeg/89cdd7c112ffc0487a2cbb9a378e03e4/gr4.jpg gr4 gr4.jpg jpg 88031 620 542 IMAGE-DOWNSAMPLED 1-s2.0-S030626192031535X-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr5/DOWNSAMPLED/image/jpeg/edadd3dbf8378d29c91bdd5690bafee1/gr5.jpg gr5 gr5.jpg jpg 77676 615 542 IMAGE-DOWNSAMPLED 1-s2.0-S030626192031535X-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr8/THUMBNAIL/image/gif/c1e54874b0df8a86c3330daaf70f869b/gr8.sml gr8 gr8.sml sml 9001 122 219 IMAGE-THUMBNAIL 1-s2.0-S030626192031535X-gr11.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr11/THUMBNAIL/image/gif/ca27fc6bb950250c110d20dac6dda0a7/gr11.sml gr11 gr11.sml sml 7242 164 145 IMAGE-THUMBNAIL 1-s2.0-S030626192031535X-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr9/THUMBNAIL/image/gif/7d0227204b8afb56a0fbe53357580d61/gr9.sml gr9 gr9.sml sml 7779 164 146 IMAGE-THUMBNAIL 1-s2.0-S030626192031535X-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr2/THUMBNAIL/image/gif/4431959c353460d92eec9605e1a6e356/gr2.sml gr2 gr2.sml sml 12654 103 219 IMAGE-THUMBNAIL 1-s2.0-S030626192031535X-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr3/THUMBNAIL/image/gif/5810c20fb9a3e2793f10e5bf4f9884f6/gr3.sml gr3 gr3.sml sml 5102 121 219 IMAGE-THUMBNAIL 1-s2.0-S030626192031535X-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr1/THUMBNAIL/image/gif/d0880781ede6e2da5bb3477c64b84fc9/gr1.sml gr1 gr1.sml sml 7188 85 219 IMAGE-THUMBNAIL 1-s2.0-S030626192031535X-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr6/THUMBNAIL/image/gif/ed9f5f8fc7bda5a72676c5fb9233b8b3/gr6.sml gr6 gr6.sml sml 9964 164 146 IMAGE-THUMBNAIL 1-s2.0-S030626192031535X-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr7/THUMBNAIL/image/gif/0c4b49387c7e23dd0a43f2b4e07bcb1f/gr7.sml gr7 gr7.sml sml 8678 163 149 IMAGE-THUMBNAIL 1-s2.0-S030626192031535X-gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr10/THUMBNAIL/image/gif/cc9fb55f01684ad8dd7e2f3327ef2653/gr10.sml gr10 gr10.sml sml 7099 163 143 IMAGE-THUMBNAIL 1-s2.0-S030626192031535X-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr4/THUMBNAIL/image/gif/3507a17396cf6ed398ba24013678d14a/gr4.sml gr4 gr4.sml sml 8940 164 143 IMAGE-THUMBNAIL 1-s2.0-S030626192031535X-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr5/THUMBNAIL/image/gif/17e8ed3a5cdb9d9eb7b52c9021ad1878/gr5.sml gr5 gr5.sml sml 8725 163 144 IMAGE-THUMBNAIL 1-s2.0-S030626192031535X-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr8/HIGHRES/image/jpeg/c2dfe75010efacf7434b1c0137d5ea38/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 153233 925 1654 IMAGE-HIGH-RES 1-s2.0-S030626192031535X-gr11_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr11/HIGHRES/image/jpeg/81ee75ca0e111b1fdcb8d8b51daacd47/gr11_lrg.jpg gr11 gr11_lrg.jpg jpg 314682 2716 2402 IMAGE-HIGH-RES 1-s2.0-S030626192031535X-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr9/HIGHRES/image/jpeg/b9ae2cbd18b606a82d4f804db842de92/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 351231 2692 2402 IMAGE-HIGH-RES 1-s2.0-S030626192031535X-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr2/HIGHRES/image/jpeg/a6633feb3b9ae0a4155b542ae54c95da/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 371875 1301 2756 IMAGE-HIGH-RES 1-s2.0-S030626192031535X-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr3/HIGHRES/image/jpeg/141fab521f8caf8f5d1f1dd8deb851b2/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 105680 917 1654 IMAGE-HIGH-RES 1-s2.0-S030626192031535X-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr1/HIGHRES/image/jpeg/076c33ac215a146f09e31c17c9525eec/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 266263 928 2402 IMAGE-HIGH-RES 1-s2.0-S030626192031535X-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr6/HIGHRES/image/jpeg/f31e888c81ac53602de515e3674a5229/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 407591 2698 2402 IMAGE-HIGH-RES 1-s2.0-S030626192031535X-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr7/HIGHRES/image/jpeg/ae1e1c28b8d21b62a66c12f2669c2ef1/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 366612 2631 2402 IMAGE-HIGH-RES 1-s2.0-S030626192031535X-gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr10/HIGHRES/image/jpeg/4590b9cf648e89ad77596deb4de19682/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 313728 2740 2402 IMAGE-HIGH-RES 1-s2.0-S030626192031535X-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr4/HIGHRES/image/jpeg/a7af47d3d751a60b8a95c66e5d9c85b0/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 432683 2748 2402 IMAGE-HIGH-RES 1-s2.0-S030626192031535X-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/gr5/HIGHRES/image/jpeg/bf15ba4b108ce1973c401375a899b38d/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 371647 2724 2402 IMAGE-HIGH-RES 1-s2.0-S030626192031535X-si1.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/STRIPIN/image/svg+xml/64e1d5d59fb17a9caee612e31557047e/si1.svg si1 si1.svg svg 42048 ALTIMG 1-s2.0-S030626192031535X-si1000.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/STRIPIN/image/svg+xml/d6bd19ca6e05161852c916d5b9eaf32e/si1000.svg si1000 si1000.svg svg 4947 ALTIMG 1-s2.0-S030626192031535X-si2.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/STRIPIN/image/svg+xml/6f3b49b9563f1681724536f2d37f101b/si2.svg si2 si2.svg svg 32576 ALTIMG 1-s2.0-S030626192031535X-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/STRIPIN/image/svg+xml/bd68c7d0ed85a4d43ea3d5003aa66635/si3.svg si3 si3.svg svg 33676 ALTIMG 1-s2.0-S030626192031535X-si4.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/STRIPIN/image/svg+xml/6952f589f526f59bd86765328157665f/si4.svg si4 si4.svg svg 8618 ALTIMG 1-s2.0-S030626192031535X-si5.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/STRIPIN/image/svg+xml/ccf4d8fd0113e422eeb2bef099ae6746/si5.svg si5 si5.svg svg 28878 ALTIMG 1-s2.0-S030626192031535X-si6.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/STRIPIN/image/svg+xml/a003512d275c372a1da0743f5dcaf326/si6.svg si6 si6.svg svg 27919 ALTIMG 1-s2.0-S030626192031535X-si7.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S030626192031535X/STRIPIN/image/svg+xml/1e63982b5b865d4f2e8fde63288fee47/si7.svg si7 si7.svg svg 7785 ALTIMG 1-s2.0-S030626192031535X-am.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:10RC600BB5D/MAIN/application/pdf/89e6c122a6fe71c97bd568e802b28f47/am.pdf am am.pdf pdf false 1708561 AAM-PDF APEN 116117 116117 S0306-2619(20)31535-X 10.1016/j.apenergy.2020.116117 Fig. 1 DNN structure for function approximation in RL. Fig. 2 Multi-zone HVAC control framework with DDPG. Fig. 3 Convergence of different deep RL methods. Fig. 4 Setpoint control strategy based on DDPG for 10 test days (top: zone 1; bottom: zone2). Fig. 5 Setpoint control strategy based on DQN for 10 test days (top: zone 1; bottom: zone2). Fig. 6 Setpoint control strategy from the rule-based case for 10 test days (top: zone 1; bottom: zone2). Fig. 7 Setpoint control strategy from the fixed setpoint case for 10 test days (top: zone 1; bottom: zone2). Fig. 8 Comparisons of cost and penalty from three control methods. Fig. 9 Setpoint control strategy based on DDPG under PJM price for 10 test days (top: zone 1; bottom: zone2). Fig. 10 Setpoint control strategy based on DQN under PJM price for 10 test days (top: zone 1; bottom: zoomed part of zone 1). Fig. 11 Setpoint control strategy in the fixed setpoint case under PJM price for 10 test days (top: zone 1; bottom: zoomed part of zone 1). Table 1 Daily user comfort level. Time period 0:00 \u2013 6:00 6:00 \u2013 12:00 12:00 \u2013 18:00 18:00 \u2013 24:00 Tlower (°C) 18 17 18 19 Table 2 DNN structure applied in DDPG and DQN algorithms. Algorithm DDPG DQN critic network actor network Size of input [1,7] [1,5] [1,5] No. of hidden layers 2 2 2 Size of each hidden layer [7,20], [20,10] [5,20], [20,10] [5,20], [20,10] Size of output [1] [2] [25] Activation function for the hidden layer ReLU ReLU ReLU Optimizer Adam Adam Adam Learning rate (η) 0.001 0.01 0.01 Discount factor (γ) 0.99 \u2013 0.99 Batch size 48 Weights of the reward ωc : 10, ωp : 1 Table 3 Test results of different HVAC control methods. Control method DDPG DQN Rule-based Fixed setpoint Total cost ($) 55.21 65.03 39.08 71.48 Temperature violation (min) 48 230 2617 0 Average temperature violation (°C) 0.13 0.93 1.85 0 Table 4 Comparison of optimization results for different building models. Building index DDPG Rule-based Fixed setpoint Cost ($) Temperature violation (min) Cost ($) Temperature violation (min) Cost ($) Temperature violation (min) 1 42.22 31 27.78 1296 57.98 0 2 44.13 41 29.22 1586 60.13 0 3 52.14 45 36.51 2347 68.52 0 4 59.66 101 43.94 3364 75.61 0 5 45.84 41 31.30 1879 62.91 0 6 42.49 39 27.68 1398 59.06 0 7 37.47 24 23.51 1012 53.42 0 8 61.21 81 45.42 3520 76.44 0 9 35.34 25 21.98 818 49.90 0 10 43.19 59 28.41 1323 58.46 0 Table 5 Test results of different control methods (under PJM price). Control method DDPG DQN Fixed setpoint Total cost ($) 32.90 31.80 32.71 Temperature violation (min) 0 222 31 Average temperature violation (°C) 0 1.00 0.27 ☆ DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-public-access-plan.) Intelligent multi-zone residential HVAC control strategy based on deep reinforcement learning Yan Du Investigation Methodology Writing - original draft a Helia Zandi Conceptualization Funding acquisition Methodology Project administration Supervision Writing - review & editing b Olivera Kotevska Investigation Methodology Writing - review & editing b Kuldeep Kurte Investigation Methodology Writing - review & editing b Jeffery Munk Methodology Software Writing - review & editing b Kadir Amasyali Investigation Methodology Writing - review & editing b Evan Mckee Investigation Methodology Writing - review & editing a Fangxing Li Funding acquisition Methodology Project administration Writing - review & editing a \u204e a Dept. of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA Dept. of Electrical Engineering and Computer Science University of Tennessee Knoxville USA Dept. of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA b Oak Ridge National Laboratory, Oak Ridge, TN, USA Oak Ridge National Laboratory Oak Ridge TN USA Oak Ridge National Laboratory, Oak Ridge, TN, USA \u204e Corresponding author. Residential heating, ventilation, and air conditioning (HVAC) has been considered as an important demand response resource. However, the optimization of residential HVAC control is no trivial task due to the complexity of the thermal dynamic models of buildings and uncertainty associated with both occupant-driven heat loads and weather forecasts. In this paper, we apply a novel model-free deep reinforcement learning (RL) method, known as the deep deterministic policy gradient (DDPG), to generate an optimal control strategy for a multi-zone residential HVAC system with the goal of minimizing energy consumption cost while maintaining the users\u2019 comfort. The applied deep RL-based method learns through continuous interaction with a simulated building environment and without referring to any prior model knowledge. Simulation results show that compared with the state-of-art deep Q network (DQN), the DDPG-based HVAC control strategy can reduce the energy consumption cost by 15% and reduce the comfort violation by 79%; and when compared with a rule-based HVAC control strategy, the comfort violation can be reduced by 98%. In addition, experiments with different building models and retail price models demonstrate that the well-trained DDPG-based HVAC control strategy has high generalization and adaptability to unseen environments, which indicates its practicability for real-world implementation. Keywords Actor-critic learning Demand response Deep deterministic policy gradient (DDPG) Deep reinforcement learning (deep RL) Multi-zone residential HVAC Nomenclature Tout(t) Outdoor temperature at time step t Tin,z (t) Indoor temperature for room zone z at time step t Tlower (t) Lower bound of the user comfort level at time step t λretail(t) Retail price at time step t Setptz(t) Setpoint for room zone z at time step t EHVAC (t) Power consumption of HVAC system at time step t Δt Control interval of the HVAC system cpenalty (t) Penalty for user comfort violation at time step t θQ , θπ Neural network parameters of the critic network and the actor network in the DDPG algorithm Q(s,a; θQ ) Action-value of the state-action pair (s,a) under the current critic network θQ π(s; θπ ) Control policy under the current actor network θπ qtarget (t) Target action-value for updating the critic network 1 Introduction In the worldwide scope, buildings account for 40% of total primary energy consumption and 30% of all CO2 emissions, among which a large portion can be attributed to thermal comfort overhead [1,2]. Therefore, it is important to study the effective energy management of building demand to achieve economic and environmental benefits. The heating, ventilation, and air conditioning (HVAC) system is currently the most widely used device for maintaining building thermal comfort. It also serves as an important demand response resource for peak load reduction and stabilizing system-wide operation via proper demand-side energy management strategies [3]. In the literature, there are many studies focusing on optimizing HVAC control strategies for improving energy efficiency. In [4], the energy management of HVAC systems is modelled under load forecast errors, where a primal\u2013dual algorithm is applied to seek the optimal operating states of HVAC for the consumer, and the pricing strategy for the energy provider. In another work, a regression approach is applied for temperature forecast in day-ahead scheduling of responsive residential HVAC demand [5]. The authors in [6,7] discuss the potential of using the HVAC system to provide primary frequency regulation to the bulk system via a hierarchical control strategy. A Lyapunov optimization technique is introduced in [8] for HVAC load control without needing to estimate the uncertain system factors such as price and temperature. A distributed transactive control market mechanism for commercial building HVAC systems is presented in [9] to demonstrate the effectiveness of HVAC in peak shaving and load shifting. All the above methods can be categorized as model-based methods, where the detailed thermal dynamics of the HVAC with consideration of ambient environment effects need to be modelled, along with the requirement of analytical solution toolboxes for practical runtime control. The model-based methods may suffer from measurement errors (e.g., building model inaccuracy), as well as computational inefficiency, since the building and equipment models must be tailored to a specific building to achieve accurate results. This represents a serious challenge for widespread deployment of model-based methods. Meanwhile, there has been significant development in machine learning technologies such as deep learning and reinforcement learning evidenced by the achievement of AlphaGo [10]. In power systems research community, general vision of new research directions related to machine learning is discussed in [11] with a number of research applications [12,13]. In industrial applications, AI-based implementation starts to be deployed in real control centers such as [14], which is the first reported control-room application of AI-driven distributed feature selection for a large, real power grid. More specifically, in recent years, deep reinforcement learning (RL), which is a combination of a deep neural network (DNN) and RL, has attracted broad attention in solving high-dimensional control and optimization problems with tremendous complexity. A double Q learning method [15] and a continuous deep deterministic policy gradient (DDPG) method [16] have been applied for optimizing the energy management strategies of hybrid electric vehicles, respectively. In [17], the asynchronous advantage actor-critic is employed to find the economic operation schedules of multiple distributed energy resources within an energy Internet. In [18], a deep Q learning method is designed for supporting the maintenance decision-making of the bulk power system. Given the potential operation constraints encountered during the implementation of deep RL-based control actions, a safe deep RL method is explored in [19] to obtain the optimal control scheme of the active distribution network with the consideration of voltage level limits, which introduces a safe layer on top of the conventional actor network to avoid any possible violations of the voltage constraints. With specific respect to the HVAC system control problem, there have also been some pioneering works in the literature focusing on utilizing the powerful deep RL approach to achieve higher energy efficiency and economic efficiency. In [20], a deep Q network (DQN) is constructed for coordinated control of joint datacenter and HVAC load, in which the neural network is utilized to estimate the Q value of a state-action pair. In [21], a convolutional neural network (CNN) is deployed as the approximator of the state-action value function to better capture the spatial and temporal correlations within the input state data with its convolutional operation. A deep policy gradient (DPG) method is investigated in [22] for controlling multiple responsive demands including ACs, electric vehicles and dishwashers. In [23], an actor-critic method is applied for optimizing the thermal comfort and energy consumption of HVAC. In [24], a practical HVAC control framework based on advantage actor critic is established for a whole building energy model. In [25], the DQN is applied to achieve optimal control balancing between different HVAC systems. All the above research works have demonstrated the effectiveness of the applied deep RL methods in optimizing the HVAC thermal control strategy compared with the designed benchmarks. However, the majority of the existing researches treat the continuous control actions of the HVAC system, such as HVAC setpoint or air flow rate, in a discretized way to narrow down the search space. Discretization can achieve satisfying performance when the granularity is low or without the combination of action spaces. However, it encounters the issue of exponential explosion when the action space is high-dimensional, for example, multiple room zones in the case of HVAC control. As a result, more simulations are needed for training the deep RL methods and the algorithm performance decreases. In [26], the authors adopt the DDPG method to realize the continuous thermal control of HVAC without discretization. However, this research work still focuses on single-zone HVAC control, which has been previously addressed by the above-mentioned discretion methods. In addition, the method applied is only compared with other RL methods, and no benchmark cases are designed to verify the optimality and the generalization of the obtained control strategy. In [27], a multi-agent deep RL method with an attention mechanism is applied to minimize the energy costs of an HVAC system in a multi-zone commercial building, where a set of actor and critic networks are designed for each zone, and they are updated in parallel during the training. While this research work provides some inspiring insights, one concern is that in the proposed algorithm, the number of neural networks needing training will grow with the number of zones, which could cause excessive computational burden. In [28], the long-short-term-memory (LSTM) recurrent neural network is combined with the DDPG to better simulate the real-world operation of multiple air handling units (AHUs), where a deep RL agent is designed for each AHU to control a separate section of the building. The same concern occurs regarding the number of RL agents and the growing computational cost. Motivated by the above concerns, in this paper, we also apply the DDPG method for optimizing the continuous thermal control strategy of residential HVAC. The main contributions of this work, as compared with the existing research, are summarized as follows: \u2022 We apply the DDPG RL method to optimize the continuous control of multi-zone residential HVAC. The multi-zone residential HVAC control involves more complex thermal dynamics and environment uncertainties, and a high-dimensional action space, which requires more delicate problem formulation including the definitions of state, action, and reward during the learning process; \u2022 We conduct a comprehensive comparison between the applied DDPG method and the widely-used DQN method to demonstrate the effectiveness of the former in dealing with the continuous action space, which is a more common case in many real-world situations; we also design benchmark cases without RL to prove that the applied DDPG can achieve higher economic benefits while maintaining user comfort; \u2022 We verify that the well-trained deep RL method has obtained high generalization and robustness, and can adapt to new environment with different price signals and physical conditions to provide the optimal HVAC control strategy. The rest of the paper is organized as follows. The HVAC control problem formulation is introduced in Section 2; in Section 3, the two representative deep RL methods, the DQN and DPG methods are first briefly reviewed, followed by a detailed explanation of the DDPG method, which is an extension of the former two; the simulation results of the DDPG method are presented in Section 4, plus comparison with the DQN and benchmark cases; finally, Section 5 concludes the paper. 2 Multi-zone residential HVAC system control problem formulation 2.1 A brief introduction of the multi-zone HVAC system control problem In this study, we consider a residential building with multiple zones. The indoor temperature of each zone can be controlled by adjusting the setpoint of the HVAC system. The HVAC system can work in various modes including \u201cCooling\u201d, \u201cHeating\u201d and \u201cAuto\u201d. The \u201cAuto\u201d mode means that the HVAC system can automatically switch between cooling and heating according to the indoor temperature and the assigned setpoint. Whenever there is a difference between the indoor temperature and the setpoint, the HVAC system will be automatically turned on to push the indoor temperature near to the setpoint to maintain user comfort. Without losing generality, in this work, we will focus on the case when all zones need heating. The goal of controlling the HVAC system is to minimize the energy cost while keeping the indoor temperature within the user comfort band. 2.2 Mapping HVAC control problem to Markov decision process (MDP) In this subsection, we will formulate the above multi-zone residential HVAC control problem as a Markov Decision Process (MDP), which will later be solved by a model-free deep RL-based algorithm in Section 3. According to the simplified thermal dynamics model of HVAC in [29], the indoor temperature at the current time interval is only related to the previous state parameters such as the indoor temperature at the previous time interval, and is not affected by indoor temperature at any other time intervals. Therefore, the HVAC control problem can be regarded as a finite Markov process and be solved using the RL method. An MDP is composed of four essential elements: state (s), action (a), state transition probability (p), and reward (r). In the context of a multi-zone residential HVAC control problem, the four elements are defined as follows: \u2022 State: 1) current outdoor temperature Tout(t); 2) current indoor temperature Tin,z(t) for the all the zones z; 3) the lower bound of the user comfort level Tlower(t); 4) retail price λretail(t), where t is the current time step. Note that the state parameters include the lower bound of the user comfort level, which changes along with the time. This is because we assume that the HVAC users have a time-variant comfort preference. This is reasonable since during the daily work hours when no one is at home, the comfort range of the indoor temperature can be lowered to save the energy cost. The comfort range can be brought back during the off-work hours when the house is occupied. The state parameters also include the current retail price to realize the pre-heating effect of HVAC. Pre-heating means setting the setpoint of the HVAC at a relatively high value when the retail price of energy is low to heat up the indoor temperature in advance, thus avoiding excessive energy consumption when cold outdoor temperatures occurs, when the retail price of energy is higher. \u2022 Action: the setpoint Setptz(t) for the zone z; The HVAC setpoint in each zone is a continuous variable. Given the setpoint, the on/off status of the HVAC unit with a thermostat at each zone obeys the following control logic: (1) HVAC s t a t u s = 1 , i f T in ( t ) < s e t p o i n t - d e a d b a n d 0 , i f T in ( t ) > s e t p o i n t r e m a i n a t t h e c u r r e n t s t a t u s , e l s e w i s e The HVAC model considered in this paper is only utilized for heating. In Eq. (1), the deadband is a small temperature span, in which the thermostat will not change its on/off status to prevent short cycles. It can be observed in Eq. (1) that if the indoor temperature is above the setpoint, the HVAC will remain off; otherwise, the HVAC will be started automatically to heat the room to maintain the user comfort. \u2022 Reward: the energy consumption cost plus the comfort violation cost for the control interval, which is defined as follows: (2) r ( t ) = - ω c ∑ t \u2032 = t - Δ t t λ retail ( t \u2032 ) E HVAC ( t \u2032 ) - ω p ∑ t \u2032 = t - Δ t t c penalty ( t \u2032 ) In Eq. (2), the first term is the energy cost of the HVAC system, where λretail (t\u2019) is the retail price, EHAVC (t\u2019) is the power consumption, and Δt is the control interval; the second term is the penalty for user comfort violation, which is calculated as follows: (3) c penalty ( t \u2032 ) = 1 , f o r T in ( t \u2032 ) < T lower ( t \u2032 ) - T th 0 , e l s e w i s e In Eq. (3), Tth is a threshold with a small value. The temperature violation is not counted if the magnitude of the violation is smaller than Tth . Given the existence of the deadband within the HVAC system, it is not possible to always keep the indoor temperature at the exact setpoint. The threshold allows for some deviations of the indoor temperature. Because the reward encloses both the energy cost and the penalty, which leads to a multi-objective function, weight factors are added to the two objectives, which are represented by ωc and ωp in Eq. (2). The final objective of HVAC thermal control is to minimize the total energy consumption cost plus the penalty over the entire control cycle, which can be written as the cumulative sum of r(t): ∑ t = 1 N T r ( t ) . Therefore, a far-sighted control strategy is needed to prevent against uncertain future circumstances, which leads to a multi-stage decision making problem. Notice that the state transition probability p is not defined for the above MDP. The state transition probability refers to the probability of transferring to a certain next state after taking action Setptz(t). With a known state transition probability, the MDP is fully observed and the cumulative reward can be analytically solved via model-based dynamic programming or other iterative methods. However, in the HVAC control problem, to obtain an accurate probability model of the state transitions is not a trivial task, because it is difficult to formulate the exact thermal-dynamic model of HVAC buildings. The heat transfer within the building is related to multiple resistances (R) and capacitors (C) from different building components, like the exterior walls, the interior walls and furnishings, and the attic, the values of which require estimation and validation through experimenting. All these factors can have a significant impact on the temperature response of the indoor air [30]. Furthermore, the indoor temperature is also affected by uncertain external factors such as outdoor temperature, solar irradiance, and wind, which calls for additional modelling and computational efforts. As a consequence, a model-based method is not a robust or adaptive solution for HVAC system optimization. Driven by the above considerations, in this paper the model-free deep RL method is leveraged to overcome the unobservability in the multi-zone residential HVAC control problem. The model-free RL method does not require any knowledge of the environment or the state transitions in advance. It gradually improves its decision-making strategy by continuously interacting with the environment and receiving feedback. In this way, the forecast errors of uncertain factors, as well as the measurement errors of building thermal mass, can be avoided. More details of the deep RL method will be revealed in the next section. 3 DDPG-based control strategy for multi-zone HVAC system 3.1 A brief review of deep reinforcement learning methods The RL method is a type of machine learning method that optimizes the decision-making strategy in an MDP. In the RL algorithm, the reward defined in the MDP is served as the guideline for algorithm evolution. A large, positive reward will encourage the algorithm to search deep in the current direction, and vice versa. The RL method is especially suitable for handling decision-making problems with temporal constraints or with hidden state space. There are two main types of RL method: the value-based RL method and the policy-based RL method. The difference between the two methods lie in their action evaluation strategies. The value-based method estimates the Q value of a state-action pair (s,a), which is the cumulative discounted reward starting from taking action a at state s, and selects the action with the highest Q value; the policy-based RL method generates the probabilities of all the feasible actions at the current state, and selects the action with the highest probability. The combination of RL with a DNN is called the deep RL method. In deep RL, the DNN is utilized as a regression tool to estimate either the Q value, as in the value-based RL method; or the action probability, as in the policy-based RL method. A general DNN structure for regression in RL is shown in Fig. 1 . The main advantage of the deep RL method over the conventional RL method is that the application of the DNN makes it possible to achieve high level control for extremely complex problems, such as with continuous state space or action space, without the tabular constraints. In deep RL a more generalized regression model is established instead of maintaining a concrete Q table to store all the possible action values, as in the case of traditional Q learning. This generalized regression model offers more robust and flexible strategies against unseen states in the case of continuous control. In the following section, we will first introduce the DQN, as a representative of the valued-based deep RL methods; and the DPG method, as a representative of the policy-based deep RL methods. Then, a continuous control method, the DPG method, which is a combination of the above two methods, will be explained in detail for solving the optimal multi-zone residential HVAC control problem. 3.2 Understanding the basic principles behind typical deep RL methods 1) Deep Q Network (DQN) The DQN is a combination of Q-learning and a DNN. In the DQN, the input is the current state, and the output is the Q value for each potential action at the current state. The advantage of the DQN over the tabular Q-learning method is that when the state and action are slightly changed, the DQN can still estimate the associated Q value without re-training, which is highly time-efficient. Unlike the supervised learning algorithm, in deep RL there are no labeled samples for the DNN to learn. To handle this issue, two DNNs are designed for the DQN algorithm: one is called the target network, and the other is called the behavior network. The function of the target network is to serve as a reference, similar to the ground truth in the supervised learning, to guide the evolution of the algorithm. Both networks are initialized with the same parameters and the same structure. As the training proceeds, the behavior network is updated at a faster speed than the target network. The loss function in the DQN is defined as the mean square error (MSE) between the target Q value and the behavior Q value. Once the loss function is calculated, the parameters of the behavior network will be updated based on its gradient to the loss function. The algorithm will continue updating until the output from the target network and the behavior network are close to each other, which indicates the convergence of the learning. More details of the DQN method can be found in [31]. 2) Deep Policy Gradient (DPG) The DPG method utilizes a strategy different from the DQN for control optimization. The output from the DNN is the probabilities of each potential action at the current state, or the policy. The policy refers to the probability of selecting action a(t) at state s(t), and can be written as π(a|s,θ)\u202f=\u202fPr{a(t)\u202f=\u202fa|s(t)\u202f=\u202fs, θ(t)\u202f=\u202fθ}. θ stands for the parameters of the probability function. The loss function of the DPG method is also different from that of the DQN, which intends to maximize the expected total reward under the policy π(a|s,θ), and can be expressed as follows: (4) max J ( θ ) = E π ( a | s , θ ) ( ∑ t = 1 N T r ( t ) ) = ∑ τ π θ ( τ ) R ( τ ) In Eq. (4), τ is called an episode generated under the policy π(a|s,θ): τ={s(1), a(1), s(2), a(2),\u2026, s(NT), a(NT)}. R(τ) = ∑ t = 1 N T r ( t ) , which is the total reward of the episode. The goal of the DPG method is to get the parameters of the policy π that leads to the maximum value of the expected total reward. More details of DPG algorithm can be found in [22]. 3.3 Realizing continuous control of HVAC system with the DDPG 1) An introduction to the DDPG The DDPG method is specially designed for solving problems with continuous variables. Unlike the DQN or DPG, where the Q values or action probabilities of all feasible actions are generated by the DNN for the agent to select, the term \u201cdeterministic\u201d in the DDPG refers to the fact that there is only one output from the DNN, which is determined. In this way, the action space can be continuous since there is only one output unit. Another advantage of the DDPG over the DQN and DPG is that it is a combination of the two methods. In the DDPG, there are two types of neural networks applied: the actor network, which assembles the DPG, and the critic network, which assembles the DQN. Their functions are explained as follows. The input to the actor network is the current state, and the output is a deterministic action; the input to the critic network is the current state plus the action generated by the actor network, and the output is the Q value of the state-action pair. This Q value will be further used to update the parameters of the actor network. The loss function of the actor network is defined to maximize the Q value with the current policy, which follows the logic of the DPG method; and the loss function of the critic network is the MSE of the Q value, which follows the logic of the DQN method. In summary, the function of the actor network is to select actions, and the function of the critic network is to evaluate the selected action. In addition, similar to the DQN algorithm, for both actor network and critic network in the DDPG, two neural networks are designed, a behavior network and a target network. Hence there are four neural networks in total. The reason for applying the target network is to stabilize the algorithm convergence. More details of the DDPG algorithm are presented in the next subsection. 2) DDPG algorithm for developing optimal HVAC control strategy The details of the proposed DDPG algorithm are shown in Algorithm 1, which is customized from a general-purpose DDPG algorithm in [32]. The DDPG algorithm follows a process similar to that of the DQN, except that an actor network is built to select a deterministic action. The applied DDPG algorithm is further explained as follows: To begin with, two neural networks, i.e., the actor network and the critic network are randomly initialized, and their associated target networks are initialized with the same set of parameters, as shown in lines 1\u20132. Starting from line 3, for each iteration, the system state is first initialized, then an HVAC control action, i.e. the setpoint, is chosen based on the current actor network π(s;θπ ), as shown by line 7. A noise is added to the selected action to boost the exploration of the algorithm. Next, in lines 8\u20139, the selected action is executed in the environment for the entire control interval Δt, and the received reward and the next state are observed. The transition (s(t), Setptz(t), r(t), s(t\u202f+\u202fΔt)) is stored in a replay buffer to be further used for algorithm training. When a sufficient number of transitions is collected, a mini-batch of transitions is randomly selected to update the parameters of the actor network and the behavior network, as shown by line 11. The random selection can cut off the temporal correlations among the transitions, which will maintain the independent, identically distributed assumption in the learning model. Also, the transitions can be sampled multiple times, which increases their utilization efficiency. The neural network parameters θQ and θπ are updated according to the loss functions. The loss function of the critic network is defined as the MSE between the target Q value and the current Q value from the behavior critic network, as shown by line 12. The temporal-difference error is used to update the Q value, where the target Q value is the sum of the current reward plus a discounted Q value from the target critic network θQ\u2019 for the next control interval t\u202f+\u202fΔt. γ is called the discount factor. Once the loss function is calculated, the parameters of the behavior critic network θQ are updated based on the gradient, as shown by line 13. ηQ is called the learning rate. The loss function of the actor network is defined to maximize the Q value: (5) max 1 M ∑ i = 1 M Q ( s ( i ) ( t ) , a ( i ) ( t ) ; θ Q ) | a ( i ) ( t ) = π ( s ( i ) ( t ) ; θ π ) In Eq. (5), a ( i )(t) is generated from the actor network π(s;θ π). Hence, the chain rule is applied in line 14 to calculate the gradient of the Q value to the θ π. In line 16, the parameters of the target critic network and the target actor network, θQ\u2019 and θ π\u2019, are updated at a slower rate than the behavior network, where τ is a number between 0 and 1 and close to 1. The function of this slower update is to increase the stability of the learning. The complete deep RL-based control framework of a multi-zone HVAC system is shown in Fig. 2 . Algorithm 1 DDPG method for multi-zone HVAC control 1: Initialize the parameters of the critic network Q(s,a;θQ ) and the actor network π(s;θπ ) 2: Initialize the target networks Q(s,a;θQ\u2019 ) and π(s;θπ\u2019 ) with θQ and θπ 3: for episode\u202f=\u202f1 to arbitrary number do 4: Initialize system state s(Tout (0), Tin,z (0), Tlower (0), λretail (0)) 5: for t\u202f=\u202f1 to NT do 6: if t == kΔt, where k is an integer, do 7: Select the multi-zone HVAC control action Setptz(t) with π(s;θπ ) plus noise 8: Execute Setptz(t), receives the immediate reward r(t) and the next state s(t\u202f+\u202fΔt) 9: Store the transition (s(t), Setptz(t), r(t), s(t\u202f+\u202fΔt)) in the replay buffer 10: end if 11: Collect a mini-batch of transitions (s ( i )(t), S e t p t z ( i ) (t), r ( i )(t), s ( i )(t\u202f+\u202fΔt)) with the size M from the replay buffer 12: Calculate the MSE of the Q value: qtarget ( i ) (t)\u202f=\u202fr ( i )(t)\u202f+\u202fγQ(s ( i )(t\u202f+\u202fΔt), π(s ( i )(t\u202f+\u202fΔt);θπ\u2019 );θQ\u2019 ) L(θQ )\u202f=\u202f1/M ∑ i = 1 M (qtarget ( i )(t) - Q(s ( i )(t),π(s ( i )(t);θπ );θQ ) 13: Update the parameters of the critic network: θQ \u202f=\u202fθQ - ηQ ▽ θ Q L(θQ ) 14: Calculate the gradient of the Q value to the actor network parameter θπ : ▽ θ πJ≈1/M ∑ i = 1 M ▽π Q(s ( i )(t),π(s ( i )(t);θπ );θQ )▽ θ π π(s ( i )(t);θπ ) 15: Update the parameters of the actor network: θπ \u202f=\u202fθπ - ηπ ▽J 16: Update the parameters of the target network with a smaller step: θQ\u2019 = (1 \u2013 τ)θQ \u202f+\u202fτ θQ\u2019 θπ\u2019 = (1 \u2013 τ) θπ \u202f+\u202fτ θπ\u2019 17: end for 18: end for 4 Case study In this section, the effectiveness of the applied DDPG-based continuous control method for multi-zone residential HVAC is demonstrated through simulations with real-world data, as well as by comparison with the DQN-based discrete control method and the benchmark cases, to fully verify the advantages of the DDPG method. Further, the generalization of the deep RL method is demonstrated by experimenting with unseen physical environments. 4.1 Simulation environment A two-zone residential HVAC model [33] is implemented for training and testing the applied deep RL method, with real-world weather data from 2019 to 2020 obtained from [34]. For price signals, a simulated retail price sequence is generated, which includes a high price value and a low price value. The price is regularly switched between the two values every three hours. The reason for applying such a frequently changing price sequence is to find if the deep RL agent can identify the effect of price signals on the reward function and properly adjust its control strategies. It is further assumed that the lower bound of the user comfort level changes four times during the daily cycle, as shown in Table 1 : The control interval of the RL agent is set to 60\u202fmin, i.e., Δt\u202f=\u202f60. Since we only focus on the heating effect of the HVAC system, the November weather data is used as the training data. During the training, one episode is defined as 24\u202fh. In this way, 24 (s ( i )(t), S e t p t z ( i ) (t), r ( i )(t), s ( i )(t\u202f+\u202fΔt)) transitions will be generated from each episode. In total 300 episodes are simulated for the RL agent to learn. After the training, the RL agent will be applied to new test days with different weather conditions to examine its generalization and adaptability. 4.2 Design of the DNN structure in deep RL The detailed design of the actor and critic network in the DDPG is shown in Table 2 . The design of the DQN is also listed for comparison. The designs of both the DDPG and the DQN are obtained via a trial-and-error process, and the current configurations provide the best possible results among all the trials. For the DDPG method, the input to the critic network is a vector containing both state variables and action variables, and the output is the estimated Q value, which is a scalar; the input to the actor network is a vector containing only state variables, and the output is a vector containing the setpoint for each zone. Although the setpoint is a continuous variable, in reality there is always a range of the setpoint for maintaining user comfort. Therefore, the output layer from the actor network utilizes tanh as the activation function, which confines the output with a range of [−1, 1]. The actual setpoint is calculated as Setptz\u202f=\u202fTlower\u202f+\u202fΔT·(yout\u202f+\u202f1), where yout is the output from the actor network, and ΔT is the upper range of the setpoint. In the simulation, ΔT is set to 2\u202f°C. Therefore, the setpoint selected by the DDPG lies within the range of [Tlower, Tlower\u202f+\u202f2]. For the DQN method, the inputs are also the state variables. Since the DQN requires a discrete action space, we discretize the range of setpoints with a step size of 0.5\u202f°C. As a result, there are 5 actions for each zone and 25 combinations of actions for the 2-zone HVAC. The output from DQN is a vector containing 25 Q values, with each corresponding to one combination of actions. 4.3 Performance of the continuous HVAC control method 1) Convergence of the DDPG In Fig. 3 , the average returns gained after each episode during the training process in the DDPG and the DQN are presented. Notice that the average returns in the first few episodes appear to be higher than that of the last few episodes. This is because for each episode, one training day is randomly chosen. Some training days may have moderate outdoor temperatures, which can lead to low energy cost and low penalty, and vice versa. However, as the training proceeds, the number of episodes grows, and the average return is neutralized. It can be observed that both curves gradually become steady as the training evolves. However, the average return gained by the DDPG method is higher than that of the DQN method. This is because the size of the output from the DQN is larger than that of the DDPG, and the combination of actions have not been fully explored after 300 episodes, leading to a lower average return. 2) Computational efficiency After the training process, the DDPG RL agent is applied to 10 test days in January 2020 from the real-world data in [34] to generate the optimal HVAC control strategy. The time cost is around 19\u202fs for testing, which is highly time-efficient. The code is written in Python 3.6 with the open-source deep learning platform TensorFlow [35]. The hardware environment is a laptop with Intel®CoreTM i7-7600U 2.8\u202fGHz CPU, and 16.00 GM RAM. 3) Comparison of the DDPG with the DQN and the benchmark cases In this study, the well-trained deep RL agents from both the DDPG and the DQN are run on new test days to verify their learning performance. We also design two benchmark cases without the RL agent as comparisons. The benchmark cases are described as follows: a) Rule-based case: the setpoint is set at the lowest value at the peak price hours, and the highest value at the off-peak price hours, to realize the pre-heating effect to save energy cost; b) Fixed setpoint case: the setpoint is always at the highest value of the setpoint range to avoid any temperature violation. The final optimized results of the RL methods and the benchmark cases are shown in Table 3 : In Table 3, the well-trained deep RL agents are applied to generate the HVAC control strategies for the first 10\u202fdays in January 2020. The weather conditions of the test days are different from those of the training days, since the outdoor temperature is much lower in January than in November. The total cost in the table refers to the total energy cost over the 10\u202fdays, and the temperature violation in the table refers to the total number of minutes that the indoor temperature falls below Tlower - Tth, as shown by Eq. (3). Tth is set to 0.3\u202f°C. The average temperature violation indicates on average by how many degrees the indoor temperature is lower than the setpoint. As shown in the table, the control strategy derived from the DDPG method has both lower energy cost and fewer temperature violations than that of the DQN. With regard to the benchmark cases, in the rule-based case, because the pre-heating logic is applied based on the price structure, it obtained the lowest cost among all four cases. However, by always setting the setpoint to the lowest value at peak price hours, this control strategy results in severe temperature violation. In the fixed setpoint case, since the setpoint is always set at the highest value, there is no temperature violation. However, the energy cost is also the highest among the four cases. The control strategy and the associated indoor temperature in the four cases are further illustrated in Figs. 4\u20136 . In all the figures, the yellow rectangular area represents the feasible region of the setpoint [Tlower, Tlower\u202f+\u202f2\u202f°C]. As can be observed, the setpoint range changes at a daily cycle. In addition, the indoor temperature in zone 1 is lower than that of zone 2, this is because in the building model, zone 1 is at the 1st floor and zone 2 is at the 2nd floor, and the warmer air goes to upper floors. In Fig. 4, the DDPG RL agent develops a setpoint control strategy such that when the outdoor temperature is relatively high, i.e. in the first 4000\u202fmin, the setpoint will be set at the lowest value at the peak price hour, and at the highest value at the off-peak hour, to realize the pre-heating effect and to reduce energy cost, which is similar to the control logic of the rule-based case. When the outdoor temperature is low, i.e., in the last 2000\u202fmin, the setpoint is always set at the highest value to avoid the indoor temperature violation. On the contrary, in the rule-based case, the control strategy still follows the price structure even when the outdoor temperature is extremely low, which results in severe indoor temperature violation, as shown in Fig. 6. Such comparisons indicate that after the training, the DDPG RL agent has acquired the knowledge that the price signal and the outdoor temperature have a significant impact on the reward, and it learns to intelligently set the setpoint based on this state information to reach a higher reward value. The control strategy of the DQN RL agent is shown in Fig. 5. It can be observed that when the outdoor temperature is relatively high, i.e. in the first 4000\u202fmin, the setpoint is set at a relatively high value, and it does not follow the change of retail price. When the outdoor temperature is extremely low, i.e., around 12,000\u202fmin, the setpoint is set at the lower bound, which results in temperature violation. The DQN RL agent does not successfully capture the impacts of the state variables on the reward function. This can be attributed to the large number of action combinations encountered by the Q network. In such cases, the DQN RL agent has not fully explored all the possible action combinations to maximize the reward, and thus obtains a control strategy with a higher energy cost and more temperature violations. Finally, in Fig. 7 , the fixed setpoint case, since the setpoint is always set at the highest value, the indoor temperature for both zones also remains at the highest level among the four test cases. However, this fixed setpoint case results in the highest energy cost. 4) Generalization of the DDPG Algorithm a) Extending the DDPG RL agent to different residential buildings The well-trained DDPG RL agent is further tested in new residential building models with HVAC systems to fully validate its generalization and robustness. Ten building models are generated with different thermal mass parameters, the variation of which follows a normal distribution. The same 10 test days in January 2020 are applied in this case. The energy cost and the temperature violation for the 10 building models under the DDPG control strategy and under the two benchmark cases are compared in Table 4 and Fig. 8 . As can be read from the table, similar to the results in Table 3, the rule-based control strategy provides the lowest energy cost, while the fixed setpoint control strategy provides the lowest violation. The well-trained DDPG RL agent can obtain an HVAC control strategy that properly weighs the two objectives, resulting in a relatively lower energy cost and fewer temperature violations for different test building models. Therefore, it can be safely concluded that the DDPG RL agent can flexibly adapt to unseen physical environments and provides an economic HVAC control strategy after its offline training with the fixed environment. b) DDPG performance under different retail price signals In the above simulations, a simulated retail price sequence is generated for training and testing the deep RL agent, which is composed of only two price signals. To demonstrate that the well-trained DDPG RL agent has developed high generalization to an unseen environment without additional training, the DDPG RL agent is further tested with a retail price sequence that is generated from the PJM wholesale hourly locational marginal price (LMP) data [36]. The retail price is set as triple of the wholesale market price. The PJM price is very irregular, changing hourly and fluctuating within a large range. The final optimized results of the two deep RL methods and the benchmark case are shown in Table 5 : In Table 5, the fixed setpoint case applies a control strategy where the setpoint is always set at the middle of the setpoint range. This is because the PJM price sequence contains more than just two values, and it cannot be simply divided into two groups as high price and low price. As a result, the setpoint is set at the middle point to avoid possible temperature violations while minimizing the energy cost. The control strategy and the associated indoor temperature in the three cases are further illustrated by Figs. 9\u201311 . As can be observed in the figure, the PJM price demonstrates a very different pattern from the simulated price sequence. For most of the time the price remains at a relatively low level, with some occasional spikes and fluctuations. However, the well-trained DDPG RL agent still attempts to follow the price tendency, and intelligently sets the setpoint to realize the pre-heating effect. For example, a price spike appears around 12,500\u202fmin. The DDPG RL agent catches this sudden change, and lowers the setpoint. At around 13,500\u202fmin the retail price sequence demonstrates some fluctuations, and the DDPG RL agent also adjusts the setpoint accordingly. Note that under the price signals that are more time-variants like the PJM market price, it is difficult to develop a simple rule-based control strategy, because the price range is uncertain. However, the well-trained DDPG RL agent can still work intelligently under such an uncertain environment, and it obtains satisfying economic benefits. Therefore, it can be safely concluded that the DDPG algorithm has gained adaptability after training and has potential for real-world online applications. In Fig. 10, the HVAC control strategy developed by the DQN RL agent also intends to follow the retail price tendency. However, at the price spike period (12,500\u202fmin) and the price variation period (13,500\u202fmin), the DQN RL agent chooses the lowest setpoint values, which results in a temperature violation in zone 1, as shown in the bottom figure. Finally, in Fig. 11, the fixed setpoint case also leads to some temperature violations in zone 1 when the outdoor temperature is extremely low (after 10,000\u202fmin). 5 Conclusion In this paper, the DDPG RL method is applied for controlling a multi-zone residential HVAC system to minimize the energy consumption cost while maintaining the user comfort. The DDPG can realize continuous control of the HVAC setpoint due to its application of DNNs. Simulation results demonstrate that the well-trained DDPG RL agent can act intelligently to balance the different optimization objectives, and that it also gains generalization and adaptability to unseen environment, which signifies its potential for future online applications in solving MDP problems with hidden information or with continuous search space. For future works, we will mainly look into two directions for further improving the robustness of the RL-based control strategy: 1) considering different seasoning scenarios, the deep RL agent should learn to automatically switch between different operation modes, i.e. cooling and heating, in order to be applied to a longer control period, i.e. one year, to provide economic control strategies for HVAC users; 2) considering different user preferences, the deep RL agent should be able to learn a more variant setpoint schedule customized by users, and provide more flexible HVAC control strategies. By investigating these two directions, the deep RL agent will become more generalized and robust against uncertainties in real-world operation scenarios. CRediT authorship contribution statement Yan Du: Investigation, Methodology, Writing-original draft. Helia Znadi: Conceptualization, Funding acquisition, Methodology, Project administration, Supervision, Writing - review & editing. Olivera Kotevska: Investigation, Methodology, Writing - review & editing. Kuldeep Kurte: Investigation, Methodology, Writing - review & editing. Jeffery Munk: Methodology, Software, Writing - reviewing & editing. Kadir Amasyali: Investigation, Methodology, Writing - review & editing. Evan Mckee: Investigation, Methodology, Writing - review & editing. Fangxing Li: Funding acquisition, Methodology, Project administration, Writing-review & editing. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgement The authors would like to acknowledge the support in part by the U.S. Department of Energy (DOE), including Office of Energy Efficiency and Renewable Energy under the Buildings Technologies Program, in part by CURENT which is an Engineering Research Center (ERC) funded by the U.S. National Science Foundation (NSF) and DOE under the NSF award EEC-1041877, and in part by the U.S. NSF award ECCS-1809458. References [1] L. Pérez-Lombard J. Ortiz C. Pout A review on buildings energy consumption information Energ Build 40 2008 394 398 Pérez-Lombard L, Ortiz J, Pout C. A review on buildings energy consumption information. Energ Buildings 2008;40:394-8. [2] A. Costa M.M. Keane J.I. Torrens E. Corry Building operation and energy performance: monitoring, analysis and optimisation toolkit Appl Energy 101 2013 310 316 Costa A, Keane MM, Torrens JI, Corry E. Building operation and energy performance: monitoring, analysis and optimisation toolkit. Appl Energy 2013;101:310-6. [3] X. Kou F. Li J. Dong A scalable and distributed algorithm for managing residential demand response programs using alternating direction method of multipliers (ADMM) IEEE Trans Smart Grid 2020 in-press Kou X, Li F, Dong J, et al. A Scalable and Distributed Algorithm for Managing Residential Demand Response Programs Using Alternating Direction Method of Multipliers (ADMM). IEEE Transactions on Smart Grid 2020; in-press. [4] K. Ma G. Hu C.J. Spanos Energy management considering load operations and forecast errors with application to HVAC systems IEEE Trans Smart Grid 9 2016 605 614 Ma K, Hu G, Spanos CJ. Energy management considering load operations and forecast errors with application to HVAC systems. IEEE Trans Smart Grid 2016;9:605-14. [5] O. Erdinc A. Taşcıkaraoğlu N.G. Paterakis Y. Eren J.P. Catalão End-user comfort oriented day-ahead planning for responsive residential HVAC demand aggregation considering weather forecasts IEEE Trans Smart Grid 8 2016 362 372 Erdinc O, Taşcıkaraoğlu A, Paterakis NG, Eren Y, Catalão, JP. End-user comfort oriented day-ahead planning for responsive residential HVAC demand aggregation considering weather forecasts. IEEE Trans Smart Grid 2016;8:362-72. [6] X. Wu J. He Y. Xu J. Lu N. Lu X. Wang Hierarchical control of residential HVAC units for primary frequency regulation IEEE Trans Smart Grid 9 2017 3844 3856 Wu X, He J, Xu Y, Lu J, Lu N, Wang X. Hierarchical control of residential HVAC units for primary frequency regulation. IEEE Trans Smart Grid 2017;9:3844-56. [7] Y. Lin P. Barooah J.L. Mathieu Ancillary services through demand scheduling and control of commercial buildings IEEE Trans Power Syst 32 2016 186 197 Lin Y, Barooah P, Mathieu JL. Ancillary services through demand scheduling and control of commercial buildings. IEEE Trans Power Syst 2016;32:186-97. [8] L. Yu T. Jiang Y. Zou Online energy management for a sustainable smart home with an HVAC load and random occupancy IEEE Trans Smart Grid 10 2017 1646 1659 Yu L, Jiang T, Zou Y. Online energy management for a sustainable smart home with an HVAC load and random occupancy. IEEE Trans Smart Grid 2017;10:1646-59. [9] H. Hao C.D. Corbin K. Kalsi R.G. Pratt Transactive control of commercial buildings for demand response IEEE Trans Power Syst 32 2016 774 783 Hao H, Corbin CD, Kalsi K, Pratt RG. Transactive control of commercial buildings for demand response. IEEE Trans Power Syst 2016;32:774-83. [10] D. Silver A. Huang C.J. Maddison A. Guez L. Sifre G. Van Den Driessche Mastering the game of go with deep neural networks and tree search Nature 529 2016 484 489 Silver D, Huang A, Maddison CJ, Guez A, Sifre L, Van Den Driessche G, et al. Mastering the game of go with deep neural networks and tree search. Nature 2016;529:484-9. [11] F. Li Y. Du From AlphaGo to power system AI: what engineers can learn from solving the most complex board game IEEE Power Energ Mag 16 2018 76 84 F. Li, Y. Du. From AlphaGo to Power System AI: What Engineers Can Learn from Solving the Most Complex Board Game. IEEE Power and Energy Magazine 2018;16:76-84. [12] Y. Du F. Li J. Li T. Zheng Achieving 100x acceleration for N-1 contingency screening with uncertain scenarios using deep convolutional neural network IEEE Trans Power Syst 34 2019 3303 3305 Du Y, Li F, Li J, Zheng T. Achieving 100x Acceleration for N-1 Contingency Screening with Uncertain Scenarios using Deep Convolutional Neural Network. IEEE Trans Power Syst 2019;34:3303-5. [13] Y. Du F. Li Multi-microgrid energy management based on deep neural network and model-free reinforcement learning IEEE Trans Smart Grid 11 2020 1066 1076 Du Y, Li F. Multi-microgrid Energy Management based on Deep Neural Network and Model-free Reinforcement Learning. IEEE Trans Smart Grid 2020;11:1066-76. [14] T. Huang Q. Guo H. Sun A distributed computing platform supporting power system security knowledge discovery based on online simulation IEEE Trans Smart Grid 8 2017 1513 1524 Huang T, Guo Q, Sun H. A Distributed Computing Platform Supporting Power System Security Knowledge Discovery Based on Online Simulation. IEEE Trans Smart Grid 2017;8:1513-24. [15] Y. Wu H. Tan J. Peng H. Zhang H. He Deep reinforcement learning of energy management with continuous control strategy and traffic information for a series-parallel plug-in hybrid electric bus Appl Energy 247 2019 454 466 Wu Y, Tan H, Peng J, Zhang H, He H. Deep reinforcement learning of energy management with continuous control strategy and traffic information for a series-parallel plug-in hybrid electric bus. Appl Energy 2019;247:454-66. [16] X. Han H. He J. Wu J. Peng Y. Li Energy management based on reinforcement learning with double deep Q-learning for a hybrid electric tracked vehicle Appl Energy 254 2019 113708 Han X, He H, Wu J, Peng J, Li Y. Energy management based on reinforcement learning with double deep Q-learning for a hybrid electric tracked vehicle. Appl Energy 2019; 254:113708. [17] H. Hua Y. Qin C. Hao J. Cao Optimal energy management strategies for energy Internet via deep reinforcement learning approach Appl Energy 239 2019 598 609 Hua H, Qin Y, Hao C, Cao J. Optimal energy management strategies for energy Internet via deep reinforcement learning approach. Appl Energy 2019; 239:598-609. [18] R. Rocchetta L. Bellani M. Compare E. Zio E. Patelli A reinforcement learning framework for optimal operation and maintenance of power grids Appl Energy 241 2019 291 301 Rocchetta R, Bellani L, Compare M, Zio E, Patelli E. A reinforcement learning framework for optimal operation and maintenance of power grids. Appl Energy 2019; 241:291-301. [19] P. Kou D. Liang C. Wang Z. Wu L. Gao Safe deep reinforcement learning-based constrained optimal control scheme for active distribution networks Appl Energy 264 2020 114772 Kou P, Liang D, Wang C, Wu Z, Gao L. Safe deep reinforcement learning-based constrained optimal control scheme for active distribution networks. Appl Energy 2020; 264:114772. [20] T. Wei S. Ren Q. Zhu Deep reinforcement learning for joint datacenter and HVAC load control in distributed mixed-use buildings IEEE Trans Sustainable Comput 2019 early access Wei T, Ren S, Zhu Q. Deep Reinforcement Learning for Joint Datacenter and HVAC Load Control in Distributed Mixed-Use Buildings. IEEE Trans Sustainable Computing 2019; early access. [21] B.J. Claessens P. Vrancx F. Ruelens Convolutional neural networks for automatic state-time feature extraction in reinforcement learning applied to residential load control IEEE Trans Smart Grid 9 2016 3259 3269 Claessens BJ, Vrancx P, Ruelens F. Convolutional neural networks for automatic state-time feature extraction in reinforcement learning applied to residential load control. IEEE Trans Smart Grid 2016;9:3259-69. [22] E. Mocanu D.C. Mocanu P.H. Nguyen A. Liotta M.E. Webber M. Gibescu On-line building energy optimization using deep reinforcement learning IEEE Trans Smart Grid 10 2018 3698 3708 Mocanu E, Mocanu DC, Nguyen PH, Liotta A, Webber ME, Gibescu M, Slootweg JG. On-line building energy optimization using deep reinforcement learning. IEEE Trans Smart Grid 2018;10:3698-708. [23] Y. Wang K. Velswamy B. Huang A long-short term memory recurrent neural network based reinforcement learning controller for office heating ventilation and air conditioning systems Processes 5 2017 46 63 Wang Y, Velswamy K, Huang B. A long-short term memory recurrent neural network based reinforcement learning controller for office heating ventilation and air conditioning systems. Processes 2017;5:46-63. [24] Z. Zhang A. Chong Y. Pan C. Zhang K.P. Lam Whole building energy model for HVAC optimal control: a practical framework based on deep reinforcement learning Energy Build 199 2019 472 490 Zhang Z, Chong A, Pan Y, Zhang C, Lam KP. Whole building energy model for HVAC optimal control: A practical framework based on deep reinforcement learning. Energy and Buildings. 2019;199:472-90. [25] K.U. Ahn C.S. Park Application of deep Q-networks for model-free optimal control balancing between different HVAC systems Sci Technol Built Environ 26 2019 61 74 Ahn KU, and Park CS. Application of deep Q-networks for model-free optimal control balancing between different HVAC systems. Science and Technology for the Built Environment 2019;26:61-74. [26] Gao G, Li J, Wen Y. DeepComfort: energy-efficient thermal comfort control in buildings via reinforcement learning. IEEE Internet of Things J 2020 (early access). [27] Yu L, Sun Y, Xu Z, Shen C, Yue D, Jiang T, et al. Multi-agent deep reinforcement learning for HVAC control in commercial buildings. IEEE Trans Smart Grid 2020 (early access). [28] Z. Zou X. Yu S. Ergan Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network Build Environ 168 2020 1 15 Zou Z, Yu X, Ergan S. Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network. Building and Environment. 2020;168:1-15. [29] N. Lu An evaluation of the HVAC load potential for providing load balancing service IEEE Trans Smart Grid 3 2012 1263 1270 Lu N. An evaluation of the HVAC load potential for providing load balancing service. IEEE Trans Smart Grid 2012;3:1263-70. [30] B. Cui J. Joe J. Munk J. Sun T. Kuruganti Load flexibility analysis of residential HVAC and water heating and commercial refrigeration 2019 Oak Ridge National Lab Oak Ridge, TN (United States) Cui B, Joe J, Munk J, Sun J, Kuruganti T. Load Flexibility Analysis of Residential HVAC and Water Heating and Commercial Refrigeration. Oak Ridge National Lab, Oak Ridge, TN (United States); 2019 Sep 1. [31] V. Mnih K. Kavukcuoglu D. Silver A.A. Rusu J. Veness M.G. Bellemare Human-level control through deep reinforcement learning Nature 518 2015 529 533 Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, Graves A, Riedmiller M, Fidjeland AK, Ostrovski G, Petersen S. Human-level control through deep reinforcement learning. Nature 2015;518:529-33. [32] Lillicrap TP, Hunt JJ, Pritzel A, Heess N, Erez T, Tassa Y, Silver D, Wierstra D. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971. [33] Cui B, Munk J, Jackson R, Fugate D, Starke M. Building thermal model development of typical house in US for virtual storage control of aggregated building loads based on limited available information. In: 30th International Conference on Efficiency, Cost, Optimisation, Simulation and Environmental Impact of Energy Systems. San Diego, California, US; 2017. [34] Clean Power Research. [online]: https://www.cleanpower.com/. [35] TensorFlow. [online]: https://www.tensorflow.org/. [36] PJM market. [online]: https://www.pjm.com/.",
    "scopus-id": "85095440588",
    "coredata": {
        "eid": "1-s2.0-S030626192031535X",
        "dc:description": "Residential heating, ventilation, and air conditioning (HVAC) has been considered as an important demand response resource. However, the optimization of residential HVAC control is no trivial task due to the complexity of the thermal dynamic models of buildings and uncertainty associated with both occupant-driven heat loads and weather forecasts. In this paper, we apply a novel model-free deep reinforcement learning (RL) method, known as the deep deterministic policy gradient (DDPG), to generate an optimal control strategy for a multi-zone residential HVAC system with the goal of minimizing energy consumption cost while maintaining the users\u2019 comfort. The applied deep RL-based method learns through continuous interaction with a simulated building environment and without referring to any prior model knowledge. Simulation results show that compared with the state-of-art deep Q network (DQN), the DDPG-based HVAC control strategy can reduce the energy consumption cost by 15% and reduce the comfort violation by 79%; and when compared with a rule-based HVAC control strategy, the comfort violation can be reduced by 98%. In addition, experiments with different building models and retail price models demonstrate that the well-trained DDPG-based HVAC control strategy has high generalization and adaptability to unseen environments, which indicates its practicability for real-world implementation.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2021-01-01",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S030626192031535X",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Du, Yan"
            },
            {
                "@_fa": "true",
                "$": "Zandi, Helia"
            },
            {
                "@_fa": "true",
                "$": "Kotevska, Olivera"
            },
            {
                "@_fa": "true",
                "$": "Kurte, Kuldeep"
            },
            {
                "@_fa": "true",
                "$": "Munk, Jeffery"
            },
            {
                "@_fa": "true",
                "$": "Amasyali, Kadir"
            },
            {
                "@_fa": "true",
                "$": "Mckee, Evan"
            },
            {
                "@_fa": "true",
                "$": "Li, Fangxing"
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S030626192031535X"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S030626192031535X"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S0306-2619(20)31535-X",
        "prism:volume": "281",
        "articleNumber": "116117",
        "prism:publisher": "Published by Elsevier Ltd.",
        "dc:title": "Intelligent multi-zone residential HVAC control strategy based on deep reinforcement learning",
        "prism:copyright": "© 2020 Published by Elsevier Ltd.",
        "openaccess": "0",
        "prism:issn": "03062619",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "Actor-critic learning"
            },
            {
                "@_fa": "true",
                "$": "Demand response"
            },
            {
                "@_fa": "true",
                "$": "Deep deterministic policy gradient (DDPG)"
            },
            {
                "@_fa": "true",
                "$": "Deep reinforcement learning (deep RL)"
            },
            {
                "@_fa": "true",
                "$": "Multi-zone residential HVAC"
            }
        ],
        "openaccessArticle": "false",
        "prism:publicationName": "Applied Energy",
        "openaccessSponsorType": null,
        "prism:pageRange": "116117",
        "pubType": "fla",
        "prism:coverDisplayDate": "1 January 2021",
        "prism:doi": "10.1016/j.apenergy.2020.116117",
        "prism:startingPage": "116117",
        "dc:identifier": "doi:10.1016/j.apenergy.2020.116117",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "209",
            "@width": "373",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr8.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "23423",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "613",
            "@width": "542",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr11.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "65811",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "607",
            "@width": "542",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr9.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "74091",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "294",
            "@width": "622",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "58497",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "207",
            "@width": "373",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "13705",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "209",
            "@width": "542",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "35523",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "609",
            "@width": "542",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "81685",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "594",
            "@width": "542",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "76192",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "618",
            "@width": "542",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr10.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "63727",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "620",
            "@width": "542",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "88031",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "615",
            "@width": "542",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "77676",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "122",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr8.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "9001",
            "@ref": "gr8",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "145",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr11.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7242",
            "@ref": "gr11",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "146",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr9.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7779",
            "@ref": "gr9",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "103",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "12654",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "121",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "5102",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "85",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7188",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "146",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "9964",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "149",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "8678",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "143",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr10.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7099",
            "@ref": "gr10",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "143",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "8940",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "144",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "8725",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "925",
            "@width": "1654",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr8_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "153233",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2716",
            "@width": "2402",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr11_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "314682",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2692",
            "@width": "2402",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr9_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "351231",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1301",
            "@width": "2756",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "371875",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "917",
            "@width": "1654",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "105680",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "928",
            "@width": "2402",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "266263",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2698",
            "@width": "2402",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "407591",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2631",
            "@width": "2402",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "366612",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2740",
            "@width": "2402",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr10_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "313728",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2748",
            "@width": "2402",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "432683",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2724",
            "@width": "2402",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "371647",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-si1.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "42048",
            "@ref": "si1",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-si1000.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4947",
            "@ref": "si1000",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-si2.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "32576",
            "@ref": "si2",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-si3.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "33676",
            "@ref": "si3",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-si4.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8618",
            "@ref": "si4",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-si5.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "28878",
            "@ref": "si5",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-si6.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "27919",
            "@ref": "si6",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-si7.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7785",
            "@ref": "si7",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S030626192031535X-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "1708561",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85095440588"
    }
}}