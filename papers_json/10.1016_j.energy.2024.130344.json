{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85184757832",
    "originalText": "serial JL 271090 291210 291702 291711 291731 291877 291878 31 Energy ENERGY 2024-01-15 2024-01-15 2024-01-18 2024-01-18 2024-05-21T02:57:18 1-s2.0-S0360544224001154 S0360-5442(24)00115-4 S0360544224001154 10.1016/j.energy.2024.130344 S300 S300.1 FULL-TEXT 1-s2.0-S0360544223X00279 2024-05-21T02:20:57.477531Z 0 0 20240315 2024 2024-01-15T14:29:23.845882Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref 0360-5442 03605442 true 291 291 C Volume 291 38 130344 130344 130344 20240315 15 March 2024 2024-03-15 2024 Full Length Articles article fla © 2024 Elsevier Ltd. All rights reserved. SUCCESSFULAPPLICATIONPREDICTIVEINFORMATIONINDEEPREINFORCEMENTLEARNINGCONTROLACASESTUDYBASEDOFFICEBUILDINGHVACSYSTEM GAO Y 1 Introduction 1.1 Background 1.2 Reinforcement learning for HVAC system control 1.3 Predictive model integrated with RL for HVAC system control 1.4 Objective of this research 2 Methods 2.1 Emulator environment 2.2 Basic definitions and methods of RL 2.3 DRL algorithms 2.3.1 Actor\u2013critic method 2.3.2 Selected algorithms 2.4 How to combine predictive information with RL algorithms? 3 Experiment setting 3.1 Building envelope and HVAC system 3.2 Design of state space and ablation study for prediction observations 3.3 Design of state space and ablation study for prediction observations 3.4 Design of action space 3.5 Design of reward function 3.5.1 Reward based on user comfort and running costs 3.5.2 Reward based on thermostatic control 3.6 Experimental setting 4 Results and discussion 4.1 Statistic experiments in four testing episodes 4.1.1 Reward based on user comfort and running costs 4.1.2 Reward based on thermostatic control 4.2 Detail visualization analysis 5 Conclusion, limitation, and future work CRediT authorship contribution statement Acknowledgments References LIU 2019 113359 M GONZALEZTORRES 2022 626 637 M CAI 2023 127188 W GAO 2023 121106 Y BUYAK 2023 129076 N CHEN 2022 103751 W VAZQUEZCANTELI 2019 1072 1089 J HWANG 2022 118689 R HWANG 2022 109684 R WANG 2020 115036 Z SINGHAL 2007 A ATTARAN 2016 613 624 S ALANNE 2022 103445 K LI 2023 128284 Y LI 2023 127627 Y MNIH 2015 529 533 V DU 2021 116117 Y BRANDI 2020 110225 S HE 2023 107158 K ZHANG 2019 472 490 Z VALLADARES 2019 105 117 W GUPTA 2021 101739 A BIEMANN 2021 117164 M YANG 2021 117335 T FANG 2022 118552 X GAO 2022 119783 Y BRANDI 2022 104128 S XU 2023 106774 Y ZHUANG 2023 120936 D LI 2023 107365 Y WANG 2023 120430 D FAN 2019 700 710 C HOCHREITER 1997 1735 1780 S ESRAFILIANNAJAFABADI 2022 111808 M GAO 2020 110156 Y SOMU 2021 108133 N YANG 2022 109568 G ZOU 2020 106535 Z PINTO 2021 117642 G BLAD 2022 125290 C WETTER 2009 143 161 M BLUM 2021 586 610 D SUTTON 2018 R REINFORCEMENTLEARNINGINTRODUCTION SONG 2000 618 624 H LILLICRAP 2015 T CONTINUOUSCONTROLDEEPREINFORCEMENTLEARNING GAO 2022 120021 Y ZHANG 2017 348 365 X SILVER 2014 387 395 D INTERNATIONALCONFERENCEMACHINELEARNING DETERMINISTICPOLICYGRADIENTALGORITHMS HAARNOJA 2018 1861 1870 T INTERNATIONALCONFERENCEMACHINELEARNING SOFTACTORCRITICOFFPOLICYMAXIMUMENTROPYDEEPREINFORCEMENTLEARNINGASTOCHASTICACTOR BRANDI 2022 329 339 S SUSTAINABILITYINENERGYBUILDINGS2021 ENERGYMANAGEMENTARESIDENTIALHEATINGSYSTEMTHROUGHDEEPREINFORCEMENTLEARNING MENG 2021 5619 5626 L 2021IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMS MEMORYBASEDDEEPREINFORCEMENTLEARNINGFORPOMDPS GAO 2022 119288 Y BLUM 2022 D INFORMATIONABOUTTESTCASEINBOPTEST BROCKMAN 2016 G OPENAIGYM2016 RAFFIN 2021 1 8 A GAOX2024X130344 GAOX2024X130344XY NR_JUSTICE publishAcceptedManuscriptIndexable http://www.elsevier.com/open-access/userlicense/1.0/ 2026-01-18T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2024 Elsevier Ltd. All rights reserved. 2024-02-16T15:31:03.426Z http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/aggregated-refined Kyushu University http://data.elsevier.com/vocabulary/SciValFunders/501100004096 http://sws.geonames.org/1861060 International Institute for Carbon Neutral Energy Research WPI-I2CNER I2CNER International Institute for Carbon-Neutral Energy Research, Kyushu University http://data.elsevier.com/vocabulary/SciValFunders/501100007068 http://sws.geonames.org/1861060/ JSPS 23KJ0513 JSPS Japan Society for the Promotion of Science http://data.elsevier.com/vocabulary/SciValFunders/501100001691 http://sws.geonames.org/1861060/ Japanese Ministry of Education, Culture, Sports, Science and Technology MEXT Ministry of Education, Culture, Sports, Science and Technology http://data.elsevier.com/vocabulary/SciValFunders/501100001700 http://sws.geonames.org/1861060/ This work was supported by JSPS KAKENHI Grant Number 23KJ0513 . We would like to thank Editage ( www.editage.jp ) for English language editing. The author(s) gratefully acknowledge the support of the International Institute for Carbon Neutral Energy Research (WPI-I2CNER), sponsored by the Japanese Ministry of Education, Culture, Sports, Science and Technology . https://doi.org/10.15223/policy-017 https://doi.org/10.15223/policy-037 https://doi.org/10.15223/policy-012 https://doi.org/10.15223/policy-029 https://doi.org/10.15223/policy-004 item S0360-5442(24)00115-4 S0360544224001154 1-s2.0-S0360544224001154 10.1016/j.energy.2024.130344 271090 2024-05-21T02:20:57.477531Z 2024-03-15 1-s2.0-S0360544224001154-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/MAIN/application/pdf/a054a9c141b09a522b8b07df63faa897/main.pdf main.pdf pdf true 1298216 MAIN 13 1-s2.0-S0360544224001154-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/PREVIEW/image/png/1d5c5f110940ba7659cb87048282a9e6/main_1.png main_1.png png 56483 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0360544224001154-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr9/DOWNSAMPLED/image/jpeg/c12dbf9812e60882fa97a357225d5750/gr9.jpg gr9 gr9.jpg jpg 96447 131 470 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224001154-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr7/DOWNSAMPLED/image/jpeg/74547530cc6fb4947a89796af09204b1/gr7.jpg gr7 gr7.jpg jpg 81542 165 483 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224001154-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr8/DOWNSAMPLED/image/jpeg/ea297f12d2d413511cd1b2834ce94d7b/gr8.jpg gr8 gr8.jpg jpg 91287 258 473 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224001154-gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr10/DOWNSAMPLED/image/jpeg/c00b9b8fd8234a08b11c64e7311219ad/gr10.jpg gr10 gr10.jpg jpg 95007 131 470 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224001154-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr1/DOWNSAMPLED/image/jpeg/b220b08fed8564002ad6a8ed68229cd7/gr1.jpg gr1 gr1.jpg jpg 109548 338 528 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224001154-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr2/DOWNSAMPLED/image/jpeg/e69965e05eff55aad55577ae37a42451/gr2.jpg gr2 gr2.jpg jpg 126552 301 524 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224001154-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr5/DOWNSAMPLED/image/jpeg/8f062f2b2b51b9b00232bfcbb9a868b9/gr5.jpg gr5 gr5.jpg jpg 93713 303 376 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224001154-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr6/DOWNSAMPLED/image/jpeg/6cb60fab12eebf00f842f91b917537a8/gr6.jpg gr6 gr6.jpg jpg 82047 191 481 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224001154-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr3/DOWNSAMPLED/image/jpeg/1245bd0150256e601b211543878864f7/gr3.jpg gr3 gr3.jpg jpg 110576 293 521 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224001154-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr4/DOWNSAMPLED/image/jpeg/d950c8713e378941d52fca6840b201ea/gr4.jpg gr4 gr4.jpg jpg 124442 285 518 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224001154-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr9/THUMBNAIL/image/gif/af52c02e0dc9b17948b1afe5c7bcba7b/gr9.sml gr9 gr9.sml sml 71051 61 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224001154-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr7/THUMBNAIL/image/gif/6e7e3272b98718beb9f1926d7fc3f30a/gr7.sml gr7 gr7.sml sml 68329 75 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224001154-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr8/THUMBNAIL/image/gif/51f5a2693c670e1e6ead225b67d6805e/gr8.sml gr8 gr8.sml sml 71472 119 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224001154-gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr10/THUMBNAIL/image/gif/4c2e6e0a5d74157714b7193fc2103ca0/gr10.sml gr10 gr10.sml sml 70839 61 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224001154-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr1/THUMBNAIL/image/gif/67fa11585eb13e7add67c8ef2e22205a/gr1.sml gr1 gr1.sml sml 75108 140 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224001154-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr2/THUMBNAIL/image/gif/5e4b1bb3ec5ed5f8b006cfb5e0c8cdca/gr2.sml gr2 gr2.sml sml 76048 126 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224001154-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr5/THUMBNAIL/image/gif/c6e69e975cad12baa2deeec653793c20/gr5.sml gr5 gr5.sml sml 72709 164 203 IMAGE-THUMBNAIL 1-s2.0-S0360544224001154-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr6/THUMBNAIL/image/gif/846734e7e4874f2796880bd0afb43615/gr6.sml gr6 gr6.sml sml 67952 87 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224001154-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr3/THUMBNAIL/image/gif/a45bfc573c9138218a7e7a1ac02dd4af/gr3.sml gr3 gr3.sml sml 73332 123 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224001154-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/gr4/THUMBNAIL/image/gif/808c66517ad131df37ef511638e45cd4/gr4.sml gr4 gr4.sml sml 75522 120 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224001154-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/HIGHRES/image/jpeg/6da784d82c746842dd881d14ea4cc1fe/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 289131 581 2080 IMAGE-HIGH-RES 1-s2.0-S0360544224001154-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/HIGHRES/image/jpeg/5e48bf0cdca3e63e7e4b5bb66fa159d9/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 181011 733 2141 IMAGE-HIGH-RES 1-s2.0-S0360544224001154-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/HIGHRES/image/jpeg/ba1eaae87e96e220d74617bcb24f7523/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 240558 1144 2097 IMAGE-HIGH-RES 1-s2.0-S0360544224001154-gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/HIGHRES/image/jpeg/3e62683323039be68aaa77cd16b63371/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 278646 581 2080 IMAGE-HIGH-RES 1-s2.0-S0360544224001154-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/HIGHRES/image/jpeg/05ba1380181e1600c36b0164856df6f3/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 319523 1497 2340 IMAGE-HIGH-RES 1-s2.0-S0360544224001154-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/HIGHRES/image/jpeg/eaf5f3a3ca2ffca5dd17c34ed730533d/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 492637 1333 2320 IMAGE-HIGH-RES 1-s2.0-S0360544224001154-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/HIGHRES/image/jpeg/92e35f84225665abd1381df0def40742/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 236741 1346 1668 IMAGE-HIGH-RES 1-s2.0-S0360544224001154-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/HIGHRES/image/jpeg/6c50da1b6c4ddcf473959218eb870aab/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 184003 846 2130 IMAGE-HIGH-RES 1-s2.0-S0360544224001154-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/HIGHRES/image/jpeg/6c93a278645e32534ab76c6d6be06d28/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 373841 1297 2307 IMAGE-HIGH-RES 1-s2.0-S0360544224001154-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/HIGHRES/image/jpeg/318e42099ee46a1345ddb7478ceb2f51/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 497965 1262 2295 IMAGE-HIGH-RES 1-s2.0-S0360544224001154-si1.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/774b1e55a4cb98b64557d8afac1861ef/si1.svg si1 si1.svg svg 1528 ALTIMG 1-s2.0-S0360544224001154-si12.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/935f307fd42d07e5115c086f909ffd85/si12.svg si12 si12.svg svg 1365 ALTIMG 1-s2.0-S0360544224001154-si13.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/9cec9b37cb305e79b57eb0231ea0002f/si13.svg si13 si13.svg svg 2309 ALTIMG 1-s2.0-S0360544224001154-si14.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/bc0087331e45e7592caef570517a1159/si14.svg si14 si14.svg svg 1447 ALTIMG 1-s2.0-S0360544224001154-si15.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/804f1f7bc9b0cc364d9bfc102f1b9ea0/si15.svg si15 si15.svg svg 2387 ALTIMG 1-s2.0-S0360544224001154-si17.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/d99c4a6a9f3cbae9376e0529292c8464/si17.svg si17 si17.svg svg 17237 ALTIMG 1-s2.0-S0360544224001154-si18.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/d77168a2e06ce8f9eb1b8c973b777c09/si18.svg si18 si18.svg svg 2007 ALTIMG 1-s2.0-S0360544224001154-si19.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/e8fa88f0862f43cef34c39a15e289e7c/si19.svg si19 si19.svg svg 19095 ALTIMG 1-s2.0-S0360544224001154-si2.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/c9dadfdf2adbd91d91a286485a8167c6/si2.svg si2 si2.svg svg 1210 ALTIMG 1-s2.0-S0360544224001154-si21.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/c3c218618f01b6159cd1bf71eaec1299/si21.svg si21 si21.svg svg 14990 ALTIMG 1-s2.0-S0360544224001154-si22.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/0805199cde033d116781ff83962f071d/si22.svg si22 si22.svg svg 15254 ALTIMG 1-s2.0-S0360544224001154-si23.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/3faf37ab9f407cc0068b6d16d3a304f3/si23.svg si23 si23.svg svg 22117 ALTIMG 1-s2.0-S0360544224001154-si24.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/c3eadfcf969b183c5f81d9cfd53f7894/si24.svg si24 si24.svg svg 11537 ALTIMG 1-s2.0-S0360544224001154-si25.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/d18580f015dc85001b2874a61b83b996/si25.svg si25 si25.svg svg 2503 ALTIMG 1-s2.0-S0360544224001154-si26.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/3c754dbf027d2ff7349c06bda42eabc3/si26.svg si26 si26.svg svg 1085 ALTIMG 1-s2.0-S0360544224001154-si27.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/043c4d18ec2dd135b1226c9f273df24f/si27.svg si27 si27.svg svg 4197 ALTIMG 1-s2.0-S0360544224001154-si29.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/97d2463fdbc62613c608c18732a6b526/si29.svg si29 si29.svg svg 2136 ALTIMG 1-s2.0-S0360544224001154-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/25f850f713adb1aae386f30b505a9ddf/si3.svg si3 si3.svg svg 1425 ALTIMG 1-s2.0-S0360544224001154-si30.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/ec7f182bfeca37f8161495cac3f9c983/si30.svg si30 si30.svg svg 2428 ALTIMG 1-s2.0-S0360544224001154-si31.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/86922946ae7aa40e6e8f026f047e2906/si31.svg si31 si31.svg svg 2626 ALTIMG 1-s2.0-S0360544224001154-si33.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/f84de7e6c21624d9fd17ccd784f304c7/si33.svg si33 si33.svg svg 4504 ALTIMG 1-s2.0-S0360544224001154-si35.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/84361bf6ff5783197ab829b7b31d6010/si35.svg si35 si35.svg svg 6750 ALTIMG 1-s2.0-S0360544224001154-si36.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/2b02016c7ccbece27943072d52cb08cd/si36.svg si36 si36.svg svg 5172 ALTIMG 1-s2.0-S0360544224001154-si37.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/3c0a08ed8bc669cb1e41483d2794ef2e/si37.svg si37 si37.svg svg 6291 ALTIMG 1-s2.0-S0360544224001154-si38.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/9bc9ad958125c1d2a9d56f14dc3d63a7/si38.svg si38 si38.svg svg 6816 ALTIMG 1-s2.0-S0360544224001154-si39.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/57a662d207f3e6851cb422729ea15c91/si39.svg si39 si39.svg svg 1239 ALTIMG 1-s2.0-S0360544224001154-si4.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/11d7af5f670da860d1b295514f25bdc7/si4.svg si4 si4.svg svg 1530 ALTIMG 1-s2.0-S0360544224001154-si40.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/caaf70ed518da8e73bd30241f0652ed1/si40.svg si40 si40.svg svg 1333 ALTIMG 1-s2.0-S0360544224001154-si41.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/7836829b56d9b6793fa4c48f1022b477/si41.svg si41 si41.svg svg 1448 ALTIMG 1-s2.0-S0360544224001154-si42.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/e6a9a4a7c3c7d1542ff66f432f908748/si42.svg si42 si42.svg svg 6570 ALTIMG 1-s2.0-S0360544224001154-si47.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/884fcc0221a38ef12e4c92fe10cf34da/si47.svg si47 si47.svg svg 15281 ALTIMG 1-s2.0-S0360544224001154-si48.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/2ecb375cb64a36e7cc86167b60244132/si48.svg si48 si48.svg svg 20447 ALTIMG 1-s2.0-S0360544224001154-si49.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/7f471cb06be91d7402248b523a51e1b1/si49.svg si49 si49.svg svg 1627 ALTIMG 1-s2.0-S0360544224001154-si5.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/bda9fff4202de83e1f9c58066a935524/si5.svg si5 si5.svg svg 1147 ALTIMG 1-s2.0-S0360544224001154-si50.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/01ba77327540d67ca5e7d88c2762bb2c/si50.svg si50 si50.svg svg 1416 ALTIMG 1-s2.0-S0360544224001154-si51.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224001154/image/svg+xml/4a6384978c6ce3bbefc71e3f5aa5245a/si51.svg si51 si51.svg svg 4347 ALTIMG 1-s2.0-S0360544224001154-am.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:10P76F1QL70/MAIN/application/pdf/a317c421bbbefb2d7488c6fc64109605/am.pdf am am.pdf pdf false 877298 AAM-PDF EGY 130344 130344 S0360-5442(24)00115-4 10.1016/j.energy.2024.130344 Elsevier Ltd Fig. 1 Conceptual framework of BOPTEST [42]. Fig. 2 Training procedures of Soft Actor\u2013Critic (SAC). Fig. 3 Combination of Recurrent Neural Network and Reinforcement Learning. Fig. 4 Observation spaces generation methods for M.3. Fig. 5 Structure of thermostatic control reward function. Fig. 6 Evolution of average return per episode of three SAC agents in cost and thermal comfort. Fig. 7 Evolution of average return per episode of three SAC agents in thermostatic control. Fig. 8 Comparison of overall operational results based on thermal comfort and running costs. Fig. 9 Zone air temperature changing in typical heat. Fig. 10 Zone air temperature changing in typical cool. Table 1 Summary of studies applying DRL in building HVAC system control. Ref. Target system Control action Optimization objective Algorithm Training environment [18] Boiler heating system Supply water temperature setpoint Energy consumption; indoor air temperature DQN EnergyPlus [19] Chiller plant Chilled water temperature Energy consumption; thermal comfort DQN EnergyPlus [31] Heat pump Heat pump signal Operation cost; thermal comfort DDQN; DDPG; SAC BOPTEST [20] Radiant heating system Supply water temperature setpoint Energy consumption; thermal comfort A3C EnergyPlus [21] Air conditioning units Zone temperature; ventilation fan Energy consumption; air quality; DQN EnergyPlus [22] Heating system Thermostat Energy consumption; thermal comfort DQN MATLAB [23] VAV system Temperature set; fan mass flow Energy consumption; indoor air temperature SAC; TD3; PPO; TRPO EnergyPlus [24] Smart HVAC Power usage; air volume Energy cost; indoor air quality DDQN-PRE Measured data [25] VAV Air temperature; supply water setpoint Energy consumption; indoor air quality DQN EnergyPlus [26] Building energy system Operating ratio Off-grid operation DDPG;TD3 Measured data [27] Thermal storage Operation of the cold-water storage tank Cost SAC EnergyPlus Table 2 Summary of studies integrated predictive model with DRL. Ref. Target system Optimization objective Algorithm Predictive model [38] AHU Energy consumption; thermal comfort DDPG LSTM [39] Energy storage Energy consumption; thermal comfort SAC LSTM [40] Heat pump Energy cost; indoor air quality Q-learning LSTM [29] AHU Energy consumption; PMV values SAC LSTM Table 3 State variable description and associated the minimum and maximum value. State variable [Unit] Min. Value Max. Value Has prediction Location of the state in episode [1] 0 167 No Zone air temperature [K] 280 310 No Direct normal radiation [ w / m 2 ] 0 982.62 Yes Global horizontal solar irradiation [ w / m 2 ] 0 1027.25 Yes Outside relative humidity [1] 0 1 Yes Outside drybulb temperature [K] 248 310 Yes Wet bulb temperature [K] 248 294 Yes Sky cover [1] 0 1 Yes Power price [$] 0.0444 0.13814 Yes Lower bounds of the comfort range [K] 280 310 Yes Upper bounds of the comfort range [K] 280 310 Yes Table 4 The hyperparameters of three SAC agents. Hyperparameter SAC for M.1 SAC for M.2 SAC for M.3 Learning rate 0.0002 0.0002 0.0002 Discount factor 0.99 0.99 0.99 Soft update 0.005 0.005 0.005 Memory size 50000 50000 50000 Minibatch sampling size 64 64 64 Training steps 168000 168000 168000 Network architecture Linear: 400 * 300 Linear 400 * 300 GRU: 1 layer; 32 hidden size Table 5 Performance results of different controllers during the heating periods under the dynamic pricing scenario. Peak heat Typical heat M.1 M.2 M.3 M.1 M.2 M.3 Operational cost ( $ / m 2 ) 0.045 0.038 0.036 0.049 0.044 0.039 Indoor thermal comfort index 0.253 0 0 0 0.061 0.033 Table 6 Performance results of different controllers during the cooling periods under the dynamic pricing scenario. Peak cool Typical cool M.1 M.2 M.3 M.1 M.2 M.3 Operational cost ( $ / m 2 ) 0.099 0.094 0.092 0.063 0.055 0.052 Indoor thermal comfort index 0.029 0.005 0.080 1.758 0.122 0.204 Table 7 Performance results of different controllers under the dynamic pricing scenario and thermostatic control. Average temperature difference per episode [°C] M.1 M.2 M.3 Peak heat 0.369 0.322 0.283 Typical heat 0.309 0.309 0.232 Peak cool 0.414 0.287 0.231 Typical cool 0.390 0.243 0.238 Successful application of predictive information in deep reinforcement learning control: A case study based on an office building HVAC system Yuan Gao Conceptualization Data curation Formal analysis Funding acquisition Investigation Methodology Resources Software Validation Visualization Writing \u2013 original draft a \u204e Shanrui Shi Validation Visualization Writing \u2013 original draft Writing \u2013 review & editing b Shohei Miyata Funding acquisition Project administration b Yasunori Akashi Funding acquisition Project administration b a The Center for Energy Systems Design (CESD), International Institute for Carbon-Neutral Energy Research (WPI-I2CNER), Kyushu University, Japan The Center for Energy Systems Design (CESD), International Institute for Carbon-Neutral Energy Research (WPI-I2CNER), Kyushu University Japan International Institute for Carbon-Neutral Energy Research (WPI-I2CNER); Kyushu University, 744 Motooka, Nishi-ku, Fukuoka-shi, Fukuoka, 819-0395, Japan b Department of Architecture, Graduate School of Engineering, The University of Tokyo, Japan Department of Architecture, Graduate School of Engineering, The University of Tokyo Japan Department of Architecture, Graduate School of Engineering, The University of Tokyo, Japan \u204e Corresponding author. Reinforcement Learning (RL), a promising algorithm for the operational control of Heating, Ventilation, and Air Conditioning (HVAC) systems, has garnered considerable attention and applications. However, traditional RL algorithms typically do not incorporate predictive information for future scenarios, and only a limited number of studies have examined the enhancement and impact of predictive information on RL algorithms. To address the issue of coupling RL and predictive information in HVAC system operation optimization, we employed an open-source framework to examine the impact of various predictive information strategies on RL outcomes. We propose a joint gated recurrent unit (GRU)-RL algorithm to handle situations where a time-series exists in state space. The results from four classic test cases demonstrate that the proposed GRU-RL method can reduce operating costs by approximately 14.5% and increase comfort performance by 88.4% in indoor comfort control and cost-management tasks. Moreover, the GRU-RL method outperformed the conventional DRL method and was merely augmented with prediction information. In indoor temperature regulation, the GRU-RL algorithm improves control efficacy by 14.2% compared to models without predictive information and offers an approximately 5% improvement over traditional network models. Finally, all models were made open source for easy replication and further research. Keywords Recurrent neural network Deep reinforcement learning Time-series prediction HVAC system Data availability Data will be made available on request. 1 Introduction 1.1 Background Most individuals spend approximately 80%\u201390% of their lives within buildings [1]. At the same time, buildings are responsible for approximately one-third of global energy consumption and a quarter of CO2 emissions [2,3]. Notably, in building facilities, Heating, Ventilation, and Air Conditioning (HVAC) systems are substantial energy consumers, accounting for 38% of the overall energy usage, which is equivalent to 12% of the final energy consumption [2,4]. As global warming continues to be a growing concern, the energy needs of HVAC systems are predicted to rise even more [5,6]. Well-designed and efficient HVAC system controls play a crucial role in providing occupants with a healthy and comfortable indoor environment while effectively managing energy consumption and carbon emissions. However, the complexity of HVAC system control has increased significantly in recent years. Modern HVAC systems must accommodate various factors, such as integrating on-site renewable energy sources, incorporating energy storage solutions, and responding to grid signals for load shifting to enhance grid stability and security [7]. In contrast to the need for advanced control strategies, the most conventional approach for HVAC system control is rule-based feedback control. This method relies on predetermined schedules to set parameters such as temperature setpoints and utilizes setpoint tracking algorithms such as proportional\u2013integral\u2013derivative (PID) to regulate the system. Although rule-based control (RBC) is simple and effective in maintaining a comfortable range to achieve occupant comfort, it has limitations in considering exogenous factors and predictive information, such as occupancy, electricity price, weather conditions, and demand response signals [8,9]. Consequently, RBC has proven to be a suboptimal approach [10]. Moreover, tuning the gains of PID controllers can be cumbersome, and the control performance may degrade or lead to system oscillation or sluggishness when the operational conditions deviate significantly from those during controller tuning [11,12]. Considerable research attention has shifted towards developing optimal control strategies, particularly those based on machine learning (ML) methods owing to the need to enhance the HVAC system control performance. The advent of big data, powerful computing capabilities, and advancements in algorithms have enabled the exploration and implementation of ML-based control approaches for building HVAC systems [13]. 1.2 Reinforcement learning for HVAC system control Reinforcement Learning (RL), a prominent category of machine learning alongside unsupervised and supervised learning, stands apart because of its unique learning paradigm based on dynamic data acquired during the learning process [14]. The primary objective of RL is to learn the optimal actions that lead to a predefined goal. Through rewards, this reinforcement guides the agent to honor its decision-making abilities and achieve the desired objective [10,15]. Deep Reinforcement Learning (DRL) shares the foundational principles of classical RL; however, it distinguishes itself by employing complex deep neural networks (DNNs) as RL agents instead of simple tabular settings or linear functions. Leveraging DNNs endows the RL agent with a higher representational capability, enabling adaptation to intricate control problems and facilitating the resolution of real-world complexities that demand a large number of states and actions to properly represent the control problem [16]. DRL is a highly suitable approach for HVAC system control since building HVAC systems is nonlinear, time-varying, and uncertain complex [17]. Many studies have applied DRL techniques to various aspects of HVAC system control, including the primary subsystem [18,19], secondary subsystem [20\u201323], entire HVAC system [24,25], and associated energy management system [26\u201328]. An overview of studies applying DRL to HVAC system control is presented in Table 1. These advanced control strategies have shown remarkable potential for HVAC system control, significantly improving indoor comfort conditions while simultaneously reducing energy consumption. However, the DRL process involves testing and evaluating new strategies through trial and error and subsequently using these evaluations to refine its control policy. This experimental approach poses challenges for the implementation of DRL controllers in real-world buildings. To address this limitation, an effective method involves pretraining the DRL controller offline by utilizing a surrogate virtual environment [27]. Many studies have developed customized HVAC system models based on simulation platforms such as EnergyPlus and MATLAB, enabling the DRL controller to interact within these virtual environments [10]. Nevertheless, accurately modeling the full spectrum of real-world environmental features is challenging, potentially leading to poor generalizability of the controller when applied to actual buildings [29,30]. Furthermore, the influence of future information, such as weather conditions, significantly influences the performance of HVAC system control, particularly regarding precooling and preheating operations [10]. To address this issue, an increasingly promising approach involves integrating predictive information into a DRL controller. 1.3 Predictive model integrated with RL for HVAC system control Recurrent Neural Networks (RNNs) are a type of Artificial Neural Network (ANN) specifically designed to address tasks like language modeling, machine translation, and speech recognition. Their defining feature lies in their recurrent connections, which enable them to maintain a hidden state that carries information from previous time steps, thereby influencing the current step. This characteristic makes RNNs suitable for processing sequential data such as natural language and time-series sequences. Considering that data related to buildings inherently involve time-series data and that building performance is affected by short-term and long-term variations, such as occupancy and seasonal changes, intrinsic temporal dependencies are prevalent [32]. As a modified version of RNN, Long Short-Term Memory (LSTM) addresses the vanishing gradient problem and long-term dependency issues associated with traditional RNNs [33]. Instead of using a recurrent layer, the LSTM recurrent unit processes information by employing various gates (forget and input gates) to control the information that should be remembered, forgotten, or updated in the cell state. Subsequently, it produces a new hidden state that passes through the output gate for use in the subsequent time step. Given this perspective, LSTM has found widespread application in the field of building information prediction, including control performance, energy consumption, thermal comfort, and air quality [34\u201337]. For successful integration with a DRL controller, the predictive model must be capable of offering continuous feedback for control operations, which makes the RNN model (specifically, LSTM) a suitable candidate [29]. Despite its potential to improve control performance, the application of integrating predictive information into optimal HVAC control using DRL remains limited in current literature. Zou et al. [38] utilized LSTM in combination with a DDPG agent to control air handling units (AHU), achieving 27%\u201330% energy savings while ensuring thermal comfort. LSTM models utilize historical Building Automation System (BAS) data, approximate HVAC operations, consider the current state and action as inputs, and provide the next state and reward as outputs. Pinto et al. [39] employed an LSTM coupled with an SAC agent to optimize heat pumps, chilled water, and domestic hot water storage across four buildings. Their approach effectively reduced electricity consumption costs by 23%. Blad et al. [40] addressed the issue of poor behavior during the early training of an RL algorithm by coupling LSTM with a Q-learning-based multi-agent algorithm for the online fine-tuning of HVAC systems. As a result, they successfully reduced heating costs by 19.4% compared to traditional control policies. Zhuang et al. [29] proposed a set of 16 LSTM-based model architectures that incorporated different combinations of convolutional neural networks (CNN), bidirectional processing, and attention mechanisms (AM) to identify the best-performing model setups. The findings indicate that the attention mechanism notably enhances the prediction performance in both recursive and independent prediction scenarios. These optimal models were then integrated with an SAC RL agent to analyze sensor metadata and optimize HVAC operations, resulting in significant energy savings of 17.4% and a remarkable 16.9% improvement in thermal comfort within the surrogate environment. Table 2 summarizes the current studies that integrate predictive models with RL for HVAC control. However, further research is required to explore and harness the potential of integrated controllers in this domain. 1.4 Objective of this research This study aims to address the limitations of current predictive models integrated with reinforcement learning for HVAC system control. Through a comprehensive review and discussion, the authors identified the following limitations of the existing approaches: \u2022 Lack of Reproducibility and Comparison: Many studies use customized virtual simulation models as the training environment for DRL controllers. This approach hinders the reproducibility and testing of developed algorithms, making it challenging to compare different control algorithms for specific applications. Additionally, the use of simulation platforms like EnergyPlus and MATLAB, which do not fully model the dynamics of HVAC systems and sometimes use idealized controllers [41], may limit the generalizability of the developed algorithms to real buildings. \u2022 Table 2 summarizes the studies combining Reinforcement Learning (RL) and forecasting information in HVAC systems. Most of these studies incorporate networks such as LSTM for future predictions and then feed the predictive data into RL. The number of studies is relatively small, indicating that the research is not yet comprehensive. The discussion has not yet touched on how to correctly utilize forecasted data in scenarios where such information already exists. This situation is quite prevalent in optimizing HVAC systems, for instance, using weather forecasts for anticipated outdoor conditions. Considering these limitations, this study focuses on exploring the effective application of predictive data in reinforcement learning. We compare traditional networks and RNN in terms of their ability to handle predictive information from observational data. Simultaneously, we explored the potential impact of this predictive information on the optimization of the HVAC system operation. The presented work encompasses the development of an open-source test platform and algorithms using the BOPTEST platform [42] and Modelica-based models, focusing on the dynamic behavior of HVAC systems to enhance reproducibility. It also explores the integration of RL and predictive information in optimizing HVAC operations, evaluating the efficacy of different approaches in handling predictive data. The proposed combination of RNN and RL demonstrates superior performance over conventional neural network approaches in both tasks, establishing a valuable reference for future research in this field. 2 Methods 2.1 Emulator environment To fully verify the effect of the proposed reinforcement learning algorithm, we used a building optimization testing (BOPTEST) framework as a benchmark [42]. BOPTEST is an open-source standardized building simulator that includes several types of buildings and HVAC systems. The open-source framework improved the transparency of the testing algorithm, and the reproducibility of the framework was very strong. A conceptual diagram of BOPTEST is illustrated in Fig. 1. All BOPTEST\u2019s building models are developed using Modelica and exported as Functional Model Units (FMUs). These are bundled into Docker containers complemented by boundary conditions and external disturbances such as a year\u2019s worth of weather data. The application programming interface (API) allows an external controller to access observed and forecasted data, modify control signals, choose test scenarios, establish simulation environments, and proceed with the simulation. BOPTEST provides a standardized set of key performance indicators (KPIs) for objective performance assessments, incorporating factors such as energy consumption, thermal discomfort, operational expenses, indoor air quality discomfort, emissions, and proportion of computing time. 2.2 Basic definitions and methods of RL Before discussing the algorithms, we first established a foundational understanding of reinforcement learning and its fundamental parameters. An in-depth explanation is provided in Ref. [43]; here we provide a concise overview to ensure the coherence of the article. RL engages in a process of learning via trial and error, directing behavior based on rewards achieved from environmental interactions, aiming to maximize cumulative rewards. Each RL challenge can be represented using a Markov Decision Process (MDP) [44]. The Markov Decision Process (MDP), characterized by quintuples ( S , A , P , R , γ ), is employed for scheduling optimization within a building renewable energy system. \u2022 S : Reinforcement learning agents utilize accessible information, encompassing current, historical, and predictive data, to inform their decisions. For instance, in managing a building\u2019s HVAC system temperature, an RL agent bases its decisions on factors such as indoor and outdoor temperatures, solar radiation intensity, and future temperature forecasts. \u2022 A : Actions reflect the variety of decisions an agent makes in response to diverse environments; for instance, factors like airflow and air temperature. \u2022 P : State transition probability depicts the state\u2019s distribution at the subsequent time step given a certain action in a particular state. If the RL algorithm learns an explicit state transition probability, it is considered model-based. Its counterpart is referred to as model-free RL [26]. \u2022 R : The reward function establishes the computation of the immediate reward achievable for a specific state\u2013action pair. This reward calculation could be a defined value or a mathematical expectation. The reward magnitude directs the optimization course. \u2022 γ : In RL problems, due to feedback delays, the current step\u2019s action might require several steps to take effect. Therefore, the agent should not merely select the action yielding the highest immediate reward but should anticipate the outcomes of future steps. The discount factor, γ , enables RL to equilibrate short- and long-term objectives. By discounting future rewards, we can ascertain the value of the agent\u2019s present actions, which equates to the cumulative future rewards and signifies the true optimization target of RL. The foregoing delineates the core elements and enhancement objectives of RL. Conventional RL approaches typically employ tables to manage finite actions and states. This strategy is effective for simplified states and offers a fast convergence. However, in real-world scenarios, these discrete states and actions are often reduced. The exterior temperature of a building is an example of a typical continuous-state value. DRL and RL share the same theoretical underpinnings. This unique distinction lies in DRL\u2019s utilization of a DNN instead of a conventional table to approximate cumulative rewards, offering more effective management of continuous vector spaces. This study primarily focuses on research involving DRL. 2.3 DRL algorithms Deep reinforcement learning agents require deep neural networks for function approximation to handle expansive state spaces. The deep Q network (DQN) algorithm made significant strides in 2016, outperforming humans in the Atari games [16]. Nevertheless, the DQN\u2019s inability to effectively manage the RL problem in a continuous action space presents a considerable limitation to its further progression and widespread application. The Deep deterministic policy gradient (DDPG) algorithm, which employs an actor\u2013critic (AC) strategy and a deterministic policy gradient (DPG), refines the DQN algorithm for application in continuous action spaces [45]. 2.3.1 Actor\u2013critic method In a DQN, the algorithm lacks a distinct actor; instead, it opts for action selection via a greedy strategy. However, this approach falls short of continuous action spaces, rendering the DQN algorithm inapplicable to such contexts. Compared with the DQN, the Actor\u2013Critic (AC) approach enables reinforcement learning in a continuous action space via a standalone actor network [46,47]. The AC model comprises two networks: an actor network μ (parameterized by θ μ ) and a critic network Q (parameterized by θ Q ). The network configurations of the actor and the critic may differ depending on the algorithm used. Critic network Q is tasked with evaluating the current action and computing the action-value function. The actor network is updated based on the feedback from the critic network, and it uses the input state vector to derive the current action. This action is computed by maximizing the value of the critic network. In the Actor\u2013Critic (AC) framework, the Q network update approach employs the Bellman equation, as shown in Eq. (1). (1) Δ Q t = r t + γ \u2022 Q s t + 1 , μ s t + 1 ∣ θ μ ∣ θ Q − Q s t , a t ∣ θ Q In Eq. (1), the sum of the current immediate reward r t and the discounted future action-value function serves as our update target. By subtracting the present action-value function, we obtain the updated gradient of the Q network. We present an update to the actor network in Eq. (2), following [48]. The computation formula only involves gradients with respect to the actor network parameters. This approach enables the actor network to produce continuous actions and discover actions that optimize the Q network\u2019s output. (2) ∇ θ μ μ = ∇ a Q s , a ∣ θ Q s = s t , a = μ s t ∇ θ μ μ s ∣ θ μ s = s t 2.3.2 Selected algorithms DDPG leverages the principles of AC and DQN to construct actor networks by introducing a reinforcement learning algorithm capable of handling continuous action spaces. However, the update process of the DDPG algorithm has many problems with unstable training, and it is easy to enter a local optimal solution prematurely. As an improved model of DDPG, Soft Actor\u2013Critic (SAC) uses the maximum entropy model to fully improve the exploration efficiency of the model so that the model will not fall into the local optimal solution in the later stage of training [49]. This study examines the performance of this algorithm, which follows an actor\u2013critic paradigm, namely, SAC. SAC employs entropy regularization to address the inefficiency encountered during the later stages of DDPG training. The entropy regularization\u2019s goal is to maximize randomness in the strategy to avoid stagnation at local optima while simultaneously optimizing the expected returns. We selected SAC based on the following considerations: \u2022 Our HVAC control problem features a continuous action space. SAC algorithms are inherently adept at managing both continuous state and action spaces. \u2022 SAC performs well in previous work on building energy management. Brandi et al. employed an SAC agent to control a heating system, resulting in 2%\u20136% energy conservation and up to a 65% increase in thermal comfort [50]. Wang and colleagues also evaluated the performance of SAC and DDPG within the BOPTEST framework [31]. \u2022 In this study, the authors focus on the impact of introducing predictive data on RL algorithms and how to better process predictive information. Then the introduction of prediction information should take effect for all RL algorithms. Therefore, the authors also selected SAC to verify the validity of the predicted data. Fig. 2 presents a flow diagram of the SAC algorithm, which is a variant of the Actor\u2013critic algorithm family. SAC distinguishes itself by incorporating the concept of entropy to enhance policy exploration. This approach helps in avoiding local optima, as indicated by the red text in the diagram. 2.4 How to combine predictive information with RL algorithms? This study focused on investigating how incorporating prediction information can enhance and optimize reinforcement learning algorithms. From an algorithmic perspective, the incorporation of predictive data should primarily consider alterations to the state vector, meaning that predictive information is integrated directly into the state vector itself. In the context of HVAC systems, future information, which is frequently presented as time-series data such as outdoor air temperature and solar radiation fluctuations over the next 24 h, is often vital. Hence, employing an RNN to process predictive data using the DRL algorithm is intuitive. Some researchers considered combining DDPG and RNN networks [51]. Therefore, we discuss the influence of the combination of the DRL algorithm and RNN on the final operation control effect for HVAC system operation control. Fig. 3 shows how we introduce prediction information and use RNN to process the prediction information. Given that SAC utilizes AC frameworks with distinct actor and critic networks, we incorporated an RNN layer at their base. This enabled the processing of sequential data for prediction information. In essence, we integrated forecast data and current observations to form a multivariate time-series, which we subsequently processed using the RNN network. For Actor and Critic networks, we simply regard the RNN network as a feature extractor, hoping to obtain more information from the features of the time-series to help in decision-making. In this study, we employed a Gated Recurrent Unit (GRU) layer to process the observations for the RL algorithm. In essence, the computations involved in the GRU are parallel to those in LSTM [52]; however, the GRU streamlines the gating mechanism, thereby simplifying the calculation process. The GRU performs subsequent functions for each component of the input series (Eq. (3)): (3) r t = σ W i r x t + b i r + W h r h ( t − 1 ) + b h r z t = σ W i z x t + b i z + W h z h ( t − 1 ) + b h z n t = tanh W i n x t + b i n + r t ∗ W h n h ( t − 1 ) + b h n h t = 1 − z t ∗ n t + z t ∗ h ( t − 1 ) The term x t denotes the value at different time steps in the state vector, such as outdoor temperature prediction at time t . h ( t − 1 ) represents the hidden layer state computed at the previous time step. The symbols r t , z t , and n t denote reset, update, and new gates, respectively. These computational methods bear a resemblance to the LSTM. For a more comprehensive explanation of the computations and principles underlying the GRU, readers can refer to the official PyTorch documentation [53]. 3 Experiment setting 3.1 Building envelope and HVAC system The study employs \u2018BESTEST air\u2019 as a test subject, which refers to a singular room situated in proximity to Denver, CO, USA, occupying a floor area of 48 m 2 . The room configuration included four exterior walls facing the principal directions along with a flat rooftop. A pair of 6 m 2 windows was embedded in the southern wall. The room was primarily utilized as a two-person office, marked by a light load density. The thermal management system of the office consisted of an idealized four-pipe fan coil unit (FCU) that handled both heating and cooling tasks. This unit included a fan, cooling coil, heating coil, and filter. It operates by drawing room air into the unit, propelling it over the coils, filtering it, and redistributing the conditioned air into the room. A variable-speed drive powered the fan motor. The cooling coil used chilled water from the chiller, whereas the heating coil used hot water from the gas boiler. The central plant was not included in the model. We manage the HVAC system by modulating the supply air temperature within a range of 12 °C to 40 °C and standardizing the air mass flow rate in proportion to the designated design flow rate, which ranges from 0 to 1. Further details on the case studies can be found on the reference webpage [54]. 3.2 Design of state space and ablation study for prediction observations The emulator model offers an abundance of outputs. The selection of the state space is critical for the performance of the RL controller. The state variable should encapsulate both present and projected states, providing information pertinent to the agent\u2019s decision-making process. In addition, acknowledging the possibility that detailed measurements, such as those provided by BOPTEST, might not be available in practical engineering applications, we selected our observations not only considering their potential influence on indoor temperature control but also favoring more commonly available measurements. After several trials, we identified the state space by incorporating the eight variables listed in Table 3. The rationale for choosing these specific state variables is outlined below: \u2022 The states of \u2018Location in the Episode\u2019 and \u2018Zone Air Temperature\u2019 are based on the relevant features of the current timestep in the HVAC system\u2019s reinforcement learning. It is necessary to know the location of the current timestep within the episode to accelerate convergence. Meanwhile, indoor temperature is one of the control targets. However, predictions cannot be made due to the uncertainty of future actions. \u2022 Outdoor temperature, humidity, and solar radiation parameters can impact the heating and cooling load of HVAC systems, so we have incorporated these parameters into the state vector. Predictions for these variables can usually be sourced from weather forecasts; hence, we utilize these forecasted state vector values in our corresponding models. \u2022 The changes in electricity costs in this study adopt a dynamic mechanism where the cost of system power purchasing varies at different times of the day. The authors anticipate that the RL agent can learn the optimal strategy in the context of dynamically changing electricity prices. Detailed information regarding the variations in electricity prices can be found on the related case web page. \u2022 The comfort temperature range significantly influences the satisfaction of office users and is an essential aspect of this study. Moreover, the variations in the comfort temperature range can also provide information about the current occupancy status of the room, which can be obtained through RL algorithms. Moreover, the authors wish to emphasize that BOPTEST maintains a set temperature for office comfort, specifically between 21 to 24 degrees Celsius during occupancy, and 15 to 30 degrees Celsius when unoccupied, regardless of the season. 3.3 Design of state space and ablation study for prediction observations We have clarified the reasons for choosing these observations. Most of these data points yielded forecasts for future events. For example, outdoor temperature predictions can be obtained from weather forecasts, while future electricity prices can be obtained directly and accurately. We posit that predictive information can positively contribute to RL optimization and propose an effective method for handling such predictive information. It is important to note that this study directly employs the predictive API provided within the BOPTEST framework, without developing any independent predictive models or algorithms. We designed three distinct methods for processing predictive information to verify the efficacy of GRU. Other configurations of the RL algorithm, along with the random seed, remained consistent to ensure the validity and reproducibility of the ablation studies. The three processing methods and their corresponding observational space dimensions are as follows: \u2022 M.1 (baseline): M.1 will employ the most traditional reinforcement learning algorithm and state space configuration, namely choosing the actual measurements of the current time step as observations without incorporating any forecast information about the future. The performance of the remaining models incorporating predictive information will be compared with M.1, serving to validate the enhancement effect of predictive information on RL algorithms. From Table 3, it can be observed that the state space dimension for M.1 is 11, indicating that all 11 variables only take the value at the current time step. \u2022 M.2 (prediction by flatten): Starting from M.2, we incorporate predictive information into the state space. However, despite introducing predictive information in M.2, we will not alter the Actor and Critic network structures. Instead, we flatten all the information into a column vector and feed it into the reinforcement learning process for training. In Table 3, we have a total of 9 states containing future predictive information. We incorporate the current and future predictive states of these 9 variables into the state space. The two variables without predictive information are included with their current information only. For instance, when the predictive timestep is 23, the dimension of the state vector is 9 ∗ 24 + 2 = 218 . \u2022 M.3 (prediction by GRU): M.3 employs the same predictive information as M.2; however, it utilizes GRU networks in the Actor and Critic to process states. Fig. 4 illustrates the method M.3 employs to process the state space. Due to the sequential nature of GRU inputs at each time step, we designed the network to remember the current indoor temperature at every step of computing predictive information. This approach emphasizes the significance of the current temperature, preventing it from being overlooked by the network due to its small dimensional representation. Additionally, the input at each step allows the network to consider the relationship between future time steps and the current step, enhancing its ability to make more informed decisions based on a comprehensive understanding of temporal dynamics. To comply with the input requirements of the GRU network, we duplicate the two values with no predictions across all forecasted time steps. For instance, when the predictive timestep is 23, the dimension of the state vector is 11 ∗ 24 = 264 . 3.4 Design of action space The indoor temperature and operational costs were regulated using air-conditioning control signals. In this scenario, the decision variables were the supply air temperature and airflow signals, which aim to maintain the building within a comfortable range. The range of the airflow signal spans [0,1], which is a variable normalized in advance within the BOPTEST framework. The range of the supply air temperature was [288.15, 313.15], as measured in Kelvin. In summary, the action space encompasses two dimensions, both of which constitute a continuous action space. 3.5 Design of reward function The design of the reward function establishes the optimization direction and goals of the RL algorithms. To better evaluate the role of predictive information in RL, we devised two distinct reward functions and optimization objectives. 3.5.1 Reward based on user comfort and running costs Our objective was to minimize thermal discomfort and operational costs as much as possible. Consequently, the reward function was structured as a weighted sum of these two goals. Thermal discomfort signifies the total deviation of a region\u2019s operative temperature from the established comfort zone over a specified time period, expressed in K h. Operational cost represents the expenditure incurred under a given pricing scenario within a specific timeframe, measured in terms of monetary units ( $ / m 2 ). Moreover, within the BOPTEST framework, three distinct pricing models are identified: constant, dynamic, and highly dynamic electricity prices. Here, we used the Dynamic Electricity Price. BOPTEST automatically calculates the operation costs according to the electricity price corresponding to user selection. A mathematical representation of the objective function is given in Eq. (4): (4) J k = ω ⋅ C + δ Here, C represents the operational cost, and δ signifies the thermal discomfort. ω serves as a weight parameter, providing a balanced tradeoff between operational cost and thermal discomfort; this is another crucial hyperparameter that requires further tuning. The operational cost and thermal discomfort values can be sourced from the BOPTEST\u2019s APIs. Considering our aim to optimize the cumulative reward, we can define the immediate reward at a specific time as follows (Eq. (5)): (5) r k + 1 = − J k + 1 − J k With this computational methodology, the maximum value of the reward was zero. A pivotal decision in designing the reward function is to determine the value of the weight ω , which aims to balance the tradeoff between operational cost and thermal comfort. A higher value of ω indicates a preference for reduced operational costs over comfort. Theoretically, ω should be selected according to the preferences of the building occupants. However, a survey on how to choose ω is beyond the scope of this study because the aim is to compare the impact of predictive information on RL algorithms. Moreover, the selection of weights should consider the absolute numerical differences between the two objectives. We assigned a value of 16 to the weight parameter, which can be adjusted for other applications. 3.5.2 Reward based on thermostatic control In Section 3.5.1, we develop a reward function using BOPTEST\u2019s inherent API to fulfill the requirements of both indoor thermal comfort and operating cost reduction. However, the presence of two criteria in the optimization objective is disadvantageous when comparing the degrees of enhancement of the predictive information, and the GRU contributes to the RL algorithm. This is because of the potential improvement in one criterion, whereas the other worsens. In this study, the authors compared another control method for HVAC systems, specifically, thermostat control. The objective of this system was to maintain the indoor temperature at a fixed value. Here, we disregard the operational cost of the system and focus solely on a single objective to facilitate a comparison of the ultimate effects of diverse predictive information on RL. Here, we establish a reward function for constant temperature control by creating a Gaussian distribution. The viability of this reward function was confirmed in prior studies using RL algorithms [26,46]. First, we compute the temperature disparity for the present time interval by deriving the optimization strategy for each stage. Here, we make the assumption that the HVAC system aims to maintain the indoor temperature at 22 degrees Celsius. Upon completion of the calculations, we were guided by the properties of the Gaussian distribution (Eq. (6)), have set the mean of the Gaussian distribution to 295.15 and the standard deviation to 2 for the purposes of this study. Following this, we scaled the Gaussian distribution to a certain extent, resulting in a maximum value of 1, a minimum value of −0.6, and both positive and negative values. The scaling technique is expressed in Eq. (7). We obtained the reward function shown in Fig. 5 by leveraging the probability density function of the Gaussian distribution. For ease of display, Celsius is used to plot in this context. (6) f ( x ) = 1 σ 2 π e − ( x − μ ) 2 2 σ 2 (7) f ( x ) = 8 ∗ 1 σ 2 π e − ( x − μ ) 2 2 σ 2 − 0 . 6 where x is the zone air temperature from BOPTEST and π indicates the control target of our optimization, 293.15 K for instance. This Gaussian distribution transforms the computed error value into a continuous reward value, ranging from −0.6 to 1, an interval ideal for gradient descent. Moreover, the optimization target can be conveniently altered by adjusting the mean of the Gaussian distribution. For instance, setting the mean to 23 degrees Celsius indicates our aim to control the room temperature at 23 degrees Celsius. 3.6 Experimental setting The target building dataset incorporates hourly data measurements. The model was trained using data from an entire year. During the training process, to maximize the use of data, each episode randomly selects an initial time step from the training set and then employs a fixed one-week warm-up to reset the environment and capture the initial state. Optimization was performed on an hourly basis over one week for each episode. The optimization time step for the entire process was set to one hour. We utilized four standard testing scenarios provided in BOPTEST to evaluate model performance. Descriptions of these scenarios are as follows: \u2022 Peak Heat Day: The testing period comprises a two-week assessment that incorporates a one-week warm-up phase using baseline control. This fortnight is strategically positioned around the day featuring the highest 15-minute system heating load annually. \u2022 Typical Heat Day: The evaluation period constitutes a two-week test, preceded by a one-week warm-up phase employing baseline control. The two-week interval is anchored around the day featuring the maximum quarter-hourly system heating load, which is nearest yet lower than the median of all annual highest quarter-hourly heating loads. \u2022 Peak Cool Day: This study entails a testing period of two weeks, which includes a one-week preliminary phase using baseline control. The two-week duration is strategically aligned to encompass the day of the highest quarterly-hour system cooling load throughout the year. \u2022 Typical Cool Day: This experiment encompasses a two-week testing interval preceded by a one-week acclimation phase, implementing baseline control. The two-week span is aligned with the day that features the maximum 15-minute system cooling load closest, but not exceeding, the median of all daily 15-minute peak cooling loads throughout the year. All algorithms were implemented using PyTorch and accelerated using an RTX 4090 graphics card. The environmental code was written using the OpenAI Gym framework [55], and the Soft Actor\u2013Critic (SAC) algorithm was trained and tested using the Stable Baselines 3 open-source library [56]. To ensure a fair comparison, we fixed the random seed in all the training and testing processes, ensuring that our experiments could be consistently replicated on any machine. The final hyperparameters for the SAC are summarized in Table 4, which were determined through manual experimentation. We also attempted to use Optuna, a Python library, for automatic hyperparameter optimization [57]. However, the lack of parallel computational support in BOPTEST-Gym results in excessively long adjustment times. Thus, we could not successfully implement it for our task, nor could we find an optimal global configuration for the hyperparameters. However, because our study compares the effects of different prediction information usages on RL, maintaining consistency in the hyperparameters is sufficient. The learning rate (0.0002) was chosen based on multiple experiments and experiences. In addition to the learning rate, the hyperparameter settings are fundamentally based on the stable-baseline library as they optimize the hyperparameters, endowing them with improved generalization. 4 Results and discussion In reinforcement learning problems, we typically focus on improving and evolving the agent\u2019s performance during training. Unlike in supervised learning, we are less concerned with variations in the loss function. Figs. 6 and 7 depict the changes in cumulative rewards for different agents throughout their training. All models were trained for 1000 episodes in the BOPTEST environment. Given that each episode consisted of 168 time steps (corresponding to 168 h per week), the total number of time steps for all episodes was 168,000. After a certain training period, all sessions converged to a relatively stable form. The training curves were consistent with those of general RL algorithms. Notably, regardless of the task reward function, M.1 converged at the fastest rate, whereas M.3 converged at the slowest. This slowdown was expected because a GRU neural network was used in M.3. The GRU network processes sequences based on temporal variations, which precludes the possibility of rapid, parallel computations on a large scale. Additionally, the network requires sequential processing of the state vectors to capture the characteristics of time-series changes as effectively as possible. From the perspective of the final converging results, it appears that the convergence speed has not been significantly impacted. 4.1 Statistic experiments in four testing episodes 4.1.1 Reward based on user comfort and running costs The overall performance indicators of the four test episodes are listed in Tables 5 and 6. The Operational Cost and the Indoor Thermal Comfort Index were calculated within the BOPTEST framework. The Indoor Thermal Comfort Index indicates the extent to which the system violates the upper and lower bounds of comfort during operation. Higher values indicate poorer performance in terms of thermal comfort. The results in Table 5 suggest that the reinforcement learning model employing predictive information outperformed Model M.1, which did not utilize a predictive approach, in both heating cycles. When tests were conducted during the peak heat period, the M.2 device, with added predictive information, reduced operational costs by 15.6%. The M.3 model, which employs a GRU network for handling predictive information, is capable of reducing operational costs by 20%, marking a 5 percentage point improvement over conventional networks. In terms of indoor thermal comfort, both M.2 and M.3 utilized predictive information and successfully maintained the indoor temperature within the comfort range for 168 h. When the experimental case shifts to a typical heat scenario, the M.2 model, which solely uses predictive information, can reduce the operational costs by 10.2%. The M.3 model, which uses a GRU network, can decrease expenses by 20.4%, which is the best performance among the three models; this confirmed the effectiveness of employing a GRU network for processing predictive data. In terms of indoor thermal comfort, Model 1 (M.1) exhibited the best performance. Model 3 (M.3), with the integration of a GRU network, exhibited a comfort level improvement of 46% over Model 2 (M.2). Given that our reward function primarily emphasizes operating cost considerations, these results are reasonable and expected. The cooling cycle presented in Table 6 corroborates the conclusions of previous studies. M.3 consistently achieved the best performance in terms of operational costs. In peak cooling scenarios, M.3 saves 7% of the operational costs compared to M.1; in typical cooling scenarios, the savings increase to 17.5%. In terms of indoor temperature control, M.3 performed the worst during all cooling periods. This showed an 88.4% improvement over Model M.1, which did not utilize predictive information. However, given that we assigned a 16-fold weight to the operational costs of the reward function, this performance is reasonable. This demonstrates the efficacy of the GRU model in handling predictive information for reinforcement learning and current tasks. In this context, we observe that during peak cooling periods, Model M.3 does not maintain optimal indoor thermal comfort, underperforming compared to Models M.1 and M.2, and particularly against M.1. This disparity is likely attributable to the configuration of the reward function. In experiments balancing operational costs and thermal comfort, we assigned a 16-fold greater weight to the reward for operational cost efficiency. Since reinforcement learning aims to maximize the overall reward, this naturally led to a greater focus on operational costs. The rationale behind this 16-fold weighting was primarily to achieve a numerical balance. Within the BOPTEST framework, the absolute numerical value of operational costs is lower than that of the thermal comfort index. Consequently, it became necessary to appropriately balance the rewards for these two objectives. This equilibrium in reward allocation posed no issues in the initial stages of training. However, as training progressed, the calculated numerical value of indoor thermal comfort tended to decrease. This decline resulted in the DRL algorithm increasingly favoring the operational cost objective in the later stages of training, thereby leading to the observed outcomes. Finally, we computed the sum of the performance metrics for different models across the four scenarios and plotted the results in bar graphs for ease of overall comparison (Fig. 8). 4.1.2 Reward based on thermostatic control The overall performance indicators of the four test episodes are listed in Table 7. At this point, the optimization objective of RL is a univariate function. At this point, RL focuses solely on maintaining a constant room temperature without introducing other optimization objectives that could distract from RL\u2019s performance. The term \u201cAverage Temperature Difference\u201d denotes the average magnitude of the absolute difference between the indoor temperature and the target temperature of 22 degrees Celsius across all time steps in an episode. The performances of models M.2 and M.3 surpass that of model M.1, which lacks predictive data; this is consistent with our understanding of the benefits of incorporating predictive data, particularly for delicate tasks such as temperature regulation. Likewise, M.3 consistently outperformed M.2 across all scenarios, demonstrating the enhanced predictive capability of the GRU networks. In the peak cooling scenario, the thermal control index of M.3 improved by approximately 44.2% compared to M.1. This is the most significant improvement among the four scenarios. Furthermore, a typical heat scenario is worth noting. The performance of M.2 and M.1 remained consistent, suggesting that the mere addition of predictive information did not enhance the model\u2019s performance in this case. However, the performance of M. 3 improved by approximately 24.9% when using the GRU network; this substantiates the necessity of employing the GRU network. Experiments focusing on a single optimization objective and reward function have validated the effectiveness of the GRU-RL approach. This further illustrates that the suboptimal results observed in previous experiments were due to an imbalance in the setting of the reward function. 4.2 Detail visualization analysis In addition to statistical data, the authors aimed to illustrate specific strategies and temperature variations in practical operations, thereby enabling readers better understand the control effects of RL under different tasks. Given the length constraints, the authors opted for an in-depth analysis of typical heating and cooling scenarios in terms of user comfort and running costs. As the authors will make the model\u2019s training outcomes and testing environments open-source, interested readers can download them to test various scenarios independently. Figs. 9 and 10 respectively depict the detailed temperature variations controlled by different intelligent agent controllers in typical heating and cooling scenarios. The dashed lines in the figures represent the range of indoor comfort bounds throughout the cycle. The range of comfortable temperature limits is narrower, between 21 and 24 degrees Celsius when there is human activity indoors. In contrast, without such activity, the range widens, stretching from 15 to 30 degrees Celsius. From the operational control results, the models utilizing predictive information saved more energy than Model M.1 during periods when the building was unoccupied. For instance, at time step 125 in Fig. 9, the indoor temperatures of Models M.2 and M.3 were lower than those of Model M.1. During this time, the building is unoccupied, and air conditioning can be turned off while maintaining the indoor temperatures within the comfort zone. In certain instances, M.3 outperformed M.2, as demonstrated by the 50th time step in Fig. 9, where M.3 displays a lower temperature, thus resulting in more energy savings. Analogous conclusions were drawn for the typical cooling scenarios. For instance, at Step 25 in Fig. 10, both M.3 and M.2 have learned similar pre-cooling controls. However, M.3 applied a gentler pre-cooling approach, avoiding excessive temperature reduction, thereby reducing operational costs. Models M. 2 and M. 3, which incorporate predictive information, learned to perform pre-cooling operations similar to those that the M.1 model failed to grasp in typical cool situations. This finding underscores the importance of using predictive data. The slightly superior performance of M.3 reflects the effectiveness of the GRU network in processing the observations. Finally, the authors emphasized the impact of incorporating predictive information. During heating cycles, this information can be used to appropriately turn off the air conditioning during building downtime and preheat ahead of use, effectively saving energy (as the operational costs during non-peak times are lower). The cooling process also allows suitable precooling operations to be learned. Moreover, M.3 proved to be more reasonable than M.2 during precooling and preheating, resulting in significant cost savings. This further improvement was due to the advantages of the GRU network over pure prediction. 5 Conclusion, limitation, and future work RL shows significant potential for addressing the crucial demand for reducing the energy use and carbon footprint of HVAC systems while ensuring indoor comfort. The authors developed an RL algorithm that uses GRU networks to handle predictive data based on the open-source framework BOPTEST to better address the issue of incorporating predictive information into RL algorithms. The experimental objectives included the joint control of indoor thermal comfort and operational costs, as well as the maintenance of indoor temperature stability. The proposed method was compared with models that did not use predictive information and those that processed predictive information with regular networks. Optimization tests were conducted under four built-in heating and cooling scenarios of BOPTEST, with the following key findings: \u2022 The proposed GRU-RL combined model achieves the best overall operational costs and thermal comfort index across four scenarios when the operational objectives are indoor thermal comfort and cost-efficiency. The overall operational cost was reduced by 14.5% compared to the model that did not utilize predictive information, while the indoor thermal comfort performance improved by 88.4%. While the performance of the GRU-RL model may be inferior to models using standard networks in terms of indoor comfort, this result is justifiable considering the reduction in operational costs and the significant computational weightage of operational costs in the reward function. \u2022 In the context of indoor temperature control tasks, we solely utilize a single optimization objective to better compare the effects of incorporating predictive information and the use of GRU models. Compared to models that do not utilize predictive information, the GRU-RL model enhances the performance of thermostatic control by approximately 14.2%. Furthermore, it outperforms models that use ordinary networks to incorporate predictive information, showing an improvement of around 5%. The GRU-RL model demonstrated superior performance in all four scenarios, validating the efficacy of incorporating the GRU model. \u2022 Through detailed analysis of two scenarios, we found that the introduction of predictive information enables the model to learn pre-cooling and pre-heating actions, thereby controlling both indoor comfort and operating costs. The model will appropriately turn off the air conditioning during unoccupied periods and initiate pre-cooling and pre-heating in advance. The GRU-RL model\u2019s operations are smoother during this pre-cooling and pre-heating process, further reducing operating expenses. \u2022 To tackle the challenge of reproducibility in our research, we have developed a test platform grounded in the BOPTEST open-source framework. Additionally, we have ensured that all algorithms employed in our study are open-source, thereby facilitating independent replication and validation of our findings by the broader research community. Our integration of the BOPTEST framework with reinforcement learning algorithms is also readily accessible, providing a valuable reference and tool for future researchers in this domain. However, because of the lack of parallel computing support in BOPTEST, we were unable to select optimal hyperparameters. Furthermore, a detailed discussion of the setting of the reward function weights is precluded owing to computational time constraints. Future research will apply this algorithm to more complex scenarios, develop fixed and efficient algorithms for the design of reward functions, and replace the GRU model with other models to enhance the interpretability of RL. Moreover, in the experiments balancing operational costs and indoor thermal comfort, we demonstrated that the design of the reward function in reinforcement learning significantly affects the optimization results. Therefore, a crucial future research direction will be how to effectively and automatically design adaptive reward functions for multi-objective optimization. CRediT authorship contribution statement Yuan Gao: Conceptualization, Data curation, Formal analysis, Funding acquisition, Investigation, Methodology, Resources, Software, Validation, Visualization, Writing \u2013 original draft. Shanrui Shi: Validation, Visualization, Writing \u2013 original draft, Writing \u2013 review & editing. Shohei Miyata: Funding acquisition, Project administration. Yasunori Akashi: Funding acquisition, Project administration. Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments This work was supported by JSPS KAKENHI Grant Number 23KJ0513. We would like to thank Editage (www.editage.jp) for English language editing. The author(s) gratefully acknowledge the support of the International Institute for Carbon Neutral Energy Research (WPI-I2CNER), sponsored by the Japanese Ministry of Education, Culture, Sports, Science and Technology . References [1] Liu M. Ooka R. Choi W. Ikeda S. Experimental and numerical investigation of energy saving potential of centralized and decentralized pumping systems Appl Energy 251 2019 113359 M. Liu, R. Ooka, W. Choi, S. Ikeda, Experimental and numerical investigation of energy saving potential of centralized and decentralized pumping systems, Applied Energy 251 (2019) 113359. [2] González-Torres M. Pérez-Lombard L. Coronel J.F. Maestre I.R. Yan D. A review on buildings energy information: Trends, end-uses, fuels and drivers Energy Rep 8 2022 626 637 M. González-Torres, L. Pérez-Lombard, J. F. Coronel, I. R. Maestre, D. Yan, A review on buildings energy information: Trends, end-uses, fuels and drivers, Energy Reports 8 (2022) 626\u2013637. [3] Cai W. Wen X. Li C. Shao J. Xu J. Predicting the energy consumption in buildings using the optimized support vector regression model Energy 273 2023 127188 W. Cai, X. Wen, C. Li, J. Shao, J. Xu, Predicting the energy consumption in buildings using the optimized support vector regression model, Energy 273 (2023) 127188. [4] Gao Y. Miyata S. Akashi Y. Energy saving and indoor temperature control for an office building using tube-based robust model predictive control Applied Energy 341 2023 121106 title=Energy saving and indoor temperature control for an office building using tube-based robust model predictive control, author=Gao, Yuan and Miyata, Shohei and Akashi, Yasunori, journal=Applied Energy, volume=341, pages=121106, year=2023, publisher=Elsevier [5] Buyak N. Deshko V. Bilous I. Pavlenko A. Sapunov A. Biriukov D. Dynamic interdependence of comfortable thermal conditions and energy efficiency increase in a nursery school building for heating and cooling period Energy 2023 129076 N. Buyak, V. Deshko, I. Bilous, A. Pavlenko, A. Sapunov, D. Biriukov, Dynamic interdependence of comfortable thermal conditions and energy efficiency increase in a nursery school building for heating and cooling period, Energy (2023) 129076. [6] Chen W.-A. Lim J. Miyata S. Akashi Y. Methodology of evaluating the sewage heat utilization potential by modelling the urban sewage state prediction model Sustainable Cities Soc 80 2022 103751 W.-A. Chen, J. Lim, S. Miyata, Y. Akashi, Methodology of evaluating the sewage heat utilization potential by modelling the urban sewage state prediction model, Sustainable Cities and Society 80 (2022) 103751. [7] Vázquez-Canteli J.R. Nagy Z. Reinforcement learning for demand response: A review of algorithms and modeling techniques Appl Energy 235 2019 1072 1089 J. R. Vázquez-Canteli, Z. Nagy, Reinforcement learning for demand response: A review of algorithms and modeling techniques, Applied energy 235 (2019) 1072\u20131089. [8] Hwang R.-L. Chen W.-A. Creating glazed facades performance map based on energy and thermal comfort perspective for office building design strategies in Asian hot-humid climate zone Applied Energy 311 2022 118689 title=Creating glazed facades performance map based on energy and thermal comfort perspective for office building design strategies in Asian hot-humid climate zone, author=Hwang, Ruey-Lung and Chen, Wei-An, journal=Applied Energy, volume=311, pages=118689, year=2022, publisher=Elsevier [9] Hwang R.-L. Chen W.-A. Identifying relative importance of solar design determinants on office building façade for cooling loads and thermal comfort in hot-humid climates Building and Environment 226 2022 109684 title=Identifying relative importance of solar design determinants on office building façade for cooling loads and thermal comfort in hot-humid climates, author=Hwang, Ruey-Lung and Chen, Wei-An, journal=Building and Environment, volume=226, pages=109684, year=2022, publisher=Elsevier [10] Wang Z. Hong T. Reinforcement learning for building controls: The opportunities and challenges Appl Energy 269 2020 115036 Z. Wang, T. Hong, Reinforcement learning for building controls: The opportunities and challenges, Applied Energy 269 (2020) 115036. [11] Singhal A. Salsbury T.I. Characterization and cancellation of static nonlinearity in HVAC systems ASHRAE Trans 113 1 2007 A. Singhal, T. I. Salsbury, Characterization and cancellation of static nonlinearity in hvac systems., ASHRAE Transactions 113 (1) (2007). [12] Attaran S.M. Yusof R. Selamat H. A novel optimization algorithm based on epsilon constraint-RBF neural network for tuning PID controller in decoupled HVAC system Appl Therm Eng 99 2016 613 624 S. M. Attaran, R. Yusof, H. Selamat, A novel optimization algorithm based on epsilon constraint-rbf neural network for tuning pid controller in decoupled hvac system, Applied Thermal Engineering 99 (2016) 613\u2013624. [13] Alanne K. Sierla S. An overview of machine learning applications for smart buildings Sustainable Cities Soc 76 2022 103445 K. Alanne, S. Sierla, An overview of machine learning applications for smart buildings, Sustainable Cities and Society 76 (2022) 103445. [14] Li Y. Wang J. Wang W. Liu C. Li Y. Dynamic pricing based electric vehicle charging station location strategy using reinforcement learning Energy 281 2023 128284 Y. Li, J. Wang, W. Wang, C. Liu, Y. Li, Dynamic pricing based electric vehicle charging station location strategy using reinforcement learning, Energy 281 (2023) 128284. [15] Li Y. Wang Z. Xu W. Gao W. Xu Y. Xiao F. Modeling and energy dynamic control for a ZEH via hybrid model-based deep reinforcement learning Energy 277 2023 127627 Y. Li, Z. Wang, W. Xu, W. Gao, Y. Xu, F. Xiao, Modeling and energy dynamic control for a zeh via hybrid model-based deep reinforcement learning, Energy 277 (2023) 127627. [16] Mnih V. Kavukcuoglu K. Silver D. Rusu A.A. Veness J. Bellemare M.G. Human-level control through deep reinforcement learning Nature 518 7540 2015 529 533 V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., Human-level control through deep reinforcement learning, nature 518 (7540) (2015) 529\u2013533. [17] Du Y. Zandi H. Kotevska O. Kurte K. Munk J. Amasyali K. Intelligent multi-zone residential HVAC control strategy based on deep reinforcement learning Appl Energy 281 2021 116117 Y. Du, H. Zandi, O. Kotevska, K. Kurte, J. Munk, K. Amasyali, E. Mckee, F. Li, Intelligent multi-zone residential hvac control strategy based on deep reinforcement learning, Applied Energy 281 (2021) 116117. [18] Brandi S. Piscitelli M.S. Martellacci M. Capozzoli A. Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings Energy Build 224 2020 110225 S. Brandi, M. S. Piscitelli, M. Martellacci, A. Capozzoli, Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings, Energy and Buildings 224 (2020) 110225. [19] He K. Fu Q. Lu Y. Wang Y. Luo J. Wu H. Predictive control optimization of chiller plants based on deep reinforcement learning J Build Eng 2023 107158 K. He, Q. Fu, Y. Lu, Y. Wang, J. Luo, H. Wu, J. Chen, Predictive control optimization of chiller plants based on deep reinforcement learning, Journal of Building Engineering (2023) 107158. [20] Zhang Z. Chong A. Pan Y. Zhang C. Lam K.P. Whole building energy model for HVAC optimal control: A practical framework based on deep reinforcement learning Energy Build 199 2019 472 490 Z. Zhang, A. Chong, Y. Pan, C. Zhang, K. P. Lam, Whole building energy model for hvac optimal control: A practical framework based on deep reinforcement learning, Energy and Buildings 199 (2019) 472\u2013490. [21] Valladares W. Galindo M. Gutiérrez J. Wu W.-C. Liao K.-K. Liao J.-C. Energy optimization associated with thermal comfort and indoor air control via a deep reinforcement learning algorithm Build Environ 155 2019 105 117 W. Valladares, M. Galindo, J. Gutiérrez, W.-C. Wu, K.-K. Liao, J.-C. Liao, K.-C. Lu, C.-C. Wang, Energy optimization associated with thermal comfort and indoor air control via a deep reinforcement learning algorithm, Building and Environment 155 (2019) 105\u2013117. [22] Gupta A. Badr Y. Negahban A. Qiu R.G. Energy-efficient heating control for smart buildings with deep reinforcement learning J Build Eng 34 2021 101739 A. Gupta, Y. Badr, A. Negahban, R. G. Qiu, Energy-efficient heating control for smart buildings with deep reinforcement learning, Journal of Building Engineering 34 (2021) 101739. [23] Biemann M. Scheller F. Liu X. Huang L. Experimental evaluation of model-free reinforcement learning algorithms for continuous HVAC control Appl Energy 298 2021 117164 M. Biemann, F. Scheller, X. Liu, L. Huang, Experimental evaluation of model-free reinforcement learning algorithms for continuous hvac control, Applied Energy 298 (2021) 117164. [24] Yang T. Zhao L. Li W. Wu J. Zomaya A.Y. Towards healthy and cost-effective indoor environment management in smart homes: A deep reinforcement learning approach Appl Energy 300 2021 117335 T. Yang, L. Zhao, W. Li, J. Wu, A. Y. Zomaya, Towards healthy and cost-effective indoor environment management in smart homes: A deep reinforcement learning approach, Applied Energy 300 (2021) 117335. [25] Fang X. Gong G. Li G. Chun L. Peng P. Li W. Deep reinforcement learning optimal control strategy for temperature setpoint real-time reset in multi-zone building HVAC system Appl Therm Eng 212 2022 118552 X. Fang, G. Gong, G. Li, L. Chun, P. Peng, W. Li, X. Shi, X. Chen, Deep reinforcement learning optimal control strategy for temperature setpoint real-time reset in multi-zone building hvac system, Applied Thermal Engineering 212 (2022) 118552. [26] Gao Y. Matsunami Y. Miyata S. Akashi Y. Operational optimization for off-grid renewable building energy system using deep reinforcement learning Appl Energy 325 2022 119783 Y. Gao, Y. Matsunami, S. Miyata, Y. Akashi, Operational optimization for off-grid renewable building energy system using deep reinforcement learning, Applied Energy 325 (2022) 119783. [27] Brandi S. Fiorentini M. Capozzoli A. Comparison of online and offline deep reinforcement learning with model predictive control for thermal energy management Autom Constr 135 2022 104128 S. Brandi, M. Fiorentini, A. Capozzoli, Comparison of online and offline deep reinforcement learning with model predictive control for thermal energy management, Automation in Construction 135 (2022) 104128. [28] Xu Y. Gao W. Li Y. Xiao F. Operational optimization for the grid-connected residential photovoltaic-battery system using model-based reinforcement learning J Build Eng 73 2023 106774 Y. Xu, W. Gao, Y. Li, F. Xiao, Operational optimization for the grid-connected residential photovoltaic-battery system using model-based reinforcement learning, Journal of Building Engineering 73 (2023) 106774. [29] Zhuang D. Gan V.J. Tekler Z.D. Chong A. Tian S. Shi X. Data-driven predictive control for smart HVAC system in IoT-integrated buildings with time-series forecasting and reinforcement learning Appl Energy 338 2023 120936 D. Zhuang, V. J. Gan, Z. D. Tekler, A. Chong, S. Tian, X. Shi, Data-driven predictive control for smart hvac system in iot-integrated buildings with time-series forecasting and reinforcement learning, Applied Energy 338 (2023) 120936. [30] Li Y. Jia Z. Zhang X. Liu Y. Xiao F. Gao W. Energy flexibility analysis and model predictive control performances of space heating in Japanese zero energy house J Build Eng 76 2023 107365 Y. Li, Z. Jia, X. Zhang, Y. Liu, F. Xiao, W. Gao, Y. Xu, Energy flexibility analysis and model predictive control performances of space heating in japanese zero energy house, Journal of Building Engineering 76 (2023) 107365. [31] Wang D. Zheng W. Wang Z. Wang Y. Pang X. Wang W. Comparison of reinforcement learning and model predictive control for building energy system optimization Appl Therm Eng 228 2023 120430 D. Wang, W. Zheng, Z. Wang, Y. Wang, X. Pang, W. Wang, Comparison of reinforcement learning and model predictive control for building energy system optimization, Applied Thermal Engineering 228 (2023) 120430. [32] Fan C. Wang J. Gang W. Li S. Assessment of deep recurrent neural network-based strategies for short-term building energy predictions Appl Energy 236 2019 700 710 C. Fan, J. Wang, W. Gang, S. Li, Assessment of deep recurrent neural network-based strategies for short-term building energy predictions, Applied energy 236 (2019) 700\u2013710. [33] Hochreiter S. Schmidhuber J. Long short-term memory Neural Comput 9 8 1997 1735 1780 S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural computation 9 (8) (1997) 1735\u20131780. [34] Esrafilian-Najafabadi M. Haghighat F. Impact of occupancy prediction models on building HVAC control system performance: Application of machine learning techniques Energy Build 257 2022 111808 M. Esrafilian-Najafabadi, F. Haghighat, Impact of occupancy prediction models on building hvac control system performance: Application of machine learning techniques, Energy and Buildings 257 (2022) 111808. [35] Gao Y. Ruan Y. Fang C. Yin S. Deep learning and transfer learning models of energy consumption forecasting for a building with poor information data Energy Build 223 2020 110156 Y. Gao, Y. Ruan, C. Fang, S. Yin, Deep learning and transfer learning models of energy consumption forecasting for a building with poor information data, Energy and Buildings 223 (2020) 110156. [36] Somu N. Sriram A. Kowli A. Ramamritham K. A hybrid deep transfer learning strategy for thermal comfort prediction in buildings Build Environ 204 2021 108133 N. Somu, A. Sriram, A. Kowli, K. Ramamritham, A hybrid deep transfer learning strategy for thermal comfort prediction in buildings, Building and Environment 204 (2021) 108133. [37] Yang G. Yuan E. Wu W. Predicting the long-term CO2 concentration in classrooms based on the BO\u2013EMD\u2013LSTM model Build Environ 224 2022 109568 G. Yang, E. Yuan, W. Wu, Predicting the long-term co2 concentration in classrooms based on the bo\u2013emd\u2013lstm model, Building and Environment 224 (2022) 109568. [38] Zou Z. Yu X. Ergan S. Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network Build Environ 168 2020 106535 Z. Zou, X. Yu, S. Ergan, Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network, Building and Environment 168 (2020) 106535. [39] Pinto G. Deltetto D. Capozzoli A. Data-driven district energy management with surrogate models and deep reinforcement learning Appl Energy 304 2021 117642 G. Pinto, D. Deltetto, A. Capozzoli, Data-driven district energy management with surrogate models and deep reinforcement learning, Applied Energy 304 (2021) 117642. [40] Blad C. Bøgh S. Kallesøe C.S. Data-driven offline reinforcement learning for HVAC-systems Energy 261 2022 125290 C. Blad, S. Bøgh, C. S. Kallesøe, Data-driven offline reinforcement learning for hvac-systems, Energy 261 (2022) 125290. [41] Wetter M. Modelica-based modelling and simulation to support research and development in building energy and control systems J Build Perform Simul 2 2 2009 143 161 M. Wetter, Modelica-based modelling and simulation to support research and development in building energy and control systems, Journal of Building Performance Simulation 2 (2) (2009) 143\u2013161. [42] Blum D. Arroyo J. Huang S. Drgoňa J. Jorissen F. Walnum H.T. Building optimization testing framework (BOPTEST) for simulation-based benchmarking of control strategies in buildings J Build Perform Simul 14 5 2021 586 610 D. Blum, J. Arroyo, S. Huang, J. Drgoňa, F. Jorissen, H. T. Walnum, Y. Chen, K. Benne, D. Vrabie, M. Wetter, et al., Building optimization testing framework (boptest) for simulation-based benchmarking of control strategies in buildings, Journal of Building Performance Simulation 14 (5) (2021) 586\u2013610. [43] Sutton R.S. Barto A.G. Reinforcement learning: An introduction 2018 MIT Press R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction, MIT press, 2018. [44] Song H. Liu C.-C. Lawarrée J. Dahlgren R.W. Optimal electricity supply bidding by Markov decision process IEEE Trans Power Syst 15 2 2000 618 624 H. Song, C.-C. Liu, J. Lawarrée, R. W. Dahlgren, Optimal electricity supply bidding by markov decision process, IEEE transactions on power systems 15 (2) (2000) 618\u2013624. [45] Lillicrap T.P. Hunt J.J. Pritzel A. Heess N. Erez T. Tassa Y. Continuous control with deep reinforcement learning 2015 arXiv preprint arXiv:1509.02971 T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, D. Wierstra, Continuous control with deep reinforcement learning, arXiv preprint arXiv:1509.02971 (2015). [46] Gao Y. Matsunami Y. Miyata S. Akashi Y. Multi-agent reinforcement learning dealing with hybrid action spaces: A case study for off-grid oriented renewable building energy system Appl Energy 326 2022 120021 Y. Gao, Y. Matsunami, S. Miyata, Y. Akashi, Multi-agent reinforcement learning dealing with hybrid action spaces: A case study for off-grid oriented renewable building energy system, Applied Energy 326 (2022) 120021. [47] Zhang X. Bao T. Yu T. Yang B. Han C. Deep transfer Q-learning with virtual leader-follower for supply-demand stackelberg game of smart grid Energy 133 2017 348 365 X. Zhang, T. Bao, T. Yu, B. Yang, C. Han, Deep transfer q-learning with virtual leader-follower for supply\u2013demand stackelberg game of smart grid, Energy 133 (2017) 348\u2013365. [48] Silver D. Lever G. Heess N. Degris T. Wierstra D. Riedmiller M. Deterministic policy gradient algorithms International conference on machine learning 2014 Pmlr 387 395 D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, M. Riedmiller, Deterministic policy gradient algorithms, in: International conference on machine learning, Pmlr, 2014, pp. 387\u2013395. [49] Haarnoja T. Zhou A. Abbeel P. Levine S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor International conference on machine learning 2018 PMLR 1861 1870 T. Haarnoja, A. Zhou, P. Abbeel, S. Levine, Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor, in: International conference on machine learning, PMLR, 2018, pp. 1861\u20131870. [50] Brandi S. Coraci D. Borello D. Capozzoli A. Energy management of a residential heating system through deep reinforcement learning Sustainability in energy and buildings 2021 2022 Springer 329 339 S. Brandi, D. Coraci, D. Borello, A. Capozzoli, Energy management of a residential heating system through deep reinforcement learning, in: Sustainability in Energy and Buildings 2021, Springer, 2022, pp. 329\u2013339. [51] Meng L. Gorbet R. Kulić D. Memory-based deep reinforcement learning for POMDPs 2021 IEEE/RSJ international conference on intelligent robots and systems 2021 IEEE 5619 5626 L. Meng, R. Gorbet, D. Kulić, Memory-based deep reinforcement learning for pomdps, in: 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), IEEE, 2021, pp. 5619\u20135626. [52] Gao Y. Miyata S. Akashi Y. Interpretable deep learning models for hourly solar radiation prediction based on graph neural network and attention Appl Energy 321 2022 119288 Y. Gao, S. Miyata, Y. Akashi, Interpretable deep learning models for hourly solar radiation prediction based on graph neural network and attention, Applied Energy 321 (2022) 119288. [53] Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library. [54] Blum D. Information about test case in BOPTEST 2022 URL https://ibpsa.github.io/project1-boptest/testcases/ibpsa/testcases_ibpsa_bestest_air/ D. Blum, https://ibpsa.github.io/project1-boptest/testcases/ibpsa/testcasesibpsabestestair/Information about test case in boptest (2022). https://ibpsa.github.io/project1-boptest/testcases/ibpsa/testcasesibpsabestestair/ [55] Brockman G. Cheung V. Pettersson L. Schneider J. Schulman J. Tang J. Openai gym (2016) 2016 arXiv preprint arXiv:1606.01540 G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, W. Zaremba, Openai gym (2016), arXiv preprint arXiv:1606.01540 476 (2016). [56] Raffin A. Hill A. Gleave A. Kanervisto A. Ernestus M. Dormann N. Stable-Baselines3: Reliable reinforcement learning implementations J Mach Learn Res 22 268 2021 1 8 URL http://jmlr.org/papers/v22/20-1364.html A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, N. Dormann, http://jmlr.org/papers/v22/20-1364.htmlStable-baselines3: Reliable reinforcement learning implementations, Journal of Machine Learning Research 22 (268) (2021) 1\u20138. http://jmlr.org/papers/v22/20-1364.html [57] Akiba T, Sano S, Yanase T, Ohta T, Koyama M. Optuna: A next-generation hyperparameter optimization framework. In: Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining. 2019, p. 2623\u201331.",
    "scopus-id": "85184757832",
    "coredata": {
        "eid": "1-s2.0-S0360544224001154",
        "dc:description": "Reinforcement Learning (RL), a promising algorithm for the operational control of Heating, Ventilation, and Air Conditioning (HVAC) systems, has garnered considerable attention and applications. However, traditional RL algorithms typically do not incorporate predictive information for future scenarios, and only a limited number of studies have examined the enhancement and impact of predictive information on RL algorithms. To address the issue of coupling RL and predictive information in HVAC system operation optimization, we employed an open-source framework to examine the impact of various predictive information strategies on RL outcomes. We propose a joint gated recurrent unit (GRU)-RL algorithm to handle situations where a time-series exists in state space. The results from four classic test cases demonstrate that the proposed GRU-RL method can reduce operating costs by approximately 14.5% and increase comfort performance by 88.4% in indoor comfort control and cost-management tasks. Moreover, the GRU-RL method outperformed the conventional DRL method and was merely augmented with prediction information. In indoor temperature regulation, the GRU-RL algorithm improves control efficacy by 14.2% compared to models without predictive information and offers an approximately 5% improvement over traditional network models. Finally, all models were made open source for easy replication and further research.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2024-03-15",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S0360544224001154",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Gao, Yuan"
            },
            {
                "@_fa": "true",
                "$": "Shi, Shanrui"
            },
            {
                "@_fa": "true",
                "$": "Miyata, Shohei"
            },
            {
                "@_fa": "true",
                "$": "Akashi, Yasunori"
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S0360544224001154"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S0360544224001154"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S0360-5442(24)00115-4",
        "prism:volume": "291",
        "articleNumber": "130344",
        "prism:publisher": "Elsevier Ltd.",
        "dc:title": "Successful application of predictive information in deep reinforcement learning control: A case study based on an office building HVAC system",
        "prism:copyright": "© 2024 Elsevier Ltd. All rights reserved.",
        "openaccess": "0",
        "prism:issn": "03605442",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "Recurrent neural network"
            },
            {
                "@_fa": "true",
                "$": "Deep reinforcement learning"
            },
            {
                "@_fa": "true",
                "$": "Time-series prediction"
            },
            {
                "@_fa": "true",
                "$": "HVAC system"
            }
        ],
        "openaccessArticle": "false",
        "prism:publicationName": "Energy",
        "openaccessSponsorType": null,
        "prism:pageRange": "130344",
        "pubType": "fla",
        "prism:coverDisplayDate": "15 March 2024",
        "prism:doi": "10.1016/j.energy.2024.130344",
        "prism:startingPage": "130344",
        "dc:identifier": "doi:10.1016/j.energy.2024.130344",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "131",
            "@width": "470",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr9.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "96447",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "165",
            "@width": "483",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "81542",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "258",
            "@width": "473",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr8.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "91287",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "131",
            "@width": "470",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr10.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "95007",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "338",
            "@width": "528",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "109548",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "301",
            "@width": "524",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "126552",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "303",
            "@width": "376",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "93713",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "191",
            "@width": "481",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "82047",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "293",
            "@width": "521",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "110576",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "285",
            "@width": "518",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "124442",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "61",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr9.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "71051",
            "@ref": "gr9",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "75",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "68329",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "119",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr8.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "71472",
            "@ref": "gr8",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "61",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr10.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "70839",
            "@ref": "gr10",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "140",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "75108",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "126",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "76048",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "203",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "72709",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "87",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "67952",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "123",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "73332",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "120",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "75522",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "581",
            "@width": "2080",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr9_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "289131",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "733",
            "@width": "2141",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "181011",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1144",
            "@width": "2097",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr8_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "240558",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "581",
            "@width": "2080",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr10_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "278646",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1497",
            "@width": "2340",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "319523",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1333",
            "@width": "2320",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "492637",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1346",
            "@width": "1668",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "236741",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "846",
            "@width": "2130",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "184003",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1297",
            "@width": "2307",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "373841",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1262",
            "@width": "2295",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "497965",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si1.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1528",
            "@ref": "si1",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si12.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1365",
            "@ref": "si12",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si13.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2309",
            "@ref": "si13",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si14.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1447",
            "@ref": "si14",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si15.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2387",
            "@ref": "si15",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si17.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "17237",
            "@ref": "si17",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si18.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2007",
            "@ref": "si18",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si19.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "19095",
            "@ref": "si19",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si2.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1210",
            "@ref": "si2",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si21.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "14990",
            "@ref": "si21",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si22.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "15254",
            "@ref": "si22",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si23.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "22117",
            "@ref": "si23",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si24.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11537",
            "@ref": "si24",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si25.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2503",
            "@ref": "si25",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si26.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1085",
            "@ref": "si26",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si27.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4197",
            "@ref": "si27",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si29.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2136",
            "@ref": "si29",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si3.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1425",
            "@ref": "si3",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si30.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2428",
            "@ref": "si30",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si31.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2626",
            "@ref": "si31",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si33.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4504",
            "@ref": "si33",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si35.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6750",
            "@ref": "si35",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si36.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5172",
            "@ref": "si36",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si37.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6291",
            "@ref": "si37",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si38.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6816",
            "@ref": "si38",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si39.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1239",
            "@ref": "si39",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si4.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1530",
            "@ref": "si4",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si40.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1333",
            "@ref": "si40",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si41.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1448",
            "@ref": "si41",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si42.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6570",
            "@ref": "si42",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si47.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "15281",
            "@ref": "si47",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si48.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "20447",
            "@ref": "si48",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si49.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1627",
            "@ref": "si49",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si5.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1147",
            "@ref": "si5",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si50.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1416",
            "@ref": "si50",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-si51.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4347",
            "@ref": "si51",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224001154-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "877298",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85184757832"
    }
}}