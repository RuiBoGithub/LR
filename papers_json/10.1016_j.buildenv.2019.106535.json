{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85074928449",
    "originalText": "serial JL 271434 291210 291731 291800 291881 31 Building and Environment BUILDINGENVIRONMENT 2019-11-08 2019-11-08 2019-11-16 2019-11-16 2020-06-30T08:29:35 1-s2.0-S0360132319307474 S0360-1323(19)30747-4 S0360132319307474 10.1016/j.buildenv.2019.106535 S300 S300.1 FULL-TEXT 1-s2.0-S0360132319X00190 2020-11-23T22:00:11.442167Z 0 0 20200115 2020 2019-11-08T17:37:46.039143Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast highlightsabst primabst ref 0360-1323 03601323 true 168 168 C Volume 168 50 106535 106535 106535 20200115 15 January 2020 2020-01-15 2020 Research Papers article fla © 2019 Elsevier Ltd. All rights reserved. TOWARDSOPTIMALCONTROLAIRHANDLINGUNITSUSINGDEEPREINFORCEMENTLEARNINGRECURRENTNEURALNETWORK ZOU Z 1 Introduction 2 Deep reinforcement learning control for HVAC systems 3 Proposed framework 3.1 DRL training environment 3.1.1 Preprocessing of historical BAS data 3.1.2 Training LSTM networks to approximate HVAC operations 3.1.3 Creating the training environment for the DRL agent 3.2 DRL agent 4 Introduction of the testbeds 5 Evaluation of the framework 5.1 Creating the DRL training and testing environments 5.1.1 Preprocessing of the stored BAS data 5.1.2 Training LSTM to approximate AHU operations and creating the DRL training environments 5.2 Training and testing of the DRL agent 6 Conclusion 7 Limitation and future work References 2012 ANNUALENERGYREVIEW2011 PEREZLOMBARD 2008 394 398 L ZHANG 2018 148 157 Z PROCEEDINGS5THCONFERENCESYSTEMSFORBUILTENVIRONMENTS PRACTICALIMPLEMENTATIONEVALUATIONDEEPREINFORCEMENTLEARNINGCONTROLFORARADIANTHEATINGSYSTEM WANG 2008 3 32 S HAN 1999 568 C TARIQ 2012 1 6 M 2012IEEEINTERNATIONALCONFERENCEPOWERSYSTEMTECHNOLOGYPOWERCON OCTOBERSMARTGRIDSTANDARDSFORHOMEBUILDINGAUTOMATION DEDEMEN 2018 488 497 G WORKSHOPEUROPEANGROUPFORINTELLIGENTCOMPUTINGINENGINEERING QUANTIFYINGPERFORMANCEDEGRADATIONHVACSYSTEMSFORPROACTIVEMAINTENANCEUSINGADATADRIVENAPPROACH YU 2019 113497 X AFRAM 2014 343 355 A SUTTON 1998 R INTRODUCTIONREINFORCEMENTLEARNING LIU 2006 148 161 S NAGY 2018 A DEEPREINFORCEMENTLEARNINGFOROPTIMALCONTROLSPACEHEATING PENG 2016 K IEEEINTERNATIONALCONFERENCEAUTOMATICCOMPUTINGFEEDBACKCOMPUTING JULYMODELPREDICTIVEPRIORREINFORCEMENTLEARNINGFORAHEATPUMPTHERMOSTAT QIN 2016 15 25 M BUILDINGSIMULATION FEBRUARYEVALUATIONDIFFERENTTHERMALMODELSINENERGYPLUSFORCALCULATINGMOISTUREEFFECTSBUILDINGENERGYCONSUMPTIONINDIFFERENTCLIMATECONDITIONS MNIH 2013 V PLAYINGATARIDEEPREINFORCEMENTLEARNING COSTANZO 2016 81 90 G DALAMAGKIDIS 2007 2686 2698 K ZHENG 2019 2713 2718 Z KOTSIANTIS 2007 249 268 S FAN 2015 75 89 C SEEM 2007 52 58 J JAYALAKSHMI 2011 1793 8201 T EVANGELINE 2013 268 274 D ADVANCEDCOMPUTINGICOAC2013FIFTHINTERNATIONALCONFERENCEADVANCEDCOMPUTINGCHENNAIINDIA FEATURESUBSETSELECTIONFORIRRELEVANTDATAREMOVALUSINGDECISIONTREEALGORITHM ZAREMBA 2014 W RECURRENTNEURALNETWORKREGULARIZATION SUNDERMEYER 2012 M THIRTEENTHANNUALCONFERENCEINTERNATIONALSPEECHCOMMUNICATIONASSOCIATION LSTMNEURALNETWORKSFORLANGUAGEMODELING DEDEAR 2002 549 561 R FANGER 1970 P THERMALCOMFORTANALYSISAPPLICATIONSINENVIRONMENTALENGINEERING WAJCMAN 2010 257 275 J GAO 2019 G ENERGYEFFICIENTTHERMALCOMFORTCONTROLINSMARTBUILDINGSVIADEEPREINFORCEMENTLEARNING LILLICRAP 2015 T CONTINUOUSCONTROLDEEPREINFORCEMENTLEARNING BATISTA 2006 193 198 L CORTEZ 2018 315 324 B SCHMIDHUBER 2015 85 117 J MOHANDES 2019 55 75 S MOHANRAJ 2015 150 172 M HOCHREITER 1998 107 116 S REN 2019 101673 C REN 2019 101498 J CAO 2018 316 333 S ZHANG 2019 472 490 Z LI 2018 179 193 H WESTPHALEN 1999 D ZHANG 2016 1822 1830 S ADVANCESINNEURALINFORMATIONPROCESSINGSYSTEMS ARCHITECTURALCOMPLEXITYMEASURESRECURRENTNEURALNETWORKS SAK 2014 H FIFTEENTHANNUALCONFERENCEINTERNATIONALSPEECHCOMMUNICATIONASSOCIATION LONGSHORTTERMMEMORYRECURRENTNEURALNETWORKARCHITECTURESFORLARGESCALEACOUSTICMODELING SCHWEIKER 2016 M ISO 2005 e615 I DEVESA 1995 300 304 S ZOUX2020X106535 ZOUX2020X106535XZ 2021-11-16T00:00:00.000Z 2021-11-16T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2019 Elsevier Ltd. All rights reserved. item S0360-1323(19)30747-4 S0360132319307474 1-s2.0-S0360132319307474 10.1016/j.buildenv.2019.106535 271434 2020-11-23T22:00:11.442167Z 2020-01-15 1-s2.0-S0360132319307474-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/MAIN/application/pdf/546ec6a95a2bd0c4b968692325cec2c8/main.pdf main.pdf pdf true 3431369 MAIN 15 1-s2.0-S0360132319307474-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/PREVIEW/image/png/b986ae03167ec2b73a51ec23a21a428d/main_1.png main_1.png png 49603 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0360132319307474-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr9/DOWNSAMPLED/image/jpeg/19c953752a2a713a164ccdf282ee2fff/gr9.jpg gr9 gr9.jpg jpg 68091 478 713 IMAGE-DOWNSAMPLED 1-s2.0-S0360132319307474-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr8/DOWNSAMPLED/image/jpeg/b1eca4475f7f3667640658e84a1ffe27/gr8.jpg gr8 gr8.jpg jpg 266972 796 802 IMAGE-DOWNSAMPLED 1-s2.0-S0360132319307474-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr7/DOWNSAMPLED/image/jpeg/48f2563b33edf4f110a69185b453168d/gr7.jpg gr7 gr7.jpg jpg 70031 336 669 IMAGE-DOWNSAMPLED 1-s2.0-S0360132319307474-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr6/DOWNSAMPLED/image/jpeg/9978995fab67601c8c1815ac04f5e95e/gr6.jpg gr6 gr6.jpg jpg 150417 711 713 IMAGE-DOWNSAMPLED 1-s2.0-S0360132319307474-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr5/DOWNSAMPLED/image/jpeg/ca4d04739a2b2ea6a3482ed2faaf6529/gr5.jpg gr5 gr5.jpg jpg 70864 231 713 IMAGE-DOWNSAMPLED 1-s2.0-S0360132319307474-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr4/DOWNSAMPLED/image/jpeg/b9722f4f283891db7e7c874ebf2f8e30/gr4.jpg gr4 gr4.jpg jpg 35449 181 580 IMAGE-DOWNSAMPLED 1-s2.0-S0360132319307474-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr3/DOWNSAMPLED/image/jpeg/79a007a597fad5ff4172e3fbd3e2c843/gr3.jpg gr3 gr3.jpg jpg 59617 310 580 IMAGE-DOWNSAMPLED 1-s2.0-S0360132319307474-fx1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/fx1/DOWNSAMPLED/image/jpeg/e42aa6ca840317f4d5f3d41d82b32fd3/fx1.jpg fx1 fx1.jpg jpg 37946 209 533 IMAGE-DOWNSAMPLED 1-s2.0-S0360132319307474-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr2/DOWNSAMPLED/image/jpeg/9eeb65034e8cd94d6e21b5bb29eb0838/gr2.jpg gr2 gr2.jpg jpg 107305 500 624 IMAGE-DOWNSAMPLED 1-s2.0-S0360132319307474-gr11.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr11/DOWNSAMPLED/image/jpeg/1cf887a38df42bc49f702f497fad11b3/gr11.jpg gr11 gr11.jpg jpg 111830 400 624 IMAGE-DOWNSAMPLED 1-s2.0-S0360132319307474-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr1/DOWNSAMPLED/image/jpeg/604eebf755b332d893a45f9cfdcf432f/gr1.jpg gr1 gr1.jpg jpg 28754 137 388 IMAGE-DOWNSAMPLED 1-s2.0-S0360132319307474-gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr10/DOWNSAMPLED/image/jpeg/3c9d71df955b987a96b575b4624ea83a/gr10.jpg gr10 gr10.jpg jpg 57339 172 579 IMAGE-DOWNSAMPLED 1-s2.0-S0360132319307474-fx2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/fx2/DOWNSAMPLED/image/jpeg/1d2a5faa509766284d8037aaef157a4b/fx2.jpg fx2 fx2.jpg jpg 61964 389 493 IMAGE-DOWNSAMPLED 1-s2.0-S0360132319307474-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr9/THUMBNAIL/image/gif/7ed68457922139a6dcbc6634fb1223b0/gr9.sml gr9 gr9.sml sml 12601 147 219 IMAGE-THUMBNAIL 1-s2.0-S0360132319307474-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr8/THUMBNAIL/image/gif/648b3de20e942a358b8878f05ad3f411/gr8.sml gr8 gr8.sml sml 27613 164 165 IMAGE-THUMBNAIL 1-s2.0-S0360132319307474-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr7/THUMBNAIL/image/gif/59ca940966ee1a39521ad88591e34d5e/gr7.sml gr7 gr7.sml sml 14554 110 219 IMAGE-THUMBNAIL 1-s2.0-S0360132319307474-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr6/THUMBNAIL/image/gif/c302d81437aeb8f4b03e4e1b0c5c0763/gr6.sml gr6 gr6.sml sml 19192 163 164 IMAGE-THUMBNAIL 1-s2.0-S0360132319307474-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr5/THUMBNAIL/image/gif/9a92a5d47584713ee6ab8d58aa39c5c9/gr5.sml gr5 gr5.sml sml 16696 71 219 IMAGE-THUMBNAIL 1-s2.0-S0360132319307474-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr4/THUMBNAIL/image/gif/80826108c01e5cd22cad776f638322e2/gr4.sml gr4 gr4.sml sml 9243 68 219 IMAGE-THUMBNAIL 1-s2.0-S0360132319307474-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr3/THUMBNAIL/image/gif/30f8a1655e1c8899a31d59f16de2872d/gr3.sml gr3 gr3.sml sml 13714 117 219 IMAGE-THUMBNAIL 1-s2.0-S0360132319307474-fx1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/fx1/THUMBNAIL/image/gif/72f80e47329e4ac2faceffc66449c9f1/fx1.sml fx1 fx1.sml sml 9177 86 219 IMAGE-THUMBNAIL 1-s2.0-S0360132319307474-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr2/THUMBNAIL/image/gif/dbca404a764ed57d963ad1221c1ae3cb/gr2.sml gr2 gr2.sml sml 20607 164 205 IMAGE-THUMBNAIL 1-s2.0-S0360132319307474-gr11.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr11/THUMBNAIL/image/gif/13a127bb68ba8e12ecad7de30f8d10c9/gr11.sml gr11 gr11.sml sml 21717 141 219 IMAGE-THUMBNAIL 1-s2.0-S0360132319307474-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr1/THUMBNAIL/image/gif/8a134b70ddb8c6d89242f692921312c2/gr1.sml gr1 gr1.sml sml 8526 77 219 IMAGE-THUMBNAIL 1-s2.0-S0360132319307474-gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr10/THUMBNAIL/image/gif/6aa81162e50f4e855142cab98cfc5095/gr10.sml gr10 gr10.sml sml 12850 65 219 IMAGE-THUMBNAIL 1-s2.0-S0360132319307474-fx2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/fx2/THUMBNAIL/image/gif/54efffd64f491b229ba08f70cdf374aa/fx2.sml fx2 fx2.sml sml 13972 164 208 IMAGE-THUMBNAIL 1-s2.0-S0360132319307474-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr9/HIGHRES/image/jpeg/620cc9ff57a8deaf0bd2ffb7af07bae6/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 445667 2117 3158 IMAGE-HIGH-RES 1-s2.0-S0360132319307474-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr8/HIGHRES/image/jpeg/016a9bd1b194daba918bc6bf3c2e93c5/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 2032187 3523 3551 IMAGE-HIGH-RES 1-s2.0-S0360132319307474-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr7/HIGHRES/image/jpeg/ec8434ec77d1a5fdf57569b67c4dbfad/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 459323 1485 2961 IMAGE-HIGH-RES 1-s2.0-S0360132319307474-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr6/HIGHRES/image/jpeg/9522f0a95fe9a45d12b78f401353550a/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 1041142 3147 3158 IMAGE-HIGH-RES 1-s2.0-S0360132319307474-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr5/HIGHRES/image/jpeg/9f5f10e2885507dbb8c766a09375ddd8/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 492623 1025 3158 IMAGE-HIGH-RES 1-s2.0-S0360132319307474-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr4/HIGHRES/image/jpeg/2ab51f7e5a2d96c552898a82abf9dbe8/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 140127 800 2567 IMAGE-HIGH-RES 1-s2.0-S0360132319307474-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr3/HIGHRES/image/jpeg/a99d2ae8e3c8d6569704925dc07848d1/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 297257 1371 2567 IMAGE-HIGH-RES 1-s2.0-S0360132319307474-fx1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/fx1/HIGHRES/image/jpeg/8b85f2b7b6e91b46a59cebc49857f88b/fx1_lrg.jpg fx1 fx1_lrg.jpg jpg 231158 924 2362 IMAGE-HIGH-RES 1-s2.0-S0360132319307474-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr2/HIGHRES/image/jpeg/d525aef2007c0ab07bd9ececccd51f01/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 644830 2213 2764 IMAGE-HIGH-RES 1-s2.0-S0360132319307474-gr11_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr11/HIGHRES/image/jpeg/06e7331686a80381962a7097555d21c7/gr11_lrg.jpg gr11 gr11_lrg.jpg jpg 666448 1773 2763 IMAGE-HIGH-RES 1-s2.0-S0360132319307474-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr1/HIGHRES/image/jpeg/dcbc712e72041127629a3b2f6632cc3a/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 99640 609 1721 IMAGE-HIGH-RES 1-s2.0-S0360132319307474-gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/gr10/HIGHRES/image/jpeg/ae6cb507f6055bc935ed476f079835fe/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 293817 764 2566 IMAGE-HIGH-RES 1-s2.0-S0360132319307474-fx2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132319307474/fx2/HIGHRES/image/jpeg/bbec2f075cdfcf833a6efecf9d89943f/fx2_lrg.jpg fx2 fx2_lrg.jpg jpg 378254 1722 2185 IMAGE-HIGH-RES 1-s2.0-S0360132319307474-am.pdf am am.pdf pdf 3018195 AAM-PDF https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:10064H4W7ZH/MAIN/application/pdf/c56ad1543fef83290698ea169de4f90e/am.pdf BAE 106535 106535 S0360-1323(19)30747-4 10.1016/j.buildenv.2019.106535 Elsevier Ltd Fig. 1 Interactions between an agent and an environment in RL. Fig. 1 Fig. 2 Overview of the proposed framework for optimal HVAC control, one-time data flows are represented using dashed arrows and repeated data flows are in solid bold arrows. Fig. 2 Fig. 3 LSTM in loops and unfolded in sequence. Fig. 3 Fig. 4 Standard RNN and LSTM nodes. Fig. 4 Fig. 5 Schematics of an AHU, control actions are in bold, AHU states are in italic, and environmental states are underlined. Fig. 5 Fig. 6 Collinearity among outside air velocity (VAO), outside air flow (FAO), and power (ahu_kw). Fig. 6 Fig. 7 Lookback in LSTM using the supply air temperature from AHU-1 as an example. Fig. 7 Fig. 8 LSTM HVAC operational data predicted values (black) vs. ground truth values (red). (For interpretation of the references to colour in this figure legend, the reader is referred to the Web version of this article.) Fig. 8 Fig. 9 Convergence of the DRL agents' reward for AHU-1, 2 and 3, the solid line is the raw reward received, the dotted line is the logarithmic trend line. Fig. 9 Fig. 10 Comparison between RBC and DRL control of AHU-1 for average energy consumption and PPD over three months, occupied hours (8 a.m.\u20135 p.m. are shaded green). (For interpretation of the references to colour in this figure legend, the reader is referred to the Web version of this article.) Fig. 10 Fig. 11 Comparison between RBC and DRL control of AHU-2 (a) and AHU-3 (b) for average energy consumption and PPD over three months, occupied hours (8 a.m.\u20135 p.m. are shaded green). (For interpretation of the references to colour in this figure legend, the reader is referred to the Web version of this article.) Fig. 11 Table 1 State and action parameters from BAS data. Table 1 Environmental states Month, hour, day in week, outside air temperature AHU states Reference temperature, return air temperature, supply air temperature, mixed air temperature, leaving liquid temperature, hot gas temperature, supply air CO2 concentration, static air pressure after cooling coil, supply air relative humidity, mixed air relative humidity, hot water temperature in, hot water temperature out, outside air flow, indoor air temperature, indoor air relative humidity, power consumption Actions Damper position, heating valve status, fan speed, liquid solenoid status Table 2 Average MSE for different number of LSTM layers (L) (vertical) and LSTM nodes (N) per layer (horizontal). Table 2 L↓ N→ 32 64 128 256 2 0.0021 0.0018 0.0014 0.0015 3 0.0023 0.0018 0.0017 0.0016 4 0.0028 0.0024 0.0033 0.0028 Table 3 Best performing parameter set of ( ψ , β , ω ) for average energy savings during weekday daytimes and nights, and PPD reduction during weekday daytimes in training environments. Table 3 AHU ψ β ω Average energy saving (DRL over RBC) Average PPD reduction (DRL over RBC) 1 3 8 1.5 25.62% 2.01% 2 5 12 2.0 28.74% 2.28% 3 5 8 3.0 30.51% 1.74% Table 4 Average energy consumption, PPD, and control actions for AHU-1 during day and night on weekdays over three months. Table 4 Time Energy consumption (kW) PPD (%) Damper position (%) Fan speed (rpm) Outside air temperature ( ℉ ) 12 a.m.\u20133 a.m. 0.10 26.68 15.84 509.99 73.19 3 a.m.\u20136 a.m. 1.72 14.44 27.63 946.79 71.30 6 a.m.\u20139 a.m. 3.72 11.56 30.23 1367.85 74.61 9 a.m. to 12 p.m. 3.46 9.90 12.57 1304.68 81.45 12 p.m.\u20133 p.m. 3.03 8.41 12.53 1149.64 84.03 3 p.m.\u20136 p.m. 1.77 9.63 12.54 1048.71 81.43 6 p.m.\u20139 p.m. 0.43 12.64 12.79 673.61 78.01 9 p.m. to 12 a.m. 0.19 15.57 13.51 536.67 75.23 Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network Zhengbo Zou ∗ Xinran Yu Semiha Ergan Department of Civil and Urban Engineering, New York University, 15 MetroTech Center, Brooklyn, NY, United States Department of Civil and Urban Engineering New York University 15 MetroTech Center Brooklyn NY United States Department of Civil and Urban Engineering, New York University, 15 MetroTech Center, Brooklyn, NY, United States ∗ Corresponding author. Optimal control of heating, ventilation and air conditioning systems (HVACs) aims to minimize the energy consumption of equipment while maintaining the thermal comfort of occupants. Traditional rule-based control methods are not optimized for HVAC systems with continuous sensor readings and actuator controls. Recent developments in deep reinforcement learning (DRL) enabled control of HVACs with continuous sensor inputs and actions, while eliminating the need of building complex thermodynamic models. DRL control includes an environment, which approximates real-world HVAC operations; and an agent, that aims to achieve optimal control over the HVAC. Existing DRL control frameworks use simulation tools (e.g., EnergyPlus) to build DRL training environments with HVAC systems information, but oversimplify building geometrics. This study proposes a framework aiming to achieve optimal control over Air Handling Units (AHUs) by implementing long-short-term-memory (LSTM) networks to approximate real-world HVAC operations to build DRL training environments. The framework also implements state-of-the-art DRL algorithms (e.g., deep deterministic policy gradient) for optimal control over the AHUs. Three AHUs, each with two-years of building automation system (BAS) data, were used as testbeds for evaluation. Our LSTM-based DRL training environments, built using the first year's BAS data, achieved an average mean square error of 0.0015 across 16 normalized AHU parameters. When deployed in the testing environments, which were built using the second year's BAS data of the same AHUs, the DRL agents achieved 27%\u201330% energy saving comparing to the actual energy consumption, while maintaining the predicted percentage of discomfort (PPD) at 10%. Keywords HVAC control Energy consumption Thermal comfort Deep reinforcement learning Long-short-term-memory network 1 Introduction Building sector (i.e., commercial and residential) consumes more than 70% of the total energy produced in the U.S [1]. Heating Ventilation and Air Conditioning (HVAC) systems account for almost half of the building energy consumption [2]. The amount of energy consumed by the building sector is expected to increase another 70% by 2050, if the current trend continues [37,41]. Therefore, researchers and practitioners have been actively pursuing control methods for HVAC systems that are capable of minimizing energy consumption while improving the indoor environment quality (IEQ) for occupants, including thermal comfort and indoor air quality [3,4,38,39]. From the perspective of current engineering applications, a wide variety of HVAC systems (e.g., packaged, central, individual, and uncooled) are deployed in buildings [42]. In practice, these HVAC systems operate based on the designed occupancy, which often exceeds the actual occupancy, and leads to energy waste. Moreover, rapid control over HVAC systems is also needed because of the constantly changing occupancy and varying heating/cooling needs of the occupants. Currently, most buildings use rule-based control methods (RBC), also known as the classic control, for their HVAC systems. RBC follows a set of rules designed by mechanical engineers before the HVAC systems become operational. These rules are mostly static (e.g., static temperature set-points for the HVAC system to turn on and off), and only differ based on preset schedules (i.e., weekday, weekends), and seasonal changes (i.e., summer, winter). When using the RBC control, facility managers are expected to continuously monitor HVAC systems\u2019 behavior and adjust the systems dynamically during the operational phase. However, in practice, facility managers are almost always passively reacting to alarms caused by system abnormalities, and leave systems running using preset rules when no such alarms are present [5]. Reasons for this behavior include the limited usage of Building Automation Systems (BAS) [6], and the lack of data analytic tools for facility managers [7,8]. As a result, most HVAC systems only obey preset rules, which were created with limited optimization intent to minimize energy consumption. Studies aiming to improve from the classic HVAC control methods are abundant and can generally be categorized as either hard control or soft control [9]. Hard control (e.g., model predictive control, optimal control) builds rigorous physics-based models and uses optimization techniques to find HVAC controlling strategies. On the other hand, soft control (e.g., neural network-based control) does not require extensive knowledge on the physical configuration of an HVAC system. Rather, it builds data-driven models (e.g., neural networks) to optimize for a certain cost function (e.g., energy consumption, thermal comfort or indoor air quality). In recent years, researchers started paying attention to Reinforcement Learning (RL) as a possible control method for HVAC systems, because it exhibits advantages over hard and soft control methods by eliminating the need of rigorous mathematical representations of HVAC systems, and can handle HVAC systems with time delays (e.g., an action of changing the room temperature only takes effect after some time delays). In a nutshell (Fig. 1 ), RL is concerned with how an agent (e.g., an HVAC controlling software embedded in the BAS) can take an action a (e.g., set room temperature to 78\u202f°F), when it is at a state s (e.g., current room temperature of 80\u202f°F) in an environment (e.g., an office space controlled by an AHU), to maximize/minimize some reward r (e.g., the thermal comfort level of occupants). The goal of RL is to learn the optimal actions to take at each state, which is called the optimal policy function π ( a | s ) ) [10]. In HVAC systems, both states (e.g., room temperature) and control actions (e.g., damper position set percentage) can be continuous. Deep reinforcement learning (DRL) is a branch of RL that deals with continuous states and actions by using deep neural networks to approximate the policy function π ( a | s ) . Efforts have been made in adopting DRL in HVAC controls, but drawbacks persist as notable research efforts have a common theme of using simulation tools (e.g., EnergyPlus and Simulink) to build DRL training environments [11\u201313], which are rarely accurate when not presented with complete HVAC schematics and building information (e.g., location, layout, facade material) [9,14]. This study fills the gap of providing an accurate approximation of a real-world HVAC controlled environment for the DRL agent to train for optimal control (i.e., achieve low energy consumption while maintaining thermal comfort). We propose a framework that combines DRL methods and long-short-term-memory networks (LSTM) with historical time series HVAC operational data. The framework includes two parts. First, we propose an LSTM-based method that uses historical HVAC operational data to build a training environment for the DRL agent to interact with. The environment takes the current state and the action chosen by the DRL agent as inputs and returns the new state and reward for action as outputs. Next, we enable the DRL agent to learn the optimal control policy for an HVAC by interacting with the training environment using state-of-the-art DRL algorithms (e.g., deep deterministic policy gradient algorithm). To evaluate the framework, we implemented it in an office building equipped with three Air Handling Units (AHUs), each with two years of operational data stored through Building Automation Systems (BAS). We used the first year's data to create three training environments (i.e., one for each AHU) for training the DRL agents, and used the second year's data to create three testing environments for testing the performance of our DRL agents. The results showed that (1) our LSTM-based DRL training and testing environments achieved a mean square error of 0.0015 across 16 normalized AHU parameters when predicting AHU sensor readings, which suggests an accurate approximation of the AHU operations using our LSTM models, and (2) the AHUs controlled by the DRL agent in testing environments achieved a 27%\u201330% lower energy consumption in three months as compared to the existing rule-based control in the same building, while maintaining an average thermal comfort level at 10% predicted percentage of dissatisfaction (PPD) during working hours on weekdays, which falls in the American Society of Heating, Refrigerating and Air-Conditioning Engineers (ASHRAE) recommended comfort level of 5%\u201320% PPD. 2 Deep reinforcement learning control for HVAC systems DRL gained attention in the HVAC control domain, because of its three main advantages. First, it does not require a complex thermodynamic model to represent real-world HVAC systems [15]. Second, DRL can handle systems with time delaying actions. For example, the effect of changing the set-point of a thermostat does not have an effect on indoor temperature immediately, and this could be captured by DRL-based methods through the concept of \u201cdelayed reward\u201d. Delayed reward refers to the reward/punishment (e.g., improved thermal comfort level due to indoor air temperature drops during summer) a DRL agent receives after some time delay (e.g., 30\u202fmin after changing the set-point of thermostat). Finally, DRL has the ability to handle continuous states and actions, which is the case for HVAC systems with continuous sensor readings and actuator controls. Despite the benefits of implementing DRL as a control method for HVAC systems, there exists one major drawback in training the DRL agent. Ideally, an HVAC controlling DRL agent should be trained in a real-world HVAC controlled space with real-time readings from sensors, actuators, and feedbacks from occupants regarding thermal comfort levels [3]. However, it is unfeasible to let the DRL agent explore the state space (i.e., all possible states) fully, including extreme states (e.g., set the room temperature to 90\u202f°F), and receive rewards or punishment, which is how the DRL agent learns through this trial and error process. Moreover, it takes an extensive time for DRL agents to converge to an optimal policy if trained in a real-world environment [16,17]. As a result, studies implementing DRL to control HVAC systems often use simulation tools (e.g., EnergyPlus, Simulink) to build the training environment for the DRL agent, with the assumption that simulations can accurately estimate the states and transitions of a real-world HVAC controlled space. However, this assumption is flawed when the detailed schematics of HVAC systems and/or the detailed information about the building (e.g., location, layout) is unavailable. Another drawback of using simulation is the limited actions that can be controlled by existing simulation tools, such as on/off and thermostat set-points. This is insufficient because HVAC systems contain other controllable parameters, such as fan speed and damper positions. Simply ignoring these actions can result in suboptimal control of HVAC systems. There exist efforts in addressing the inaccuracies of using simulation tools to build the DRL training environment by providing additional training for the DRL agent in real-world HVAC controlled buildings after the agent had already been trained in the environment built using simulation tools [3,11]. However, these efforts are still insufficient, because they only train the DRL agent in the real-world HVAC controlled building for a short period of time (e.g., one week) [3,11,40], hence the DRL agent's exposure to real-world HVAC operational data is limited. In addition, previous studies fail to address the problem of using only a subset of all controllable actions. 3 Proposed framework We propose a DRL based HVAC control framework with the goal of minimizing energy consumption while maintaining thermal comfort levels for occupants. The framework includes two parts (as seen in Fig. 2 ), which echoes the standard reinforcement learning paradigm introduced in Fig. 1, where Fig. 2c is the equivalent to the environment in Fig. 1, 2d and e in combination is the equivalent to the agent in Fig. 1. First, this framework uses historical BAS data (Fig. 2a) to build LSTM models to approximate HVAC operations (Fig. 2b), which addresses the previously mentioned problems of inaccurate representations of HVAC controlled spaces when using simulation tools without detailed HVAC schematics/building information. The reason for choosing the LSTM method roots in its ability to achieve a higher accuracy for time-series predictions [18] due to its capability of retaining information from previous inputs by using loops within LSTM layers. The preprocessing of raw BAS data (2a) and the training of LSTM models (2b) are one-time processes that provide inputs (dashed arrows in Fig. 2) for creating the DRL training environment in the later step (Fig. 2c). The DRL training environment (Fig. 2c) is generated by wrapping the trained LSTM network in a function, which takes in the current state s and action a , and then returns the next state s ' and reward r as experienced by the agent. This interaction between the DRL agent and the DRL training environment is repeated (bold solid arrows in Fig. 2) until the DRL agent converges to an optimal policy, which is the second part of the framework. The DRL agent contains an actor network (Fig. 2d), which takes the current state s as the input, and chooses an action a ; and a critic network, which takes the current state s , action a , and reward r as inputs, and returns the quality of the action Q ( s , a ) , which represents the cumulated reward the agent is expecting to receive. The DRL agent learns to control an HVAC by observing the quality of its actions through a time period (i.e., training period), until the reward it receives converges to a stable value, hence producing an optimal policy [30]. Each part of the framework is introduced in detail in the following sections. 3.1 DRL training environment 3.1.1 Preprocessing of historical BAS data The first step is preprocessing of historical HVAC systems\u2019 operational data collected through BAS (Fig. 2a), as BAS data is often incomplete and noisy [19]. In our framework, the data preprocessing step includes refill of missing values, removal of outliers, normalization of HVAC parameters, and removal of collinear parameters. The common reasons for missing values in BAS are sensor malfunctions and system shutdowns due to overloading. We used the moving average method to handle missing values since it is effective in grasping the update rates over time in HVAC parameters [20]. As for outliers in BAS data, which is mainly due to sensor mis-readings, we used the standard deviation method with the z score set to three (i.e., any parameter value beyond three standard deviations of its mean value is to be eliminated), because it is a common method for outlier removal for BAS data [21]. Next, BAS data should be normalized because the measurements of different HVAC operational parameters have varying scales (e.g., 0\u202frpm\u20131300\u202frpm for the fan speed, whereas −3.5 to 2 inches for the static pressure). Min-max normalization with a feature range of zero to one (i.e., rescaling the maximum value to one, minimum value to zero, and linearly inserting other values in between) was used since it usually provides the best result for training a neural network [22]. The final data preprocessing step is the elimination of collinear parameters in the BAS data. Collinearity in parameters duplicates information and creates redundancy in calculations [23]. We used Pearson correlation coefficient (calculated as Equation (1)) in calculating collinearity, because it shows the degree of linear association among parameters. In Equation (1), the Pearson correlation coefficient ( ρ X , Y ) between two random variables X and Y are calculated as the covariance of X and Y ( c o v ( X , Y ) ) , divided by the product of both variables\u2019 standard deviations ( σ X a n d σ Y ). The extreme values of the Pearson correlation coefficient (i.e., +1 and −1) indicate linear/inverse-linear relationship between two parameters. (1) ρ X , Y = c o v ( X , Y ) σ X σ Y 3.1.2 Training LSTM networks to approximate HVAC operations Once the historical BAS data is preprocessed, it can be used to train an LSTM network (one type of recurrent neural network, or RNN) to approximate the HVAC operations in the real-world (Fig. 2b). LSTM was chosen due to its capability of handling sequential data by using loops to retain information from previous inputs, and its ability to handle the \u201cvanishing gradient\u201d problem existing in the standard RNN, which refers to the phenomenon that the gradients of RNN nodes converge to zero after an extended time of training, which hinders learning of the network [36]. Furthermore, the computational complexity of LSTM, which is usually represented by the number of weights associated with the network, does not drastically increase as compared to the regular RNN [43]. It is shown from previous studies [43,44], that the number of weights in LSTM is only a constant multiplication of the regular RNN. A typical LSTM structure is shown in Fig. 3 a [24], where x t is the input from time t (e.g., the supply air temperature at time t ); A 1 is the first hidden layer of the LSTM containing multiple LSTM nodes, and multiple layers can be added (i.e., from A 1 to A n ); and h t is the output at time t (e.g., the predicted supply air temperature for time t . This LSTM representation with loops can be unfolded into a sequential formation, as seen in Fig. 3b, where the input of one LSTM layer not only contains the new input x t at time t (e.g., the supply air temperature at time t ), but also contains previous outputs from the same LSTM layer { h t − l b , \u2026 , h t − 1 } (e.g., the supply air temperatures predicted from time t − l b to time t − 1 ). Here l b stands for lookback (as seen in Fig. 2b), which refers to the number of time-stamps an LSTM node \u201cremembers\u201d from previous inputs (i.e., { x t − l b , \u2026 , x t − 1 } ), in order to predict the current output [32]. A closer look at an LSTM node (i.e., Fig. 4 b) shows that, one LSTM node contains three gate functions ( f t , i t , o t ), interacting with each other to control the flow of inputs and outputs as compared to a standard RNN node, which usually only contains a single t a n h function (Fig. 4a). Besides the standard output of an RNN node (i.e., h t , such as the predicted supply air temperature at time t ), an LSTM node also has a secondary output called a cell state (i.e., C t ), deciding whether to update the information (e.g., whether to update the current supply air temperature) conveyed from previous inputs to address the vanishing gradient problem [25,36]. In Fig. 4b, σ stands for the sigmoid function, which can be written in σ X = 1 / ( 1 − e − X ) for a random variable X , and operations in circles represent pointwise operations of matrices. Three gate functions are written as f t , i t , a n d o t . Among these three, f t is the forget gate (i.e., Equation (2)), deciding what information to discard by outputting a number between 0 and 1 using the sigmoid function; i t is the input gate (i.e., Equation (3)), deciding what new information to store from the new input also using the sigmoid function; o t is the output gate (Equation (4)), deciding what information to output from the node with the sigmoid function. Weight (i.e., W ) and bias (i.e., b ) matrices of the LSTM network are learned by minimizing the differences between the LSTM outputs (e.g., the predicted fan speed given historical readings of fan speed and other parameters) and the actual training samples (e.g., the actual fan speed readings) using optimization methods such as the stochastic gradient descent (SGD) method. (2) f t = σ ( W f ⋅ [ h t − 1 , x t ] + b f ) (3) i t = σ ( W i ⋅ [ h t − 1 , x t ] + b i ) (4) o t = σ ( W o ⋅ [ h t − 1 , x t ] + b o ) 3.1.3 Creating the training environment for the DRL agent After training the LSTM network (Fig. 2b), an interactive training environment (Fig. 2c) is needed to help the DRL agent learn the optimal control strategy (i.e., policy π ( a | s ) ). To construct this interactive training environment, we need to decide the state and reward design. The state design is intuitive. In our framework, the DRL agent learns to control HVAC systems by observing a set of HVAC parameters collected from the BAS. The combination of these parameters at time t is defined as states S t (see Equation (5)), which can be further divided into environmental states E S t (i.e., states determined by the environment and deeply rooted with time sequence, such as outside air temperature and time) and HVAC states H S t , (i.e., states observed by the sensors embedded in HVAC systems monitoring the operation of HVAC, such as supply air temperature and supply air CO2 concentration). (5) S t = { E S t , 1 , E S t , 2 , E S t , 3 , \u2026 E S t , m , H S t , 1 , H S t , 2 , \u2026 , H S t , n } On the other hand, to define the reward for HVAC control, we need to consider energy consumption and thermal comfort simultaneously. In other words, to achieve an optimal control policy, the DRL agent should be able to minimize the energy consumption, while not causing discomfort for occupants. In this framework, energy consumption is designed to have a negative reward, since higher energy consumption should serve as a punishment for the agent. For representing the thermal comfort, we adopted the widely accepted predicted percentage of dissatisfied (PPD) value proposed by ASHRAE 55 [26]. In a nutshell, the PPD is a percentage value, from 5% to 100%, describing the discomfort level of humans in spaces. A PPD value closer to 5% means that people are more comfortable, while a value closer to 100% means less comfortable. Therefore, the PPD value is also considered as a negative reward (i.e., the higher the PPD value, the smaller the reward). PPD was calculated based on the predicted mean vote (PMV), as shown in Equation (6). PMV was developed by Ref. [27] to define the thermal comfort of occupants and it takes into account the indoor air temperature, humidity, air speed, metabolic rate, and clothing insulation of occupants. In this study, PMV was calculated using an R package \u201ccomf\u201d developed by Ref. [45], based on ASHRAE 55 [26] and ISO 7730 [46]. Data for parameters used to calculate PMV are from two sources:(1) BAS historical data, including indoor air temperature, indoor air relative humidity, and indoor air velocity (which was inferred by the fan speed values); and (2) values indicated on standards (i.e., ASHRAE 55 and ISO 7730) for office spaces, including clothing insulation level (i.e., CLO as 1.0) and metabolic rate (i.e., MET as 1.2) for summer months. The ASHRAE 55 standard defines \u201ccomfortable\u201d as a PMV value between −0.5 and 0.5, which corresponds to 5%\u201310% of PPD. (6) P P D = 100 − 95 × exp ( − 0.03353 × P M V 4 − 0.2179 × P M V 2 ) The overall reward design can be represented as Equation (7). Note that we separated the reward mechanism into occupied and unoccupied periods. Since unoccupied spaces do not have the constraint of maintaining thermal comfort, the only reward is the negative energy consumption. In our implementation, occupied hours were defined as 8 a.m. to 5 p.m. during weekdays (as given by the schedule of the HVAC equipment), which followed the traditional working hours for occupants in office buildings [28]. There are three tunable parameters (i.e., ψ , β , ω ). Parameter ψ is a positive value penalizing energy consumption during unoccupied periods. Parameter β and ω are positive values mediating the relative importance between energy consumption and thermal comfort level (i.e., PPD). A larger β means a higher weight for the thermal comfort in the reward function, whereas a larger ω means a higher weight for the energy consumption. Additionally, β is assumed to only have a non-zero value when PPD is larger than 10%, because lower PPD values indicate the space to be comfortable, and the DRL agent should not be punished if PPD does not exceed 10%. During the training of the DRL agents, we tested multiple combinations of the tunable parameters ψ , β , a n d ω , and the details will be shown in the next section. (7) R t = { − ω × e n e r g y c o n s u m p t i o n − ( β × P P D ) 2 | o c c u p a n c y = 1 − ψ × e n e r g y c o n s u m p t i o n − 0 × P P D | o c c u p a n c y = 0 Finally, we need to determine the training period for the DRL agent, which is defined by the number of episodes and the length of one episode. An episode can contain a number of loops (as shown in Fig. 2 with bold arrows), when the agent interacts with the training environment by choosing various actions. At the end of each episode, the agent receives the cumulated reward and determines how well were the actions taken during that episode. While the number of episodes depends on the convergence of the optimal policy, the length of one episode needs to be defined by looking at the HVAC operational schedule. For example, in an office space, HVAC systems are supposed to keep the space cool/warm during the working hours of weekdays to maintain thermal comfort levels, while saving energy during non-working hours of weekdays and weekends. Therefore, an HVAC system is often operating on a weekly schedule. For the DRL agent to capture this type of fluctuations of operations and learns to control the HVAC systems accordingly during a week, an episode should at least be of one-week length. On the other hand, choosing any longer period of time as an episode can risk overfitting. For this purpose, we defined a Boolean value is_done, to represent if one training episode is done for the DRL agent. In our implementation, we defined is_done to be true after the agent has seen one week of training data, and the cumulated reward in this week will be reported back to the DRL agent for learning the optimal control. After deciding the state and reward design, an algorithm was developed to enable interactions between the DRL agent and the training environment, which is done through the step function shown below (Algorithm 1). Algorithm 1 Deep Reinforcement Learning Agent Training Environment Image 1 3.2 DRL agent In the second part of the framework (Fig. 2d and e), we train the DRL agent to control HVAC systems by letting it interact with the training environment (Fig. 2c) created in the previous step. The DRL agent's performance will be measured by the reward it received calculated using Equation (7), which takes into consideration of the energy consumption as well as thermal comfort levels. The DRL agent learns to control the HVAC systems through a \u201ctrial and error\u201d process. During this process, the DRL agent explores many state-action pairs in the beginning of the training to understand the reward associated with them. After a number of episodes, the reward would converge to a relatively stable value, because the DRL agent stopped exploring new state-action pairs and started to exploit the learned policy. The balance of \u201cexploration vs. exploitation\u201d can be achieved using the ε greedy method, where ε is the coefficient of the rate of exploration. In our implementation, we set ε to be 50% at the start of the training, and ε decays with a rate of 0.9995, so the exploration would stop after a number of episodes. During exploration, the DRL agent can generate an action that is out of the range of the historical BAS data for that specific action parameters (e.g., set the fan speed to 5000\u202frpm, while the fan speed was recorded to have a boundary of 0\u20131300\u202frpm). To regulate this behavior, we closely monitor the action parameter values generated by the DRL agent. If the DRL agent generates any action parameter values that fall out of the range of the BAS historical data for that specific action, we simply ignored the out-of-range actions and regenerate a set of actions for the same timestamp. The most commonly used reinforcement learning algorithms for controlling HVAC systems are Q-learning and its variants because of two main advantages they have in addressing HVAC control problems. First, Q-learning is model free, which eliminates the need for creating complex mathematical models of HVAC systems. Furthermore, Q-learning is off-policy, meaning instead of evaluating a set of possible policies (i.e., a set of instructions describing actions to take for each state the agent is in), Q-learning agent observes the reward it received in previous interactions with the environment and updates its policy based on the expected cumulated reward in the future. This process can be described using Equation (8), where G t is the cumulated reward (e.g., cumulated energy consumption of an AHU in three months) of R t from time t (e.g., the energy consumption of an AHU at 8:00 a.m.), and γ is the discount factor ( 0 < γ ≤ 1 ), used to gradually discount the reward the agent expects to receive in the future [10] (e.g., the expected energy consumption of an AHU from 8:00 a.m. to the end of the day), and k is the index of future time stamps. (8) G t = ∑ k = 0 ∞ γ k R t + k + 1 The Q-learning agent then uses the action-value function (defined in Equation (9)) to assess the expected return of a state action pair ( s , a ), also called a Q value, where S t and A t are the random variables representing the state and action at time t . In a nutshell, Q value is the expected value of the cumulated reward G t from all possible pairs of states and actions at time t . The agent updates its Q values using Equation (10), where α is the learning rate determining the extent of updates for each episode, and Q m a x \u2032 ( s ' , a ' ) is the maximum Q value for the next state action pair ( s ' , a ' ) . (9) Q ( s , a ) = Ε [ G t | S t = s , A t = a ] (10) Q ( s , a ) = Q ( s , a ) + α [ R ( s , a ) + γ ( Q max \u2032 ( s \u2032 , a \u2032 ) − Q ( s , a ) ) ] The standard Q-learning implementation described in Equations (8)\u2013(10) works well with discrete states and actions, but falls apart when handling continuous states and actions, which is often the case for HVAC systems. Therefore, variants of Q-learning were developed to address this issue, including Deep Q Network (DQN), Asynchronous Advantage Actor Critic (A3C), and Deep Deterministic Policy Gradient (DDPG). One common theme of these variants is the use of deep neural networks to approximate Q values instead of calculating them based on the discrete states and reward functions (Equation (9)). The inputs for these deep neural networks are state action pairs, which are allowed to have multiple continuous parameters (e.g., states can be a combination of AHU and environmental states, and actions can be a series of control parameters present in the BAS). The outputs of the networks are the calculated Q values. Among Q-learning variants, DDPG has been proven to work well with HVAC system control problems [29]. Hence, DDPG was used to train the DRL agent in our framework. In DDPG, there are two deep neural networks, an actor network and a critic network [30], as seen in Fig. 2d and e. Essentially, the actor network performs an action, and the critic network judges the quality of the action by calculating the Q value for that action. More specifically, the actor network, often written as π ( s | θ π ), takes in the current state ( s ) and returns an action to take ( A t = π ( s | θ π ) ) by the DRL agent. Here θ π , stands for the weight matrices in the actor network. On the other hand, the critic network, often written as Q ( s , a | θ Q ), takes in the current state and action produced by the action networks, and returns the quality of the action taken (i.e., Q value). Here θ Q stands for the weight matrices in the critic network. To perform loss calculations and weight updates (i.e., the learning process for deep neural networks) for both the actor and critic networks ( π and Q ), DDPG also builds two target networks ( π ' and Q ' ). The target networks have the same input/output pair as the actor/critic networks, but they update their weights differently ( θ π ' a n d θ Q ' ) by using a parameter τ (as seen in Equation (11) and (12)). τ is a number between 0 and 1, managing the extent of updates for the target networks. Loss function of the target networks is defined as the difference of the outputs of the target networks and the actor/critic networks. After training, target networks are the final outputs for the DDPG algorithm (i.e., the DRL agent). Finally, the details of our DDPG algorithm implementation (adapted from Ref. [30]) can be found in Algorithm 2. (11) θ π ' = τ θ π + ( 1 − τ ) θ π ' (12) θ Q ' = τ θ Q + ( 1 − τ ) θ Q ' Algorithm 2 Deep Deterministic Policy Gradient Algorithm Image 2 4 Introduction of the testbeds The testbeds include three Air Handling Units (AHUs) installed in a functional office building and each AHU controls a separate section of the building. All three AHUs are generic all-air single-duct variable air volume systems, consisting of filter modules, mixing boxes, heating/cooling coils, and fans (see Fig. 5 for the schematics). The AHUs are rated with a supply airflow of about 181.23 cubic meter per minute (i.e., 6400\u202fcubic feet per minute). The historical AHU data is collected through the Building Automation System (BAS), to which all AHUs are connected. Overall, 33 operational parameters of AHUs were recorded at a frequency of 15\u202fmin for two years. After the data preprocessing step, the original 33 parameters were reduced to 24, (as will be shown in Table 1 ). The details of data preprocessing will be introduced in the next section. To avoid repetition, only the eliminated nine parameters are listed here: static air pressure after filter (in), supply air static pressure (in), return air static pressure (in), outside air velocity (fpm), supply air velocity (fpm), return air relative humidity (%), supply air flow (cfm), static air pressure after heating coil (in), return air CO 2 concentration (ppm). 5 Evaluation of the framework The proposed framework was evaluated on three testbed AHUs. For each AHU, we created a DRL training environment, and a testing environment (i.e., six in total), using trained LSTM networks to approximate AHU operations (Fig. 2b and c). The reason of creating separate training and testing environments for individual AHUs is that each AHU controls one section of the testbed building, and the historical BAS data differs in parameter values due to the dynamic operation and occupancy of the individual sections. We used the first year BAS data to build the training environments and the second year BAS data for the testing environments. We also trained one DRL agent per AHU (three in total) using the DDPG algorithm. The results for each part of the framework are reported in the following sections. 5.1 Creating the DRL training and testing environments 5.1.1 Preprocessing of the stored BAS data The input for the preprocessing step is the raw BAS data for three testbed AHUs, which was recorded every 15\u202fmin for two years. The whole two-year AHUs data was preprocessed, and later split into training (i.e., first year) and testing (i.e., second year). We performed missing value refill, outlier removal, normalization, and collinear parameter removal individually for each AHU. Looking at the results, there were about 4.1% missing values, and about 1.2% outliers in the raw data. For collinearity detection, we set the thresholds of Pearson correlation coefficient ( ρ , as calculated in Equation (1)) between two parameters as +0.90 and −0.90, since any higher/lower values indicate linear/inverse-linear relationship between parameters [31]. Fig. 6 illustrates an example of the collinearity among three parameters, outside air velocity (VAO), outside air flow (FAO), and power (ahu_kw). It is apparent that outside air velocity and outside air flow have a linear relationship ( ρ = 0.98 ). Therefore, only outside air flow was retained for further steps. After filtering out the collinear parameters, nine parameters were dropped from the original 33 for the three testbed AHUs, due to high correlation coefficient values (i.e., ρ > 0.9 o r ρ < − 0.9 ). The nine parameters were chosen as an intersection from three AHUs to retain as much information from BAS as possible. Next, we separated the remaining 24 parameters into states and actions from the reinforcement learning perspective (i.e., Fig. 1). As seen in Table 1, for AHU 1\u20133, state parameters are defined as the ones that can only be observed by the DRL agent, while action parameters are the ones that can be controlled by the agent. Next, we further separated the state parameters into environmental states (i.e., states determined by the environment and deeply rooted with time sequence, such as outside air temperature and day in week) and HVAC states, which in our case are AHU states (i.e., states observed by the sensors embedded in AHUs to monitor system operation, such as supply air temperature and supply air CO2 concentration). Among which, note that the indoor air temperature and the indoor air relative humidity are captured through sensors equipped in variable-air-volume (VAV) boxes, which are installed in conditioned spaces (zones) as shown in Fig. 5. VAV boxes control the volume of conditioned air delivered to spaces based on the setpoints set by occupants, and can provide more accurate air temperature control. Hence the overall PPD of occupants in the testbed building can be represented by averaging the sensor readings from VAV boxes. Finally, the outputs of this step are three sets of preprocessed BAS data, each containing 24 AHU parameters, separated into actions, environmental states, and AHU states. 5.1.2 Training LSTM to approximate AHU operations and creating the DRL training environments In this step, we aim to approximate the AHU operations using LSTM, and then use the trained LSTM networks to create training environments for the DRL agents (Fig. 2b and c). LSTM excels at predicting time-series data due to its ability of \u201cremembering\u201d previous inputs during prediction, which is captured by the concept \u201clookback\u201d. Lookback refers to the number of timestamps an LSTM node \u201cremembers\u201d from previous inputs in order to predict the current output [32], as seen in Fig. 2b. To use the parameter supply air temperature as an example, as seen in Fig. 7 , we assume the current time is t and the current supply air temperature is x t . In this case, at time t , the combined input for the LSTM network is the set of { x t − l b , x t − l b + 1 , \u2026 , x t } (i.e., previous supply air temperature readings from time t − l b to time t ). For our testbed AHUs, the BAS data was recorded every 15\u202fmin, so we have 96 rows of BAS data per day, containing 24 columns (from Table 1). In this study, we used 672 (i.e., 96\u202f×\u202f7) as the lookback value. This means when predicting the next state, the LSTM network refers back as far as one week's historical input data (i.e., previous states and actions). The input of LSTM networks contains all 24 AHU parameters (i.e., states and actions) filtered from the last step, and the output contains only the AHU state parameters (i.e., 16 parameters). This input/output pair is designed by considering the DRL agent control input/output. To elaborate, the DRL agent observes its current state (AHU and environmental states) to decide on an action, and the training environment takes in this state/action pair to produce the next AHU state and a reward. So, the training environment does not predict the DRL agent's next action (which is produced by the target actor network π ( S t | θ π ' ) ), nor the environmental state values (decided by historical data such as historical weather), but only the AHU states. During evaluation, we used a 70/30 split for train/test separation to examine the LSTM network accuracies. The accuracy was calculated using the average mean squared error (MSE) across all 16 predicted AHU state values against the corresponding ground truth values stored in the BAS. MSE can be calculated using the Equation (13) where y i and y ˆ i are the ground truth and predicted value for parameter y . (13) M S E = 1 n ∑ i = 1 n ( y i − y ˆ i ) 2 To find the best LSTM network architecture for each testbed AHU, we iterated through combinations of two LSTM hyper-parameters, which are the number of LSTM layers and the number of LSTM nodes per layer. Unlike standard neural networks, LSTM, or RNN in general, does not require a very deep network to produce accurate prediction results [30,32]. Rather, changing the number of LSTM nodes per layer has proven to be a more effective hyper-parameter tuning method than changing the number of LSTM layers [33]. In our evaluation, the number of LSTM layers was chosen to be smaller than five, since previous research showed that deeper LSTM networks had little improvement in prediction accuracy of AHU operational data [34,35]. On the other hand, the number of LSTM nodes per layer was kept under 256, since the marginal improvement of prediction accuracy for wider networks is low [34,35]. To conclude, we tested the network architectures with two, three and four LSTM layers, and 32, 64, 128, and 256 nodes per layer respectively for each AHU, while holding the lookback value as 672 (i.e., 96\u202f×\u202f7). We calculated the average MSE across all AHU states (16 AHU parameters) for the testing data (30% of whole data in the first year) across all combinations (total of 12) of hyper-parameters, and the results for AHU-1 are presented in Table 2 . Note that the MSE values are normalized since the inputs to the LSTM networks were normalized from the data preprocessing step. It is apparent that with the increase of LSTM layers, the model accuracy decreases (i.e., larger MSE), which indicates overfitting of deeper LSTM networks. Therefore, the final selected network has two LSTM layers for AHU-1. Next, keeping LSTM layers as two, with the increase of LSTM nodes per layer, the network accuracy first increases and then decrease, and the best network accuracy was achieved with 128 LSTM nodes per layer. Overall, the smallest MSE for AHU-1 (in italic, underlined) was achieved with two LSTM layers, and 128 nodes per LSTM layer. The same analysis was done for AHU-2 and 3. For AHU-2, the best performing LSTM architecture contains three LSTM layers and 128 nodes per layer with an MSE of 0.0016. For AHU-3, the best performing LSTM architecture has two LSTM layers and 256 nodes per layer with an MSE of 0.0015. The best performing LSTM architecture was used to train the network, and the predicted AHU state values from the testing set (30% of whole data) for AHU-1 are shown in Fig. 8 , black lines being predicted values and red lines being ground truth values from the BAS. It is apparent that our LSTM network can accurately capture the trend of every parameter observed in AHU-1, since the black lines follow closely to the red lines. The average MSE between the predicted (black) and ground truth (red) values across these 16 AHU states is 0.0014, as shown in Table 2. The results from Table 2 and Fig. 8 serve as a confirmation that our LSTM networks can accurately predict AHU operational data, which proves our LSTM-based approximation of AHU operations are close representations of real-world AHU operations, and we can use the trained LSTM networks to create training environments for DRL agents. 5.2 Training and testing of the DRL agent To evaluate the framework in the context of optimal control for AHUs, we implemented state-of-the-art DRL control algorithm, deep deterministic policy gradient (DDPG), as shown in Algorithm 2. The goodness of control is measured by the cumulated reward per episode, which takes into consideration of energy consumption measured in kilowatt, and thermal comfort levels measured in PPD. Three DRL agents were trained, one for each testbed AHU. For consistency, we kept the agents\u2019 observable data in the training environments as only summer months (June, July and August), because AHUs operate in different modes (i.e., cooling and heating) during summer and winter times. Specifically, the AHUs reduce indoor air temperature during summer to improve thermal comfort level of occupants but raise indoor air temperature during winter for the same purpose. As a result, cooling and heating modes need to be separated to prevent divergence in reward received by the DRL agents because of the opposite actions for heating and cooling modes. Rewards received by the DRL agent were calculated using Equation (7), which had three tunable parameters (i.e., ψ , β , ω ) to balance the emphasis on energy consumption and thermal comfort. Among three parameters, ψ penalizes energy consumption occurred during unoccupied hours (i.e., weekday nights and weekends), while β and ω penalize large PPD values and high energy consumption during occupied hours. The logic of selecting the possible values of the three parameters is twofold: 1) to guarantee that the DRL agent receives relatively the same amount of penalty for high energy consumption and high PPD during occupied hours; and 2) to ensure that the penalty of energy consumption during the unoccupied hours is similar to the combined penalty of energy consumption and PPD during the occupied hours. Therefore, we identified the set of ( ψ , β , ω ) that results in the same reward for energy consumption and PPD from historical data, which is (5, 10, 2.0). Next, we oscillated each parameter around this set of value (i.e., between 1 and 9 for ψ , 6\u201314 for β , and 1.0\u20133.0 for ω ) to enlarge the search space for the optimal control. In this study, ( ψ , β , ω ) were chosen among the sets of {1, 3, 5, 7, 9}, {6, 8, 10, 12, 14}, and {1.0, 1.5, 2.0, 2.5, 3.0}, respectively. It is noted that a more comprehensive search of the possible values for the parameters may result in better control policies and their impact on the results should be investigated in the subsequent study for sensitivity of the reward function to search spaces. For each AHU, we exhaustively tested every possible combination of ( ψ , β , ω ) chosen from their value sets and trained the DRL agent for 200 episodes. We observed the average energy consumption for weekday days and nights for the DRL agent and the RBC control for the testbed AHUs. Average PPD values during daytime weekdays were also calculated for the DRL and RBC control. The set of parameters that resulted in the most energy saving and PPD reduction was selected as the final set of ( ψ , β , ω ) to implement for each AHU. As shown in Table 3 , the DRL agents achieved 25%\u201330% energy savings over the RBC control in training environments of the testbed AHUs. On the other hand, the average PPD reduction of our DRL agents are not significant over the RBC control during occupied hours (daytime of weekdays), because the RBC method already provide satisfactory PPD values (less than 10%) during those hours. We trained the DRL agents on a six-core CPU, with sufficient RAM but no support from GPU. A single run for one set of ( ψ , β , ω ) took around 5\u202fmin. The total training time for three AHUs was 31\u202fh (i.e., roughly 10\u202fh per AHU). This training time is a one-time upfront cost. After training, the DRL agents can be deployed for controlling the AHUs in real-time. The convergence of DRL agents for each set of parameters ( ψ , β , ω ) was examined from the reward-episode graph (i.e., Fig. 9 ) using the elbow method (i.e., the trend of reward flattens). In Fig. 9, the solid lines are raw reward values and the dotted lines are logarithmic trend lines describing the overall trend of the reward. We selected the logarithmic function because it is best fitted as a trend line when the rate of change in the raw data is large at the beginning but quickly levels out [47]. For AHU-1, 2 and 3, we observed the reward converged to a stable cumulated reward value of −120, −200, and −300, respectively. Note that the absolute value of the reward does not have any practical units, since it is a numerical representation of energy consumption and thermal comfort level solely determined by the reward design presented in Equation (7). The important observation here is to check whether the reward converges to a stable value, since a non-convergent reward means the DRL agent failed to achieve the optimal control over the AHU. Deploying a non-convergent DRL agent to an AHU can result in sub-optimal actions such as turning the fan completely off during the day to avoid high energy consumption but disregard occupants\u2019 thermal comfort levels. Hence, when the reward cannot converge to a stable value for one set of ( ψ , β , ω ) , that particular set of parameters could be discarded, and the next set of values for parameters can be iterated. If none of the parameter sets can achieve a convergent reward, the reward function needs to be redesigned to better balance energy consumption and thermal comfort levels. Aside from the observation of convergence, it is also noticeable that the reward does not follow a smoothly increasing line, which concurs with the exploration behavior expected from the DRL agent, which has a exploration rate (i.e., ε ) of 50% (randomly take actions 50% of the time) at the beginning of the training. The exploration rate decays at a pace of 0.9995, which ensures the DRL agent does not take random actions after a number of training episodes and starts to exploit the \u201clearned\u201d knowledge of controlling an HVAC system to achieve optimal control. Finally, the converged DRL agents were deployed to the testing environments. Three testing environments were built, one for each AHU, using the same method adopted from creating the training environments. The only difference was that the testing environments used the second year BAS data as inputs as compared to the first year. This was done to ensure the AHU data in our testing environments had never been seen by the DRL agents. The DRL agents controlled the AHUs in our testing environments for three months (i.e., the same three summer months used for training, but in the second year). We compared the energy consumption and PPD values achieved by the DRL agents with the RBC control implemented in the testbed building. Results show that the DRL agents for AHU-1, 2, and 3 consumed 27.00%, 31.27%, and 30.55% less energy over three months while maintained a PPD of 9.99%, 10.00%, and 9.77% during occupied hours (i.e., weekday days). Fig. 10 provides a more detailed comparison of the energy consumption and thermal comfort levels between the RBC control and DRL agent for AHU-1. Only weekdays are shown, since neither of the RBC control nor DRL agent consumed energy during weekends. As seen in Fig. 10, the DRL agents consumed less energy during weekday daytimes and nights. Furthermore, the DRL agent for AHU-1 learned to pre-cool the spaces by turning on the AHU before the start of occupied hours (i.e., 8 a.m.), which ensures the thermal comfort levels for occupants during working hours (8 a.m.\u20135 p.m.). Especially for Mondays, the DRL agent had a higher energy peak than the rest of the weekdays, due to the higher cooling demand caused by lack of cooling activities during the weekends. The DRL agent also learned to slowly shut down the AHU to avoid wasting energy to cool the spaces right before the end of working hours. From the perspective of thermal comfort, the DRL agent maintained similar PPD (around 10%) as compared to the RBC control during occupied hours. The RBC control reaches low PPD levels earlier in the day than the DRL agent, which can also be reflected on the energy consumption, where the RBC control reached higher energy consumption peaks before the DRL control. It can also be noticed that the DRL agent had higher PPD values on Fridays, because the agent learned to use less energy close to weekends to avoid penalties caused by the energy consumption during unoccupied hours. Similar trends of energy consumption and PPD values can be observed in AHU-2 and AHU-3, which are shown in Fig. 11 . To give an overview of the control actions implemented by the DRL agent, we calculated the average values of the control actions (i.e., outside air damper position (%) and fan speed (rpm)) in response to the energy consumption and the PPD for every 3-h period over three months (as shown in Table 4 ). We also included the outside air temperature as a reference for the weather condition. The other two control actions, heating valve status and liquid solenoid status were omitted, since they had no variance during summer months (i.e., heating valve was off, and the liquid solenoid was fully open). As seen in Table 4, the DRL agent's actions were aiming for the reduction of energy consumption and PPD. Agent did not close the outside air damper and the fan fully during weeknights while the outside air temperature is lower. These actions caused the PPD level to remain below 20% until 12 a.m. during nights, which helped to curtail the energy peaks typically happen in mornings due to turning on equipment fully. During occupied hours (8 a.m.\u20135 p.m.), the fan speed remained high and the damper opened less than night hours to limit the hot air intake from the outside during summer times while blowing conditioned air to spaces. 6 Conclusion This paper proposes a novel framework that combines LSTM and DRL to achieve optimal control of AHUs. The proposed DRL approach eliminates the need of engineering experience from the facility managers to decide static set-points for HVAC systems. Furthermore, we proposed a new method to create DRL training environments by using LSTM networks to approximate HVAC operational parameter values. LSTM-based training environments can provide a more accurate approximation of the real-world AHU operations using only historical BAS data (as shown in Fig. 8) when AHU schematics and other building information (e.g., building location, layouts) are unavailable. Moreover, even with the information readily available, simulations tools demand extensive parameter tuning during the calibration process, which requires extensive knowledge of thermal dynamics. Finally, using real BAS data provides the possibility of comparing the optimal policy achieved by the DRL agent to the actual current rule-based policy implemented in the buildings. We evaluated our framework using three AHUs from real settings. Our LSTM-based training environments achieved an average MSE of 0.0015 across all AHU parameters for three AHUs. When deployed for testing, our trained DRL agents achieved an average 27%\u201330% less energy consumption in three months for three AHUs, while maintaining the thermal comfort level (average PPD of 10%) during occupied hours on weekdays for three AHUs, which is compliant with the ASHRAE recommended 5\u201320% PPD for building thermal comfort. Finally, our proposed framework can be scaled to any HVAC systems with historical sensor readings, which is becoming increasingly common these days due to wider deployment of internet of things and accumulated BAS data. The trained DRL agent will be completely tailored toward its own HVAC system, and achieve optimal control given enough training data and training episodes. 7 Limitation and future work The limitations and corresponding future work of this study can be summarized in the following four aspects. First, the authors only deployed the proposed framework on AHUs for the purpose of evaluation. However, there are other components included in HVAC systems, such as chillers and boilers, which are part of a larger central system. As part of the future work, the authors will extend the evaluation of the proposed framework to include other HVAC components to achieve the optimal control over a complete central system. Second, the authors trained the DRL agents under the assumption that the optimal control policy should minimize the energy consumption of AHUs, while maintaining the thermal comfort level of occupants. However, there are other indoor environment quality metrics concerning other aspects of the occupants' comfort levels, such as indoor air quality and CO2 level, which can also be controlled by HVAC systems. As a result, the authors will extend the current reward function and control action design to incorporate indoor air quality metrics for achieving a more comprehensive optimal control policy of the AHUs. Furthermore, the trained DRL agents in this study are building specific, because the main goal of this study is to propose an alternative method to create DRL training environments using historical BAS data instead of energy simulations. When deploying this framework on a new building, the LSTM networks and the DRL agents have to be retrained for optimal control of the new building. For future work, the authors will explore methods to minimize this repeated effort of retraining, with the assumption that a new building's operation can be inferred from another similarly configured highly sensed building in terms of HVAC systems and building characteristics (e.g., building type, number of floors). Finally, while the authors argue the superiority of the proposed method of using data-driven approaches to build DRL training environments over the current practice of using energy simulation tools, a direct comparison of the HVAC parameter prediction accuracy between the LSTM-based method and the simulation tools is preferred. Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. References [1] U.S. EIA Annual Energy Review 2011 2012 U.S. Energy Information Administration U.S. EIA. (2012). \u201cAnnual Energy Review 2011\u201d. U.S. Energy Information Administration. [2] L. Pérez-Lombard J. Ortiz C. Pout A review on buildings energy consumption information Energy Build. 40 3 2008 394 398 Perez-Lombard, L., Ortiz, J., & Pout, C. (2008). \u201cA review on buildings energy consumption information\u201d. Energy and Buildings, 40(3), 394-398. [3] Z. Zhang K.P. Lam Practical implementation and evaluation of deep reinforcement learning control for a radiant heating system\u201d Proceedings of the 5th Conference on Systems for Built Environments 2018, November ACM 148 157 Zhang, Z., & Lam, K. P. (2018, November). \u201cPractical implementation and evaluation of deep reinforcement learning control for a radiant heating system\u201d. In Proceedings of the 5th Conference on Systems for Built Environments (pp. 148-157). ACM. [4] S. Wang Z. Ma Supervisory and optimal control of building HVAC systems: a review HVAC R Res. 14 1 2008 3 32 Wang, S., & Ma, Z. (2008). \u201cSupervisory and optimal control of building HVAC systems: A review\u201d. HVAC&R Research, 14(1), 3-32. [5] C.Y. Han Y. Xiao C.J. Ruther Fault detection and diagnosis of HVAC systems ASHRAE Transact. 105 1999 568 Han, C. Y., Xiao, Y., & Ruther, C. J. (1999). \u201cFault detection and diagnosis of HVAC systems\u201d. Ashrae Transactions, 105, 568. [6] M. Tariq Z. Zhou J. Wu M. Macuha T. Sato October). \u201cSmart grid standards for home and building automation\u201d 2012 IEEE International Conference on Power System Technology (POWERCON) 2012 IEEE 1 6 Tariq, M., Zhou, Z., Wu, J., Macuha, M., & Sato, T. (2012, October). \u201cSmart grid standards for home and building automation\u201d. In 2012 IEEE International Conference on Power System Technology (POWERCON) (pp. 1-6). IEEE. [7] G. Dedemen S. Ergan Quantifying performance degradation of HVAC systems for proactive maintenance using a data-driven approach Workshop of the European Group for Intelligent Computing in Engineering 2018, June Springer Cham 488 497 Dedemen, G., & Ergan, S. (2018, June). \u201cQuantifying Performance Degradation of HVAC Systems for Proactive Maintenance Using a Data-Driven Approach\u201d. In Workshop of the European Group for Intelligent Computing in Engineering (pp. 488-497). Springer, Cham. [8] X. Yu S. Ergan G. Dedemen A data-driven approach to extract operational signatures of HVAC systems and analyze impact on electricity consumption Appl. Energy 253 2019 113497 Yu, X., Ergan, S., & Dedemen, G. (2019). \u201cA data-driven approach to extract operational signatures of HVAC systems and analyze impact on electricity consumption\u201d. Applied Energy, 253, 113497. [9] A. Afram F. Janabi-Sharifi Theory and applications of HVAC control systems\u2013A review of model predictive control (MPC) Build. Environ. 72 2014 343 355 Afram, A., & Janabi-Sharifi, F. (2014). \u201cTheory and applications of HVAC control systems-A review of model predictive control (MPC)\u201d. Building and Environment, 72, 343-355. [10] R.S. Sutton A.G. Barto Introduction to Reinforcement Learning vol. 135 1998 MIT press Cambridge Sutton, R. S., & Barto, A. G. (1998). \u201cIntroduction to reinforcement learning\u201d (Vol. 135). Cambridge: MIT press. [11] S. Liu G.P. Henze Experimental analysis of simulated reinforcement learning control for active and passive building thermal storage inventory: Part 2: results and analysis Energy Build. 38 2 2006 148 161 Liu, S., & Henze, G. P. (2006). \u201cExperimental analysis of simulated reinforcement learning control for active and passive building thermal storage inventory: Part 2: Results and analysis\u201d. Energy and buildings, 38(2), 148-161. [12] A. Nagy H. Kazmi F. Cheaib J. Driesen Deep Reinforcement Learning for Optimal Control of Space Heating 2018 arXiv preprint arXiv:1805.03777 Nagy, A., Kazmi, H., Cheaib, F., & Driesen, J. (2018). \u201cDeep Reinforcement Learning for Optimal Control of Space Heating\u201d. arXiv preprint arXiv:1805.03777. [13] K.S. Peng C.T. Morrison July). \u201cModel predictive prior reinforcement learning for a heat pump thermostat\u201d IEEE International Conference on Automatic Computing: Feedback Computing vol. 16 2016 Peng, K. S., & Morrison, C. T. (2016, July). \u201cModel Predictive Prior Reinforcement Learning for a Heat Pump Thermostat\u201d. In IEEE International Conference on Automatic Computing: Feedback Computing (Vol. 16). [14] M. Qin J. Yang February). \u201cEvaluation of different thermal models in EnergyPlus for calculating moisture effects on building energy consumption in different climate conditions\u201d Building Simulation vol. 9 2016 Tsinghua University Press 15 25 No. 1 Qin, M., & Yang, J. (2016, February). \u201cEvaluation of different thermal models in EnergyPlus for calculating moisture effects on building energy consumption in different climate conditions\u201d. In Building Simulation (Vol. 9, No. 1, pp. 15-25). Tsinghua University Press. [15] V. Mnih K. Kavukcuoglu D. Silver A. Graves I. Antonoglou D. Wierstra M. Riedmiller Playing Atari with Deep Reinforcement Learning 2013 arXiv preprint arXiv:1312.5602 Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). \u201cPlaying atari with deep reinforcement learning\u201d. arXiv preprint arXiv:1312.5602. [16] G.T. Costanzo S. Iacovella F. Ruelens T. Leurs B.J. Claessens Experimental analysis of data-driven control for a building heating system Sustainable Energy, Grids and Networks 6 2016 81 90 Costanzo, G. T., Iacovella, S., Ruelens, F., Leurs, T., & Claessens, B. J. (2016). \u201cExperimental analysis of data-driven control for a building heating system\u201d. Sustainable Energy, Grids and Networks, 6, 81-90. [17] K. Dalamagkidis D. Kolokotsa K. Kalaitzakis G.S. Stavrakakis Reinforcement learning for energy conservation and comfort in buildings Build. Environ. 42 7 2007 2686 2698 Dalamagkidis, K., Kolokotsa, D., Kalaitzakis, K., & Stavrakakis, G. S. (2007). \u201cReinforcement learning for energy conservation and comfort in buildings\u201d. Building and environment, 42(7), 2686-2698. [18] Z. Zheng H. Chen X. Luo Spatial granularity analysis on electricity consumption prediction using LSTM recurrent neural network Energy Procedia 158 2019 2713 2718 Zheng, Z., Chen, H., & Luo, X. (2019). \u201cSpatial granularity analysis on electricity consumption prediction using LSTM recurrent neural network\u201d. Energy Procedia, 158, 2713-2718. [19] S.B. Kotsiantis Supervised machine learning: a review of classification techniques Informatica 31 2007 249 268 Kotsiantis, S. B. (2007). \u201cSupervised Machine Learning: A Review of Classification Techniques\u201d. Informatica, 31, 249-268. [20] C. Fan F. Xiao H. Madsen D. Wang Temporal knowledge discovery in big BAS data for building energy management Energy Build. 109 2015 75 89 Fan, C., Xiao, F., Madsen, H., & Wang, D. (2015). \u201cTemporal knowledge discovery in big BAS data for building energy management\u201d. Energy and Buildings, 109, 75-89. [21] J.E. Seem Using intelligent data analysis to detect abnormal energy consumption in buildings Energy Build. 39 1 2007 52 58 Seem, J. E. (2007). \u201cUsing intelligent data analysis to detect abnormal energy consumption in buildings\u201d. Energy and buildings, 39(1), 52-58. [22] T. Jayalakshmi A. Santhakumaran Statistical normalization and back propagation for classification Int. J. Compute. Theory. Eng. 3 1 2011 1793 8201 Jayalakshmi, T., & Santhakumaran, A. (2011). \u201cStatistical normalization and back propagation for classification\u201d. International Journal of Computer Theory and Engineering, 3(1), 1793-8201. [23] D.P. Evangeline C. Sandhiya P. Anandhakumar G.D. Raj T. Rajendran Feature subset selection for irrelevant data removal using Decision Tree Algorithm Advanced Computing (ICoAC), 2013 Fifth International Conference on Advanced Computing, Chennai, India 2013 268 274 Evangeline, D.P., Sandhiya, C., Anandhakumar, P., Raj, G.D. and Rajendran, T., (2013). \u201cFeature subset selection for irrelevant data removal using Decision Tree Algorithm.\u201d In Advanced Computing (ICoAC), 2013 Fifth International Conference on Advanced Computing, Chennai, India, pp. 268-274. [24] W. Zaremba I. Sutskever O. Vinyals Recurrent Neural Network Regularization 2014 arXiv preprint arXiv:1409.2329 Zaremba, W., Sutskever, I., & Vinyals, O. (2014). \u201cRecurrent neural network regularization\u201d. arXiv preprint arXiv:1409.2329. [25] M. Sundermeyer R. Schlüter H. Ney LSTM neural networks for language modeling Thirteenth Annual Conference of the International Speech Communication Association 2012 Sundermeyer, M., Schluter, R., & Ney, H. (2012). \u201cLSTM neural networks for language modeling\u201d. In Thirteenth annual conference of the international speech communication association. [26] R.J. De Dear G.S. Brager Thermal comfort in naturally ventilated buildings: revisions to ASHRAE Standard 55 Energy Build. 34 6 2002 549 561 De Dear, R. J., & Brager, G. S. (2002). \u201cThermal comfort in naturally ventilated buildings: revisions to ASHRAE Standard 55\u201d. Energy and buildings, 34(6), 549-561. [27] P.O. Fanger Thermal Comfort. Analysis and Applications in Environmental Engineering 1970 (Thermal comfort. Analysis and applications in environmental engineering) Fanger, P. O. (1970). \u201cThermal comfort. Analysis and applications in environmental engineering\u201d. Thermal comfort. Analysis and applications in environmental engineering. [28] J. Wajcman E. Rose J.E. Brown M. Bittman Enacting virtual connections between work and home J. Sociol. 46 3 2010 257 275 Wajcman, J., Rose, E., Brown, J. E., & Bittman, M. (2010). \u201cEnacting virtual connections between work and home\u201d. Journal of sociology, 46(3), 257-275. [29] G. Gao J. Li Y. Wen Energy-Efficient Thermal Comfort Control in Smart Buildings via Deep Reinforcement Learning 2019 arXiv preprint arXiv:1901.04693 Gao, G., Li, J., & Wen, Y. (2019). \u201cEnergy-Efficient Thermal Comfort Control in Smart Buildings via Deep Reinforcement Learning\u201d. arXiv preprint arXiv:1901.04693. [30] T.P. Lillicrap J.J. Hunt A. Pritzel N. Heess T. Erez Y. Tassa D. Wierstra Continuous Control with Deep Reinforcement Learning 2015 arXiv preprint arXiv:1509.02971 Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., ... & Wierstra, D. (2015). \u201cContinuous control with deep reinforcement learning\u201d. arXiv preprint arXiv:1509.02971. [31] L.H. Batista P.R. Camargo G.V. Aiello J. Oishi T.F. Salvini Knee joint range-of-motion evaluation: correlation between measurements achieved using a universal goniometer and an isokinetic dynamometer Braz. J. Phys. Ther. 10 2 2006 193 198 Batista, L. H., Camargo, P. R., Aiello, G. V., Oishi, J., & Salvini, T. F. (2006). \u201cKnee joint range-of-motion evaluation: correlation between measurements achieved using a universal goniometer and an isokinetic dynamometer\u201d. Brazilian Journal of Physical Therapy, 10(2), pp. 193-198. [32] B. Cortez B. Carrera Y.J. Kim J.Y. Jung An architecture for emergency event prediction using LSTM recurrent neural networks Expert Syst. Appl. 97 2018 315 324 Cortez, B., Carrera, B., Kim, Y. J., & Jung, J. Y. (2018). \u201cAn architecture for emergency event prediction using LSTM recurrent neural networks\u201d. Expert Systems with Applications, 97, 315-324. [33] J. Schmidhuber Deep learning in neural networks: an overview Neural Netw. 61 2015 85 117 Schmidhuber, J. (2015). \u201cDeep learning in neural networks: An overview\u201d. Neural networks, 61, 85-117. [34] S.R. Mohandes X. Zhang A. Mahdiyar A comprehensive review on the application of artificial neural networks in building energy analysis Neurocomputing 340 2019 55 75 Mohandes, S. R., Zhang, X., & Mahdiyar, A. (2019). \u201cA comprehensive review on the application of artificial neural networks in building energy analysis\u201d. Neurocomputing, 340, 55-75. [35] M. Mohanraj S. Jayaraj C. Muraleedharan Applications of artificial neural networks for thermal analysis of heat exchangers\u2013a review Int. J. Therm. Sci. 90 2015 150 172 Mohanraj, M., Jayaraj, S., & Muraleedharan, C. (2015). \u201cApplications of artificial neural networks for thermal analysis of heat exchangers-a review\u201d. International Journal of Thermal Sciences, 90, 150-172. [36] S. Hochreiter The vanishing gradient problem during learning recurrent neural nets and problem solutions Int. J. Uncertain. Fuzziness Knowledge-Based Syst. 6 02 1998 107 116 Hochreiter, S. (1998). \u201cThe vanishing gradient problem during learning recurrent neural nets and problem solutions\u201d. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02), 107-116. [37] C. Ren S.J. Cao Development and application of linear ventilation and temperature models for indoor environmental prediction and HVAC systems control Sustainable Cities and Society 51 2019 101673 Ren, C., & Cao, S. J. (2019). \u201cDevelopment and application of linear ventilation and temperature models for indoor environmental prediction and HVAC systems control\u201d. Sustainable Cities and Society, 51, 101673. [38] J. Ren S.J. Cao Incorporating online monitoring data into fast prediction models towards the development of artificial intelligent ventilation systems Sustainable Cities and Society 47 2019 101498 Ren, J., & Cao, S. J. (2019). \u201cIncorporating online monitoring data into fast prediction models towards the development of artificial intelligent ventilation systems\u201d. Sustainable Cities and Society, 47, 101498. [39] S.J. Cao C. Ren Ventilation control strategy using low-dimensional linear ventilation models and artificial neural network Build. Environ. 144 2018 316 333 Cao, S. J., & Ren, C. (2018). \u201cVentilation control strategy using low-dimensional linear ventilation models and artificial neural network\u201d. Building and Environment, 144, 316-333. [40] Z. Zhang A. Chong Y. Pan C. Zhang K.P. Lam Whole building energy model for HVAC optimal control: a practical framework based on deep reinforcement learning Energy Build. 199 2019 472 490 Zhang, Z., Chong, A., Pan, Y., Zhang, C., & Lam, K. P. (2019). \u201cWhole building energy model for HVAC optimal control: A practical framework based on deep reinforcement learning\u201d. Energy and Buildings, 199, 472-490. [41] H. Li X. Li Benchmarking energy performance for cooling in large commercial buildings Energy Build. 176 2018 179 193 Li, H., & Li, X. (2018). \u201cBenchmarking energy performance for cooling in large commercial buildings\u201d. Energy and Buildings, 176, 179-193. [42] D. Westphalen S. Koszalinski Energy consumption characteristics of commercial building HVAC systems. Volume II: thermal Distribution, auxiliary equipment, and ventilation Arthur D. Little Inc (ADLI) 20 October 1999 33745-00 Westphalen, D., & Koszalinski, S. (1999). \u201cEnergy consumption characteristics of commercial building HVAC systems. Volume II: Thermal Distribution, auxiliary equipment, and ventilation\u201d. Arthur D. Little Inc (ADLI), 20(October), 33745-00. [43] S. Zhang Y. Wu T. Che Z. Lin R. Memisevic R.R. Salakhutdinov Y. Bengio Architectural complexity measures of recurrent neural networks Advances in Neural Information Processing Systems 2016 1822 1830 Zhang, S., Wu, Y., Che, T., Lin, Z., Memisevic, R., Salakhutdinov, R. R., & Bengio, Y. (2016). \u201cArchitectural complexity measures of recurrent neural networks\u201d. In Advances in neural information processing systems (pp. 1822-1830). [44] H. Sak A. Senior F. Beaufays Long short-term memory recurrent neural network architectures for large scale acoustic modeling Fifteenth Annual Conference of the International Speech Communication Association 2014 Sak, H., Senior, A., & Beaufays, F. (2014). \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling\u201d. In Fifteenth annual conference of the international speech communication association. [45] Marcel Schweiker Comf: an R package for thermal comfort studies The R Journal 2016 https://journal.r-project.org/archive/accepted/schweiker.pdf. 10.32614/RJ-2016-050 Schweiker, Marcel. (2016). \u201ccomf: An R Package for Thermal Comfort Studies.\u201d The R Journal. https://journal.r-project.org/archive/accepted/schweiker.pdf. 10.32614/RJ-2016-050. [46] I. ISO 7730: ergonomics of the thermal environment\u2014analytical determination and interpretation of thermal comfort using calculation of the PMV and PPD indices and local thermal comfort criteria Management 3 605 2005 e615 ISO, I. (2005). \u201c7730: Ergonomics of the thermal environment-Analytical determination and interpretation of thermal comfort using calculation of the PMV and PPD indices and local thermal comfort criteria\u201d. Management, 3(605), e615 [47] S.S. Devesa J. Donaldson T. Fears Graphical presentation of trends in rates Am. J. Epidemiol. 141 4 1995 300 304 Devesa, S. S., Donaldson, J., & Fears, T. (1995). \u201cGraphical presentation of trends in rates\u201d. American journal of epidemiology, 141(4), 300-304.",
    "scopus-id": "85074928449",
    "coredata": {
        "eid": "1-s2.0-S0360132319307474",
        "dc:description": "Optimal control of heating, ventilation and air conditioning systems (HVACs) aims to minimize the energy consumption of equipment while maintaining the thermal comfort of occupants. Traditional rule-based control methods are not optimized for HVAC systems with continuous sensor readings and actuator controls. Recent developments in deep reinforcement learning (DRL) enabled control of HVACs with continuous sensor inputs and actions, while eliminating the need of building complex thermodynamic models. DRL control includes an environment, which approximates real-world HVAC operations; and an agent, that aims to achieve optimal control over the HVAC. Existing DRL control frameworks use simulation tools (e.g., EnergyPlus) to build DRL training environments with HVAC systems information, but oversimplify building geometrics. This study proposes a framework aiming to achieve optimal control over Air Handling Units (AHUs) by implementing long-short-term-memory (LSTM) networks to approximate real-world HVAC operations to build DRL training environments. The framework also implements state-of-the-art DRL algorithms (e.g., deep deterministic policy gradient) for optimal control over the AHUs. Three AHUs, each with two-years of building automation system (BAS) data, were used as testbeds for evaluation. Our LSTM-based DRL training environments, built using the first year's BAS data, achieved an average mean square error of 0.0015 across 16 normalized AHU parameters. When deployed in the testing environments, which were built using the second year's BAS data of the same AHUs, the DRL agents achieved 27%\u201330% energy saving comparing to the actual energy consumption, while maintaining the predicted percentage of discomfort (PPD) at 10%.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2020-01-15",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S0360132319307474",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Zou, Zhengbo"
            },
            {
                "@_fa": "true",
                "$": "Yu, Xinran"
            },
            {
                "@_fa": "true",
                "$": "Ergan, Semiha"
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S0360132319307474"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S0360132319307474"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S0360-1323(19)30747-4",
        "prism:volume": "168",
        "articleNumber": "106535",
        "prism:publisher": "Elsevier Ltd.",
        "dc:title": "Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network",
        "prism:copyright": "© 2019 Elsevier Ltd. All rights reserved.",
        "openaccess": "0",
        "prism:issn": "03601323",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "HVAC control"
            },
            {
                "@_fa": "true",
                "$": "Energy consumption"
            },
            {
                "@_fa": "true",
                "$": "Thermal comfort"
            },
            {
                "@_fa": "true",
                "$": "Deep reinforcement learning"
            },
            {
                "@_fa": "true",
                "$": "Long-short-term-memory network"
            }
        ],
        "openaccessArticle": "false",
        "prism:publicationName": "Building and Environment",
        "openaccessSponsorType": null,
        "prism:pageRange": "106535",
        "pubType": "fla",
        "prism:coverDisplayDate": "15 January 2020",
        "prism:doi": "10.1016/j.buildenv.2019.106535",
        "prism:startingPage": "106535",
        "dc:identifier": "doi:10.1016/j.buildenv.2019.106535",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "478",
            "@width": "713",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr9.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "68091",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "796",
            "@width": "802",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr8.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "266972",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "336",
            "@width": "669",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "70031",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "711",
            "@width": "713",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "150417",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "231",
            "@width": "713",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "70864",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "181",
            "@width": "580",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "35449",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "310",
            "@width": "580",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "59617",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "209",
            "@width": "533",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-fx1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "37946",
            "@ref": "fx1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "500",
            "@width": "624",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "107305",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "400",
            "@width": "624",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr11.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "111830",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "137",
            "@width": "388",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "28754",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "172",
            "@width": "579",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr10.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "57339",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "389",
            "@width": "493",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-fx2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "61964",
            "@ref": "fx2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "147",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr9.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "12601",
            "@ref": "gr9",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "165",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr8.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "27613",
            "@ref": "gr8",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "110",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "14554",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "164",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "19192",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "71",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "16696",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "68",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "9243",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "117",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "13714",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "86",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-fx1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "9177",
            "@ref": "fx1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "205",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "20607",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "141",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr11.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "21717",
            "@ref": "gr11",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "77",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "8526",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "65",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr10.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "12850",
            "@ref": "gr10",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "208",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-fx2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "13972",
            "@ref": "fx2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "2117",
            "@width": "3158",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr9_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "445667",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "3523",
            "@width": "3551",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr8_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "2032187",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1485",
            "@width": "2961",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "459323",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "3147",
            "@width": "3158",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1041142",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1025",
            "@width": "3158",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "492623",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "800",
            "@width": "2567",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "140127",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1371",
            "@width": "2567",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "297257",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "924",
            "@width": "2362",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-fx1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "231158",
            "@ref": "fx1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2213",
            "@width": "2764",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "644830",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1773",
            "@width": "2763",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr11_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "666448",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "609",
            "@width": "1721",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "99640",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "764",
            "@width": "2566",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-gr10_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "293817",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1722",
            "@width": "2185",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-fx2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "378254",
            "@ref": "fx2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132319307474-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "3018195",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85074928449"
    }
}}