{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85143766318",
    "originalText": "serial JL 271089 291210 291731 291800 291881 31 Energy and Buildings ENERGYBUILDINGS 2022-10-27 2022-10-27 2022-11-02 2022-11-02 2022-12-16T22:57:09 1-s2.0-S0378778822007551 S0378-7788(22)00755-1 S0378778822007551 10.1016/j.enbuild.2022.112584 S300 S300.2 FULL-TEXT 1-s2.0-S0378778822X00212 2022-12-16T23:20:47.132433Z 0 0 20221215 2022 2022-10-27T01:18:25.652845Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath affil appendices articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref 0378-7788 03787788 true 277 277 C Volume 277 43 112584 112584 112584 20221215 15 December 2022 2022-12-15 2022 Research Articles article fla © 2022 Elsevier B.V. All rights reserved. DEEPREINFORCEMENTLEARNINGCONTROLFORNONSTATIONARYBUILDINGENERGYMANAGEMENT NAUG A 1 Introduction 2 Literature review 3 Problem formulation 3.1 Problem formulation 4 Solution approach 4.1 Deployment loop 4.1.1 Performance monitor module 4.2 Relearning loop 4.2.1 Data-driven modeling of the dynamic system 4.2.2 Experience buffer 4.2.3 Supervisory controller: deep reinforcement learning agent 4.2.4 Exogenous variable predictors 5 Evaluation of approach on 5 zone testbed 5.1 System description 5.2 Problem formulation 5.3 Implementation of the dual loop relearning controller 5.3.1 Performance monitor module 5.3.2 Dynamic system model 5.3.3 Experience buffer 5.3.4 Supervisory controller 5.3.5 Exogenous variable predictors 5.4 Experimental studies and results 5.4.1 Evaluation metrics 6 Evaluation of approach on real building 6.1 System description 6.2 Problem formulation 6.3 Implementation of the solution 6.4 Results 7 Discussion and conclusions Appendix A 5-zone testbed A.1 Performance monitor module A.2 Dynamic system model A.3 Experience buffer A.4 Supervisory controller A.5 Exogenous variable predictors Appendix B Real building B.1 Dynamic system model B.2 Supervisory controller References DENG 2022 X PERERA 2014 1 5 A 7THINTERNATIONALCONFERENCEINFORMATIONAUTOMATIONFORSUSTAINABILITY DESIGNINGSMARTHYBRIDRENEWABLEENERGYSYSTEMSV2G SAHA 2013 1 6 A JAIN 2018 1 21 A YUCE 2015 1351 1363 B SERALE 2018 631 G HAZYUK 2014 98 109 I DETTORRE 2019 524 535 F AZUATALAM 2020 100020 D LI 2019 2002 2013 Y LUZI 2019 28 36 M ZHANG 2019 472 490 Z CRAWLEY 2000 49 56 D STURZENEGGER 2015 1 12 D JAIN 2014 168 178 R WANG 2018 11 25 Z CHEN 2017 1368 1373 Y 201751STASILOMARCONFERENCESIGNALSSYSTEMSCOMPUTERS MODELINGOPTIMIZATIONCOMPLEXBUILDINGENERGYSYSTEMSDEEPNEURALNETWORKS YU 2020 2751 2762 L KATHIRGAMANATHAN 2021 110120 A BARRETT 2015 3 19 E JOINTEUROPEANCONFERENCEMACHINELEARNINGKNOWLEDGEDISCOVERYINDATABASES AUTONOMOUSHVACCONTROLAREINFORCEMENTLEARNINGAPPROACH AHMADIKARVIGH 2019 184 199 S DOUNIS 2009 1246 1261 A HOMOD 2020 115255 R DAWOOD 2022 809 831 S NAGABANDI 2018 7559 7566 A 2018IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATIONICRAIEEE NEURALNETWORKDYNAMICSFORMODELBASEDDEEPREINFORCEMENTLEARNINGMODELFREEFINETUNING NAGABANDI 2020 1101 1112 A CONFERENCEROBOTLEARNING DEEPDYNAMICSMODELSFORLEARNINGDEXTEROUSMANIPULATION WETTER 2014 253 270 M WETTER 2004 989 999 M LEE 2015 2106 2111 Y LEDREAU 2016 991 1002 J AFRAM 2017 96 113 A REYNOLDS 2018 729 739 J RUANO 2016 145 158 A DRGONA 2018 199 216 J HILLIARD 2017 326 338 T KONTES 2018 3376 G ZHOU 2019 Z ENSEMBLEMETHODSFOUNDATIONSALGORITHMS COSTANZO 2016 81 90 G MOCANU 2018 3698 3708 E YU 2021 12046 12063 L YU 2020 407 419 L RING 1994 M CONTINUALLEARNINGINREINFORCEMENTENVIRONMENTS BRYHN 2011 e19241 A KOUZOUPIS 2018 863 882 D ROSOLIA 2017 1883 1896 U KIRKPATRICK 2017 3521 3526 J WILLIAMS 1992 229 256 R GRONDMAN 2012 1291 1307 I KAREVAN 2020 1 9 Z NAUG 2019 249 257 A PROCEEDINGS2019IEEEINTERNATIONALCONFERENCESMARTCOMPUTINGSMARTCOMP2019 ONLINEENERGYMANAGEMENTINCOMMERCIALBUILDINGSUSINGDEEPREINFORCEMENTLEARNING NAUGX2022X112584 NAUGX2022X112584XA 2024-11-02T00:00:00.000Z 2024-11-02T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2022 Elsevier B.V. All rights reserved. 0 https://doi.org/10.15223/policy-017 https://doi.org/10.15223/policy-037 https://doi.org/10.15223/policy-012 https://doi.org/10.15223/policy-029 https://doi.org/10.15223/policy-004 item S0378-7788(22)00755-1 S0378778822007551 1-s2.0-S0378778822007551 10.1016/j.enbuild.2022.112584 271089 2022-12-16T23:20:47.132433Z 2022-12-15 1-s2.0-S0378778822007551-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/MAIN/application/pdf/abf9beab0ec548e89c5b55958c250399/main.pdf main.pdf pdf true 3151380 MAIN 20 1-s2.0-S0378778822007551-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/PREVIEW/image/png/9bdaef6f9366ab027027957e2c898464/main_1.png main_1.png png 57936 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0378778822007551-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr2/DOWNSAMPLED/image/jpeg/99b1f26140316a3b6b83d32b3cb4f809/gr2.jpg gr2 gr2.jpg jpg 35591 267 539 IMAGE-DOWNSAMPLED 1-s2.0-S0378778822007551-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr3/DOWNSAMPLED/image/jpeg/af2e2e1e4c2f276a6d81b0764e3e0ea4/gr3.jpg gr3 gr3.jpg jpg 63469 373 539 IMAGE-DOWNSAMPLED 1-s2.0-S0378778822007551-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr4/DOWNSAMPLED/image/jpeg/00e8935425bc9d16adbb9d864e53c31b/gr4.jpg gr4 gr4.jpg jpg 44284 354 539 IMAGE-DOWNSAMPLED 1-s2.0-S0378778822007551-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr5/THUMBNAIL/image/jpeg/e8a43f6125d4ae5d1ded5f7f2c2dacfa/gr5.jpg gr5 gr5.jpg jpg 91372 505 538 IMAGE-DOWNSAMPLED 1-s2.0-S0378778822007551-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr6/HIGHRES/image/jpeg/3a3e10686a0352af34de90a972f6069a/gr6.jpg gr6 gr6.jpg jpg 51945 340 539 IMAGE-DOWNSAMPLED 1-s2.0-S0378778822007551-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr7/DOWNSAMPLED/image/jpeg/80dbf7513f63edd7e2e1802bd9e0a081/gr7.jpg gr7 gr7.jpg jpg 46437 243 539 IMAGE-DOWNSAMPLED 1-s2.0-S0378778822007551-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr8/DOWNSAMPLED/image/jpeg/e96e41af92bad72a3969cee6872cf98b/gr8.jpg gr8 gr8.jpg jpg 31198 205 539 IMAGE-DOWNSAMPLED 1-s2.0-S0378778822007551-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr9/DOWNSAMPLED/image/jpeg/2684c158ef92d7f031a15fdc9d744c82/gr9.jpg gr9 gr9.jpg jpg 38594 225 539 IMAGE-DOWNSAMPLED 1-s2.0-S0378778822007551-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr1/THUMBNAIL/image/jpeg/79ebfb05e1e7143cb99ba9d50c128acb/gr1.jpg gr1 gr1.jpg jpg 103384 482 778 IMAGE-DOWNSAMPLED 1-s2.0-S0378778822007551-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr2/THUMBNAIL/image/gif/546625ff155e06daf448ba8ca7f7187f/gr2.sml gr2 gr2.sml sml 6622 109 219 IMAGE-THUMBNAIL 1-s2.0-S0378778822007551-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr3/THUMBNAIL/image/gif/8c494870cba91c55162f9e19c94c90fb/gr3.sml gr3 gr3.sml sml 9426 152 219 IMAGE-THUMBNAIL 1-s2.0-S0378778822007551-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr4/THUMBNAIL/image/gif/14dc5a4471826038bb22792fd85ec88e/gr4.sml gr4 gr4.sml sml 7682 144 219 IMAGE-THUMBNAIL 1-s2.0-S0378778822007551-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr5/THUMBNAIL/image/gif/b5e0a43282710f26daaabace12aa5fe5/gr5.sml gr5 gr5.sml sml 11769 164 175 IMAGE-THUMBNAIL 1-s2.0-S0378778822007551-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr6/THUMBNAIL/image/gif/b81ca4a460c0f27774acf9fbabceb21d/gr6.sml gr6 gr6.sml sml 9890 138 219 IMAGE-THUMBNAIL 1-s2.0-S0378778822007551-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr7/THUMBNAIL/image/gif/44fa37366d6448c15c21baa9d6886b36/gr7.sml gr7 gr7.sml sml 9800 99 219 IMAGE-THUMBNAIL 1-s2.0-S0378778822007551-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr8/THUMBNAIL/image/gif/1c0346c7f0a7e6b62f21dfd537e7c323/gr8.sml gr8 gr8.sml sml 7064 83 219 IMAGE-THUMBNAIL 1-s2.0-S0378778822007551-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr9/THUMBNAIL/image/gif/b6a1af27b8506cc0cb9121ceb7b9ef2e/gr9.sml gr9 gr9.sml sml 9074 91 219 IMAGE-THUMBNAIL 1-s2.0-S0378778822007551-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/gr1/THUMBNAIL/image/gif/e08ba1cfbb4793d905bcf5aa2d786576/gr1.sml gr1 gr1.sml sml 10540 136 219 IMAGE-THUMBNAIL 1-s2.0-S0378778822007551-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/HIGHRES/image/jpeg/061becef203f178d65b22c09be4212e2/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 292724 1183 2387 IMAGE-HIGH-RES 1-s2.0-S0378778822007551-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/HIGHRES/image/jpeg/23610a407611c5f283b7257aef793209/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 444192 1652 2387 IMAGE-HIGH-RES 1-s2.0-S0378778822007551-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/HIGHRES/image/jpeg/29338e7443f2661ed1090b21cda1b063/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 377239 1567 2387 IMAGE-HIGH-RES 1-s2.0-S0378778822007551-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/HIGHRES/image/jpeg/18fe0a4e20f568681cf84b83a6ff76fb/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 777128 2234 2382 IMAGE-HIGH-RES 1-s2.0-S0378778822007551-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/HIGHRES/image/jpeg/1e48f9744a1e68685ba9c5fed1ac1dd1/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 385656 1504 2387 IMAGE-HIGH-RES 1-s2.0-S0378778822007551-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/HIGHRES/image/jpeg/26b33bbbaeaf7ef059138fd2a2d3627a/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 427822 1076 2387 IMAGE-HIGH-RES 1-s2.0-S0378778822007551-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/HIGHRES/image/jpeg/145502185a1dba71d6da3e3e369a429b/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 291901 909 2387 IMAGE-HIGH-RES 1-s2.0-S0378778822007551-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/HIGHRES/image/jpeg/898e67e7af9770fb56150d08491ce682/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 340773 996 2387 IMAGE-HIGH-RES 1-s2.0-S0378778822007551-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778822007551/THUMBNAIL/image/jpeg/a1c0f4e769e652be6b5433b8e945bbd0/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 900907 2134 3445 IMAGE-HIGH-RES 1-s2.0-S0378778822007551-am.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:10QW4RV78K9/MAIN/application/pdf/0851d2fc94eb0074df569e9d701ac6c9/am.pdf am am.pdf pdf false 1746345 AAM-PDF ENB 112584 112584 S0378-7788(22)00755-1 10.1016/j.enbuild.2022.112584 Elsevier B.V. Fig. 1 Performance Comparison in terms of Energy Consumption, Temperature Comfort, VAV Actuation across state-of-the-art control strategies like the Industrial Standard Rule Based Control based on ASHRAE Guideline 36, Data-Driven Model based MPC with weekly updates and online Deep Reinforcement Learning algorithm PPO demonstrated on a testbed simulating a 5 zone building setup. Fig. 2 Overview of the solution using a deployment and a relearning loop schema. Fig. 3 Dual Loop Relearning Controller for building energy control. Fig. 4 Time to adapt under each DRL based approach across four different types of non-stationary behavior. Fig. 5 Demonstration of the proposed approach for non-stationary changes in the weather and occupancy (indicated by the mean thermal load-MTL) of some zones oriented along North, South, East, West and one located in the middle. a) Ambient temperature and thermal load behavior, b) AHU discharge set-points issued by the Relearning Approach compared to PPO and MPPI, and c) immediate reward obtained for each method. Fig. 6 Simplified schematic of the HVAC system under Study. Fig. 7 Similarity of weather during performance comparison of Relearning Approach and Rule Based Controller deployed on the real building. Fig. 8 Comparison of hourly zone temperature deviation between relearning control and rule based Control. Fig. 9 Comparison of hourly VRF fan On/Off switching between relearning control and rule based Control. Table 1 Description of the testbed Dynamic System Model in terms of an MDP. Component Variables State 1.Outside Air Temperature(oat) s ¯ t 2.Outside Air Relative Humidity(orh) 3.Five Zone Temperatures( T z , z = 1 \u2026 5 ) 4.Total Energy Consumption( E tot ) 5.Air Handling Unit Discharge Air Temperature ( T disch ) Action: Air Handling Unit Discharge Temperature Set- a ¯ t point( T disch _ stpt ) Reward r energy + r cmft + r vav _ actuation where r ( s t , a t ) r energy = - E tot r cmft = - ∑ z ∈ zones max ( ( T z - ub z , t ) , 0 , ( lb z , t - T z ) ) r vav _ actuation = - ∑ z ∈ zones | v % , z , t - v % , z , t - 1 | Transition 1. Total Energy Consumption Model( M energy ) Model( p ( s t , a t , s \u2032 ) ) 2. Zone Temperature Model( M T , zone ) 3. VAV Damper Percentage Model( M vav % ) Table 2 Performance of Rule-Based, PPO, DDPG, MPPI, Relearning Approach deployed on the testbed and simulated for a period of 1 year. Metrics are recorded for a week after detection of non-stationarity using the Performance Monitor Module. Energy performance is aggregated for a week. Temperature deviation and Actuation rates are aggregated on a per-hour basis. Benchmarking is done under four conditions of non-stationarity. We report the mean and standard deviation across 20 experiments for each category. Metric Cause of Non-stationarity Guideline-36 PPO DDPG MPPI Relearning Approach E tot (kwh) Weather 1225.55 1079.33 1154.58 1096.55 1114.82 (32.24) (72.9) (17.62) (32.2) (77.72) Zone-Setpoint 1339.68 1346.78 1374.78 1284.57 1277.22 (6.08) (12.22) (10.53) (7.51) (5.33) Thermal Load 1811.44 1683.80 1808.51 1405.37 1366.09 (62.18) (35.82) (29.92) (42.28) (32.15) Combined 1735.66 1708.09 1769.20 1688.73 1600.58 (22.60) (32.82) (45.63) (26.22) (16.19) T D ( oF ) Weather 1.63 1.21 0.73 0.61 0.53 (0.08) (0.15) (0.28) (0.22) (0.20) Zone-Setpoint 2.85 3.88 3.79 3.25 2.29 (0.08) (0.13) (0.28) (0.35) (0.22) Thermal Load 6.85 7.29 6.92 4.29 3.38 (0.31) (1.22) (1.38) (0.11) (0.28) Combined 5.83 6.28 7.01 4.86 3.26 (0.52) (1.06) (1.59) (0.46) (0.33) A s ( % ) Weather 2.83 3.6 3.15 1.16 1.28 (0.05) (0.22) (0.28) (0.36) (0.15) Zone-Setpoint 6.37 5.43 5.89 4.02 3.75 (0.27) (0.38) (0.73) (0.28) (0.21) Thermal Load 1.05 1.36 1.92 0.96 0.68 (0.03) (0.06) (0.08) (0.08) (0.04) Combined 1.68 2.35 2.09 1.11 0.83 (0.04) (0.06) (0.09) (0.03) (0.02) Table 3 HVAC supervisory control problem for multi-zone building. Component Variables State 1.Outside Air Temperature ( oat ) s ¯ t 2.Outside Air Relative Humidity ( orh ), 3.Total Energy Consumption( E tot ), 4.AHU Discharge Air Temperature ( T disch ) Action AHU Preheat/Reheat Discharge a ¯ t Temperature Set-point( T disch _ stpt ) Reward r energy + r cmft where r ( s t , a t ) r energy = - E tot r cmft = - ∑ z ∈ zones | T disch _ stpt - T z , stpt | Transition Model 1. Total Heating Energy Consumption ( M heat ) p ( s \u2032 , s t , a t ) 2. Total Cooling Energy Consumption ( M cool ) 3. Steam/Heating Valve State Model ( M vlv ) Table 4 Comparison of Energy Consumption in kBTUs/Hr for our proposed Relearning Controller vs Rule-based Controller on real building. Period of deployment under consideration: 20th Feb 2020 to 20th Feb 2021 for Rule-Based Controller and 21st Feb 2021 to 20th Feb 2022 for Relearning Controller. HVAC Mode Metric RBC (kBTU) Relearning PPO (kBTU) dehumidi-fication hours 4219 4355 E cool / hr 81.26 73.80 E ee / hr 51.20 42.83 E stm / hr 0.25 0.28 non-dehum-idification hours 4539 4403 E cool / hr 31.08 23.86 E ee / hr 51.80 45.42 E stm / hr 0.61 0.53 Table A.5 Tuned values W pm 1 and W pm 2 associated with the performance monitor module under different conditions of non-stationarity. Non-stationarity Weather Zone Set Point Thermal Load Combined Hyperpara- meter W pm 1 W pm 2 W pm 1 W pm 2 W pm 1 W pm 2 W pm 1 W pm 2 Value (hrs) 6 2 3 2 3 2 6 3 Table A.6 Tuned network architecture of the Data-driven models under four possible conditions of non-stationarity. M energy predicts heating and cooling energy, M T predicts zone temperature across all zones. Each of M vav % , z predicts valve percentage in zone z . Non-stationarity Weather Zone Set Point Thermal Load Combined Models M energy M T M vav % , z M energy M T M vav % , z M energy M T M vav % , z M energy M T M vav % , z Network Inputs oat t - 1 , orh t - 1 , E cooling , t - 1 , E heating , t - 1 , T disch , t - 1 oat t - 1 , orh t - 1 , T disch , t - 1 , T z , t - 1 , T z , spt , t - 1 oat t - 1 , orh t - 1 , T z , spt , t - 1 , T z , t - 1 , vav z , % , t - 1 oat t - 1 , orh t - 1 , E cooling , t - 1 , E heating , t - 1 , T disch , t - 1 oat t - 1 , orh t - 1 , T disch , t - 1 , T z , t - 1 , T z , spt , t - 1 oat t - 1 , orh t - 1 , T z , spt , t - 1 , T z , t - 1 , vav z , % , t - 1 oat t - 1 , orh t - 1 , E cooling , t - 1 , E heating , t - 1 , T disch , t - 1 oat t - 1 , orh t - 1 , T disch , t - 1 , T z , t - 1 , T z , spt , t - 1 oat t - 1 , orh t - 1 , T z , spt , t - 1 , T z , t - 1 , vav z , % , t - 1 oat t - 1 , orh t - 1 , E cooling , t - 1 , E heating , t - 1 , T disch , t - 1 oat t - 1 , orh t - 1 , T disch , t - 1 , T z , t - 1 , T z , spt , t - 1 oat t - 1 , orh t - 1 , T z , spt , t - 1 , T z , t - 1 , vav z , % , t - 1 Network Architecture lstm(32) → lstm(32) → lstm(32) → lstm(64) → lstm(64) → lstm(32) → lstm(64) → lstm(32) → lstm(32) → lstm(64) → lstm(32) → lstm(64) → lstm(32) → lstm(32) → lstm(64) → lstm(64) → lstm(64) → lstm(32) → lstm(64) → lstm(64) → lstm(32) → lstm(64) → lstm(64) → lstm(32) → fnn(16) → fnn(16) → lstm(64) → fnn(32) → fnn(32) → fnn(64) → fnn(64) → fnn(64) → fnn(32) → fnn(128) → fnn(64) → fnn(32) → fnn(16) → fnn(16) → fnn(64) → fnn(32) → fnn(8) → fnn(64) → fnn(32) → fnn(8) → fnn(32) → fnn(64) → fnn(8) → fnn(32) → fnn(2) fnn(5) fnn(1) fnn(2) fnn(5) fnn(1) fnn(2) fnn(5) fnn(1) fnn(2) fnn(5) fnn(1) Network Outputs E cooling , t T z , t VAV % z , t E cooling , t T z , t VAV % z , t E cooling , t T z , t VAV % z , t E cooling , t T z , t VAV % z , t E heating , t E heating , t E heating , t E heating , t Table A.7 Error in the prediction models for the Dynamic System Models. Non-stationarity Weather Zone Set Point Thermal Load Combined Model Type M energy M T M vav % , z M energy M T M vav % , z M energy M T (MAPE) M vav % , z M energy M T M vav % , z (CVRMSE) (MAPE) (MAPE) (CVRMSE) (MAPE) (MAPE) (CVRMSE) (MAPE) (MAPE) (CVRMSE) (MAPE) (MAPE) Error 23.18% 14.29% 16.09% 19.06% 15.80% 13.88% 18.59% 17.92% 14.80% 24.86% 21.36% 18.91% (3.67%) (2.38%) (3.08%) (2.18%) (1.66%) (1.86%) (3.91%) (4.08%) (2.47%) (3.87%) (4.58%) (4.71%) Table A.9 Tuned size of the Experience Buffer in hours under different non-stationarities. Non-stationarity Weather Zone Set Point Thermal Load Combined M e (hrs) 36 18 18 24 Table A.8 Tuned network architecture of the Actor-Critic Network under four possible conditions of non-stationarity. Non-stationarity Weather Zone Set Point Thermal Load Combined Agent Component Actor Network Critic Network Actor Network Critic Network Actor Network Critic Network Actor Network Critic Network Network Architecture fnn(256) → fnn(64) → fnn(64) → fnn(128) → fnn(64) → fnn(128) → fnn(256) → fnn(128) → fnn(256) → fnn(64) → fnn(256) → fnn(128) → fnn(64) → fnn(64) → fnn(256) → fnn(64) → fnn(256) → fnn(64) → fnn(128) → fnn(16) → fnn(64) → fnn(32) → fnn(64) → fnn(64) → fnn(2) fnn(1) fnn(2) fnn(1) fnn(2) fnn(1) fnn(2) fnn(1) Table A.10 Tuned Values of Episode Length and Discount factor under four possible conditions of non-stationarity. Non-stationarity Weather Zone Set Point Thermal Load Combined Episode Length in days ( l ) 2 1 1 2 Discount Factor ( γ ) 0.85 0.92 0.95 0.95 Table A.11 Model Architecture of the Exogenous Variable Predictor for Outside Air Temperature and Outside Air Relative Humidity. Model Name Predictor of Outside Air Temperature Predictor of Outside Air Relative Humidity Network Inputs oat t - K : t - 1 orh t - K : t - 1 Network Architecture lstm(K,128) → lstm(K,128) → lstm(K,128) → lstm(K,128) → lstm(1,128) → lstm(1,128) → fnn(1,1) fnn(1,1) Network Outputs oat t orh t Table A.12 Error in the Exogenous Variable Predictor Models for the Dynamic System Models. Model Type OAT Predictor ORH Predictor MAPE 5.47% 11.69% (1.36%) (3.11%) Table A.13 Tuned values of the input sequence length K and the prediction horizon N under different conditions of non-stationarity. Non-stationarity Weather Zone Set Point Thermal Load Combined Hyperpara- meter K N K N K N K N Value (hrs) 18 72 6 72 6 72 12 48 Table A.14 Tuned network architecture of the Data-driven models. M heat predicts heating energy and M cool predicts the cooling energy, respectively. M vlv predicts valve status. Models M heat M vlv M cool Network Inputs oat t - 1 , orh t - 1 , oat t - 1 , orh t - 1 oat t - 1 , orh t - 1 , E heating , t - 1 , T disch , t - 1 , E cooling , t - 1 , T disch , t - 1 , T z , t - 1 T disch , t - 1 T z , spt , t - 1 Network Arch lstm(4) → lstm(8) → lstm(8) → lstm(4) → lstm(8) → lstm(8) → fnn(16) → fnn(16) → fnn(16) → fnn(16) → fnn(16) → fnn(16) → fnn(16) → fnn(16) → fnn(16) → fnn(16) → fnn(16) → fnn(16) → fnn(16) → fnn(16) → fnn(16) → fnn(1) fnn(2) fnn(1) Network Outputs E heating , t logits ( vlv z , % , t ) E cooling , t Table A.15 Performance of the data-driven models for the Dynamic System Models. Model Type M heat (CVRMSE) M vlv (Accuracy) M cool (CVRMSE) Performance Metric 23.12% 88.62% 21.20% (5.03%) (7.08%) (6.11%) Table A.16 Tuned network architecture of the Actor-Critic Network. Hyper params Actor Network Policy Network Network Arch fnn(128) → fnn(64) → fnn(128) → fnn(128) → fnn(64) → fnn(64) → fnn(2) fnn(1) Deep reinforcement learning control for non-stationary building energy management Avisek Naug a Marcos Quinones-Grueiro a \u204e Gautam Biswas a a Department of Computer Science and Institute for Software Integrated Systems, Vanderbilt University, United States Department of Computer Science and Institute for Software Integrated Systems Vanderbilt University United States Department of Computer Science and Institute for Software Integrated Systems, Vanderbilt University \u204e Corresponding author. Developing an optimal supervisory control policy for building energy management is a complex problem because the system exhibits non-stationary behaviors, and the target policy needs to evolve with changes in the state transition and reward functions. Non-stationary real-world problems often present a set of challenges: non-stationary changes are difficult to detect; and thermodynamic systems with larger time-constants can create sample-inefficiency problems for learning algorithms. In addition, the system may have to satisfy safety\u2013critical constraints, and, therefore, the policy must be learned offline unless actuation rules are correctly designed. To address these challenges, we propose a data-driven deep reinforcement learning framework. A reinforcement learning supervisory controller is firstly developed and deployed on the building for the heating, ventilation and air conditioning (HVAC) system and monitored for performance degradation by tracking an aggregate metric. When degradation is detected, a relearning loop is triggered. Then, a set of data-driven models of the building behavior is updated with the latest real data. Subsequently, the deployed controller is re-tuned by letting it interact with the model and is then redeployed on the system. Our proposed approach is demonstrated extensively on the standard ASHRAE 5-zone testbed and a real building. It is benchmarked against state-of-the-art algorithms in building supervisory control: Guideline-36, Proximal Policy Optimization, Deep Deterministic Policy Gradient, and Model Predictive Path Integral control. Our approach performs significantly better than the previously mentioned supervisory control strategies and highlights the need for a condition-based offline relearning framework in dynamic systems. Keywords Smart building management Deep reinforcement learning Non-stationary systems Continuous adaptation 1 Introduction Over the past few decades, increased energy use is causing major stresses on energy generation and distribution, while simultaneously contributing to environmental deterioration and climate change [1]. In the US, of the 92.94 Quadrillion BTUs of the energy consumed in 2020, roughly 29 % or 27 Qd BTUs was used for operating commercial buildings. 40 % of the building energy consumption is attributed to Heating Ventilation and Air Conditioning(HVAC) systems [2], which amounts to billions of dollars in operating costs. Hence, it is of primary interest to make HVAC operations in large buildings energy-efficient. Simultaneously, occupant comfort and actuation safety of the HVAC components during operation are important for sustainable HVAC performance. Designing energy efficient supervisory controllers for large buildings is a challenging task [3]. The dynamics of building energy use are governed by the complexity of the HVAC components of the building, the energy flows between different zones, thermal load dependent on occupancy levels, and the energy interactions between the building and its environment. Traditional building HVAC supervisory controllers for performance optimization adopt a sequential decision-making approach that are implemented as < condition , action > rules [4,5]. Recently, more advanced rule-based controllers have been developed, including the ASHRAE Guideline 36-2018 and Guideline36 [6,7]. However, pre-defined rule-based controllers fail to adapt to unforeseen changes in the building and its environment [8,9]. More recently, researchers have developed advanced control algorithms using Model Predictive Control (MPC) [10\u201312] and Deep Reinforcement Learning (DRL) [13\u201317] to improve energy optimization in buildings. Their effectiveness has been demonstrated primarily on physics-based Energy Plus [18] simulators, and abstracted energy transfer models for buildings using RC\u2013circuit analogs [19]. Creating accurate Energy Plus models for large commercial multi-zone buildings is time-consuming and computationally expensive, and the cost of building accurate models often outweighs the benefits accrued from the designed energy optimization control [20]. The increased ability to collect and analyze data from buildings, has led to research on developing data-driven models for building energy control [21\u201324]. These approaches learn the relevant building energy and temperature dynamics from data and subsequently train a controller using the ensemble of data-driven models. The data-driven modeling and control approach is being extensively applied to energy management for large buildings [25]. However, data-driven modeling approaches suffer from generalization problems [26,3], which may be caused by non-stationary changes [27,3] in the building energy behavior. Not accounting for non-stationary behavior may result in overfitting of the energy models as well as out-of-distribution prediction error. This is especially problematic in the case of supervised learning methods for control which can only generalize to previously experienced conditions. Current methods for supervisory control of buildings based on deep reinforcement learning [28,14,16] and Model Predictive Control [29,30] do not account for non-stationary changes in building behavior. As a result, learned models and controller performance degrade over time. Recent works have also focused on this problem from a direct control perspective, e.g. designing advanced control methods for directly actuating the components like valves instead of considering the classic PID control methods[31,32]. In these works, the authors have demonstrated promising results in terms of balancing energy consumption and comfort while guaranteeing robustness whenever it is possible to implement controllers for direct component control. However, in our work, we focus on the supervisory control problem which can be especially useful if regulations in place do not allow to replace certified PID controllers with more advanced control techniques. Fig. 1 compares the weekly energy consumption, comfort, and Variable Air Volume (VAV) system actuation rates in a building testbed described in Section 5 across two different conditions: (1) absence and (2) presence of non-stationary behaviors due to exogenous variables like weather, zone setpoints and occupancy based thermal load. Our experiment compared building energy performance under identical conditions for (1) an advanced adaptive rule based control (Guideline-36 [7]); (2) Data-driven Model Predictive Path Integral (MPPI) Control, which was updated weekly [33,34]; and (3) Online Proximal Policy Optimization (PPO) [35]. We observed that non-stationary changes increased energy consumption ( 24 % ) and VAV actuation rates ( 52 % ) while decreasing occupant comfort ( 59 % ). MPPI control performed better because it is based on periodic updates of the data-driven models. However, MPC methods are susceptible to modeling accuracy and convergence issues can result in degraded performance [36,37]. Nevertheless, a takeaway from these results is that non-stationarity in building behaviors may require making updates to the building energy model and the controllers for efficient energy management. In this paper, we propose to address the complex problem of building energy supervisory control by using a two-step approach. Our first step, develops systematic deep learning approaches for learning a set of data-driven models to support building energy management. These include the building energy consumption and the environment prediction models. The second step uses a deep RL method for learning an \u201coptimal\u201d controller to manage building energy consumption while ensuring comfort and actuation safety. This two-step approach is enclosed in a novel performance-based relearning loop that conditionally performs efficient model relearning and controller relearning when unexpected changes related to exogenous (i.e., weather related) and internal (e.g., occupancy change) variables induce non-stationary changes in building behavior. We adopt methods, such as multi-environment training, future forecasts of exogenous variables, elastic weighted regularization (EWC) to improve sample efficiency and address problems like modeling errors, overfitting with limited data, and time constraints to update data-driven models and controller. We benchmark our performance against state of the art in Rule Based Control[7], Model Predictive Path Integral Control[33] and state-of-the-art Policy Gradient Based reinforcement learning algorithms Proximal Policy Optimization(PPO) [35] and Deep Deterministic Policy Gradient(DDPG)[38] using a standard five-zone testbed. In addition, we have deployed our relearning control method for supervisory control of a central Air Handling Unit (AHU) in a real office building on our university campus, where we optimize energy consumption, temperature comfort and minimize variable refrigerant flow (VRF) fan-switching. The rest of the paper is organized as follows. Section 2 reviews the relevant literature in buildings energy control and the limitations with respect to deploying those approaches on real buildings due to non-stationarity. Section 3 outlines the problem statement, assumptions made in deriving our solution approach, and an overview of our solution in terms of an online deployment phase and an offline relearning phase. Section 3 provides our solution architecture and a detailed discussion of the individual components of our solution approach. We evaluate the performance of our approach on a standard ASHRAE 5 zone testbed in Section 5, and on a real building in Section 6. Finally, in Section 7, we discuss the contributions of our approach for building performance optimization as well as their implications by using the testbed experiments and the results of the application to a real building. In addition, we point out limitations in our current approach, and suggest future work to address them. 2 Literature review In this section, we review existing research in buildings control based on physics-driven and data-driven modeling approaches. We focus our review on state-of-the-art approaches that have been developed, i.e., Ruled-based Strategies, Model Predictive Control (MPC) and Deep Reinforcement Learning (DRL) control in buildings. Traditional sequence-based-control, based on expert developed rules, use easily implementable structures and have low initial costs for deployment. They are characterized by quick online response times, but their control efficiency drops significantly for complex systems with nonlinear behaviors and time delays. Also, they are not capable of handling changing conditions and uncertainties in the measurements and environment [39]. Adaptive Advanced Control Strategies, MPC- and DRL-based approaches are more effective in dealing with nonlinearities in system behavior and uncertainties in the measurements and environmental parameters. These approaches use a model of the building energy consumption to design or learn the optimal control strategies. As discussed, typical modeling methodologies can be model-based or data-driven. For small buildings, MPC approaches in conjunction with physics-based models have demonstrated significant energy savings. The physics-based models typically consider the thermal capacity of these buildings as passive stores of thermal energy [12,15,29]. The designed controllers typically heat or cool the buildings during off-peak hours and try to use that energy to maintain building temperature and comfort for the duration of the building operations [30,40]. Similarly, DRL-based schemes have been used in conjunction with physics-based models like Energy Plus [18]. Online model-free learning approaches have been proposed using Deep-Q Networks in [16] for optimizing energy and airflow for HVACs. Deep Deterministic Policy Gradient Approaches were used for cooling based control in data-centers in [28], and an actor-critic RL approach was used for building energy optimization in [17]. A fully offline version of DRL called Batch RL was used in [41] for residential energy management. Energy savings in the range of 10 % to 30 % were obtained using these approaches. However, these results apply to simpler building testbeds, and the energy models employed in these testbeds do not generalize well for larger systems with non-stationarity behaviors. As discussed earlier, constructing sufficiently accurate and comprehensive building models that apply across multiple building and environmental states require large amounts of development time and effort, making them infeasible from a cost-benefit viewpoint. As a result, as sensing and data collection technologies have evolved, researchers and practitioners have transitioned to focusing on data-driven models for control. Data-driven MPC implementations in buildings have focused on neural network approaches [42\u201346], Regression Trees [47,48] and Gaussian Processes [49]. These approaches have been updated to train the data-driven models periodically, for example, every 24 to 48 h, to ensure the control actions selected best match the building energy states. However, these applications have been restricted to simpler and uniform building structures with small state-spaces and well calibrated data-driven models. It is difficult to scale up these approaches to larger buildings with more complex energy behaviors, therefore, model accuracy can degrade with increasing computation horizon. According to [36,37] this leads to degradation in control performance, and affects the convergence properties of control optimization. DRL have been applied to building performance optimization using Gaussian Process Models [50], Neural Networks and ELMS[51] in [52]. The limited applications of data-driven models in DRL control can be attributed to problems with generalization because of the lack of diversity in the real building operational data (data that account for multiple operational conditions of a building may take years to collect). Therefore, the resulting data-driven models or experiences used to learn the control policies face the curse of dimensionality and sample inefficiency problems. As a result, the derived control policies for energy management may be incomplete and inaccurate due to overfitting problems [53]. Furthermore, recent challenges in DRL for single building as well as multi-building architectures have been extensively captured in [54,55] for multi-agent settings. It was observed that simultaneously balancing efficiency for control, accuracy for prediction, uncertainty in parameters, temporal and spatial constraints, and high dimensionality of certain energy optimization problems made the solution approach difficult. Based on our literature review, we observed that most of the existing studies on data-driven DRL have assumed that building behaviors are stationary over long future horizons. As a result, controllers optimized on recent information should perform optimally eternally. However, to be successfully deployed on real buildings for production purposes, any unforeseen changes in building behavior have to be adapted. Hence, our work will focus on attending to the real-life problem of adapting to changes not experienced before. However, we will assume a limit on the rate of the changes enforced by Lipschitz continuity that we discuss later in Section 3. In summary, current research and deployment of advanced controllers in large buildings face several challenges. Learning efficient control policies using exploratory data-driven strategies can be difficult due to lack of diversity in the data. Especially, when the systems involve a large amount of thermal inertia and high dimensional state-space, the optimization problem itself can become close to intractable. In this regard, some recent work in [56] demonstrates the use of a deep clustering approach to simplify a multiagent RL problem, obtaining good results compared to current state-of-the-art PID controllers. However, the problem addressed in this work differs from this proposal because we consider supervisory control instead of multi-agent direct control of the building HVAC components. Developing supervisory controllers that can simultaneously optimize energy reduction, comfort, and actuation to minimize wear and tear of HVAC equipment across multiple operating conditions of buildings is a difficult problem to solve. For DRL-based exploratory control to be effective, it has to learn online to adjust to conditions as the building and environmental operating conditions change over time. However, the uncertainties introduced by online learning are often a matter of concern for building managers because they do not have sufficient guarantees that controller actions will not adversely affect building operations [25]. On the other hand, in real world applications, a static DRL controller cannot adjust to the non-stationarities in building operations, especially if those situations have not been encountered in the past. For HVAC system components with fast moving non-stationary behavior (e.g. Fans, Valves e.t.c.), the adaptive control needs to respond quickly. This has been studied using fuzzy controllers in [31] which integrates cluster adaptive training with PI-PD Mamdani-type controllers. Our work in this regard tries to achieve similar effects under a certain class of faster non-stationarities induced by events in a building. Researchers have suggested using hybrid models to accommodate known contexts introduced by building and environmental non-stationarities. In reality, transitions in building energy use caused by non-stationarity changes, are continuous and slow. The effects of non-stationarities imply that learning-based controller design and deployment for buildings should adopt the concept of continual learning [57]. Typically, non-stationary changes may not be identifiable from past contexts, and, therefore, may have to identified as they occur. Hence, methods need to be developed for detecting non-stationary changes in building behaviors, and subsequent model and control policies may need to be relearned to ensure optimal performance under the changed behaviors. In our previous work, we have developed an initial framework for handling non-stationary changes in building behaviors [58,27,59]. Recent work by Deng, et al. [3] have developed similar approaches. In the next section, we develop a systematic approach to designing and deploying DRL controllers for large buildings that exhibit non-stationary behaviors. 3 Problem formulation Our overall DRL approach to relearning control for large buildings with non-stationary behaviors includes. \u2022 a deployment loop and \u2022 a relearning loop. The deployment loop monitors building performance, and when the performance shows a consistent degradation (attributed to non-stationarity), the relearning loop is activated to relearn the building models and subsequently retrain the control policy offline, so that it is a better match to the current building conditions. The relearning loop includes two primary iterative steps: 1. Deriving predictive, dynamic data-driven models of building energy consumption from relevant experiences of building energy data; and 2. Using a Deep RL methodology to learn a new control policy and deploy this controller for subsequent building energy management. Unlike previous work, this dual loop relearning controller decouples the controller training and deployment problem by performing the relearning task offline. This reduces the safety problems that may occur because of exploration moves during online learning that can happen with common RL approaches, like PPO and DDPG. The offline relearning framework allows us to simulate relevant behavior of the building under non-stationarities by using relatively higher sampling rate of the data-driven components constituting the system model, and thereby relearning much faster. For real buildings, learning can be severely limited by the sampling frequency, which tends to be much lower due to large system time-constants. Furthermore, as we discuss later, use of multi environment training and a forecast horizon generates diverse samples and a sufficient variety of trajectories to support the training of the RL controller. The deployment loop reduces the constant overhead of lifelong reinforcement learning commonly associated with previous approaches. 3.1 Problem formulation The dual loop relearning controller formally models the building energy consumption model as a Lipschitz Continuous Non-stationary Markov Decision Process. We start by formally defining a a Non-stationary Markov Decision Process. Definition 1 Non-Stationary Markov Decision Process (NS-MDP) A non-stationary Markov decision process is defined as a 5-tuple: M = { S , A , T , ( p t ) t ∈ T , ( r t ) t ∈ T } . S represents the set of possible states that the environment can reach at decision epoch t. A is the action space. T = { 1 , 2 , \u2026 , N } is the set of decision epochs with N ⩽ + ∞ . p t ( s \u2032 | s , a ) and r t ( s , a ) represent the transition function and the reward function at decision epoch t ∈ T , respectively. The above formulation differs from a standard MDP, where the transition dynamics and the reward function do not change with time or decision epochs. For stationary MDPs, the best policy π ∗ is stationary. In the NSMDP case, the optimal policy for a NSMDP, π t ∗ will be non-stationary as it depends on the dynamics and reward functions associated with each epoch. Therefore, learning optimal policies from Non-Stationary MDPs is particularly difficult for non-episodic problems, because the agent is unable to explore the time axis at will. However, if the non-stationary system does not change arbitrarily fast over time, i.e., it has slow time constants, its behavior can be formulated by applying the regularity hypothesis that can be formalized using the notion of Lipschitz Continuity (LC) applied to the transition and reward functions of a non-stationary MDP [60]. This results in the definition of Lipschitz Continuous NS-MDP (LC-NSMDP) as: Definition 2 ( L p , L r ) -LC-NSMDP An ( L p , L r ) -LC-NSMDP is a NSMDP whose transition and reward functions are Lipschitz Continuous with respect to time and are constrained by (1) W 1 ( p t ( s \u2032 | s , a ) , p t ̂ ( s \u2032 | s , a ) ) ⩽ L p | t - t ̂ | , ∀ ( t , t ̂ , s , s \u2032 , a ) (2) | r t ( s , a , s \u2032 ) - r t ̂ ( s , a , s \u2032 ) | ⩽ L r | t - t ̂ | , ∀ ( t , t ̂ , s , s \u2032 , a ) , where W 1 , the Wasserstein distance is used to quantify the difference between two distributions. L p and L r and the associated Lipschitz bounds on the change. t , t ̂ ∈ T , where T = { 1 , 2 , \u2026 , N } is the set of decision epochs with N ⩽ + ∞ . s , s \u2032 ∈ S , where S represents the set of possible states (possibly continuous and infinite) that the environment can reach. Although the agent does not have access to the true NSMDP model, it is possible for it to learn a quasi-optimal policy by interacting with temporal slices of the NSMDP assuming the LC-property. This means that the agent can learn using a stationary MDP of the environment at epoch t [60]. Therefore, the trajectory generated by an LC-NSMDP { s 0 , r 0 , \u2026 , s k } is assumed to be generated by a sequence of stationary MDPs { MDP t 0 , \u2026 , MDP t 0 + k - 1 } . The LC assumption translates to our relearning framework as the following condition, i.e., the temporal distances of the transition function and reward in Eqs. 1 and 2 are bounded between time slices. Building energy behaviors have large time-constants associated with most of their thermodynamic processes. These assumptions hold even when exogenous variables like ambient temperature, relative humidity, setpoint preferences and thermal loads cause changes in energy behavior of the building. Under these assumptions, the response to non-stationarity changes in the exogenous variables can be modeled as a mode change that occurs over a relatively large time interval, δ . This allows us sufficient time to detect the onset of non-stationarity using the outer loop, followed by the relearning of the models and subsequently the RL agent in the inner loop. Therefore, we formulate building energy models as Lipschitz Continuous non-stationary Markov Decision Processes (LC-NSMDP) that can be learned incrementally using deep learning approaches in the time interval specified by δ as the mode transitions occur. The existence of Lipschitz Bounds are reasonable for building transition function T under the influence of the exogenous variables except faults. Building thermodynamic behavior does not undergo unbounded changes within short periods of time. Hence, the \u201cincremental\u201d learning of building energy models is feasible in a reasonable time interval between changes, provided sufficient information is available about the new behavior. 4 Solution approach In this section, we develop the solution architecture to implement our nested deployment and relearning loop approach for RL-based control of systems, which operate as Lipschitz Continuous Non-stationary Markov Decision Processes. The approach will be demonstrated in the supervisory control of buildings for energy management. The processes associated with the building behavior are considered dynamic systems (energy, zone temperatures, and component actuation). The controller directly defines supervisory actions (set-point/reference changes) for the building at the deployment stage but there is no learning involved. At the relearning stage, the dynamic systems that describe the behavior of interest for the building are defined through data-driven models such that the controller learns from the interaction with these models. Each model captures the behavior of one or multiple variables of interest based on measurements commonly available in most buildings. An overview of our approach is presented in Fig. 2 . The deployment loop (highlighted in green) comprises two components: (1) the supervisory controller that operates on the dynamic system for overall energy management, and (2) a monitoring module that tracks a relevant performance metric to determine when the inner relearning loop needs to be triggered. The relearning loop (highlighted in red) uses recent data of building energy operations and deep learning methods to perform system model relearning and subsequent control adaption in an optimal and time efficient manner. The rest of this section provides a detailed description of the different components of our solution by unpacking our nested loop relearning control structure into a set of computational components for performance monitoring, model and controller relearning, and controller redeployment. 4.1 Deployment loop Our deployment loop discusses the online interactions between our RL-based Supervisory Controller and the Building, which is assumed to behave as a stochastic non-stationary MDP, and the operations of the Performance Monitoring Module. 4.1.1 Performance monitor module The Supervisory Controller, uses an offline RL approach to learn an \u201coptimal\u201d controller given a set of deep learning models derived from building operational data. Since building energy systems exhibit non-stationarity, the deep learning models, and therefore, the controller policies may need to be updated using incremental learning methods, especially when controller performance degrades. For our models and controller to adjust to non-stationary changes in a fast and efficient manner, we have developed a Performance Monitor Module that is sensitive to the performance changes. More specifically, our monitoring module is designed to track a calculated reward signal as our Supervisory Controller operates online. This operation is illustrated in the top half of Fig. 2. We assume that the degradation in controller performance manifests as a consistent negative trend in the reward signal, which we measure as a hypothesis testing scheme using a method described in [61]. Through empirical studies, the authors demonstrated that a trend was statistically significant if the R 2 coefficient of determination was greater than 0.65 and the p - value established a 95% confidence interval (i.e., p < 0.05 ). More complex non-linear trend detection methods may also be used, as has been recently demonstrated in a similar application in [3]. The sensitivity and detection delay of the Performance Monitoring module depends on the size of the moving window over which we track changes in the reward signal. Larger window sizes make the detection process more noise-tolerant, but also increase the detection delay. In our work, we define the window size, W pm i as a hyperparameter, and estimate the value of i ⩾ 1 for individual application domains. In our buildings applications, it is important to track both slow moving non-stationarities (that may be attributed to changes in weather conditions), and fast moving non-stationarities (that may be attributed to changes in building parameters, such as sudden and unexpected changes in the occupancy of the building). In addition to application-specific considerations, the choice of the window size also depends on the time-constants associated with the dynamics of the system, which also influences how reward values may change over time. Hence, we need to fine-tune the window sizes to match system dynamics, to make them sufficiently sensitive to trends in a timely manner. Once, the performance monitor module is triggered, it initiates the relearning loop which is discussed next. 4.2 Relearning loop Our solution architecture for the relearning loop includes deriving dynamic models of building energy consumption as a set of predictive data-driven models. After the Performance Monitor triggers the model relearning task, and the dynamic models have been relearned, the currently deployed reinforcement learning based controller is retrained offline to adapt to non-stationary behavior changes using the modified data-driven system models. Then, the controller is redeployed on the actual system. This process is illustrated in the bottom half of Fig. 2. 4.2.1 Data-driven modeling of the dynamic system Advanced control methodologies for real world systems typically use model-based or data-driven predictive models of system behavior to explore the state-action-performance relations, and then select appropriate control actions that optimize system performance [62,63]. The complexity of large building systems makes it prohibitively expensive to develop accurate physics-based models [46] for model-predictive control. Therefore, data-driven techniques are being increasingly used to develop dynamic models for large, complex systems. An important component of our approach is to develop Dynamic System Models (DSM) of the system using an ensemble of data-driven models that estimate the next state s ¯ t + 1 of the system given the current state s ¯ t and inputs u ¯ t . These inputs typically represent a combination of actions produced by the controller a ¯ t and the exogenous variables d ¯ t that affect system behavior. The purpose of this ensemble of models is to simulate the transition dynamics p t ( s \u2032 | s , a ) . In our work, given the complexities in time and cost for deriving physics models of building energy consumption, we adopt data-driven approaches to derive recurrent neural network Long Short Term-Memory networks(LSTMs) that capture the dynamic relations between system inputs and state variables, along with Fully Connected/Dense Neural Networks (FCNN) to capture the non-linearity of the processes. The specific network architecture of the deep learning models are obtained using a hyperparameter search process associated with the models. The hyperparameter search process is described in detail in [64]. Furthermore, given the non-stationary behaviors in the operating regimes of our system, it is necessary to retrain the deep learning networks, when non-stationary changes occur in the system. Retraining with limited data after a non-stationary change may cause overfitting, and, as a result cause catastrophic forgetting [65] of past behaviors that may still be relevant. To prevent this, we adapt a regularization process termed Elastic Weight Consolidation (EWC) [65]. This process is effective in retaining previously learned behavior from older data, while avoiding long learning times. The loss function for the network training updates is modified to incorporate the regularization term: (3) L ( θ ) = L old + newdata ( θ ) + ∑ i ∈ θ λ 2 F i ( θ i - θ i , old ) 2 The first loss term simply indicates the training loss on the old and new data (from the Experience Buffer) after the non-stationary change occurrence. The second term penalizes weight changes scaled by the Fisher Information Matrix ( F i ) on the new data for each parameter i of the network θ . λ is the hyperparameter that determines how much weight we want to assign to learning the new behavior. In our work, we use the value, λ = 400 . Since we need to evaluate the Fisher importance matrix for all the data points, we use a small data set for these incremental updates to enable faster computations. This requirement aligns with the provision of limited data available during relearning. 4.2.2 Experience buffer Since we consider data-driven models, we need to provide training data containing the input variables for the models. This includes variables, like a ¯ t , s ¯ t and d ¯ t and the target data to estimate s ¯ t + 1 . We collect and use real data from the actual pre-existing controller-system interactions. This data is continuously collected and stored in an Experience Buffer modeled as a FIFO queue with queue length M e . The Experience Buffer helps us retain the latest data from the real system so that the models can be adapted to the latest system behavior as needed. Choosing an optimal value for M e is an important step in the overall approach. 4.2.3 Supervisory controller: deep reinforcement learning agent In order to have an optimal policy π t ∗ for each epoch of the LC-NSMDP, we need a controller that can interact with the real system in an efficient, fast and safe manner and, when needed, adapt to the changes in the real system transition dynamics. It needs to utilize feedback from multiple factors of non-stationarity that appear as part of the state space model s ¯ t = F ( s ¯ t - 1 , u ¯ t ) . In order to adapt safely, it needs to retrain offline on a Dynamic System Model and handle a degree of inaccuracy commonly associated with data-driven Dynamic System Models. Furthermore, it needs access to numerous and a diverse set of sample transitions in order to converge faster to an optimal policy. Given these requirements, we choose to use a policy gradient based reinforcement learning algorithm with some augmentations as the learning mechanism for the Supervisory Controller. For our work, we implement a policy-gradient based Deep Reinforcement Learning Supervisory Controller. Its Policy Network, parameterized by θ , takes as input the current state of the MDP, s ¯ t , which comprises observations o ¯ t from the system and certain exogenous variables in d ¯ t that aid in the decision-making process. In response, the network outputs an action a ¯ t . Additionally, when the agent is performing model-free training, it either receives from the environment or generates its own feedback signal using the reward function r ( s t , a t ) . It then collects the information ( a ¯ t , a ¯ t , a ¯ t + 1 , r ( s t , a t ) ) as a tuple, and uses it to optimize the network loss function J θ . Depending on the algorithm used to update the loss function, the DRL agent might deploy a policy gradient network like REINFORCE [66], an Actor-Critic network like A2C [67], or advanced actor critic algorithms like Proximal Policy Optimization(PPO) [68] and Deep Deterministic Policy Gradient(DDPG) [38]. Since the measurements from the real environments are prone to disturbances and sensor noise, the data-driven models forming the Dynamic System Model may exhibit significant variance that reflects its inaccuracies. This can lead to convergence issues during the agent network training process because of diverging gradient estimates during backups of returns. This is commonly described as the Out of Distribution Error(OOD) [26,69] in data-driven reinforcement learning. Hence, during model-free training, we run multiple \u201dworker\u201d Agents on different initialization or seeds of the Dynamic System Model to generate diverse experiences in parallel so that we can reduce the variances caused by the learned the data-driven models. In turn, these diverse experiences make the agent learning process more stable via bootstrap aggregation. 4.2.4 Exogenous variable predictors Our approach aims to adapt the reinforcement learning based Supervisory Controller to the non-stationary changes in the system immediately after they are detected. RL controllers based on policy gradient approaches are typically sample inefficient. Therefore, we need sufficient data to accurately simulate the behavior of the Dynamic System Model. For slow moving systems like buildings, access to sufficient real data may not be possible. Hence, we have developed an approach to forecast the future behavior of exogenous variables that serve as inputs to our derived Dynamic System Models so that we can simulate the model behaviors into the future and generate enough data for faster updates. The inclusion of exogenous variable predictors is mainly guided by the observation that ambient conditions and zone temperature requirements (the exogenous variables) help the agent correlate the reward/feedback with the state of the system. We introduce a class of predictor models, called Exogenous Variable Predictors that learn to forecast behavior of exogenous variables affecting a dynamic system. We demonstrate the development of Exogenous Variable Predictors in the context of weather, thermal load, and occupancy schedules when we apply our approach to buildings. Given the current sequence of inputs of length K for the exogenous variables d ¯ t - K : t , these models predict the value of the exogenous variable at the next time instant as d ̂ t + 1 . For forecasting over a horizon of length N, we use the output at the t + 1 th instant and append it to the input sequence. Both K and N are hyperparameters that need to be fine-tuned based on the specific application. We have primarily used LSTM models for capturing time-correlations in these variables and Fully Connected/Dense Neural Networks(FCNN) to capture non-linearity of the processes. The specific network architecture is obtained by a hyperparameter search process associated with the models. Further, depending on the domain of application, it can keep training continuously with newer batches of data or train only when certain system non-stationary behavior is detected. Details of our overall approach are shown in the schematic, Fig. 3 . As discussed earlier, there are two distinct phases of operation. During the Deployment Phase, a pre-trained Supervisory Controller interacts with and controls the energy behavior of the Dynamic System. The interactions are collected in the Experience Buffer. The Performance Monitor Module tracks the performance of the controller and whenever it deteriorates beyond a statistically significant threshold, it generates a Relearn Trigger signal to start the Offline Relearn Phase. The Dynamic System Model is then updated using data from the Experience Buffer. The relearned models are used to train the Supervisory Controller using a model-free learning approach that generates parallel copies of the Dynamic System Model using a multi Actor Critic approach. The Exogenous Variable Predictors are used to forecast the exogenous variables over a horizon so that the agent training process can gather numerous data points during the model-free training process for faster updates. We conduct experimental analyses in the next two sections to demonstrate the effectiveness of our relearning controller approach. First, we modify a standard 5-zone building testbed and run a series of experiments to demonstrate the effectiveness of our supervisory controller across a variety of non stationary changes in building energy behavior. Then, we deploy our supervisory controller on a real office building on our Vanderbilt University campus to demonstrate the effectiveness of our approach to real world problems. 5 Evaluation of approach on 5 zone testbed As discussed, we perform studies that illustrate how our relearning supervisory control approach addresses the challenges related to its deployment for non-stationary building behavior. We compare our approach against other state of the art approaches for building supervisory control. These include (1) rule-based control e.g.: ASHRAE Guideline 36 [7]; (2) a planning based Model Predictive Control algorithm using Path Integral [33]; and (3) advanced model-free reinforcement learning based techniques, such as Proximal Policy Optimization (PPO) [35] and Deep Deterministic Policy Gradient (DDPG) [38]. We perform benchmark studies that demonstrate the improvements of our approach over existing techniques in terms of key performance indicators for buildings, such as energy consumption, occupant comfort and actuation related wear and tear. We further investigate the reasons for these improvements and discuss the contributions of our approach in terms of advancing current building supervisory control methods. In the remainder of this section, we first provide a brief description of the system and then discuss the problem formulation, solution implementation, and evaluation methods. We then discuss the results of our experiments and our contributions to the research in data-driven building supervisory control. 5.1 System description The five-zone testbed, developed by the Lawrence Berkeley National Labs, is a commonly used physics-based dynamic simulation model comprising an HVAC system with Air Handling Units(AHUs) and Variable Air Volume(VAV) Units, a building envelope model that includes air flow and leakage through open doors and other parts of the building [70]. In this work, we adapt an open source implementation of this model in Modelica [71] for our experiments. We compiled the model into a Functional Mockup Unit using a Jmodelica compiler and then used PyFMI to interact with the system using our data-driven approaches developed in Python. Our goal was to develop a supervisory controller that controlled the AHU discharge setpoint to ensure energy efficiency, comfort, and reduced VAV damper actuation in the building. Our experiments consider a number of non-stationary behaviors in the building energy consumption that can be attributed to unexpected changes in 1) weather, 2) zone temperature requirements and 3) zone thermal loads and 4) a combination of 1, 2 and 3. 5.2 Problem formulation Table 1 describes different components of the testbed represented as an MDP. The choice of variables was based on suggestions from building managers and partly inspired by relevant work in Section 2. Our designed reward function for learning a RL-based controller includes three components, as shown in Table 1: 1. r energy , which incentivizes energy efficiency. The energy variable sums three energy terms: the heating, cooling, and electrical energy consumed by the building during a pre-defined time interval after an action is taken by the supervisory controller/agent. The component energy values are obtained from the simulator at the end of each simulation interval; 2. r cmft , which focuses on comfort values for each zone of the building, and computes zone comfort violations across all zones of the building. ub z , t and lb z , t represent the upper and lower bounds for the temperature setpoints for each zone z at time instant t. Zone violations are recorded as zone temperature differences outside the set upper and lower bounds of each zone in the building, sampled at the end of each time interval after the supervisory controller takes an action; 3. r vav _ actuation , which penalizes frequent changes in the controller action T disch _ stpt from one time step to the next in each zone. Frequent changes of larger magnitude indicate aggressive damper response to the supervisory controller actions and should be avoided. Here, v % , z , t denotes the VAV damper valve percentage in zone z at time instant t, and a non-zero change in the damper valve percentage from time t - 1 to t is recorded as a penalty. Since, the scale of the individual reward terms are different, we scale them to 0\u20131 range using max and min values of each component based on historical data. The scaled violations are then summed together and reported as a single term. 5.3 Implementation of the dual loop relearning controller This section presents the implementation details of each component in the solution architecture that was discussed in Section 3. The tuned hyperparameter values for these components, and their individual performance results, are described in the Appendix A for the interested reader. 5.3.1 Performance monitor module The Performance Monitor monitored the reward function values, looking for negative trends in the reward values for the deployed controller. As discussed, two windows ( i = 2 ), i.e., W pm 1 and W pm 2 tracked the reward signal in parallel to capture slow moving non-stationarities due to weather changes, and fast moving changes due to zone temperature or load changes, respectively. The optimal values for window lengths W pm 1 and W pm 2 are discussed in Appendix A1. 5.3.2 Dynamic system model The Transition Model p ( s t , a t , s \u2032 ) captures the relevant building dynamics, and given the current state of the system s t and the action taken a t at the current time step, provides the values of the following set of observations ( s \u2032 ¯ t ) at the next time step: Total Energy Consumption ( E tot ), the zone temperatures ( T z , z = 1 \u2026 5 ), and the VAV damper percentages ( v % , z , z = 1 \u2026 5 ). Accordingly, we create a model M energy for predicting total energy consumption of the building, a model M T , zone for predicting all the zone temperatures and 5 individual models ( M vav % , z , z = 1 \u2026 5 ), each of which predicts the VAV damper percentage for each zone at the next time step. The collection of these models constitute the Dynamic System Model. Details of the tuned model architecture with the hyperparameters, training procedure and evaluation process for each data-driven model in the collection appear in Appendix A2. 5.3.3 Experience buffer The Experience Buffer was initiated with a hyperparameter-tuned memory size M e . The computation of the values for M e for each non-stationarity experiment is described in Appendix A3. 5.3.4 Supervisory controller The DRL-based Supervisory Controller was implemented using a simple Multi-Actor Critic framework [72,73]. We chose this formulation to make a determination whether the improvement in performance could be attributed to our dual loop framework versus the inherent efficiency one could gain from an online state-of-the-art algorithm like PPO [68], DDPG [38] etc. Given the computation resources, we chose to train the multi-actor ( n = 10 ) parallel environments for the inner loop, to generate a large number of diverse samples similar to [74]. We accounted for measurement noise and resulting model inaccuracies in the Dynamic System Models by bootstrap aggregating transitions across these parallel executions. We did not specify max training steps, as our training process was incremental in response to performance degradation, and used callbacks to stop training when the reward did not improve further or converged. The tuned model architecture for the Actor Critic networks, the length of an episode (l) and the discount factor ( γ ) are summarized in Appendix A4. 5.3.5 Exogenous variable predictors For the testbed, we needed to predict the values of ambient dry bulb temperature (oat) and the relative humidity (orh). For this problem, the outside air temperature and humidity prediction models are based on Long Short-Term Memory Neural Networks(LSTMs) adapted from [75,73] with some fine-tuning to adjust to the data for our specific location. The Exogenous variable predictor models are continuously learned in a batch online fashion. The tuned model architecture, training procedure and evaluation process for each exogenous variable predictor model and the values of K and N in the ensemble are provided in Appendix A5. For the zone setpoints schedules and thermal loads based exogenous variables, models were based on simple rules that looked up the current schedules or values for the variables in the real system and set them to those schedules or values for the required prediction horizon N. 5.4 Experimental studies and results We ran a set of experiments to perform a comparative analysis and evaluate the performance of our relearning supervisory controller in the presence of four non-stationary conditions that affect building energy consumption. The non-stationary conditions studied were: 1. Unexpected changes in the weather conditions. Weather-related changes included sudden increase and decrease in the outside air temperature and relative humidity, e.g., sudden warm spells during spring mostly occurring in February and March 2021 or sudden cold spells in August and October 2021. 2. Abrupt changes in zone set-point values. In our experiments, zone set-point changes resulted in abrupt changes to the upper and lower bounds for temperature set points by 4 o F to 6 o F for certain zones of the test-bed building. 3. Thermal load changes in the building. Sudden thermal load changes may be linked to occupancy changes in a building, for example, an event or a gathering in the building may lead to a large influx of people for an interval of time. 4. Combination of non-stationary changes. Here, the effects of non-stationary behavior due to unexpected weather-related changes, modifications of zone setpoints by occupants and thermal load changes in different zones were simulated for studying their effects on controller performance. We used three months of data generated by simulating the building testbed as a pre-training period for learning the building dynamics and initial supervisory controller. Then we evaluated the proposed framework over a period of one year. We used weather data from the city of Nashville for our experimental analyses. The reward function, state space, and control actions were the same for the DRL algorithms as well as MPPI. The DRL approaches were run online[35,38] to match their implementation in related reinforcement-learning based buildings control literature [28,16,41,17]. The on-policy PPO algorithm updated the controller with batches of continuously collected experiences over a period of 12 h. On the other hand, DDPG is an off-policy algorithm, and, in our experiments, we updated the controller weekly, using data sampled from the Replay Buffer which included the week of new data. The prediction horizon considered for MPPI was set similar to our relearning controller algorithm for each non-stationarity condition, to ensure fairness in the comparison. Similar to [74], we made periodic(weekly) updates for the data-driven models used for MPPI. To account for the effects of random seed initialization in recurrent neural network models, we ran 20 experiments for each one of the controllers for each non-stationary behavior condition. The purpose of our experiments was to compare the performance of our relearning controller after the building exhibited non-stationary behaviors, therefore, we computed the evaluation metrics over a period of 1-week after the non-stationary change was detected. We assumed that one week would be the maximum duration before which the building response changes. 5.4.1 Evaluation metrics We use standard metrics in building literature to evaluate our approach. 1. Energy Consumption E tot : A lower energy consumption under similar conditions indicates better energy efficiency. 2. Average Zone Temperature Deviation: T D = ∑ z ∈ Z max ( T z - ub z , 0 , lb z - T z ) or ∑ z ∈ Z | T z - T z , stpt | . A lower value indicates better thermal comfort. Here, Z represents the total number of zones, T z is the temperature in zone z , T z , stpt is the set-point for zone z, and ub z and lb z are a lower and upper comfort bounds. One of the two choices can be selected as a proxy for comfort, depending on whether the bounds are predefined or not for each zone of the building. 3. Actuation Rate: A s = ∑ t = 0 T ∑ z ∈ Z | A t + 1 , z - A t , z | , where A is the VAV damper opening state (percentage) for the testbed or fan switching state (on/off) for the VRF cassettes fans. Lesser actuation on aggregate implies that the controller enforces smoother control to reduce wear in the actuators. 4. Time to relearn: Δ T relearn : It indicates the time it took from the start of the relearning trigger to the time until the controller training converged. We measured this for our approach as well as PPO and DDPG using a callback during the training process that told us whether the episode reward over the last 10 episodes was withing 2 standard deviations of the average of the past 100 episode rewards. The results from the benchmarking study are presented in Table 2 , with the best result in each case highlighted in bold. The results are reported for each type of non-stationary behavior across each evaluation metric. Since we ran 20 experiments, we reported the mean and standard deviation of each metric under consideration.The proposed approach seems to perform well across different conditions. In terms of Energy consumption, our relearning approach performed significantly better than Guideline-36 ( p < 0.05 ). We attributed this to the reward formulation strategy, where effects of non-stationary behaviors on energy consumption are factored in, and the supervisory controller uses this information to adapt better. Adaptation is mostly absent in the Guideline 36 strategy. For the majority of the non-stationary conditions, the relearning approach performed better ( p < 0.05 ) than other RL approaches (PPO and DDPG) in spite of the same problem formulation. We learned offline, and this provided a speedup in terms of the relearning time compared to the online algorithms, which were limited mostly by the sampling rate and partly by the large time constants of the system. This speed up in adaptation is highlighted later when we compare the relearning times Δ T relearn of these individual RL algorithms. The performance was slightly better than MPC since because our approach has the ability to adjust to uncertainties in the future state estimation by bootstrap aggregating across parallel actors during training. Overall, we saw energy performance improvements of 13.18 % , when we aggregated the results across multiple experiments over a period of 1 year. Similarly, when we compared the comfort metric, we observed that the relearning approach had the least temperature deviations ( p < 0.05 ) from the zone setpoint as compared to other approaches. When compared to Guideline 36, the improvement was mainly due to the linear relation between decreasing reward due to more discomfort in the RL reward formulation versus the constant temperature requests in Guideline 36. However, under non-stationary behavior conditions, the online versions of PPO and DDPG exhibited worse comfort performance compared to relearning and Guideline 36. This is mostly attributed to the exploratory behavior of their corresponding target policy for PPO and behavior policy for DDPG. During exploration under non-stationary building behavior, their actions generate random temperature changes and hence lower comfort performance on the real system. Our relearning approach to control avoids this by learning offline. The performance of our approach was better than MPPI due to similar reasons mentioned in the previous paragraph, i.e., our approach handles uncertainties over longer horizons. When we used a bagging approach, we observed similar performance improvements with respect to the actuation metric. The online RL algorithms performed worse because the building VAV dampers responded aggressively to the exploratory setpoints of PPO and DDPG for non-stationary system behaviors. The aggregate improvement in comfort control when compared to other approaches was 12.03 % over a period of 1 year. Improved comfort control reduces decentralized heating and cooling, improving energy efficiency. For similar reasons as above, we observed that our approach had comparatively similar performance with respect to the actuation metric A s , while performing significantly better ( p < 0.05 ) than Guideline-36, PPO and DDPG. The performance in comparison to MPPI was better, but the improvement was not statistically significant. Again, the reward component in our approach had ensured the setpoints did not trigger large actuation signals for the VAV dampers under different conditions. Overall, we observed a 9.4 % reduction in VAV damper actuation over a period of 1 year. This indicated extended lifespan of the actuating components. The overall advantage of our relearning approach was mainly be attributed to the speed of adaptation. Hence, we compared the time taken to relearn Δ T relearn compared to the online algorithms PPO and DDPG in Fig. 4 . We observe that the dual loop controller approach takes 60% to 80% less time than the online approaches proposed in [16,28,17] using PPO and DDPG while using much more samples(10 times more samples) across parallel environments. This speed up is a major advantage to addressing the challenges of having a suboptimal policy under non-stationary behavior conditions when the system dynamics is slow. To investigate the reasons for performance improvement, we looked at different timelines across multiple experiments where the non-stationarities occurred and observed how different controllers reacted to the changes. We highlight one such change due to thermal load and the controller responses to explain the reasons for performance improvement in our approach. Fig. 5 .a shows the thermal load across multiple building zones. We highlight the thermal load based building non-stationarity introduced into the system, using the vertical dashed black lines. The orange dashed lines indicate the time point at which the corresponding the performance monitor triggered. Fig. 5.b compares the AHU discharge air temperature set-point of the PPO, MPPI, and the relearning controller over the above-mentioned period, where a combination of weather and thermal load based non-stationary changes occurred. Fig. 5.c compares the step reward obtained from the real system. The higher the reward value, the better the real performance. PPO incurred delays in responding to thermal load changes, and explored different set-points when recovering, leading to temperature fluctuations in the individual zones and aggressive damper response that produced an overall degraded reward over an extended period of time. The set points for MPPI and our relearning controller addressed degradation faster than PPO. This was observed after the 36-h mark, when the first non-stationary change was detected. The relearning controller leared the updated behavior faster than the MPPI through bootstrapping experiences across parallel environment copies and reduced the temperature set-point to accommodate the cooling requirement. Overall, we observed that the reward function design helped us address the challenges associated with using multiple feedback, especially under non-stationary behavior. The offline relearning helped us prevent aggressive system response and reduced the amount of discomfort and actuation. We significantly increased the speed of adaptation by simulating the system at faster rates offline. The use of parallel actors and environments helped generated diverse transitions that helped the agent to learn much faster. 6 Evaluation of approach on real building In this section, we apply and evaluate our relearning controller approach on a real building. Similar to the testbed, we used our approach for supervisory control of the Air Handling Units of the HVAC system. However, there are differences between the testbed and the real building. Instead of VAV units, the real building is retrofitted with Refrigerant Flow (VRF) units for decentralized heating and cooling. Also, the building performance evaluation was limited by given safety and resource constraints, the differences in HVAC components, and the method of evaluation and accuracy of models given the limited sensors available in the building. Given all of these limitations and other real world constraints, we designed experiments to compare the previously deployed rule-based controller [76] with our relearning approach over a period of one year. To make sure our comparisons were fair, we had to find similar day-to-day weather conditions for the comparison experiment. In the rest of this section, we describe the system, the deployment of the relearning controller, the experimental setup, and the results obtained. It is worth clarifying that non-stationary changes, in this case, are not under our control thus we cannot categorize them. Yet, we have tried to identify and report the results plus analyzing when relearning triggers happened and what possibly generated the trigger. 6.1 System description The building under consideration is a large three-story building mixed use commercial building that is LEED Gold Certified. It consists of a collection of individual office spaces, classrooms, halls, a gymnasium, a student lounge, and a small cafeteria. The building climate is controlled by two Air Handling Units (AHUs) that operate simultaneously and multiple Variable Refrigerant Flow (VRF) units that are deployed in the different zones of the building [77]. The configuration of the building HVAC system is shown in Fig. 6 . The AHUs operate in two modes defined by the wet bulb temperature ( t wb ). When t wb > 52 o F , the cooling and the reheat coils of the AHU operate to first dehumidify the air by cooling it to 52 o F , and then the air is reheated back to a reheating coil temperature set-point so as to achieve neutral humidity (which is close to 50 % ). When t wb < 52 o F , only the preheat coil operates to heat the incoming cold air. Once the air is released into individual zones, it is further conditioned depending on the temperature setpoint requirements of the individual zones. This zone level conditioning is performed by the VRF systems, comprising of local cassettes and five central condenser units. Traditionally, a rule-based supervisory controller was deployed to define the set-point of the AHU discharge temperature. This controller used fixed set-point temperatures for each operating mode, i.e., for the pre-heating mode the discharge temperature was set to 72 o F , and for the re-heating mode the discharge temperature was set to 68 o F . As proposed by the building managers, our goal was to develop an intelligent supervisory controller that minimized energy consumption without sacrificing occupant comfort and reducing wear and tear on the actuation devices. Since the rate of wear and tear could not be measured directly or estimated for the devices, we dropped this term from our reward function calculations. We collected temperature sensor data from multiple HVAC components. For the Air Handling Units(AHUs), we collected temperature and valve data related to the different coils to model the dynamics for subsequent use in supervisory control. We also collected temperature and relative humidity information from the different building zones. 6.2 Problem formulation Similar to the testbed, Table 3 describes different components of the real building mapped onto the MDP. In the designed reward function, the term, r energy incentivized energy efficiency, the term, and r cmft penalized zone comfort violations. The total energy consumption ( E tot ) includes Heating, and Cooling energy components. T z , stpt represents the temperature set-point for each zone z. The reward function was aproved by the plant ops technical managers. The managers also stressed on studying energy savings at individual zones of the building, and suggested that if we ensured that discharge air temperature ( T disch _ stpt ) was close to the zone setpoints then our energy savings for the building would be significantly higher. Hence, we formulated the second term of the reward function, so that T disch _ stpt was close to average set-point temperature across all zones. The intuition behind minimizing this difference was to ensure that the VRFs did a minimal amount of cooling or heating locally for each zone. 6.3 Implementation of the solution We again discuss the components of our relearning controller. 1. Performance Monitor Module: Similar to the testbed, we tracked the presence of negative trends in the reward function during the deployment phase using i = 2 windows W pm 1 and W pm 2 in parallel. The optimal values for W pm 1 and W pm 2 were W pm 1 = 6 hours and W pm 2 = 2 hours, respectively. 2. Dynamic System Model: For the Transition Model ( p ( s \u2032 , s t , a t ) ) we evaluated the Energy consumption ( E tot ) using the derived data-driven models: M heat for heating energy and M cool for cooling energy. In this building, the heating system has an additional heat source, steam, and the steam heating valve can be set to on/off. We created a separate model M vlv to predict the valve on/off behavior. This value, multiplied with the output of the M heat model gave us the actual heating energy consumed. A separate model helped simplify the LSTM models for deriving the heating energy consumption. The energy consumption, E tot = M heat * M vlv + M cool . The tuned model architecture, training procedure and evaluation process for each data-driven model in the ensemble are discussed in Appendix B1. 3. Experience Buffer: The Experience Buffer was initiated with an optimal memory size M e = 24 hours. 4. Supervisory Controller: The DRL based Supervisory Controller was implemented using a simple Multi-Actor PPO algorithm [68]. We chose PPO because the policy updates in PPO are conservative and there were fewer chances of the policy diverging and causing safety issues with the system. Further, we wanted to get the best performance for the actual deployment and PPO, being the state-of-the-art Actor Critic algorithm, was a natural choice for deployment purposes. 1 1 We clarify that this approach is different from deploying PPO as an on policy (i.e., online) controller on the building. This issue is further demonstrated from our benchmarking experiments on the testbed. Similar to the testbed, we chose to train in n = 10 parallel environments. The tuned model architecture for the Actor Critic networks, the length of an episode (l) and the discount factor ( γ ) are summarized in Appendix B2. 5. Exogenous Variable Predictors: Similar to the testbed experiments, for the real building, we needed to predict the values of ambient dry bulb temperature(oat) and the relative humidity(orh). We used the same models that we had developed in the testbed experiments. For the zone setpoint schedule based on the exogenous variables, models were created using simple rules, which looked up the current schedules in the real building and set them to the schedules for the required prediction horizon N. The optimal values for K and N for our experiments were K = 12 hours and N = 48 hours, respectively. 6.4 Results We benchmarked the performance of our approach against the previous Rule Based Controller for a period of one year. For the rule-based controller, we analyzed performance from Feb 2020 to Feb 2021. For the relearning approach, we analyzed performance from Feb 2021 to Feb 2022. While we could not absolutely ensure a fair comparison in terms of all the variables affecting the building, we ensured that the weather conditions on a day-to-day basis were the same for our proposed approach and the rule-based controller. We used dynamic time warping (DTW) to find similar weather conditions considering two variables: outside air temperature and relative humidity. DTW allows selecting days during which the rule-based control operated with similar weather conditions to the days during which our approach was deployed. This allows ensuring a fair comparison of the performance of the two approaches for the real building operation. The resulting day-to-day periods of similar weather variables aligned over a period of one year is shown in Fig. 7 . We compared the performance of the two controllers with respect to the key performance indicator metrics: 1) energy consumption, 2) comfort measured as temperature deviation from individual zone setpoints, and 3) actuation rate indicated by fan state changes between consecutive time-points. A lower value of each indicates better performance. First, we compared the cooling, heating/steam and electrical energy consumption. Since, there were some differences with respect to the hours spent in the de-humidification and non de-humidification modes by each type of controller, given the best match between weather conditions, we present the energy consumption on a kBTUs/hour basis in Table 4 . The distinction between the modes were of interest to the building energy managers, given the different costs of the energy sources. The savings obtained in cooling and electrical energy on a per-hour basis for our relearning controller compared against the rule based controller was 13.1 % and 14.30 % , respectively. However, our controller caused a slightly higher steam energy consumption, but we noticed that the overall scale of steam energy consumption is two orders of magnitude lower than cooling and electrical energy, and it costs less compared to electrical energy. While it is difficult to exactly infer how our framework saves cooling and electrical energy, according to the building manager, the earlier controller used to reheat the air to ensure neutral relative humidity, but our approach avoids the extra heating as the higher relative humidity air mixes with the larger amount of lower relative humidity air already in the building producing an overall increase in relative humidity that is very small. We attribute this improvement to the reward formulation, which ensured that the discharge temperature setpoint from the AHU should reduce decentralized cooling or heating energy use by the VRF systems. Next, we observed the performance of each controller with respect to the comfort and actuation metrics. We compared aggregated monthly performance across one year. The results are shown in Figs. 8 and 9 , respectively. We observed that the proposed controller maintained (1) lower deviation of zone temperature from set-point and (2) lower VRF fan-switching by virtue of our reward design. The reward function encouraged the supervisory controller to discharge air at temperatures that were close to the zone set-point requirements for the different zones (in our case, we used the average of the zone temperature). Hence, the zone deviations from set-points were on average lower and the zone VRF fans needed less on/off switching as compared to the rule-based control. Overall, we reduced zone temperature deviation by 21.31 % and VRF fan switching by about 10.79 % . According to the building manager, these numbers, over the long run, can contribute to significant energy savings and lesser actuation-related degradation of the components at the zone level. 7 Discussion and conclusions This paper has discussed a condition-triggered deep learning-based modeling and a deep reinforcement learning-based relearning approach for optimizing performance in supervisory control of real buildings which exhibit non-stationary dynamic behavior. This dual loop relearning controller augments vanilla deep reinforcement learning(DRL) approaches using a deployment phase performance monitoring loop, which triggers a relearning loop whenever controller performance on the actual system deteriorates under non-stationarities. The inner loop adapts the system model based on new data and lets the controller policy network adjust to the new behavior of the system offline by training on this new model. A set of methods were used to further improve the performance of our approach when deployed in a time and sampled constrained real building environment. Exogenous variable forecasting helps in increasing sample availability, multi-actor training across parallel environments helps in reducing variance due to approximation errors in data-driven models, and elastic weighted consolidation-based regularization helps in reducing overfitting on limited data. Overall, these augmentations support the agent training process, facilitating faster convergence to a new optimum under new building behavior. We validated our approach through extensive experiments, on both, a testbed, and a real building, and observed significant improvements in building energy performance. The key metrics used in our approach were total energy consumption of the building, comfort for occupants by minimizing the temperature deviations from setpoint values, and reduction in on/off switching (i.e., actuation) of the HVAC components. We observed that the reward function, along with the RL approach, helped our controller generate significantly better energy consumption profiles than the rule-based Guideline-36 controllers that are prevalent in current building energy control systems. Our relearning controller\u2019s performance was marginally better than MPCs because of the bootstrap aggregation to reduce effects of uncertain estimates on a longer horizon. The significant performance improvements with respect to PPO and DDPG were attributed to the faster adaptation under offline relearning and avoiding exploration and hence instability on the real system during non-stationary building behavior. In the future, we will continue to build on our success, by extending our current relearning approach to build hierarchical relearning controllers for large buildings that exhibit non-stationary behaviors. These approaches will combine an overall supervisory controller with individual controllers for the primary HVAC components (e.g., cooling towers, the AHUs and condenser units) in a distributed control architecture with cooperating controller agents. We will continue our work on refining the reward functions to further optimize RL approaches to controller relearning. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Appendix A 5-zone testbed Here we provide the details of the tuned model architectures resulting from hyperparameter optimization process. We ran distributed hyperparameter optimization using the Ray-Tune library [78]. We performed this tuning process across 4 types of building non-stationarities: 1) weather, 2) zone setpoint, 3) thermal load and 4) combination of previous three. Accordingly, tuned architectures are reported across each condition. A.1 Performance monitor module For the performance monitor module, we tuned the values of the two windows for detecting negative trends over both small intervals due to faster relatively non-stationarities and large intervals for slow-moving non-stationarities. The tuned values for W pm 1 and W pm 2 are shown in Table A.5 . A.2 Dynamic system model The network architecture for the different data-driven models in the Dynamic System Model are show in Table A.6 . These models were initially pre-trained on 6 months of data from the testbed. During evaluation, we fed the predicted output from t - 1 as input for predicting at time step t. This helped us evaluate the model error over the prediction horizon N. We evaluated the models across 100 N = 48 hour horizons from different parts of the year. The energy models were evaluated using CVRMSE error, while the temperature and zone VAV predictors were evaluated using MAE. The results are show in Table A.7 . A.3 Experience buffer The size of the Experience Buffer M e is tuned using hyperparameter optimization and we obtain the following values show in Table A.9 . A.4 Supervisory controller The Supervisory Controller consists of the Actor Critic framework [72,73]. The inputs to the actor network are the state variables in s ¯ t described in Table 1 and the output is the mean a μ and a σ from which an action a ¯ t is sampled. The critic network in this case is a Q-network that takes as input the current state s ¯ t and action a ¯ t . It is then used to estimate the Advantage using Eq. A.1. (A.1) A ( s t , a t ) = Q ( s t , a t ) - V ( s t ) = Q ( s t , a t ) - ∑ a t ∈ A π ( a t | s t ) * Q ( s t , a t ) Typically, advantages are normalized for better convergence during agent training. The tuned network architecture is shown in Table A.8 . Next, we look at the values of the episode length l and discount factor γ in Table A.10 . We believe that for slower non-stationarities with high degree of uncertainty like weather, there is less faith in future estimated returns. Hence, we have low values of γ for weather while the opposite is true for non-stationarities related to zone setpoint and thermal load changes as they tend to sustain values or maintain the same schedule in the future and not change very often. A.5 Exogenous variable predictors The Exogenous Variable Predictors models for this problem involved the prediction of outside air temperature and relative humidity. We formulated this a sequence prediction problem where, given K past temperature or relative humidity inputs, we predict the corresponding values over a horizon N. The model architectures were adapted from the paper [75,73] and fine-tuned for the weather data of Nashville, TN, USA. However, it was difficult to perform sequence to sequence prediction using the method outlined in that paper because we did not have labeled data for the future when we use these models for forecasting purposes. Hence, during prediction, we implemented a for loop where the previous predicted output ( o ¯ t ) cell state ( c ¯ t ) and the network state ( h ¯ t ) of the last LSTM is stored and fed as input during prediction of the temperature at the next time step ( o ¯ t + 1 ). This is a standard method in sequence based problems in recurrent neural network based machine translation methods as shown in [79]. Based on this, the network architecture is formulated as an encoder decoder network as shown in Table A.11 . Based on this architecture, we pre-trained the models using 1 year of data from Nashville and then performed prediction over a horizon of length N using past K = 12 hour inputs sampled at a half-hour interval. The test data in this case comprised 100 samples of data, where each sample had over N = 48 hour horizon sampled at a half-hour interval. The resulting modeling errors are reported in Table A.12 . Finally, the tuned values of the input sequence length K and the output horizon N under different conditions of non-stationarity are shown in Table A.13 . The output horizon length is tuned based on a weighted metric comprising the model error, time to relearn, energy, actuation and comfort performance. The value of N has to balance between generating a large number of samples via a larger future horizon to facilitate faster agent training and convergence and the effects of modeling errors across due to uncertainty in future estimates G t . Appendix B Real building Here, we provide the details of the tuned model architectures resulting from the hyperparameter optimization process when our approach is applied to the real building. We ran distributed hyperparameter optimization using the Ray-Tune library [78]. We performed this tuning process once, since we cannot control different exogenous variables in the real building. B.1 Dynamic system model The network architecture for the different data-driven models in the Dynamic System Model are show in Table A.14 . All the layers used Batch normalization and relu activations except the last layer for the M vlv model, which uses a softmax activation. The energy models are trained using standard MSE loss, while the valve model is trained using Binary Cross-Entropy loss. The training and testing procedure is similar to the Dynamic System Models used in the testbed. The energy models were evaluated using CVRMSE error, while the valve predictors were evaluated using classification accuracy. The evaluations were performed by setting N = 48 hours. The results are show in Table A.15 . The valve model had an ROC-AUC of 0.857 . B.2 Supervisory controller The Supervisory Controller consists of the Actor Critic framework, where policy is trained using the Proximal Policy Optimization [35]. The tuned network architecture is shown in Table A.16 . The optimal values of the episode length l and discount factor γ were found to be l = 1 day and, γ = 0.94 respectively. References [1] Oar, Learn about Energy and its Impact on the Environment, US EPA 0 (2021) 0\u20135. URL: https://www.epa.gov/energy/learn-about-energy-and-its-impact-environment. Oar, Learn about Energy and its Impact on the Environment, US EPA 0 (2021) 0\u20135. URL:https://www.epa.gov/energy/learn-about-energy-and-its-impact-environment [2] None, U.S. energy facts explained - consumption and production \u2013 U.S. Energy Information Administration (EIA), [Online; accessed 26. Jul. 2021] (May 2021). URL: https://www.eia.gov/energyexplained/us-energy-facts. None, U.S. energy facts explained - consumption and production - U.S. Energy Information Administration (EIA), [Online; accessed 26. Jul. 2021] (May 2021). URL:https://www.eia.gov/energyexplained/us-energy-facts [3] X. Deng Y. Zhang H. Qi Towards optimal hvac control in non-stationary building environments combining active change detection and deep reinforcement learning Build. Environ. 108680 2022 X. Deng, Y. Zhang, H. Qi, Towards optimal hvac control in non-stationary building environments combining active change detection and deep reinforcement learning, Building and Environment (2022) 108680. [4] A.T.D. Perera A. Wijesiri Designing smart hybrid renewable energy systems with V2G 7th International Conference on Information and Automation for Sustainability 2014 IEEE 1 5 10.1109/ICIAFS.2014.7069612 A.T.D. Perera, A. Wijesiri, Designing smart hybrid renewable energy systems with V2G, in: 7th International Conference on Information and Automation for Sustainability, IEEE, 2014, pp. 1\u20135. doi:10.1109/ICIAFS.2014.7069612. [5] A. Saha M. Kuzlu M. Pipattanasomporn Demonstration of a home energy management system with smart thermostat control 2013 IEEE PES Innovative Smart Grid Technologies Conference (ISGT), IEEE 2013 2013 1 6 A. Saha, M. Kuzlu, M. Pipattanasomporn, Demonstration of a home energy management system with smart thermostat control, in: 2013 IEEE PES Innovative Smart Grid Technologies Conference (ISGT), IEEE, 2013, pp. 1\u20136. [6] ASHRAE, Sequences of operation for common hvac systems, ASHRAE. ASHRAE, Sequences of operation for common hvac systems, ASHRAE. [7] None, Guideline 36: Best in Class HVAC Control Sequences, [Online; accessed 21. May 2021] (May 2021). URL: https://www.ashrae.org/professional-development/all-instructor-led-training/instructor-led-training-seminar-and-short-courses/guideline-36-best-in-class-hvac-control-sequences. None, Guideline 36: Best in Class HVAC Control Sequences, [Online; accessed 21. May 2021] (May 2021). URL:https://www.ashrae.org/professional-development/all-instructor-led-training/instructor-led-training-seminar-and-short-courses/guideline-36-best-in-class-hvac-control-sequences [8] A. Jain F. Smarra M. Behl R. Mangharam Data-driven model predictive control with regression trees\u2013an application to building energy management ACM Trans. Cyber-Phys. Syst. 2 1 2018 1 21 A. Jain, F. Smarra, M. Behl, R. Mangharam, Data-driven model predictive control with regression trees\u2013an application to building energy management, ACM Transactions on Cyber-Physical Systems 2 (1) (2018) 1\u201321. [9] B. Yuce Y. Rezgui An ann-ga semantic rule-based system to reduce the gap between predicted and actual energy consumption in buildings IEEE Trans. Autom. Sci. Eng. 14 3 2015 1351 1363 B. Yuce, Y. Rezgui, An ann-ga semantic rule-based system to reduce the gap between predicted and actual energy consumption in buildings, IEEE Transactions on Automation Science and Engineering 14 (3) (2015) 1351\u20131363. [10] G. Serale M. Fiorentini Model Predictive Control (MPC) for Enhancing Building and HVAC System Energy Efficiency: Problem Formulation, Applications and Opportunities Energies 11 3 2018 631 10.3390/en11030631 G. Serale, M. Fiorentini, et al., Model Predictive Control (MPC) for Enhancing Building and HVAC System Energy Efficiency: Problem Formulation, Applications and Opportunities, Energies 11 (3) (2018) 631. doi:10.3390/en11030631. [11] I. Hazyuk C. Ghiaus Model Predictive Control of thermal comfort as a benchmark for controller performance Autom. Constr. 43 2014 98 109 10.1016/j.autcon.2014.03.016 I. Hazyuk, C. Ghiaus, et al., Model Predictive Control of thermal comfort as a benchmark for controller performance, Autom. Constr. 43 (2014) 98\u2013109. doi:10.1016/j.autcon.2014.03.016. [12] F. D\u2019Ettorre P. Conti Model predictive control of a hybrid heat pump system and impact of the prediction horizon on cost-saving potential and optimal storage capacity Appl. Therm. Eng. 148 2019 524 535 10.1016/j.applthermaleng.2018.11.063 F. D\u2019Ettorre, P. Conti, et al., Model predictive control of a hybrid heat pump system and impact of the prediction horizon on cost-saving potential and optimal storage capacity, Appl. Therm. Eng. 148 (2019) 524\u2013535. doi:10.1016/j.applthermaleng.2018.11.063. [13] D. Azuatalam W.-L. Lee F. de Nijs A. Liebman Reinforcement learning for whole-building hvac control and demand response Energy and AI 2 2020 100020 D. Azuatalam, W.-L. Lee, F. de Nijs, A. Liebman, Reinforcement learning for whole-building hvac control and demand response, Energy and AI 2 (2020) 100020. [14] Y. Li Y. Wen D. Tao K. Guan Transforming cooling optimization for green data center via deep reinforcement learning IEEE Trans. Cybern. 50 5 2019 2002 2013 Y. Li, Y. Wen, D. Tao, K. Guan, Transforming cooling optimization for green data center via deep reinforcement learning, IEEE transactions on cybernetics 50 (5) (2019) 2002\u20132013. [15] M. Luzi M. Vaccarini A tuning methodology of Model Predictive Control design for energy efficient building thermal control J. Build. Eng. 21 2019 28 36 10.1016/j.jobe.2018.09.022 M. Luzi, M. Vaccarini, et al., A tuning methodology of Model Predictive Control design for energy efficient building thermal control, Journal of Building Engineering 21 (2019) 28\u201336. doi:10.1016/j.jobe.2018.09.022. [16] T. Wei, Y. Wang, Q. Zhu, Deep reinforcement learning for building hvac control, in: Proceedings of the 54th Annual Design Automation Conference 2017, 2017, pp. 1\u20136. T. Wei, Y. Wang, Q. Zhu, Deep reinforcement learning for building hvac control, in: Proceedings of the 54th Annual Design Automation Conference 2017, 2017, pp. 1\u20136. [17] Z. Zhang A. Chong Y. Pan C. Zhang K.P. Lam Whole building energy model for hvac optimal control: A practical framework based on deep reinforcement learning Energy Build. 199 2019 472 490 Z. Zhang, A. Chong, Y. Pan, C. Zhang, K.P. Lam, Whole building energy model for hvac optimal control: A practical framework based on deep reinforcement learning, Energy and Buildings 199 (2019) 472\u2013490. [18] D.B. Crawley C.O. Pedersen L.K. Lawrie F.C. Winkelmann Energyplus: Energy simulation program ASHRAE J. 42 2000 49 56 D.B. Crawley, C.O. Pedersen, L.K. Lawrie, F.C. Winkelmann, Energyplus: Energy simulation program, ASHRAE Journal 42 (2000) 49\u201356. [19] J. Leclere, F. Wurtz, E. Wurtz, A low order envelope model for optimised predictive control of indoor temperature: development methodology and calibration with a numerical model, Proceedings of BS2013. J. Leclere, F. Wurtz, E. Wurtz, A low order envelope model for optimised predictive control of indoor temperature: development methodology and calibration with a numerical model, Proceedings of BS2013. [20] D. Sturzenegger D. Gyalistras M. Morari R.S. Smith Model predictive climate control of a swiss office building: Implementation, results, and cost\u2013benefit analysis IEEE Trans. Control Syst. Technol. 24 1 2015 1 12 D. Sturzenegger, D. Gyalistras, M. Morari, R.S. Smith, Model predictive climate control of a swiss office building: Implementation, results, and cost\u2013benefit analysis, IEEE Transactions on Control Systems Technology 24 (1) (2015) 1\u201312. [21] R.K. Jain K.M. Smith P.J. Culligan J.E. Taylor Forecasting energy consumption of multi-family residential buildings using support vector regression: Investigating the impact of temporal and spatial monitoring granularity on performance accuracy Appl. Energy 123 2014 168 178 R.K. Jain, K.M. Smith, P.J. Culligan, J.E. Taylor, Forecasting energy consumption of multi-family residential buildings using support vector regression: Investigating the impact of temporal and spatial monitoring granularity on performance accuracy, Applied Energy 123 (2014) 168\u2013178. [22] Z. Wang Y. Wang R. Zeng R.S. Srinivasan S. Ahrentzen Random forest based hourly building energy prediction Energy Build. 171 2018 11 25 Z. Wang, Y. Wang, R. Zeng, R.S. Srinivasan, S. Ahrentzen, Random forest based hourly building energy prediction, Energy and Buildings 171 (2018) 11\u201325. [23] Y. Chen Y. Shi B. Zhang Modeling and optimization of complex building energy systems with deep neural networks 2017 51st Asilomar Conference on Signals, Systems, and Computers 2017 IEEE 1368 1373 Y. Chen, Y. Shi, B. Zhang, Modeling and optimization of complex building energy systems with deep neural networks, in: 2017 51st Asilomar Conference on Signals, Systems, and Computers, IEEE, 2017, pp. 1368\u20131373. [24] L. Yu W. Xie D. Xie Y. Zou D. Zhang Z. Sun L. Zhang Y. Zhang T. Jiang Deep reinforcement learning for smart home energy management IEEE Internet of Things J. 7 4 2020 2751 2762 10.1109/JIOT.2019.2957289 L. Yu, W. Xie, D. Xie, Y. Zou, D. Zhang, Z. Sun, L. Zhang, Y. Zhang, T. Jiang, Deep reinforcement learning for smart home energy management, IEEE Internet of Things Journal 7 (4) (2020) 2751\u20132762. doi:10.1109/JIOT.2019.2957289. [25] A. Kathirgamanathan M. De Rosa E. Mangina D.P. Finn Data-driven predictive control for unlocking building energy flexibility: A review Renew. Sustain. Energy Rev. 135 2021 110120 A. Kathirgamanathan, M. De Rosa, E. Mangina, D.P. Finn, Data-driven predictive control for unlocking building energy flexibility: A review, Renewable and Sustainable Energy Reviews 135 (2021) 110120. [26] D. Seita, Data-Driven Deep Reinforcement Learning, [Online; accessed 29. May 2021] (May 2021). URL: https://bair.berkeley.edu/blog/2019/12/05/bear. D. Seita, Data-Driven Deep Reinforcement Learning, [Online; accessed 29. May 2021] (May 2021). URL:https://bair.berkeley.edu/blog/2019/12/05/bear [27] A. Naug, M. Quiñones-Grueiro, et al., Continual adaptation in deep reinforcement learning-based control applied to non-stationary building environments, in: RLEM\u201920: Proceedings of the 1st International Workshop on Reinforcement Learning for Energy Management in Buildings & Cities, Association for Computing Machinery, New York, NY, USA, 2020, pp. 24\u201328. doi:10.1145/3427773.3427867. A. Naug, M. Quiñones-Grueiro, et al., Continual adaptation in deep reinforcement learning-based control applied to non-stationary building environments, in: RLEM\u201920: Proceedings of the 1st International Workshop on Reinforcement Learning for Energy Management in Buildings & Cities, Association for Computing Machinery, New York, NY, USA, 2020, pp. 24\u201328. doi:10.1145/3427773.3427867. [28] E. Barrett S. Linder Autonomous hvac control, a reinforcement learning approach Joint European conference on machine learning and knowledge discovery in databases 2015 Springer 3 19 E. Barrett, S. Linder, Autonomous hvac control, a reinforcement learning approach, in: Joint European conference on machine learning and knowledge discovery in databases, Springer, 2015, pp. 3\u201319. [29] S. Ahmadi-Karvigh B. Becerik-Gerber Intelligent adaptive automation: A framework for an activity-driven and user-centered building automation Energy Build. 188\u2013189 2019 184 199 10.1016/j.enbuild.2019.02.007 S. Ahmadi-Karvigh, B. Becerik-Gerber, et al., Intelligent adaptive automation: A framework for an activity-driven and user-centered building automation, Energy Build. 188-189 (2019) 184\u2013199. doi:10.1016/j.enbuild.2019.02.007. [30] A.I. Dounis C. Caraiscos Advanced control systems engineering for energy and comfort management in a building environment\u2013A review Renewable Sustainable Energy Rev. 13 6 2009 1246 1261 10.1016/j.rser.2008.09.015 A.I. Dounis, C. Caraiscos, Advanced control systems engineering for energy and comfort management in a building environment\u2013A review, Renewable Sustainable Energy Rev. 13 (6) (2009) 1246\u20131261. doi:10.1016/j.rser.2008.09.015. [31] R.Z. Homod K.S. Gaeid S.M. Dawood A. Hatami K.S. Sahari Evaluation of energy-saving potential for optimal time response of HVAC control system in smart buildings Appl. Energy 271 2020 115255 10.1016/j.apenergy.2020.115255 R.Z. Homod, K.S. Gaeid, S.M. Dawood, A. Hatami, K.S. Sahari, Evaluation of energy-saving potential for optimal time response of HVAC control system in smart buildings, Appl. Energy 271 (2020) 115255. doi:10.1016/j.apenergy.2020.115255. [32] S.M. Dawood A. Hatami R.Z. Homod Trade-off decisions in a novel deep reinforcement learning for energy savings in hvac systems J. Build. Performance Simulat. 15 6 2022 809 831 S.M. Dawood, A. Hatami, R.Z. Homod, Trade-off decisions in a novel deep reinforcement learning for energy savings in hvac systems, Journal of Building Performance Simulation 15 (6) (2022) 809\u2013831. [33] A. Nagabandi G. Kahn R.S. Fearing S. Levine Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning 2018 IEEE International Conference on Robotics and Automation (ICRA), IEEE 2018 IEEE Brisbane Convention Center 7559 7566 A. Nagabandi, G. Kahn, R.S. Fearing, S. Levine, Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning, in: 2018 IEEE International Conference on Robotics and Automation (ICRA), IEEE, IEEE, Brisbane Convention Center, 2018, pp. 7559\u20137566. [34] A. Nagabandi K. Konolige S. Levine V. Kumar Deep dynamics models for learning dexterous manipulation Conference on Robot Learning 2020 PMLR 1101 1112 A. Nagabandi, K. Konolige, S. Levine, V. Kumar, Deep dynamics models for learning dexterous manipulation, in: Conference on Robot Learning, PMLR, 2020, pp. 1101\u20131112. [35] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, Proximal policy optimization algorithm, [Online; accessed 4. Feb. 2022] (Aug 2017). URL: https://onikle.com/articles/211795. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, Proximal policy optimization algorithm, [Online; accessed 4. Feb. 2022] (Aug 2017). URL:https://onikle.com/articles/211795 [36] M. Wetter W. Zuo Modelica Buildings library J. Build. Perform. Simul. 7 4 2014 253 270 10.1080/19401493.2013.765506 M. Wetter, W. Zuo, et al., Modelica Buildings library, J. Build. Perform. Simul. 7 (4) (2014) 253\u2013270. doi:10.1080/19401493.2013.765506. [37] M. Wetter J. Wright A comparison of deterministic and probabilistic optimization algorithms for nonsmooth simulation-based optimization Build. Environ. 39 8 2004 989 999 10.1016/j.buildenv.2004.01.022 M. Wetter, J. Wright, A comparison of deterministic and probabilistic optimization algorithms for nonsmooth simulation-based optimization, Build. Environ. 39 (8) (2004) 989\u2013999. doi:10.1016/j.buildenv.2004.01.022. [38] T.P. Lillicrap, J.J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, D. Wierstra, Continuous control with deep reinforcement learning, arXiv preprint arXiv:1509.02971 0. T.P. Lillicrap, J.J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, D. Wierstra, Continuous control with deep reinforcement learning, arXiv preprint arXiv:1509.02971 0. [39] Y.M. Lee R. Horesh Optimal HVAC Control as Demand Response with On-site Energy Storage and Generation System Energy Procedia 78 2015 2106 2111 10.1016/j.egypro.2015.11.253 Y.M. Lee, R. Horesh, et al., Optimal HVAC Control as Demand Response with On-site Energy Storage and Generation System, Energy Procedia 78 (2015) 2106\u20132111. doi:10.1016/j.egypro.2015.11.253. [40] J. Le Dréau P. Heiselberg Energy flexibility of residential buildings using short term heat storage in the thermal mass Energy 111 2016 991 1002 10.1016/j.energy.2016.05.076 J. Le Dréau, P. Heiselberg, Energy flexibility of residential buildings using short term heat storage in the thermal mass, Energy 111 (2016) 991\u20131002. doi:10.1016/j.energy.2016.05.076. [41] H. Berlink, A.H. Costa, Batch reinforcement learning for smart home energy management, in: Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015. H. Berlink, A.H. Costa, Batch reinforcement learning for smart home energy management, in: Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015. [42] A. Afram F. Janabi-Sharifi Artificial neural network (ANN) based model predictive control (MPC) and optimization of HVAC systems: A state of the art review and case study of a residential HVAC system Energy Build. 141 2017 96 113 10.1016/j.enbuild.2017.02.012 A. Afram, F. Janabi-Sharifi, et al., Artificial neural network (ANN) based model predictive control (MPC) and optimization of HVAC systems: A state of the art review and case study of a residential HVAC system, Energy Build. 141 (2017) 96\u2013113. doi:10.1016/j.enbuild.2017.02.012. [43] J. Reynolds Y. Rezgui A zone-level, building energy optimisation combining an artificial neural network, a genetic algorithm, and model predictive control Energy 151 2018 729 739 10.1016/j.energy.2018.03.113 J. Reynolds, Y. Rezgui, et al., A zone-level, building energy optimisation combining an artificial neural network, a genetic algorithm, and model predictive control, Energy 151 (2018) 729\u2013739. doi:10.1016/j.energy.2018.03.113. [44] A.E. Ruano S. Pesteh The IMBPC HVAC system: A complete MBPC solution for existing HVAC systems Energy Build. 120 2016 145 158 10.1016/j.enbuild.2016.03.043 A.E. Ruano, S. Pesteh, et al., The IMBPC HVAC system: A complete MBPC solution for existing HVAC systems, Energy Build. 120 (2016) 145\u2013158. doi:10.1016/j.enbuild.2016.03.043. [45] J. Drgoňa D. Picard Approximate model predictive building control via machine learning Appl. Energy 218 2018 199 216 10.1016/j.apenergy.2018.02.156 J. Drgoňa, D. Picard, et al., Approximate model predictive building control via machine learning, Appl. Energy 218 (2018) 199\u2013216. doi:10.1016/j.apenergy.2018.02.156. [46] Stripping off the implementation complexity of physics-based model predictive control for buildings via deep learning, [Online; accessed 22. Sep. 2021] (Dec 2019). URL: https://www.climatechange.ai/papers/neurips2019/34.html. Stripping off the implementation complexity of physics-based model predictive control for buildings via deep learning, [Online; accessed 22. Sep. 2021] (Dec 2019). URL:https://www.climatechange.ai/papers/neurips2019/34.html [47] T. Hilliard L. Swan Experimental implementation of whole building MPC with zone based thermal comfort adjustments Build. Environ. 125 2017 326 338 10.1016/j.buildenv.2017.09.003 T. Hilliard, L. Swan, et al., Experimental implementation of whole building MPC with zone based thermal comfort adjustments, Build. Environ. 125 (2017) 326\u2013338. doi:10.1016/j.buildenv.2017.09.003. [48] A. Jain, M. Behl, et al., Data Predictive Control for Building Energy Management: Poster Abstract, in: BuildSys \u201916: Proceedings of the 3rd ACM International Conference on Systems for Energy-Efficient Built Environments, Association for Computing Machinery, New York, NY, USA, 2016, pp. 245\u2013246. doi:10.1145/2993422.2996410. A. Jain, M. Behl, et al., Data Predictive Control for Building Energy Management: Poster Abstract, in: BuildSys \u201916: Proceedings of the 3rd ACM International Conference on Systems for Energy-Efficient Built Environments, Association for Computing Machinery, New York, NY, USA, 2016, pp. 245\u2013246. doi:10.1145/2993422.2996410. [49] A. Jain, D. Nong, T.X. Nghiem, R. Mangharam, Digital twins for efficient modeling and control of buildings: An integrated solution with scada systems, in: 2018 Building Performance Analysis Conference and SimBuild, 2018. A. Jain, D. Nong, T.X. Nghiem, R. Mangharam, Digital twins for efficient modeling and control of buildings: An integrated solution with scada systems, in: 2018 Building Performance Analysis Conference and SimBuild, 2018. [50] G.D. Kontes G.I. Giannakis V. Sánchez D. Agustin-Camacho A. Romero-Amorrortu N. Panagiotidou D.V. Rovas S. Steiger C. Mutschler G. Gruen Simulation-based evaluation and optimization of control strategies in buildings Energies 11 12 2018 3376 G.D. Kontes, G.I. Giannakis, V. Sánchez, D. Agustin-Camacho, A. Romero-Amorrortu, N. Panagiotidou, D.V. Rovas, S. Steiger, C. Mutschler, G. Gruen, et al., Simulation-based evaluation and optimization of control strategies in buildings, Energies 11 (12) (2018) 3376. [51] Z.-H. Zhou Ensemble methods: foundations and algorithms 2019 Chapman and Hall/CRC Z.-H. Zhou, Ensemble methods: foundations and algorithms, Chapman and Hall/CRC, 2019. [52] G.T. Costanzo S. Iacovella Experimental analysis of data-driven control for a building heating system, Sustainable Energy Grids Networks 6 2016 81 90 10.1016/j.segan.2016.02.002 G.T. Costanzo, S. Iacovella, et al., Experimental analysis of data-driven control for a building heating system, Sustainable Energy Grids Networks 6 (2016) 81\u201390. doi:10.1016/j.segan.2016.02.002. [53] E. Mocanu D.C. Mocanu P.H. Nguyen A. Liotta M.E. Webber M. Gibescu J.G. Slootweg On-line building energy optimization using deep reinforcement learning IEEE Trans. Smart Grid 10 4 2018 3698 3708 E. Mocanu, D.C. Mocanu, P.H. Nguyen, A. Liotta, M.E. Webber, M. Gibescu, J.G. Slootweg, On-line building energy optimization using deep reinforcement learning, IEEE transactions on smart grid 10 (4) (2018) 3698\u20133708. [54] L. Yu S. Qin M. Zhang C. Shen T. Jiang X. Guan A Review of Deep Reinforcement Learning for Smart Building Energy Management IEEE IoT J. 8 15 2021 12046 12063 10.1109/JIOT.2021.3078462 L. Yu, S. Qin, M. Zhang, C. Shen, T. Jiang, X. Guan, A Review of Deep Reinforcement Learning for Smart Building Energy Management, IEEE IoT J. 8 (15) (2021) 12046\u201312063. doi:10.1109/JIOT.2021.3078462. [55] L. Yu Y. Sun Z. Xu C. Shen D. Yue T. Jiang X. Guan Multi-Agent Deep Reinforcement Learning for HVAC Control in Commercial Buildings IEEE Trans. Smart Grid 12 1 2020 407 419 10.1109/TSG.2020.3011739 L. Yu, Y. Sun, Z. Xu, C. Shen, D. Yue, T. Jiang, X. Guan, Multi-Agent Deep Reinforcement Learning for HVAC Control in Commercial Buildings, IEEE Trans. Smart Grid 12 (1) (2020) 407\u2013419. doi:10.1109/TSG.2020.3011739. [56] R.Z. Homod, H. Togun, A.K. Hussein, N. Al-Mousawi, O.A. Hussein, Dynamics analysis of a novel hybrid deep clustering for unsupervised learning by reinforcement of multi-agent to energy saving in intelligent buildings, Appl. Energy 313 (9). doi:10.1016/j.apenergy.2022.118863. R.Z. Homod, H. Togun, A.K. Hussein, N. Al-Mousawi, O.A. Hussein, Dynamics analysis of a novel hybrid deep clustering for unsupervised learning by reinforcement of multi-agent to energy saving in intelligent buildings, Appl. Energy 313 (9). doi:10.1016/j.apenergy.2022.118863. [57] M.B. Ring Continual learning in reinforcement environments 1994 University of Texas at Austin Austin Texas 78712 Ph.D. thesis M.B. Ring, Continual learning in reinforcement environments, Ph.D. thesis, University of Texas at Austin Austin, Texas 78712 (1994). [58] A. Naug, M. Q\u2019uiñones-Grueiro, et al., A Relearning Approach to Reinforcement Learning for control of Smart Buildings, PHM _ CONF 12 (1) (2020) 14. doi:10.36001/phmconf.2020.v12i1.1296. A. Naug, M. Q\u2019uiñones-Grueiro, et al., A Relearning Approach to Reinforcement Learning for control of Smart Buildings, PHM_CONF 12 (1) (2020) 14. doi:10.36001/phmconf.2020.v12i1.1296. [59] A. Naug, M. Quinones-Grueiro, et al., Sensitivity and robustness of end-to-end data-driven approach for building performance optimization, in: BuildSys \u201921: Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, Association for Computing Machinery, New York, NY, USA, 2021, pp. 314\u2013318. doi:10.1145/3486611.3488728. A. Naug, M. Quinones-Grueiro, et al., Sensitivity and robustness of end-to-end data-driven approach for building performance optimization, in: BuildSys \u201921: Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, Association for Computing Machinery, New York, NY, USA, 2021, pp. 314\u2013318. doi:10.1145/3486611.3488728. [60] E. Lecarpentier, E. Rachelson, Non-Stationary Markov Decision Processes a Worst-Case Approach using Model-Based Reinforcement Learning, in: Advances in Neural Information Processing Systems, 2019, pp. 7214\u20137223. E. Lecarpentier, E. Rachelson, Non-Stationary Markov Decision Processes a Worst-Case Approach using Model-Based Reinforcement Learning, in: Advances in Neural Information Processing Systems, 2019, pp. 7214\u20137223. [61] A.C. Bryhn P.H. Dimberg An operational definition of a statistically meaningful trend PLoS One 6 4 2011 e19241 A.C. Bryhn, P.H. Dimberg, An operational definition of a statistically meaningful trend, PLoS One 6 (4) (2011) e19241. [62] D. Kouzoupis G. Frison A. Zanelli M. Diehl Recent advances in quadratic programming algorithms for nonlinear model predictive control Vietnam J. Math. 46 4 2018 863 882 D. Kouzoupis, G. Frison, A. Zanelli, M. Diehl, Recent advances in quadratic programming algorithms for nonlinear model predictive control, Vietnam Journal of Mathematics 46 (4) (2018) 863\u2013882. [63] U. Rosolia F. Borrelli Learning model predictive control for iterative tasks. a data-driven control framework IEEE Trans. Autom. Control 63 7 2017 1883 1896 U. Rosolia, F. Borrelli, Learning model predictive control for iterative tasks. a data-driven control framework, IEEE Transactions on Automatic Control 63 (7) (2017) 1883\u20131896. [64] A. Naug, Deep learning methods applied to modeling and policy optimization in large buildings, Ph.D. thesis (May 2022). URL: https://ir.vanderbilt.edu/handle/1803/17367. A. Naug, Deep learning methods applied to modeling and policy optimization in large buildings, Ph.D. thesis (May 2022). URL:https://ir.vanderbilt.edu/handle/1803/17367 [65] J. Kirkpatrick R. Pascanu Overcoming catastrophic forgetting in neural networks Proc. Natl. Acad. Sci. U.S.A. 114 13 2017 3521 3526 10.1073/pnas.1611835114 J. Kirkpatrick, R. Pascanu, et al., Overcoming catastrophic forgetting in neural networks, Proc. Natl. Acad. Sci. U.S.A. 114 (13) (2017) 3521\u20133526. doi:10.1073/pnas.1611835114. [66] R.J. Williams Simple statistical gradient-following algorithms for connectionist reinforcement learning Mach. Learn. 8 3 1992 229 256 10.1007/BF00992696 URL:https://doi.org/10.1007/BF00992696 R.J. Williams, Simple statistical gradient-following algorithms for connectionist reinforcement learning, Machine Learning 8 (3) (1992) 229\u2013256. doi:10.1007/BF00992696. URL:https://doi.org/10.1007/BF00992696 [67] I. Grondman L. Busoniu G.A.D. Lopes R. Babuska A survey of actor-critic reinforcement learning: Standard and natural policy gradients IEEE Trans. Syst., Man, Cybernet. Part C (Appl. Rev.) 42 6 2012 1291 1307 10.1109/TSMCC.2012.2218595 I. Grondman, L. Busoniu, G.A.D. Lopes, R. Babuska, A survey of actor-critic reinforcement learning: Standard and natural policy gradients, IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42 (6) (2012) 1291\u20131307. doi:10.1109/TSMCC.2012.2218595. [68] J. Schulman, F. Wolski, et al., Proximal Policy Optimization Algorithms, arXiv arXiv:1707.06347. URL: https://arxiv.org/abs/1707.06347v2. J. Schulman, F. Wolski, et al., Proximal Policy Optimization Algorithms, arXiv arXiv:1707.06347. URL:https://arxiv.org/abs/1707.06347v2 [69] A. Kumar, J. Fu, et al., Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction, arXiv arXiv:1906.00949. URL: https://arxiv.org/abs/1906.00949v2. A. Kumar, J. Fu, et al., Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction, arXiv arXiv:1906.00949. URL:https://arxiv.org/abs/1906.00949v2 [70] Buildings.Examples.VAVReheat, [Online; accessed 15. Jul. 2021] (Jun 2021). URL: https://simulationresearch.lbl.gov/modelica/releases/v8.0.0/help/Buildings_Examples_VAVReheat.html#Buildings.Examples.VAVReheat. Buildings.Examples.VAVReheat, [Online; accessed 15. Jul. 2021] (Jun 2021). URL:https://simulationresearch.lbl.gov/modelica/releases/v8.0.0/help/Buildings_Examples_VAVReheat.html#Buildings.Examples.VAVReheat [71] lbl srg, modelica-buildings, [Online; accessed 15. Jul. 2021] (Jul 2021). URL: https://github.com/lbl-srg/modelica-buildings. lbl srg, modelica-buildings, [Online; accessed 15. Jul. 2021] (Jul 2021). URL:https://github.com/lbl-srg/modelica-buildings [72] V.R. Konda, J.N. Tsitsiklis, Actor-citic agorithms, in: NIPS\u201999: Proceedings of the 12th International Conference on Neural Information Processing Systems, MIT Press, Cambridge, MA, USA, 1999, pp. 1008\u20131014. doi:10.5555/3009657.3009799. V.R. Konda, J.N. Tsitsiklis, Actor-citic agorithms, in: NIPS\u201999: Proceedings of the 12th International Conference on Neural Information Processing Systems, MIT Press, Cambridge, MA, USA, 1999, pp. 1008\u20131014. doi:10.5555/3009657.3009799. [73] A. Zaytar, C.E. Amrani, Sequence to Sequence Weather Forecasting with Long Short-Term Memory Recurrent Neural Networks, undefined. URL: https://www.semanticscholar.org/paper/Sequence-to-Sequence-Weather-Forecasting-with-Long-Zaytar-Amrani/67cee70dd5ca40e259dd0df1ed599ef2686f20d5. A. Zaytar, C.E. Amrani, Sequence to Sequence Weather Forecasting with Long Short-Term Memory Recurrent Neural Networks, undefined. URL:https://www.semanticscholar.org/paper/Sequence-to-Sequence-Weather-Forecasting-with-Long-Zaytar-Amrani/67cee70dd5ca40e259dd0df1ed599ef2686f20d5 [74] X. Ding, W. Du, A.E. Cerpa, MB2C: Model-Based Deep Reinforcement Learning for Multi-zone Building Control, in: BuildSys \u201920: Proceedings of the 7th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, Association for Computing Machinery, New York, NY, USA, 2020, pp. 50\u201359. doi:10.1145/3408308.3427986. X. Ding, W. Du, A.E. Cerpa, MB2C: Model-Based Deep Reinforcement Learning for Multi-zone Building Control, in: BuildSys \u201920: Proceedings of the 7th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, Association for Computing Machinery, New York, NY, USA, 2020, pp. 50\u201359. doi:10.1145/3408308.3427986. [75] Z. Karevan J.A.K. Suykens Transductive LSTM for time-series prediction: An application to weather forecasting Neural Networks 125 2020 1 9 10.1016/j.neunet.2019.12.030 Z. Karevan, J.A.K. Suykens, Transductive LSTM for time-series prediction: An application to weather forecasting, Neural Networks 125 (2020) 1\u20139. doi:10.1016/j.neunet.2019.12.030. [76] G. Wei, W.D. Turner, D.E. Claridge, M. Liu, Single-Duct Constant Air Volume System Supply Air Temperature Reset: Using Return Air Temperature or Outside Air Temperature?, Taylor & Francis, 2003. G. Wei, W.D. Turner, D.E. Claridge, M. Liu, Single-Duct Constant Air Volume System Supply Air Temperature Reset: Using Return Air Temperature or Outside Air Temperature?, Taylor & Francis, 2003. [77] A. Naug I. Ahmed G. Biswas Online energy management in commercial buildings using deep reinforcement learning Proceedings \u2013 2019 IEEE International Conference on Smart Computing, SMARTCOMP 2019 2019 Institute of Electrical and Electronics Engineers Inc. 249 257 10.1109/SMARTCOMP.2019.00060 A. Naug, I. Ahmed, G. Biswas, Online energy management in commercial buildings using deep reinforcement learning, in: Proceedings - 2019 IEEE International Conference on Smart Computing, SMARTCOMP 2019, Institute of Electrical and Electronics Engineers Inc., 2019, pp. 249\u2013257. doi:10.1109/SMARTCOMP.2019.00060. [78] R. Liaw, E. Liang, R. Nishihara, P. Moritz, J.E. Gonzalez, I. Stoica, Tune: A research platform for distributed model selection and training, arXiv preprint arXiv:1807.05118. R. Liaw, E. Liang, R. Nishihara, P. Moritz, J.E. Gonzalez, I. Stoica, Tune: A research platform for distributed model selection and training, arXiv preprint arXiv:1807.05118. [79] A ten-minute introduction to sequence-to-sequence learning in Keras, [Online; accessed 27. Feb. 2022] (Sep 2020). URL: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html. A ten-minute introduction to sequence-to-sequence learning in Keras, [Online; accessed 27. Feb. 2022] (Sep 2020). URL:https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html",
    "scopus-id": "85143766318",
    "coredata": {
        "eid": "1-s2.0-S0378778822007551",
        "dc:description": "Developing an optimal supervisory control policy for building energy management is a complex problem because the system exhibits non-stationary behaviors, and the target policy needs to evolve with changes in the state transition and reward functions. Non-stationary real-world problems often present a set of challenges: non-stationary changes are difficult to detect; and thermodynamic systems with larger time-constants can create sample-inefficiency problems for learning algorithms. In addition, the system may have to satisfy safety\u2013critical constraints, and, therefore, the policy must be learned offline unless actuation rules are correctly designed. To address these challenges, we propose a data-driven deep reinforcement learning framework. A reinforcement learning supervisory controller is firstly developed and deployed on the building for the heating, ventilation and air conditioning (HVAC) system and monitored for performance degradation by tracking an aggregate metric. When degradation is detected, a relearning loop is triggered. Then, a set of data-driven models of the building behavior is updated with the latest real data. Subsequently, the deployed controller is re-tuned by letting it interact with the model and is then redeployed on the system. Our proposed approach is demonstrated extensively on the standard ASHRAE 5-zone testbed and a real building. It is benchmarked against state-of-the-art algorithms in building supervisory control: Guideline-36, Proximal Policy Optimization, Deep Deterministic Policy Gradient, and Model Predictive Path Integral control. Our approach performs significantly better than the previously mentioned supervisory control strategies and highlights the need for a condition-based offline relearning framework in dynamic systems.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2022-12-15",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S0378778822007551",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Naug, Avisek"
            },
            {
                "@_fa": "true",
                "$": "Quinones-Grueiro, Marcos"
            },
            {
                "@_fa": "true",
                "$": "Biswas, Gautam"
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S0378778822007551"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S0378778822007551"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S0378-7788(22)00755-1",
        "prism:volume": "277",
        "articleNumber": "112584",
        "prism:publisher": "Elsevier B.V.",
        "dc:title": "Deep reinforcement learning control for non-stationary building energy management",
        "prism:copyright": "© 2022 Elsevier B.V. All rights reserved.",
        "openaccess": "0",
        "prism:issn": "03787788",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "Smart building management"
            },
            {
                "@_fa": "true",
                "$": "Deep reinforcement learning"
            },
            {
                "@_fa": "true",
                "$": "Non-stationary systems"
            },
            {
                "@_fa": "true",
                "$": "Continuous adaptation"
            }
        ],
        "openaccessArticle": "false",
        "prism:publicationName": "Energy and Buildings",
        "openaccessSponsorType": null,
        "prism:pageRange": "112584",
        "pubType": "fla",
        "prism:coverDisplayDate": "15 December 2022",
        "prism:doi": "10.1016/j.enbuild.2022.112584",
        "prism:startingPage": "112584",
        "dc:identifier": "doi:10.1016/j.enbuild.2022.112584",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "267",
            "@width": "539",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "35591",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "373",
            "@width": "539",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "63469",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "354",
            "@width": "539",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "44284",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "505",
            "@width": "538",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "91372",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "340",
            "@width": "539",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "51945",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "243",
            "@width": "539",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "46437",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "205",
            "@width": "539",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr8.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "31198",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "225",
            "@width": "539",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr9.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "38594",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "482",
            "@width": "778",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "103384",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "109",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "6622",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "152",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "9426",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "144",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7682",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "175",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "11769",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "138",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "9890",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "99",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "9800",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "83",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr8.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7064",
            "@ref": "gr8",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "91",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr9.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "9074",
            "@ref": "gr9",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "136",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "10540",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "1183",
            "@width": "2387",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "292724",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1652",
            "@width": "2387",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "444192",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1567",
            "@width": "2387",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "377239",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2234",
            "@width": "2382",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "777128",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1504",
            "@width": "2387",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "385656",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1076",
            "@width": "2387",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "427822",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "909",
            "@width": "2387",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr8_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "291901",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "996",
            "@width": "2387",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr9_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "340773",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2134",
            "@width": "3445",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "900907",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778822007551-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "1746345",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85143766318"
    }
}}