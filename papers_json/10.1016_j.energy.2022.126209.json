{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85142904710",
    "originalText": "serial JL 271090 291210 291702 291711 291731 291877 291878 31 Energy ENERGY 2022-11-24 2022-11-24 2022-11-28 2022-11-28 2023-02-27T14:58:19 1-s2.0-S036054422203095X S0360-5442(22)03095-X S036054422203095X 10.1016/j.energy.2022.126209 S300 S300.1 FULL-TEXT 1-s2.0-S0360544222X00339 2023-02-27T21:48:39.521686Z 0 0 20230201 2023 2022-11-24T17:25:56.268986Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor grantsponsorid highlightsabst primabst ref 0360-5442 03605442 true 264 264 C Volume 264 23 126209 126209 126209 20230201 1 February 2023 2023-02-01 2023 Full Length Articles article fla © 2022 Elsevier Ltd. All rights reserved. ENERGYEFFICIENTHEATINGCONTROLFORNEARLYZEROENERGYRESIDENTIALBUILDINGSDEEPREINFORCEMENTLEARNING QIN H 1 Introduction 1.1 Optimal control of HVAC systems 1.2 Reinforcement learning applied to HVAC 1.3 Motivation and structure of this research 2 Methodology 2.1 Reinforcement learning algorithms 2.2 Model-free controller diagram 2.3 Configuration and initialization of two Q-networks 2.3.1 State 2.3.2 Action 2.3.3 Reward 2.4 Decision-making 2.5 Action processor 2.6 Hyper-parameters 3 Case study 3.1 Case system 3.2 Simulation system model based on measured data 3.3 Realization details of proposed method and the comparative control method 3.3.1 Proposed method 3.3.2 Rule-based method 4 Results and discussion 4.1 Randomness of the model-free controller 4.2 Learning process of the model-free controller 4.3 Performance of thermal comfort and energy saving 4.4 Performance of actual deployment in physical world 4.5 Future work 5 Conclusion Credit author statement Acknowledgment References ATTIA 2017 439 458 S WANG 2008 3 32 S DARAEI 2019 1111 1127 M BISCHI 2014 12 26 A FERREIRA 2012 238 251 P HUANG 2015 203 216 H KUSIAK 2010 3092 3102 A GARNIER 2015 847 862 A KIM 2016 666 674 W KILLIAN 2016 403 412 M PRIVARA 2013 8 22 S CANNON 2004 229 237 M CRAWLEY 2008 661 673 D MAGALHAES 2017 S MODELLINGRELATIONSHIPBETWEENHEATINGENERGYUSEINDOORTEMPERATURESINRESIDENTIALBUILDINGSTHROUGHARTIFICIALNEURALNETWORKSCONSIDERINGOCCUPANTBEHAVIORJ AFRAM 2017 96 113 A DONG 2005 545 553 B CHEN 2017 659 670 Y QIU 2019 490 500 S QIU 2018 S LI 2013 307 312 D OPTIMIZATIONHEATTREATMENTPROCESSVACUUMDIECASTINGAT72MAGNESIUMALLOYC MO 2019 109564 H FENG 2021 116814 Y BIEMANN 2021 117164 M HAN 2021 137 148 M DU 2020 106959 Y BRANDI 2020 110225 S QIU 2020 110055 S JIANG 2021 110833 Z BIEMANN 2021 117164 M HENZE 2003 259 275 G LIU 2007 S EVALUATIONREINFORCEMENTLEARNINGFOROPTIMALCONTROLBUILDINGACTIVEPASSIVETHERMALSTORAGEINVENTORYJ ZOU 2020 106535 Z SUTTON 2018 R REINFORCEMENTLEARNINGANINTRODUCTIONM CHEN 2018 195 205 Y LIU 2018 1616 1625 T LIU 2018 544 555 T DENG 2021 110860 Z YUAN 2018 31 36 W LI 2017 Y DEEPREINFORCEMENTLEARNINGOVERVIEWJ VOLODYMYR 2019 529 533 M HASSELT 2015 H DEEPREINFORCEMENTLEARNINGDOUBLEQLEARNINGJ WANG 2016 1995 2003 Z DUELINGNETWORKARCHITECTURESFORDEEPREINFORCEMENTLEARNINGCINTERNATIONALCONFERENCEMACHINELEARNING SCHAUL 2015 T PRIORITIZEDEXPERIENCEREPLAYJ ZOU 2021 120174 R JIANG 2020 265 279 J SMITH 2017 S DONTDECAYLEARNINGRATE AFRAM 2017 96 113 A MAGALHAES 2017 332 343 S QIN 2022 14137 H ZHANG 2020 X RESEARCHDETERMINATIONMETHODOUTDOORDESIGNCONDITIONSSUNSHADINGDESIGND HERSBACH 2018 H ERA5HOURLYDATASINGLELEVELS1979PRESENTCOPERNICUSCLIMATECHANGESERVICEC3SCLIMATEDATASTORECDS QINX2023X126209 QINX2023X126209XH 2024-11-28T00:00:00.000Z 2024-11-28T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2022 Elsevier Ltd. All rights reserved. 2022-12-03T03:40:59.190Z http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/aggregated-refined 0 https://doi.org/10.15223/policy-017 https://doi.org/10.15223/policy-037 https://doi.org/10.15223/policy-012 https://doi.org/10.15223/policy-029 https://doi.org/10.15223/policy-004 item S0360-5442(22)03095-X S036054422203095X 1-s2.0-S036054422203095X 10.1016/j.energy.2022.126209 271090 2023-02-27T21:48:39.521686Z 2023-02-01 1-s2.0-S036054422203095X-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/MAIN/application/pdf/f50c751a57b0bc88d292d71dd2e7d2f0/main.pdf main.pdf pdf true 9090993 MAIN 12 1-s2.0-S036054422203095X-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/PREVIEW/image/png/dbf4aea8d510c2a3bb8f43d1b3694fe6/main_1.png main_1.png png 53849 849 656 IMAGE-WEB-PDF 1 1-s2.0-S036054422203095X-gr14.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr14/DOWNSAMPLED/image/jpeg/fc9eee7bc2b33ae021105023aabd9fc6/gr14.jpg gr14 gr14.jpg jpg 99940 258 386 IMAGE-DOWNSAMPLED 1-s2.0-S036054422203095X-gr13.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr13/DOWNSAMPLED/image/jpeg/cf35cd855b492f295e4dedaa94d97f39/gr13.jpg gr13 gr13.jpg jpg 110692 275 388 IMAGE-DOWNSAMPLED 1-s2.0-S036054422203095X-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr9/DOWNSAMPLED/image/jpeg/9106f67a3fc359bb4c7416049bc19467/gr9.jpg gr9 gr9.jpg jpg 113462 301 388 IMAGE-DOWNSAMPLED 1-s2.0-S036054422203095X-gr12.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr12/DOWNSAMPLED/image/jpeg/5a09aacd06ef610574214afe45534ec5/gr12.jpg gr12 gr12.jpg jpg 120323 269 388 IMAGE-DOWNSAMPLED 1-s2.0-S036054422203095X-gr11.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr11/DOWNSAMPLED/image/jpeg/ab3d43979a45825bc059aaabb8d6cbb5/gr11.jpg gr11 gr11.jpg jpg 94208 261 387 IMAGE-DOWNSAMPLED 1-s2.0-S036054422203095X-gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr10/DOWNSAMPLED/image/jpeg/71ab8f0f31982572abd62c1582b6cfd0/gr10.jpg gr10 gr10.jpg jpg 202050 583 811 IMAGE-DOWNSAMPLED 1-s2.0-S036054422203095X-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr6/DOWNSAMPLED/image/jpeg/af83f2508dac6fe64a54b60e660fcdf1/gr6.jpg gr6 gr6.jpg jpg 146392 359 535 IMAGE-DOWNSAMPLED 1-s2.0-S036054422203095X-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr5/DOWNSAMPLED/image/jpeg/f2b242ddb4c7c7bf20d401014985b4cb/gr5.jpg gr5 gr5.jpg jpg 138851 361 811 IMAGE-DOWNSAMPLED 1-s2.0-S036054422203095X-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr8/DOWNSAMPLED/image/jpeg/b9dc2dea53b23f783ceb0ba69d8fced4/gr8.jpg gr8 gr8.jpg jpg 81872 283 388 IMAGE-DOWNSAMPLED 1-s2.0-S036054422203095X-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr7/DOWNSAMPLED/image/jpeg/a10dd8dc5ca4bd92c35df33e18d96ed3/gr7.jpg gr7 gr7.jpg jpg 137621 323 811 IMAGE-DOWNSAMPLED 1-s2.0-S036054422203095X-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr2/DOWNSAMPLED/image/jpeg/656d4f04e639c41636707a93ee26faaa/gr2.jpg gr2 gr2.jpg jpg 92197 321 388 IMAGE-DOWNSAMPLED 1-s2.0-S036054422203095X-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr1/DOWNSAMPLED/image/jpeg/47ddf81da8d2d9d023aea41e0b0b5e2c/gr1.jpg gr1 gr1.jpg jpg 173953 525 811 IMAGE-DOWNSAMPLED 1-s2.0-S036054422203095X-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr4/DOWNSAMPLED/image/jpeg/832f93e41f56779a9af9bcde90da8941/gr4.jpg gr4 gr4.jpg jpg 143826 401 535 IMAGE-DOWNSAMPLED 1-s2.0-S036054422203095X-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr3/DOWNSAMPLED/image/jpeg/bfd2f311f97aaf475c80c89d16f0acdb/gr3.jpg gr3 gr3.jpg jpg 89527 158 811 IMAGE-DOWNSAMPLED 1-s2.0-S036054422203095X-gr14.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr14/THUMBNAIL/image/gif/a56fe26cba8e8fd1297ca92314223c37/gr14.sml gr14 gr14.sml sml 74943 147 219 IMAGE-THUMBNAIL 1-s2.0-S036054422203095X-gr13.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr13/THUMBNAIL/image/gif/c916bb343d50ee7518de361b8fd7c3a5/gr13.sml gr13 gr13.sml sml 76758 155 219 IMAGE-THUMBNAIL 1-s2.0-S036054422203095X-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr9/THUMBNAIL/image/gif/6b33965c0b1893f1ff3eaf27ef7d4a94/gr9.sml gr9 gr9.sml sml 77897 164 211 IMAGE-THUMBNAIL 1-s2.0-S036054422203095X-gr12.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr12/THUMBNAIL/image/gif/78d189e6e7eb65d8a2930827d4a5b42c/gr12.sml gr12 gr12.sml sml 80044 152 219 IMAGE-THUMBNAIL 1-s2.0-S036054422203095X-gr11.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr11/THUMBNAIL/image/gif/210a3c27f655084560535a1985f0cc11/gr11.sml gr11 gr11.sml sml 73946 148 219 IMAGE-THUMBNAIL 1-s2.0-S036054422203095X-gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr10/THUMBNAIL/image/gif/1c1f46a277f6f96a129829480161cb3c/gr10.sml gr10 gr10.sml sml 79926 157 219 IMAGE-THUMBNAIL 1-s2.0-S036054422203095X-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr6/THUMBNAIL/image/gif/d6ae3fa8a171cf41107d7213ec19ad48/gr6.sml gr6 gr6.sml sml 82172 147 219 IMAGE-THUMBNAIL 1-s2.0-S036054422203095X-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr5/THUMBNAIL/image/gif/bdbf97ab7e71f21a756201b888b3ed20/gr5.sml gr5 gr5.sml sml 75513 97 219 IMAGE-THUMBNAIL 1-s2.0-S036054422203095X-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr8/THUMBNAIL/image/gif/ab03d172d0d62fde233eb16db35c3813/gr8.sml gr8 gr8.sml sml 70528 160 219 IMAGE-THUMBNAIL 1-s2.0-S036054422203095X-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr7/THUMBNAIL/image/gif/b831992a4103915337cb9ad01f1c444b/gr7.sml gr7 gr7.sml sml 72459 87 219 IMAGE-THUMBNAIL 1-s2.0-S036054422203095X-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr2/THUMBNAIL/image/gif/5916753ef05330c1dbb113f99afe4098/gr2.sml gr2 gr2.sml sml 72215 164 198 IMAGE-THUMBNAIL 1-s2.0-S036054422203095X-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr1/THUMBNAIL/image/gif/b59291e833cd7e576cc228cab5a8de4d/gr1.sml gr1 gr1.sml sml 77721 142 219 IMAGE-THUMBNAIL 1-s2.0-S036054422203095X-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr4/THUMBNAIL/image/gif/055772caf7009e086a570fdc5ecadb6e/gr4.sml gr4 gr4.sml sml 96414 164 219 IMAGE-THUMBNAIL 1-s2.0-S036054422203095X-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/gr3/THUMBNAIL/image/gif/3070df217a650f4fb7a632333db79ad8/gr3.sml gr3 gr3.sml sml 68770 43 219 IMAGE-THUMBNAIL 1-s2.0-S036054422203095X-gr14_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/HIGHRES/image/jpeg/ad1e635723a89c64e02da9aae0b875ad/gr14_lrg.jpg gr14 gr14_lrg.jpg jpg 296081 1144 1710 IMAGE-HIGH-RES 1-s2.0-S036054422203095X-gr13_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/HIGHRES/image/jpeg/dc3e078b46eb08fa9195a80b7176fbe0/gr13_lrg.jpg gr13 gr13_lrg.jpg jpg 377117 1220 1721 IMAGE-HIGH-RES 1-s2.0-S036054422203095X-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/HIGHRES/image/jpeg/1a3666806ceb4c324d05b42fff587ded/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 414059 1335 1721 IMAGE-HIGH-RES 1-s2.0-S036054422203095X-gr12_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/HIGHRES/image/jpeg/7404126debcf986e4e3938225d9e0183/gr12_lrg.jpg gr12 gr12_lrg.jpg jpg 430121 1195 1721 IMAGE-HIGH-RES 1-s2.0-S036054422203095X-gr11_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/HIGHRES/image/jpeg/79a6095d974074dd54cb02e905df664a/gr11_lrg.jpg gr11 gr11_lrg.jpg jpg 245845 1158 1715 IMAGE-HIGH-RES 1-s2.0-S036054422203095X-gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/HIGHRES/image/jpeg/c629a71e837a83dbcd16a685e4e72c7d/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 1300775 2583 3592 IMAGE-HIGH-RES 1-s2.0-S036054422203095X-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/HIGHRES/image/jpeg/c5437ee001cfa8423b4e492e09f7194a/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 804162 1592 2371 IMAGE-HIGH-RES 1-s2.0-S036054422203095X-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/HIGHRES/image/jpeg/a4679441b92758c506c3ba6b4b17efd4/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 594547 1597 3591 IMAGE-HIGH-RES 1-s2.0-S036054422203095X-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/HIGHRES/image/jpeg/c6f42a5eb7f092b9b6f49f6ec3aa0fba/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 170579 1257 1721 IMAGE-HIGH-RES 1-s2.0-S036054422203095X-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/HIGHRES/image/jpeg/24523ce6dc05d08980070cfd00ebd2b7/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 544024 1431 3591 IMAGE-HIGH-RES 1-s2.0-S036054422203095X-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/HIGHRES/image/jpeg/4d289ebc42c2ec5cda70056cfad93969/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 231554 1423 1721 IMAGE-HIGH-RES 1-s2.0-S036054422203095X-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/HIGHRES/image/jpeg/7cb2a3aebcc54b620ef7f172c731f3e8/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 731228 2325 3591 IMAGE-HIGH-RES 1-s2.0-S036054422203095X-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/HIGHRES/image/jpeg/2dc6a978edb9cedf8f80c1a6ce1c2894/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 943705 1778 2371 IMAGE-HIGH-RES 1-s2.0-S036054422203095X-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/HIGHRES/image/jpeg/021b9daa9ea7345d402614ea9cd06f88/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 278160 701 3591 IMAGE-HIGH-RES 1-s2.0-S036054422203095X-si4.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/image/svg+xml/44c360ab5403ecf4bede2f7d29486e5e/si4.svg si4 si4.svg svg 58717 ALTIMG 1-s2.0-S036054422203095X-si7.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/image/svg+xml/2fdcc1cefc0461bce29b4eecea97259d/si7.svg si7 si7.svg svg 47960 ALTIMG 1-s2.0-S036054422203095X-si8.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/image/svg+xml/586345b69d50adf77032ef5a5a39cf3c/si8.svg si8 si8.svg svg 9788 ALTIMG 1-s2.0-S036054422203095X-si6.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/image/svg+xml/45ff89a1449d9c0c90b7755b7271d344/si6.svg si6 si6.svg svg 115209 ALTIMG 1-s2.0-S036054422203095X-si13.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/image/svg+xml/24d1f2f5ac1c6a8f6778fd53909ef9dd/si13.svg si13 si13.svg svg 133781 ALTIMG 1-s2.0-S036054422203095X-si1.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/image/svg+xml/99c333750362f103bc87d07a95512b08/si1.svg si1 si1.svg svg 30077 ALTIMG 1-s2.0-S036054422203095X-si10.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/image/svg+xml/8c86643c1c8bae7f6a5e76c8c100d430/si10.svg si10 si10.svg svg 58622 ALTIMG 1-s2.0-S036054422203095X-si12.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/image/svg+xml/57afe4790cea5fb91d7a62777283763d/si12.svg si12 si12.svg svg 102317 ALTIMG 1-s2.0-S036054422203095X-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/image/svg+xml/4cf3da6800922dd7ab38025879c70ec2/si3.svg si3 si3.svg svg 13224 ALTIMG 1-s2.0-S036054422203095X-si2.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/image/svg+xml/4cb70b991c9215041b08857386f47c8e/si2.svg si2 si2.svg svg 7281 ALTIMG 1-s2.0-S036054422203095X-si9.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/image/svg+xml/c105c2b3e928d6adb0987bdc461b4a6e/si9.svg si9 si9.svg svg 11744 ALTIMG 1-s2.0-S036054422203095X-si11.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/image/svg+xml/d0f2ef617faebbfa1858bfba4b258f03/si11.svg si11 si11.svg svg 53326 ALTIMG 1-s2.0-S036054422203095X-si5.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S036054422203095X/image/svg+xml/07b92db060968c216c1915a554b1eb2b/si5.svg si5 si5.svg svg 123231 ALTIMG 1-s2.0-S036054422203095X-am.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:10V90GN1TBB/MAIN/application/pdf/5c4745c143d6a797a2b4848b913fe62d/am.pdf am am.pdf pdf false 2508929 AAM-PDF EGY 126209 126209 S0360-5442(22)03095-X 10.1016/j.energy.2022.126209 Elsevier Ltd Fig. 1 Model-free controller for optimized control of HVAC. Fig. 1 Fig. 2 Thermal comfort reward varies with indoor temperature. Fig. 2 Fig. 3 Action processor in the proposed approach. Fig. 3 Fig. 4 The future architecture laboratory of the China academy of building research. Fig. 4 Fig. 5 HVAC system diagram. Fig. 5 Fig. 6 Historical data for controlled buildings winter 2020. Fig. 6 Fig. 7 Accuracy validation of room temperature model and HVAC energy consumption model. Fig. 7 Fig. 8 Staff presence rate for rule-based control method. Fig. 8 Fig. 9 Evolution of five rounds. Fig. 9 Fig. 10 Comprehensive rewards and accumulated comprehensive rewards of learning processes. Fig. 10 Fig. 11 Model-free controller temperature distribution. Fig. 11 Fig. 12 Simulation results based on model-free controller. Fig. 12 Fig. 13 During the test period, Outdoor temperature and relative humidity forecast value and ground measured value. Fig. 13 Fig. 14 Deployment results based on proposed control strategy. Fig. 14 Table 1 Description of the state space. Table 1 Description Notation Range Unit Time of the day t hour [0,24] \u2013 Outdoor air temperature T out [-20,30] °C Direct Solar Radiation G hor [0,735] W/m2 Indoor Air Temperature T in [10,30] °C average temperature (first 4 h of the room) T his [10,30] °C Hourly outdoor temperature for the next 2 h T 1, T 2 [-20,30] °C Continuous on/off time of heat pump T hp [ 0 , + ∞ ) \u2013 Table 2 List of hyper-parameters. Table 2 Number Hyper-parameter Value 1 basic batch size 256 2 Replay memory size 10 000 3 Discount factor 0.91 4 Learning rate 0.0001 5 Initial exploration 0.9 6 Final exploration 0.1 7 Replay target frequency 1 Table 3 Main parameters of the building envelope. Table 3 Envelope Parameters Values External wall heat transfer coefficient (W/(m2· K)) 0.15 External wall solar radiation absorption coefficient (−) 0.7 Inner wall solar radiation absorption coefficient (−) 0.4 Heat transfer coefficient of outer window (W/(m2· K)) 0.8 Shading coefficient of exterior window (−) 0.5 Outer door heat transfer coefficient (W/(m2· K)) 0.8\u20131.0 Table 4 The specific parameters of the ASHP and PAU. Table 4 Devices Main parameters Air Sourced Heat Pump Rated cooling capacity 3.5 kW，Rated heating capacity 4.2 kW，COP 3.0 Primary Air Unit Rated fresh air volume 150 m3/h，Enthalpy efficiency 75% Table 5 The regression accuracy of the artificial neural network model. Table 5 Model Train set R2 Train set MSE Test set R2 Test set MSE room temperature 0.96 0.025 0.93 0.047 Equipment energy consumption 0.94 0.040 0.92 0.058 Table 6 Results of five training rounds and rule-based method. Table 6 Direction of simulation Number of convergent episodes (−) Thermal comfort reward (−) Energy consumption cost (−) Comprehensive reward (−) Round 1 6 1 962 247 3353 1 958 894 Round 2 3 1 944 000 2859 1 941 141 Round 3 3 1 959 400 2993 1 956 407 Round 4 6 1 986 000 2919 1 983 081 Round 5 1 1 931 000 3437 1 927 563 Average 3.8 1 955 071 4330 1 950 741 Standard deviation 1.9 18 537 236 18 644 Rule-based method \u2013 1 368 600 9851 1 358 749 Table 7 Results of the trade-off between thermal comfort and energy consumption. Table 7 Thermal comfort reward (−) Energy consumption cost (−) Comprehensive reward (−) rule-based method 3963 1311 2652 model-free controller 3464 418 3046 Energy-efficient heating control for nearly zero energy residential buildings with deep reinforcement learning Haosen Qin a Zhen Yu b ∗ Tailu Li a Xueliang Liu b Li Li b a Tianjin Key Laboratory of Clean Energy and Pollutant Control, School of Energy and Environmental Engineering, Hebei University of Technology, Tianjin 400301, China Tianjin Key Laboratory of Clean Energy and Pollutant Control School of Energy and Environmental Engineering Hebei University of Technology Tianjin 400301 China Tianjin key Laboratory of Clean Energy and Pollutant Control, School of Energy and Environmental Engineering, Hebei University of Technology, Tianjin 400301, China b Institute of Building Environment and Energy, China Academy of Building Research, Beijing 100013, China Institute of Building Environment and Energy China Academy of Building Research Beijing 100013 China Institute of Building Environment and Energy, China Academy of Building Research, Beijing 100013, China ∗ Corresponding author. Controlling Heating, Ventilation and Air Conditioning (HVAC) systems is critical to improving energy efficiency of demand-side. In this paper, a model-free optimal control method based on deep reinforcement learning is proposed to control the heat pump start/stop and room temperature setting in residential buildings. The optimization goal of this method is to obtain the highest comprehensive reward which considering thermal comfort and energy cost. Firstly, the randomness, learning process, thermal comfort and energy consumption of the model-free controller are systematically investigated by a simulation system based on measured data. The results show that randomness has a significant impact on the initial performance and convergence speed of the model-free controller; The model-free controller has a linear accumulation of comprehensive rewards during the learning process, and the slope of the accumulated comprehensive rewards can be used to determine whether the controller converges; The model-free controller coordinates monitoring data, weather forecasts and building thermal inertia to achieve the highest comprehensive reward. Afterwards, the model-free controller was verified in a nearly zero energy residential building in Beijing, China. The results show that model-free controller improves the comprehensive reward by 15.3% compared to rule-based method. Keywords HVAC Optimal control Reinforcement learning Deep Q learning Prioritized replay Model-free control Data availability The authors do not have permission to share data. 1 Introduction The energy systems of many countries in the world have undergone tremendous transformation in the past decade. The gradual maturity of the application of renewable energy sources in buildings and the consequent effort in decarbonization have changed the way to use and manage energy. Nearly zero energy buildings have gradually become the development direction of building energy conservation in major countries in the world [1]. The optimal operation of Heating, Ventilation and Air Conditioning (HVAC) systems is the key to achieving energy consumption indicators in nearly zero energy building. The development of the Internet of Things (IoT) and Information and Communication Technologies (ICT) has made it possible to use advanced Artificial Intelligence (AI) technology to achieve optimal management of building energy. In this context, more and more researchers worldwide are exploring various design concepts to increase the energy-saving potential of HVAC systems without sacrificing the required thermal comfort and Indoor Air Quality (IAQ) and requires minimal human intervention. 1.1 Optimal control of HVAC systems The primary function of the HVAC system is to ensure the user's needs (such as indoor temperature and humidity, indoor air quality, etc.). When optimizing the control of the HVAC system to save energy and reduce emissions, it should also be based on the premise of ensuring the needs of users. This is a process that requires intelligent decision-making to trade-off. Wang et al. [2] reviewed the existing control strategies for HVAC systems, which can be roughly divided into performance map-based control methods, rule-based control methods, model-based control methods, and model-free control methods. The performance map-based control methods and the rule-based control methods belongs to an off-line optimization method. It determines the optimal control strategy through simulation and optimization in advance, so it requires less computing power of the control system and is easier to embed into the actual system for on-site control. Hydeman et al. [3] obtained the control rules between the total cooling load, the number of cooling units and the cooling tower fan speed through simulation optimization. Wang et al. [4] developed an online control strategy based on the performance map to approximate the optimal chiller load distribution. However, as the number of optimization components increases, the complexity of the operating mode and the number of operating states will increase, resulting in a rapid increase in the number of required rules. It is not easy to consider the corresponding rules of all components in different operating states and solve the global optimal setting of the system, and it is even impossible to directly find the corresponding rules for control. Model-based method such as Model predictive control (MPC) is an online optimization method whose basic idea is to use the model of the system under study to predict the future state and minimize a certain cost function within the forecast period [5\u20139]. This kind of method is a real-time scrolling optimization process, and its optimization effect is strongly dependent on the accuracy of the architectural mode [10\u201312]. At present, there are many mature and reliable building energy simulation tools, such as TRNSYS [13], EnergyPlus [14], IES<VE> [15] and so on. A comparison of them can be found in Ref. [16]. Most of these tools use physical models, which are detailed and complex. The physical models are difficult to be applied to different buildings, as each building is unique and has its own characteristics. Data-driven methods are more suitable for model predictive control, such as Artificial Neural Network (ANN) [17,18], Support Vector Machine (SVM) [19,20], Multivariate Polynomial Regression (MPR) [21\u201323] and XGBoost [24,25]. Data-driven methods do not require physical information of the building, mainly rely on statistical evaluation and artificial intelligence to evaluate and estimate building energy consumption, it is also called the black box model. However, a challenge of this type of model is that it difficult to apply to some buildings that lack historical operating data. Deep reinforcement learning (Deep RL) is a model-free method that combines a deep neural network (Q network) and reinforcement learning (RL). Deep RL have been applied to the HVAC industry [26\u201329]. Different from the model-based method, the model-free method represented by reinforcement learning gradually forms and perfects the control strategy in the form of interactive trial and error between the agent and the environment. This kind of method does not require prior knowledge of the equipment or environment to build a model. 1.2 Reinforcement learning applied to HVAC DRL represents a novel and promising approach to the research of HVAC control and has attracted the attention of many scholars. Qiu et al. [30] implemented the control of cooling tower fan and cooling water pump frequency based on Q-learning with ambient wet bulb temperature and system cooling load as the state and system COP as the reward; Jiang et al. [31] used Double-DQN (DDQN) to optimize the room temperature setpoint while considering time-varying electricity prices; Biemann et al. [32] used each of the four RL algorithms to achieve optimal control of data center HVAC systems, and provided a comparative analysis of these four methods. All the above research works have demonstrated the RL-based control meths can be effective in reducing building energy consumption. However, due to some significant challenges related to the infrastructure required to effectively deploy the controllers, most of the existing research are limited to simulations and few implement the developed controllers on real testbed. Even with a calibrated simulation model, there will still be a gap between it and the data distribution of the actual building. Whether the performance of the controller obtained by pre-training in the simulated environment can meet the expectations after replicating it to the real building is a question worth discussing. Under ideal conditions, the training of the RL-based controller should be implemented directly online in a real building, and its control strategy should be refined through a trial-and-error process by constantly interacting with the control environment. However, RL-based controller may require considerable time to converge to acceptable control effects, as the Q-network may explore extreme states of the environment (e.g., poorer thermal comfort conditions) to adequately map the relationship between the state-action space and the corresponding rewards obtained. Currently, the most widespread solution to this problem is to create a simulated environment to generate the necessary data needed to train the algorithm, after which the controller is replicated to the physical building and continue training there [33,34]. However, the development of accurate simulation models requires considerable labor costs. In some cases, the models cannot simulate the complexity of HVAC system operation and the effects of occupant behavior. The use of historical data collected by building automation systems (BAS) to build a black box model is an alternative [35]. In contrast to MPC, the model is only used for the generation of training data and not for the computation of control strategies. A further advantage of RL is that the algorithms operate without the need for weather or price forecasts, as they can be learned using training data. Once training is complete, the operation is much less computationally expensive than MPC [32]. Indeed, DRL is a novel approach to HVAC control with great potential. However, it is still in precocial state and further research is needed to evaluate its performance in comparison with other solutions. 1.3 Motivation and structure of this research As mentioned earlier, the greatest defect in model-based control methods is that it is highly dependent on accurate system model, while current model-free control methods have poor initial performance and take a long time to converge. To improve the above problems, this paper proposes a DRL-based model-free control method for room temperature setting and heat pump start-stop setting. The developed model-free controller combines the advantages of the D3QN and the PR-DQN and has a faster convergence speed compared to the DQN-based controller. In this regard, the specific objectives of this research are as follows: (1) Considering two advanced RL methods, Dueling Double DQN (D3QN) and Prioritized Replay DQN (PR-DQN), and combining the characteristics of HVAC system, a model free controller with faster convergence speed and better control effect is developed. (2) Problem formulation including the definitions of state, action, and reward during the learning process is carefully designed, to ensure that the model-free controller can adapt to the complex thermal dynamics and environmental uncertainties involved in residential HVAC control. (3) Systematic research on model-free controller stochasticity, learning process, thermal comfort and energy consumption is conducted to explore the possibility of improving controller performance. (4) Deploy model-free controller in real buildings to verify its control effectiveness under uncertainty conditions. The rest of the paper is organized as follows. Section 2 presents the control methodology. Section 3 discusses the simulation case based on the measured data, describes the implementation details of the proposed method and the rule-based method used for comparison, Section 4 illustrates and analyzes the simulation and experimental results, and Section 5 concludes the paper. 2 Methodology 2.1 Reinforcement learning algorithms Reinforcement learning is suitable for solving continuous multi-step decision-making problems under uncertain conditions [36]. Reinforcement learning problems can be defined as Markov Decision Processes (MDP), which represents the interaction between an agent and the environment. A MDP is composed of four essential elements: state (S), action (A), state-transition probability (p), and reward (r). The state space, action space, and reward are artificially set, which will be discussed in detail later. Unlike the MPC, the state transition probability matrix is unknown and requires the agent to update iteratively in the form of trial and error. Deep RL algorithms can be divided into two families: value-based methods [29\u201331,37\u201339] and Policy-based methods [28,32,35,40\u201342]. Compared with the Policy-based method, the value-based method can only handle discrete actions, but the computational cost for training is relatively low. This is because the Policy-based method needs to maintain action-network and the critic-network separately, while the value-based method based solely on the Bellman equation and only needs to calculate Q network during iteration. The value-based method is effective in a smaller action space [31], thus is suitable for the control of HVAC systems in a residential building. Q-learning is a commonly used value-based method. Q-Learning is a classical algorithm for reinforcement learning and has been widely used in different fields [43]. The Q-Learning algorithm bases its decisions on the state-action value function. The state-action value function (Q-value) represents the reward expectation obtained by choosing action an among the available actions at state S. The Q-learning algorithm uses Q-tables to store and iterate over Q-values. In HVAC systems with a high number of sensors and devices and many state-space dimensions, the Q-learning algorithm is unable to create an effective Q-table. To address this problem, Mnih et al. [44] used a neural network instead of a Q table to estimate the action-state value function. The input of this neural network is state information and the output is the Q-value corresponding to each action, a method known as Deep Q-Learning (DQN). The DQN algorithm suffers from slow convergence and unstable operation in use. Researchers have improved it in three ways: the Double Deep Q-learning (Double DQN) algorithm uses two Q-networks with identical structures to improve the stability of the DQN algorithm [45]. The current Q-network ( Q ( S , A ; w ) ) is used to select actions a \u2032 and the target Q-network ( Q ( S , A ; w ) ) is used to calculate the target Q-value; Dueling Deep Q-learning (Dueling DQN) adds a value function and an advantage function between the hidden and output layers of the Q-network and improves the convergence speed of the DQN algorithm by updating the value function of the remaining actions simultaneously with the Q-value of one action [46]; The Dueling Double Deep Q-learning (D3QN) algorithm combines the ideas of Double DQN and Dueling DQN algorithms to further improve the performance of the algorithm; Prioritized Replay Deep Q-Learning (PR-DQN) uses Temporal Difference-error (TD-error) to evaluate the importance of samples and further improves the convergence speed of the DQN algorithm by prioritizing samples with high TD-error [47]. 2.2 Model-free controller diagram The model-free controller used combines the ideas of D3QN and PR-DQN to further improve the performance, and the overall training diagram is shown in Fig. 1 . The diagram is adapted from Ref. [48], but differs in Q-network structure, reward shaping and Decision-making. In addition, an action processor was added to the process. An advantage function is introduced in the Q-network for accelerating the convergence of the algorithm. Reward shaping is mainly developed to overcome the issue of reward sparsity. Decision-making will determine the balance between improving performance and ensuring stability of the model-free controller. The action processor decides to start or stop the heat pump based on the current temperature of the room and the set temperature output from the Q-network. 2.3 Configuration and initialization of two Q-networks In this method, the temperature setting value of the room is controlled by the Q network. And the Q-network should be configured and initialized in the following way. 2.3.1 State The state space is shown in Table 1 , including two parts: weather data and building sensor data. The average temperature of the room in the previous 4 h and the continuous on/off time of the heat pump are input into the Q-network as a series of previous states. This can provide important additional information to the algorithm and help its decision-making. For example, it can determine whether the temperature has been rising in the first few hours and take corresponding actions. 2.3.2 Action The action is defined as the room temperature setting value, the temperature control interval is 16\u201326 °C, and the temperature control step is 1 °C. The start or stop of the heat pump is controlled by the action processor. During the heating season, the room comfort temperature range is set at 20\u201322 °C. 2.3.3 Reward The aim of the training is to exploit the full energy saving potential by using the thermal inertia of the building to achieve a balance between energy consumption and thermal comfort. Referring to the principles proposed in the relevant literature [49], this research defines the reward function as: (1) r ( s , a ) = λ 1 R c m f − λ 2 W r represents the comprehensive reward of the system between thermal comfort reward and energy consumption cost. The larger the r, the better the overall performance of the algorithm in maintaining indoor temperature and reducing energy consumption. R cmf is the thermal comfort reward, as shown in Eq. (2) and Fig. 2 . When the temperature is in the comfort zone, R cmf is the positive reward value; When the temperature is higher or lower than the comfort limit, R cmf is a negative cost value, and the comfort cost is calculated according to the size of its out-of-range. The farther the out-of-range is, the greater the comfort cost. W is the energy consumption of the device, obtained directly from the energy consumption measurement. The effect of parameters with a small range of variation may be overwhelmed by parameters with a large range of variation, so the weighting factor λ needs to be set so that the energy consumption term has the same order of magnitude as the thermal comfort term. In this research, λ 1 = 1 and λ 2 = 100. (2) R c m f = { − 21950 + 2100 × T i n − 50 × T i n 2 20 ≤ T i n ≤ 22 − 2700 + 262.5 × T i n − 6.25 × T i n 2 18 ≤ T i n < 20 o r 22 < T i n ≤ 24 − 1350 + 131.25 × T i n − 3.125 × T i n 2 16 ≤ T i n < 18 o r 24 < T i n ≤ 26 − 500 T i n < 16 o r T i n > 26 2.4 Decision-making At each time step, the algorithm determines the next action based on the Q-value and a certain policy. In this paper, an improved ε-greedy policy is used to determine the action. The ε-greedy policy will set a small value of ε, so that the Q-network has a probability of 1-ε to greedily choose the action with the largest Q-value at present, and the Q-network has a probability of ε to explore among other m-1 actions. In this research, the ε-value is gradually updated as the algorithm gains experience, making it possible to encourage the algorithm to explore at the beginning of training and to run smoothly at the later stages of training: (3) π ( a | s ) = { 1 − ε t + ε t m i f a = arg max a ∈ A Q ( s , a ) ε t m i f a ≠ arg max a ∈ A Q ( s , a ) (4) ε t + 1 = ε t − ε i n i t a l − ε f i n a l N s Among them, ε inital represents the ε value at the beginning of training, ε final represents the ε value at the end of training, and N s represents the number of training steps required for an episode. 2.5 Action processor The start/stop of the heat pump is a very important control item, but if the Q-network controls this item directly, the number of action space dimensions will increase from 11 to 22 (from the room temperature setpoint to the full arrangement of temperature setpoint and heat pump start/stop), which greatly increases the computational cost. Therefore, this research uses an action processor to control the start/stop of the heat pump. Fig. 3 shows how the action processor works in the proposed method. Let a t denote the original control action output by the Q-network at time step t and let a ˆ t be the processing action sent to the system. If historical data is available for the controlled building, the action processor calls the data-driven model to calculate the temperature range the room is likely to be at the next time step when the heat pump is turned on. If the set temperature output by the Q-network is in that interval, the heat pump unit is turned on, otherwise the heat pump unit is turned off. If the building history data is not available, the action processor controls the start/stop of the heat pump based on the difference between the current temperature of the room and the set temperature of the Q-network. If the set temperature of the Q network is 3 °C below the current temperature of the room, the heat pump is turned off. 2.6 Hyper-parameters Although algorithm can continue to fine-tune some hyper-parameters to get greater rewards, this is extremely costly and time-consuming. In addition, when deployed in a physical building, hyper-parameters search become impossible, so the algorithm needs to be able to always learn the task, even for suboptimal hyper-parameters. The training of the Q-network relies on the following hyper-parameters: (1) Replay memory size, i.e., the maximum capacity of experience replay buffer max capacity, if it exceeds the capacity limit, it will delete the oldest memory. (2) Minibatch size, i.e., the number of samples used for each training iteration. (3) Discount factor, i.e., the attenuation factor γ used to update the Q-value. (4) Learning rate, i.e., the learning rate used for back propagation to update Q-network parameters. (5) Initial exploration, i.e., the initial value of ε in the ε-greedy policy. (6) Final exploration, i.e., the final value of ε in the ε-greedy policy. (7) Replay target frequency, i.e., the frequency to update target Q-network. Minibatch size should be modified and increased proportionally according to the amount of data in Replay, as shown in Eq. (5) and Eq. (6) [50]. Other hyper-parameter values are shown in Table 2 . (5) k = 1 + r e p l y _ l e n r e p l y _ m a x (6) b a t c h _ s i z e = k × b a s i c _ b a t c h _ s i z e 3 Case study 3.1 Case system As shown in Fig. 4 , a typical residence (2035 Experimental House) of the Future Building Laboratory (FBL) of the Chinese Academy of Building Research (CABR) in Beijing is adopted as the case system. The building achieves nearly zero energy building energy efficiency level, and the main parameters of the envelope are shown in Table 3 . The heat storage capacity of the building itself and the facilities within the building (i.e., floors, ceilings, walls, and furniture) can be fully utilized, allowing a range of options for the adjustment of HVAC equipment, thus providing the potential for improved overall control performance. The building is heated by an Air Sourced Heat Pump (ASHP) rated at 4.2 kW and fresh air is supplied by duct and Primary Air Unit (PAU). The main parameters of the equipment are shown in Table 4 , and the floor area of each main functional room and the HVAC system diagram are shown in Fig. 5 . Measurement operations data (Room temperature and equipment energy consumption) and meteorological data for the winter of 2020 were used to build the simulation model, as shown in Fig. 6 a and Fig. 6b, respectively. 3.2 Simulation system model based on measured data Based on the measured data, an artificial neural network model of the building was built as a simulation environment using the approach described in the literature [51]. The model is implemented in python 3.8 platform. The input of the building model is the current time step indoor temperature, outdoor temperature, outdoor relative humidity, solar radiation and thermostat set point, and the output is the next time step indoor temperature and equipment energy consumption. The prediction coefficient of determination (R2, Eq. (7)) and the mean square error (MSE, Eq. (8)) are adopted as the error indices to assess the accuracy of the equipment models [52]. The accuracy of the models is illustrated in Fig. 7 and Table 5 . (7) R 2 = ∑ i = 1 n [ y p r e d i c t e d − y \u203e o b s e r v e d ] 2 ∑ i = 1 n [ y o b s e r v e d − y \u203e o b s e r v e d ] 2 i = 1,2 , \u2026 , n (8) M S E = ∑ i = 1 n [ y o b s e r v e d − y p r e d i c t e d ] 2 n ， ∈ [ 0 , + ∞ ) , i = 1,2 , \u2026 , n 3.3 Realization details of proposed method and the comparative control method In this research, the rule-based control method is simulated together with the proposed method to validate the performance of the proposed method. 3.3.1 Proposed method The proposed method uses the algorithm described previously to control the start and stop of the ASHP and the temperature setting of the room. The PAU always operates in full heat recovery mode and the shading device is not enabled. 3.3.2 Rule-based method The rule-based control method will be used as the baseline strategy for this research to compare the control performance of model-free controller. The control rules of the rule-based control method are as follows: The ASHP is set to start or stop according to the room rate of personnel shown in Fig. 8 , and the room temperature is set to 20 °C when the ASHP is on. When the indoor air enthalpy is greater than the outdoor air enthalpy of 13 kJ/kg, the PAU operates in full heat recovery mode, otherwise it operates in bypass mode. This enthalpy difference is set to the set value that minimizes the energy consumption of the system and is determined by iterative simulation of the building TRNSYS simulation model [53]. Referring to the conclusion of Zhang et al. [54], when the solar radiation is greater than 140 W/m2, the shading equipment is turned on to block 50% of the solar radiation. 4 Results and discussion This research first uses the historical meteorological data [55] of Beijing in 2020 to test the performance of the proposed model-free controller in terms of thermal comfort and energy saving. The model-free controller is then deployed in an actual building to further explore how the algorithm performs under uncertain conditions. The code is written in Python 3.8 with the open-source deep learning platform TensorFlow [56]. The hardware environment is a notebook computer with Intel®corem™ i7-8750H 3.5 GHz CPU, RTX 2070 GDDR6@6 GB, 32.00 GM RAM. 4.1 Randomness of the model-free controller As shown in Eq. (3), the optimization strategy of the proposed model-free controller contains uncertainty and randomness. Therefore, in this research, the simulations under model-free control are independently repeated five times to determine the effect of stochasticity on the control performance. Table 6 shows the results of these five training rounds. The standard deviation of the number of convergent episodes is 50% of the corresponding mean. This shows that the randomness explored during the training of the model-free controller has a huge impact on the convergence speed. If the Q network explores actions with high rewards early in the training, the model-free controller quickly converges to optimal control. Conversely, the model-free controller takes a considerable period to learn from the experience. The standard deviation of both the thermal comfort reward and the comprehensive reward is less than 2.5% of the corresponding average values, while the standard deviation of the energy consumption cost is 5.4% of the corresponding mean. This shows that (1) the randomness of exploration does not affect the final performance of the model-free controller; (2) there exists more than one path that makes the comprehensive reward of thermal comfort and energy consumption optimal, and the randomness of exploration affects the final path chosen by the controller. Fig. 9 illustrates the comprehensive reward variation obtained after each episode of training in these five rounds of simulation. One episode refers to the model-free controller training for one heating season. The Q network is initialized randomly, and the exploration rate is high at the beginning of training, so even the same algorithm has different effects in the first episode of each training. The figure further validates the findings in Table 6: poor strategies are explored early in the first round and thus require longer learning time; good strategies are explored early in the second, third, and fifth rounds and thus converge quickly. In these five rounds of simulation, the final performance of the model-free controller does not differ much. In addition, it is noted in Fig. 9 that the control performance of the model-free controller does not always increase monotonically with training before convergence. This is because in deep reinforcement learning, Q values are stored in the form of neural networks, and a change in any weight within the Q network necessarily affects all state-action pairs. The significant impact of the explored stochasticity on the initial performance and convergence speed of the model-free controller suggests that: some pre-training of the controller using a simulated environment based on historical building data is necessary before deploying the model-free controller to a real building. In addition, using prior knowledge at the beginning of exploration to specify the direction of exploration is a possible solution to avoid poor performance in the early stages of reinforcement learning model deployment. 4.2 Learning process of the model-free controller Fig. 10 depicts in detail the learning process for the first and fifth rounds of simulation. Figs. 10a and c shows the comprehensive rewards obtained by the model-free controller at each simulation step during episode 1, episode 5 and episode 8. It is observed that the comprehensive reward obtained by the model-free controller at the same time step increases significantly with the number of training episodes. As shown in Fig. 10b and d, the comprehensive reward of the model-free controller gradually becomes linearly cumulative as the training proceeds. This is because after continuous exploration, the Q value of different actions in various states are gradually modified. As training proceeds, common states and their available actions are gradually explored, allowing the model-free controller to select actions that receive higher rewards. Therefore, the slope of the accumulated comprehensive reward gradually increases in the simulations of the subsequent episode (the performance of the model-free controller gradually improves). The theoretical maximum slope of the accumulated comprehensive reward is 700 (i.e., the building is always kept at 20\u201322 °C with 0 energy consumption). After sufficient training in the simulated environment, the slope of the accumulated comprehensive reward no longer increases significantly with training and the model-free controller converges. At this time the model-free controller has completed the exploration of most of the states and is able to stably select the action with the greatest combined gain in thermal comfort and energy consumption in different states. Therefore, the model-free controller can be determined whether it converges or not based on the change in the slope of its accumulated comprehensive reward. 4.3 Performance of thermal comfort and energy saving As shown in Table 6, the model-free controller outperforms the rule-based control strategy in terms of thermal comfort reward, energy cost, and comprehensive reward after sufficient learning to establish its own experience. The model-free controller can obtain thermal comfort rewards above the rule-based control strategy because (1) the controlled building is highly insulated and sealed and may overheat on sunny days due to the greenhouse effect if left unchecked. (2) The rule-based control strategy controls the air conditioning on and off based on the presence or absence of people in the room, and the room temperature may be too low when the room is unoccupied. The model-free controller prevents the building from overcooling and overheating through intelligent regulation based on outdoor weather parameters and the historical state of the room, while making full use of the natural outdoor heat and thermal inertia of the building. The room temperature distribution diagram of the model-free controller throughout the heating season is shown in Fig. 11 . The model-free controller allows the room temperature to fluctuate temporarily outside the comfort range, and the thermal inertia of the building is used by pre-cooling or pre-heating to balance energy consumption and comfort. As the training continues, the performance of the algorithm will continue to improve. Fig. 12 intercepts the data of the last week of December, and further shows the control strategy of the fully trained model-free controller. The first three, fifth and sixth days were cloudy, the fourth and seventh days were sunny. Firstly, the Q network witty \u2018recognized\u2019 the action processor and learned to turn off the AHSP by setting the setpoint to a lower temperature. Second, the Q-network learns that weather data have a significant impact on rewards and intelligently sets the setting values to achieve higher reward values based on this state information. It can be observed that the control strategies of model-free controller in general meteorology have some similarity daily. Continuously turn on the ASHP at night when there is no sunlight to maintain the room temperature, turn on the ASHP intermittently in the morning to use natural heat to raise the room temperature stepwise, and turn off the ASHP at noon and afternoon to use the thermal inertia of the building. In weather with more complicated changes (such as the fifth day), the controller can flexibly select the temperature setpoint. During these seven days, due to insufficient solar radiation on continuous cloudy days, the controller set the night indoor temperature at 18\u201319 °C to save energy. The energy consumption of model-free controller strategy for these 7 days is 11.80 kWh, the energy consumption of the rule-based control strategy is 14.64 kWh, and the energy saving is 19.40%. The model-free controller gains considerable energy savings by sacrificing acceptable comfort level. 4.4 Performance of actual deployment in physical world Pre-trained model-free controller are deployed in controlled building from December 4, 2021 to December 6, 2021. The indoor temperature of the controlled building was set at 24\u201326 °C one week before the start of the experiment, while the controller was set at a comfortable temperature range of 20\u201322 °C in winter. To reduce the influence of building heat storage, the controlled building was ventilated by opening Windows from 15:00 to 18:00 on December 3, 2021, and the room temperature was set at 20 °C from 18:00 to 24:00. During deployment, the PAU operates in full heat recovery mode and the shading system is turned off. Fig. 13 a shows the outdoor dry bulb temperature during model-free controller and baseline strategy deployment, and Fig. 13b shows the outdoor relative humidity during model-free controller and baseline strategy deployment. The two strategies had similar outdoor weather data during deployment, with clear weather except for cloudy conditions from 20:00 on December 4 to 03:00 on December 5. The forecast data are from the weather forecasting service of China Weather Network, and the actual data are from the relevant sensors of the controlled building, and there is an error between them as shown in Fig. 13. During the pre-training period, the input parameters of the controller state space are the weather forecast data and the input parameters of the building model are the actual measured data at the same moment, so the controller has considered the effect of the error between the weather forecast and the measured data when making decisions. The maximum absolute errors of outdoor temperature and relative humidity in the pre-training data set are 12 °C and 82.47%, respectively, and the maximum absolute errors of outdoor temperature and relative humidity during the actual deployment are 7 °C and 74.12%, respectively, and the errors between the predicted and measured data during the actual deployment meet the controller accuracy requirements. Fig. 14 a shows the room temperature and energy consumption during the deployment of the model-free controller. Fig. 14b shows the room temperature and energy consumption when the baseline control strategy is deployed from December 11 to December 13, 2021. The solid red line in Fig. 14a represents the set temperature output by the reinforcement learning algorithm, which is processed by the action processor and then applied to the air conditioner. The same as the simulation results, the model-free controller keenly discovered the existence of the action processor and learned to set the temperature at 17 °C or 18 °C to turn off the ASHP without any human intervention. The baseline strategy sets the indoor temperature at 20 °C. The average temperature of the controlled building during the deployment of the baseline strategy was 20.2 °C. In order to maintain the indoor temperature at the set value, the ASHP was always on, and although the hour-by-hour energy consumption was low, the total energy consumption was high. The model-free controller sets the indoor comfort temperature range at 20\u201322 °C and will evaluate the results based on the calculations in Eq. (1) and Eq. (2), allowing the indoor temperature to briefly deviate from the comfort temperature range to exploit the energy saving potential. The average room temperatures during the three days of deployment of the model-free controller were 20.0 °C, 19.7 °C, and 19.5 °C, and the lowest room temperatures during the three days were 19.4 °C, 19.5 °C, and 19.3 °C, respectively. The model-free controller is dynamically changing for heat pump start/stop and room temperature settings, and the controller takes into account not only current monitoring information but also future environmental changes and building thermal inertia when making adjustments to the system's settings. During the model-free controller deployment, the heat pump was on for only 4 h, so although the ASHP ran at high power each time, the overall energy consumption was much lower than the baseline strategy in which the ASHP was on all the time. Table 7 shows the trade-off results of the model-free controller between thermal comfort and energy consumption. In the simulated environment, a fully trained model free controller can reduce energy consumption while also improving thermal comfort. However, the performance during the actual deployment was worse than the simulated environment. This is because even with a calibrated simulation model, the data distribution will still be different from the real-world data distribution. The model-free controller reduced room comfort by 12.6% and saved 63% of energy during deployment compared to the rule-based strategy. The reason is that during periods of high outdoor temperature, the baseline strategy keeps the set point at 20 °C, which keeps the HVAC system \u201con\u201d for a long time, resulting in excessive energy consumption. The model free controller achieves a better trade-off between thermal comfort and energy saving by flexibly adjusting the temperature setting and the on/off of the ASHP and obtains the optimization effect of increasing the total reward value by 15.3%. The model-free controller continues to be trained in the controlled building for a period, and its control performance will also gain substantial improvement. 4.5 Future work Continuing the discussion of this research, future work can be carried out in the following two aspects: (1) Introduce more Suitable comfort parameters in the objective function. The current evaluation of comfort is only related to temperature. The impact of other parameters (e.g. air velocity, relative humidity) obtained by the monitoring will be considered in the future work. (2) Developing methods to determine the direction of exploration through prior knowledge at the beginning of training rather than random exploration may be a solution to poor initial performance. Instead of learning weights from scratch for each new building, transfer the Q-network weights from one building to another via transfer learning. 5 Conclusion A model-free optimal control method for residential building HVAC system is proposed based on the classical reinforcement learning algorithm. In this paper, a measured data-based simulation is first conducted to investigate the effect of randomness of exploration on the controller, the learning process of the controller and its convergence capability, and the performance of the controller in terms of thermal comfort and energy saving. Afterwards, the model-free controller was deployed and validated in a nearly zero energy residential building in Beijing. The main conclusions of this research are as follows: (1) The randomness of exploration does not have an impact on the final performance of the model free controller but will determine the performance of the controller at the beginning of its deployment and its convergence speed, and pre-training can significantly reduce the deployment cost of the controller in actual buildings. (2) The comprehensive reward of the model-free controller is linearly accumulated, and the model-free controller converges when the slope of the accumulated comprehensive reward no longer increases significantly with training. (3) Compared to the rule-based control strategy, during deployment in the actual building, the model-free controller saved 63% in energy consumption at the cost of a 12.6% reduction in room thermal comfort reward, ultimately improving comprehensive reward by 15.3%. (4) When replicating a model-free controller fully trained in a simulated environment to a real building, the controller performance drops significantly, but still outperforms the rule-based control method. Model-free controller still needs to be iterated after actual deployment using building instant monitoring data. Credit author statement Haosen Qin: Methodology, Software, Writing- Original draft preparation. Zhen Yu: Conceptualization, Supervision, Writing - Review & Editing. Tailu Li: Methodology, Writing - Review & Editing. Xueliang Liu: Investigation, Data Curation. Li Li: Software, Validation. Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgment The author gratefully acknowledges the financial support from National Key R&D Program of China (Project No.2019YFE0100300). References [1] S. Attia P. Eleftheriou F. Xeni Overview and future challenges of nearly zero energy buildings (nZEB) design in Southern Europe[J] Energy Build 155 2017 439 458 Attia S, Eleftheriou P, Xeni F, et al. Overview and future challenges of nearly zero energy buildings (nZEB) design in Southern Europe[J]. Energy and Buildings, 2017, 155: 439-458. [2] S. Wang Z. Ma Supervisory and optimal control of building HVAC systems: a review[J] HVAC R Res 14 1 2008 3 32 Wang S, Ma Z. Supervisory and optimal control of building HVAC systems: A review[J]. Hvac&R Research, 2008, 14(1): 3-32. [3] M. Daraei A. Avelin E. Thorin Optimization of a regional energy system including CHP plants and local PV system and hydropower: scenarios for the County of Västmanland in Sweden[J] J Clean Prod 230 2019 1111 1127 Daraei M, Avelin A, Thorin E. Optimization of a regional energy system including CHP plants and local PV system and hydropower: Scenarios for the County of Vastmanland in Sweden[J]. Journal of Cleaner Production, 2019, 230: 1111-1127. [4] A. Bischi L. Taccari E. Martelli A detailed MILP optimization model for combined cooling, heat and power system operation planning[J] Energy 74 9 2014 12 26 Bischi A, Taccari L, Martelli E, et al. A detailed MILP optimization model for combined cooling, heat and power system operation planning[J]. Energy, 2014, 74(9): 12-26. [5] P.M. Ferreira A.E. Ruano S. Silva Neural networks based predictive control for thermal comfort and energy savings in public buildings[J] Energy Build 55 2012 238 251 Ferreira P M, Ruano A E, Silva S, et al. Neural networks based predictive control for thermal comfort and energy savings in public buildings[J]. Energy & Buildings, 2012, 55:238-251. [6] H. Huang L. Chen E. Hu A new model predictive control scheme for energy and cost savings in commercial buildings: an airport terminal building case study[J] Build Environ 89 2015 203 216 jul. Huang H, Chen L, Hu E. A new model predictive control scheme for energy and cost savings in commercial buildings: An airport terminal building case study[J]. Building & Environment, 2015, 89(jul.):203-216. [7] A. Kusiak M. Li F. Tang Modeling and optimization of HVAC energy consumption[J] Appl Energy 87 10 2010 3092 3102 Kusiak A, Li M, Tang F . Modeling and optimization of HVAC energy consumption[J]. Applied Energy, 2010, 87(10):3092-3102. [8] A. Garnier J. Eynard M. Caussanel Predictive control of multizone heating, ventilation and air-conditioning systems in non-residential buildings[J] Appl Soft Comput 37 2015 847 862 Garnier A, Eynard J, Caussanel M, et al. Predictive control of multizone heating, ventilation and air-conditioning systems in non-residential buildings[J]. Applied Soft Computing, 2015, 37: 847-862. [9] W. Kim Y. Jeon Y. Kim Simulation-based optimization of an integrated daylighting and HVAC system using the design of experiments method[J] Appl Energy 162 2016 666 674 JAN.15 Kim W, Jeon Y, Kim Y. Simulation-based optimization of an integrated daylighting and HVAC system using the design of experiments method[J]. Applied Energy, 2016, 162(JAN.15):666-674. [10] M. Killian M. Kozek Ten questions concerning model predictive control for energy efficient buildings[J] Build Environ 105 2016 403 412 aug. Killian M, Kozek M. Ten questions concerning model predictive control for energy efficient buildings[J]. Building & Environment, 2016, 105(aug.):403-412. [11] S. Privara J. Cigler Z. Váňa Building modeling as a crucial part for building predictive control[J] Energy Build 56 2013 8 22 Privara S, Cigler J, Vana Z, et al. Building modeling as a crucial part for building predictive control[J]. Energy and Buildings, 2013, 56: 8-22. [12] M. Cannon Efficient nonlinear model predictive control algorithms[J] Annu Rev Control 28 Pt2 2004 229 237 Cannon M . Efficient nonlinear model predictive control algorithms[J]. ANNUAL REVIEWS IN CONTROL, 2004, 28(Pt2):p.229-237. [13] TRNSYS http://www.trnsys.com/ TRNSYS, Transient System Simulation Tool, http://www.trnsys.com/ [14] EnergyPlus https://energyplus.net/ EnergyPlus, https://energyplus.net/ [15] IES-VE https://www.iesve.com/ IES-VE, https://www.iesve.com/ [16] D. Crawley J. Hand Contrasting the capabilities of building energy performance simulation programs[J] Build Environ 43 4 2008 661 673 Crawley D, Hand J. Contrasting the capabilities of building energy performance simulation programs[J]. Building & Environment, 2008, 43(4):661-673. [17] Sara M.C. Magalhães Vítor M.S. Leal I.M. Horta Modelling the relationship between heating energy use and indoor temperatures in residential buildings through Artificial Neural Networks considering occupant behavior[J] 2017 Energy and Buildings Sara M.C. Magalhaes, Vitor M.S. Leal, Horta I M. Modelling the relationship between heating energy use and indoor temperatures in residential buildings through Artificial Neural Networks considering occupant behavior[J]. Energy and Buildings, 2017. [18] A. Afram F. Janabi-Sharifi A.S. Fung Artificial neural network (ANN) based model predictive control (MPC) and optimization of HVAC systems: a state of the art review and case study of a residential HVAC system[J] Energy Build 141 2017 96 113 Afram A, Janabi-Sharifi F, Fung A S, et al. Artificial neural network (ANN) based model predictive control (MPC) and optimization of HVAC systems: A state of the art review and case study of a residential HVAC system[J]. Energy and Buildings, 2017, 141: 96-113. [19] B. Dong C. Cao S.E. Lee Applying support vector machines to predict building energy consumption in tropical region[J] Energy Build 37 5 2005 545 553 Dong B, Cao C, Lee S E. Applying support vector machines to predict building energy consumption in tropical region[J]. Energy and Buildings, 2005, 37(5): 545-553. [20] Y. Chen P. Xu Y. Chu Short-term electrical load forecasting using the Support Vector Regression (SVR) model to calculate the demand response baseline for office buildings[J] Appl Energy 195 2017 659 670 Chen Y, Xu P, Chu Y, et al. Short-term electrical load forecasting using the Support Vector Regression (SVR) model to calculate the demand response baseline for office buildings[J]. Applied Energy, 2017, 195: 659-670. [21] S. Qiu S. Li F. Wang An energy exchange efficiency prediction approach based on multivariate polynomial regression for membrane-based air-to-air energy recovery ventilator core[J] Build Environ 149 2019 490 500 FEB. Qiu S, Li S, Wang F, et al. An energy exchange efficiency prediction approach based on multivariate polynomial regression for membrane-based air-to-air energy recovery ventilator core[J]. Building and Environment, 2019, 149(FEB.):490-500. [22] Shunian Qiu Zhengwei Li Zhihong Pang Weijie Zhang Zhenhai Li A quick auto-calibration approach based on normative energy models Energy Build 172 2018 10.1016/j.enbuild.2018.04.053 Qiu, Shunian & Li, Zhengwei & Pang, Zhihong & Zhang, Weijie & Li, Zhenhai. (2018). A quick auto-calibration approach based on normative energy models. Energy and Buildings. 172. 10.1016/j.enbuild.2018.04.053. [23] D. Li Y. Liu J. Xu Optimization of heat treatment process on vacuum die-casting AT72 magnesium alloy[C] 2013 //Trans Tech Publications 307 312 Li D, Liu Y, Xu J, et al. Optimization of Heat Treatment Process on Vacuum Die-Casting AT72 Magnesium Alloy[C]// Trans Tech Publications, 2013:307-312. [24] H. Mo H. Sun J. Liu Developing window behavior models for residential buildings using XGBoost algorithm[J] Energy Build 205 2019 109564 Mo H, Sun H, Liu J, et al. Developing window behavior models for residential buildings using XGBoost algorithm[J]. Energy and Buildings, 2019, 205:109564-. [25] Y. Feng Q. Duan X. Chen Space cooling energy usage prediction based on utility data for residential buildings using machine learning methods[J] Appl Energy 291 2021 116814 Feng Y, Duan Q, Chen X, et al. Space cooling energy usage prediction based on utility data for residential buildings using machine learning methods[J]. Applied energy, 2021, 291: 116814. [26] M. Biemann F. Scheller X. Liu Experimental evaluation of model-free reinforcement learning algorithms for continuous HVAC control[J] Appl Energy 298 2021 117164 Biemann M, Scheller F, Liu X, et al. Experimental evaluation of model-free reinforcement learning algorithms for continuous HVAC control[J]. Applied Energy, 2021, 298: 117164. [27] M. Han J. Zhao X. Zhang The reinforcement learning method for occupant behavior in building control: a review[J] Energy and Built Environment 2 2 2021 137 148 Han M, Zhao J, Zhang X, et al. The reinforcement learning method for occupant behavior in building control: A review[J]. Energy and Built Environment, 2021, 2(2): 137-148. [28] Y. Du F. Li J. Munk Multi-task deep reinforcement learning for intelligent multi-zone residential HVAC control[J] Elec Power Syst Res 192 2 2020 106959 Du Y, Li F, Munk J, et al. Multi-task deep reinforcement learning for intelligent multi-zone residential HVAC control[J]. Electric Power Systems Research, 2020, 192(2):106959. [29] S. Brandi M.S. Piscitelli M. Martellacci Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings[J] Energy Build 224 2020 110225 Brandi S, Piscitelli M S, Martellacci M, et al. Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings[J]. Energy and Buildings, 2020, 224: 110225. [30] S. Qiu Z. Li Z. Li Model-free control method based on reinforcement learning for building cooling water systems: validation by measured data-based simulation[J] Energy Build 218 2020 110055 Qiu S, Li Z, Li Z, et al. Model-free control method based on reinforcement learning for building cooling water systems: Validation by measured data-based simulation[J]. Energy and Buildings, 2020, 218: 110055. [31] Z. Jiang M.J. Risbeck V. Ramamurti Building HVAC control with reinforcement learning for reduction of energy cost and demand charge[J] Energy Build 239 2021 110833 Jiang Z, Risbeck M J, Ramamurti V, et al. Building HVAC control with reinforcement learning for reduction of energy cost and demand charge[J]. Energy and Buildings, 2021, 239: 110833. [32] M. Biemann F. Scheller X. Liu Experimental evaluation of model-free reinforcement learning algorithms for continuous HVAC control[J] Appl Energy 298 2021 117164 Biemann M, Scheller F, Liu X, et al. Experimental evaluation of model-free reinforcement learning algorithms for continuous HVAC control[J]. Applied Energy, 2021, 298: 117164. [33] G.P. Henze J. Schoenmann Evaluation of reinforcement learning control for thermal energy storage systems[J] HVAC R Res 9 3 2003 259 275 Henze G P, Schoenmann J. Evaluation of reinforcement learning control for thermal energy storage systems[J]. HVAC&R Research, 2003, 9(3): 259-275. [34] S. Liu G.P. Henze Evaluation of reinforcement learning for optimal control of building active and passive thermal storage inventory[J] 2007 Liu S, Henze G P. Evaluation of reinforcement learning for optimal control of building active and passive thermal storage inventory[J]. 2007. [35] Z. Zou X. Yu S. Ergan Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network[J] Build Environ 168 2020 106535 Zou Z, Yu X, Ergan S. Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network[J]. Building and Environment, 2020, 168: 106535. [36] R. Sutton A. Barto Reinforcement learning:an introduction[M] 2018 MIT Press Sutton R, Barto A . Reinforcement Learning:An Introduction[M]. MIT Press, 2018. [37] Y. Chen L.K. Norford H.W. Samuelson Optimal control of HVAC and window systems for natural ventilation through reinforcement learning[J] Energy Build 169 JUN 2018 195 205 Chen Y, Norford L K, Samuelson H W, et al. Optimal Control of HVAC and Window Systems for Natural Ventilation Through Reinforcement Learning[J]. Energy and Buildings, 2018, 169(JUN.):195-205. [38] T. Liu X. Hu A Bi-level control for energy efficiency improvement of a hybrid tracked vehicle IEEE Trans Ind Inf 14 2018 1616 1625 Liu T, Hu X. A Bi-level control for energy efficiency improvement of a hybrid tracked vehicle. IEEE Trans Ind Inform 2018;14:1616-1625. [39] T. Liu B. Wang C. Yang Online Markov chain-based energy management for a hybrid tracked vehicle with speedy Q-learning Energy 160 2018 544 555 Liu T, Wang B, Yang C. Online Markov Chain-based energy management for a hybrid tracked vehicle with speedy Q-learning. Energy 2018;160:544-555 [40] Yan D A, Hz B, Ok B, et al. Intelligent multi-zone residential HVAC control strategy based on deep reinforcement learning[J]. Applied Energy, vol. 281. [41] Z. Deng Q. Chen Reinforcement learning of occupant behavior model for cross-building transfer learning to various HVAC control systems[J] Energy Build 238 5\u20136 2021 110860 Deng Z, Chen Q. Reinforcement learning of occupant behavior model for cross-building transfer learning to various HVAC control systems[J]. Energy and Buildings, 2021, 238(5-6):110860. [42] Wang Yuan Kirubakaran A novel approach to feedback control with deep reinforcement learning - ScienceDirect[J] IFAC-PapersOnLine 51 18 2018 31 36 Yuan, Wang, Kirubakaran, et al. A Novel Approach to Feedback Control with Deep Reinforcement Learning - ScienceDirect[J]. IFAC-PapersOnLine, 2018, 51(18):31-36. [43] Y. Li Deep reinforcement learning: an overview[J] 2017 arXiv preprint arXiv:1701.07274 Li Y. Deep reinforcement learning: An overview[J]. arXiv preprint arXiv:1701.07274, 2017. [44] M. Volodymyr K. Koray S. David Human-level control through deep reinforcement learning[J] Nature 518 7540 2019 529 533 Volodymyr M, Koray K, David S, et al. Human-level control through deep reinforcement learning[J]. Nature, 2019, 518(7540):529-533. [45] H.V. Hasselt A. Guez D. Silver Deep reinforcement learning with Double Q-learning[J] 2015 Computer ence Hasselt H V, Guez A, Silver D. Deep Reinforcement Learning with Double Q-learning[J]. Computer ence, 2015. [46] Z. Wang T. Schaul M. Hessel Dueling network architectures for deep reinforcement learning[C]//International conference on machine learning 2016 PMLR 1995 2003 Wang Z, Schaul T, Hessel M, et al. Dueling network architectures for deep reinforcement learning[C]//International conference on machine learning. PMLR, 2016: 1995-2003. [47] T. Schaul J. Quan I. Antonoglou Prioritized experience replay[J] 2015 Computer Science Schaul T, Quan J, Antonoglou I, et al. Prioritized Experience Replay[J]. Computer Science, 2015. [48] R. Zou L. Fan Y. Dong DQL energy management: an online-updated algorithm and its application in fix-line hybrid electric vehicle[J] Energy 1 2021 120174 Zou R, Fan L, Dong Y, et al. DQL Energy Management: An Online-updated Algorithm and Its Application in Fix-Line Hybrid Electric Vehicle[J]. Energy, 2021(1):120174. [49] J. Jiang X. Zeng D. Guzzetti Path planning for asteroid hopping rovers with pre-trained deep reinforcement learning architectures[J] Acta Astronaut 171 2020 265 279 J Jiang, X Zeng, Guzzetti D, et al. Path planning for asteroid hopping rovers with pre-trained deep reinforcement learning architectures[J]. Acta Astronautica, 2020, 171:265-279. [50] S.L. Smith P.J. Kindermans C. Ying Don't decay the learning rate 2017 Increase the Batch Size[J Smith S L, Kindermans P J, Ying C, et al. Don't Decay the Learning Rate, Increase the Batch Size[J]. 2017. [51] A. Afram F. Janabi-Sharifi A.S. Fung Artificial neural network (ANN) based model predictive control (MPC) and optimization of HVAC systems: a state of the art review and case study of a residential HVAC system[J] Energy Build 141 APR 2017 96 113 Afram A, Janabi-Sharifi F, Fung A S, et al. Artificial neural network (ANN) based model predictive control (MPC) and optimization of HVAC systems: A state of the art review and case study of a residential HVAC system[J]. Energy & Buildings, 2017, 141(APR.):96-113. [52] S.M.C. Magalhães V.M.S. Leal I.M. Horta Modelling the relationship between heating energy use and indoor temperatures in residential buildings through Artificial Neural Networks considering occupant behavior[J] Energy Build 151 2017 332 343 Magalhaes S M C, Leal V M S, Horta I M. Modelling the relationship between heating energy use and indoor temperatures in residential buildings through Artificial Neural Networks considering occupant behavior[J]. Energy and Buildings, 2017, 151: 332-343. [53] H. Qin Z. Yu T. Li Heating control strategy based on dynamic programming for building energy saving and emission reduction[J] Int J Environ Res Publ Health 19 21 2022 14137 Qin H, Yu Z, Li T, et al. Heating Control Strategy Based on Dynamic Programming for Building Energy Saving and Emission Reduction[J]. International Journal of Environmental Research and Public Health, 2022, 19(21): 14137. [54] X. Zhang Research on the determination method of outdoor design conditions of sun shading design [D] 2020 Xi'an University of Architecture and Technology [In Chinese, Master Thesis)] Zhang X. Research on the Determination Method of Outdoor Design Conditions of Sun Shading Design [D]. Xi'an University of Architecture and Technology, 2020. (In Chinese, Master Thesis) [55] H. Hersbach B. Bell P. Berrisford G. Biavati A. Horányi J. Muñoz Sabater J. Nicolas C. Peubey R. Radu I. Rozum D. Schepers A. Simmons C. Soci D. Dee J.-N. Thépaut ERA5 hourly data on single levels from 1979 to present. Copernicus climate change service (C3S) climate data store (CDS) 2018 10.24381/cds.adbb2d47 Accessed on < DD-MMM-YYYY > Hersbach, H., Bell, B., Berrisford, P., Biavati, G., Horanyi, A., Munoz Sabater, J., Nicolas, J., Peubey, C., Radu, R., Rozum, I., Schepers, D., Simmons, A., Soci, C., Dee, D., Thepaut, J-N. (2018): ERA5 hourly data on single levels from 1979 to present. Copernicus Climate Change Service (C3S) Climate Data Store (CDS). (Accessed on < DD-MMM-YYYY >), 10.24381/cds.adbb2d47 [56] [online]: TensorFlow https://www.tensorflow.org/ TensorFlow. [online]: https://www.tensorflow.org/.",
    "scopus-id": "85142904710",
    "coredata": {
        "eid": "1-s2.0-S036054422203095X",
        "dc:description": "Controlling Heating, Ventilation and Air Conditioning (HVAC) systems is critical to improving energy efficiency of demand-side. In this paper, a model-free optimal control method based on deep reinforcement learning is proposed to control the heat pump start/stop and room temperature setting in residential buildings. The optimization goal of this method is to obtain the highest comprehensive reward which considering thermal comfort and energy cost. Firstly, the randomness, learning process, thermal comfort and energy consumption of the model-free controller are systematically investigated by a simulation system based on measured data. The results show that randomness has a significant impact on the initial performance and convergence speed of the model-free controller; The model-free controller has a linear accumulation of comprehensive rewards during the learning process, and the slope of the accumulated comprehensive rewards can be used to determine whether the controller converges; The model-free controller coordinates monitoring data, weather forecasts and building thermal inertia to achieve the highest comprehensive reward. Afterwards, the model-free controller was verified in a nearly zero energy residential building in Beijing, China. The results show that model-free controller improves the comprehensive reward by 15.3% compared to rule-based method.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2023-02-01",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S036054422203095X",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Qin, Haosen"
            },
            {
                "@_fa": "true",
                "$": "Yu, Zhen"
            },
            {
                "@_fa": "true",
                "$": "Li, Tailu"
            },
            {
                "@_fa": "true",
                "$": "Liu, Xueliang"
            },
            {
                "@_fa": "true",
                "$": "Li, Li"
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S036054422203095X"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S036054422203095X"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S0360-5442(22)03095-X",
        "prism:volume": "264",
        "articleNumber": "126209",
        "prism:publisher": "Elsevier Ltd.",
        "dc:title": "Energy-efficient heating control for nearly zero energy residential buildings with deep reinforcement learning",
        "prism:copyright": "© 2022 Elsevier Ltd. All rights reserved.",
        "openaccess": "0",
        "prism:issn": "03605442",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "HVAC"
            },
            {
                "@_fa": "true",
                "$": "Optimal control"
            },
            {
                "@_fa": "true",
                "$": "Reinforcement learning"
            },
            {
                "@_fa": "true",
                "$": "Deep Q learning"
            },
            {
                "@_fa": "true",
                "$": "Prioritized replay"
            },
            {
                "@_fa": "true",
                "$": "Model-free control"
            }
        ],
        "openaccessArticle": "false",
        "prism:publicationName": "Energy",
        "openaccessSponsorType": null,
        "prism:pageRange": "126209",
        "pubType": "fla",
        "prism:coverDisplayDate": "1 February 2023",
        "prism:doi": "10.1016/j.energy.2022.126209",
        "prism:startingPage": "126209",
        "dc:identifier": "doi:10.1016/j.energy.2022.126209",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "258",
            "@width": "386",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr14.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "99940",
            "@ref": "gr14",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "275",
            "@width": "388",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr13.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "110692",
            "@ref": "gr13",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "301",
            "@width": "388",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr9.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "113462",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "269",
            "@width": "388",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr12.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "120323",
            "@ref": "gr12",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "261",
            "@width": "387",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr11.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "94208",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "583",
            "@width": "811",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr10.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "202050",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "359",
            "@width": "535",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "146392",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "361",
            "@width": "811",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "138851",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "283",
            "@width": "388",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr8.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "81872",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "323",
            "@width": "811",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "137621",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "321",
            "@width": "388",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "92197",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "525",
            "@width": "811",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "173953",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "401",
            "@width": "535",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "143826",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "158",
            "@width": "811",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "89527",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "147",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr14.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "74943",
            "@ref": "gr14",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "155",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr13.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "76758",
            "@ref": "gr13",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "211",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr9.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "77897",
            "@ref": "gr9",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "152",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr12.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "80044",
            "@ref": "gr12",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "148",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr11.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "73946",
            "@ref": "gr11",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "157",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr10.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "79926",
            "@ref": "gr10",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "147",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "82172",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "97",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "75513",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "160",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr8.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "70528",
            "@ref": "gr8",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "87",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "72459",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "198",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "72215",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "142",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "77721",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "96414",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "43",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "68770",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "1144",
            "@width": "1710",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr14_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "296081",
            "@ref": "gr14",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1220",
            "@width": "1721",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr13_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "377117",
            "@ref": "gr13",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1335",
            "@width": "1721",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr9_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "414059",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1195",
            "@width": "1721",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr12_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "430121",
            "@ref": "gr12",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1158",
            "@width": "1715",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr11_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "245845",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2583",
            "@width": "3592",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr10_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1300775",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1592",
            "@width": "2371",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "804162",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1597",
            "@width": "3591",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "594547",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1257",
            "@width": "1721",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr8_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "170579",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1431",
            "@width": "3591",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "544024",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1423",
            "@width": "1721",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "231554",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2325",
            "@width": "3591",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "731228",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1778",
            "@width": "2371",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "943705",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "701",
            "@width": "3591",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "278160",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-si4.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "58717",
            "@ref": "si4",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-si7.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "47960",
            "@ref": "si7",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-si8.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9788",
            "@ref": "si8",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-si6.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "115209",
            "@ref": "si6",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-si13.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "133781",
            "@ref": "si13",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-si1.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "30077",
            "@ref": "si1",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-si10.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "58622",
            "@ref": "si10",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-si12.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "102317",
            "@ref": "si12",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-si3.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "13224",
            "@ref": "si3",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-si2.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7281",
            "@ref": "si2",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-si9.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11744",
            "@ref": "si9",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-si11.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "53326",
            "@ref": "si11",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-si5.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "123231",
            "@ref": "si5",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S036054422203095X-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "2508929",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85142904710"
    }
}}