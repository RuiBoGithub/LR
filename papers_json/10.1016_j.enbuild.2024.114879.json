{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85206186834",
    "originalText": "serial JL 271089 291210 291731 291800 291881 31 Energy and Buildings ENERGYBUILDINGS 2024-10-09 2024-10-09 2024-10-13 2024-10-13 2025-08-18T02:59:09 1-s2.0-S0378778824009952 S0378-7788(24)00995-2 S0378778824009952 10.1016/j.enbuild.2024.114879 S300 S300.1 FULL-TEXT 1-s2.0-S0378778824X00195 2025-08-18T02:07:55.950635Z 0 0 20241201 2024 2024-10-09T18:35:36.369921Z absattachment articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref specialabst 0378-7788 03787788 true 324 324 C Volume 324 19 114879 114879 114879 20241201 1 December 2024 2024-12-01 2024 Research Articles article fla © 2024 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies. DYNAPINNPHYSICSINFORMEDDEEPDYNAQREINFORCEMENTLEARNINGFORINTELLIGENTCONTROLBUILDINGHEATINGSYSTEMINLOWDIVERSITYTRAININGDATAREGIMES SAEED M 1 Introduction 1.1 Classical and physics-based approaches for building heating control 1.2 Reinforcement Learning for building heating control 1.3 Physics-informed Reinforcement Learning 1.4 Challenges in training with low-diversity data regimes 1.5 Contribution 2 Methodology 2.1 Markov decision process (MDP) formulation 2.2 Physics informed neural networks (PINNs) 2.3 Deep Dyna-Q (DDQ) architecture: integrated learning, acting, and planning 2.4 Physics-informed neural-network based deep Dyna-Q (dyna-PINN) control: 2.5 Neural network based Deep-Dyna-Q (NN-DDQ) control 2.6 RC-model based Deep-Dyna-Q (RC-DDQ) control 2.7 Baslines 2.7.1 Model-free RL - first baseline 2.7.2 Rule-based control (RBC) - second baseline 3 Experimental setup 3.1 Building models: BOPTEST framework 3.2 Initial configurations 3.3 Evaluation criteria and success metrics 3.4 System setup and hyperparameter tuning for RL architecture and DDQ models 4 Results and discussion 4.1 Prediction performance of the models 4.2 Control performance of the proposed DDQ-Controllers 4.2.1 Performance in low-diversity data regime 4.2.2 Performance in high-diversity data regime 4.2.3 Complete overview of all controllers 5 Discussion 6 Conclusion Declaration of the use of generative AI and AI-assisted technologies in writing CRediT authorship contribution statement Acknowledgements Appendix A Hyper-parameter sweep ranges and initial guesses with network architectures Appendix B Finalized hyperparameters and network architectures for DQN policy network and DDQ agent models B.1 RC-model parameters Appendix C Parameters of the heat pump References TRACKING I DUSSAULT 2016 680 691 J SALPAKARI 2016 425 436 J SVETOZAREVIC 2022 118127 B DRGONA 2020 190 232 J KILLIAN 2016 403 412 M NAGY 2023 110435 Z HUCHUK 2021 B WANG 2020 115036 Z BIEMANN 2021 M WANG 2023 113696 X ARROYO 2022 118346 J BHARDWAJ M ZHANG 2019 287 296 C PROCEEDINGS6THACMINTERNATIONALCONFERENCESYSTEMSFORENERGYEFFICIENTBUILDINGSCITIESTRANSPORTATION BUILDINGHVACSCHEDULINGUSINGREINFORCEMENTLEARNINGVIANEURALNETWORKBASEDMODELAPPROXIMATION CHEN 2022 19160 19173 L AN 2023 149 158 Z PROCEEDINGS10THACMINTERNATIONALCONFERENCESYSTEMSFORENERGYEFFICIENTBUILDINGSCITIESTRANSPORTATIONBUILDSYS2023ASSOCCOMPMACHINERYACMSPECIALINTERESTGRPENERGYSYSTINFORMAT CLUESAFEMODELBASEDRLHVACCONTROLUSINGEPISTEMICUNCERTAINTYESTIMATION AFROZ 2018 64 84 Z GOKHALE 2022 G DINATALE 2022 119806 L ALSAYED 2024 110085 K WESTERMANN 2019 170 186 P YOUSIF M KORIVAND 2023 S ZHAO 2023 5356 5366 T BANERJEE C VONRUEDEN 2023 614 633 L CHEN 2023 110149 Y GOKHALE G ZHANG 2018 148 157 Z PROCEEDINGS5THCONFERENCESYSTEMSFORBUILTENVIRONMENTS PRACTICALIMPLEMENTATIONEVALUATIONDEEPREINFORCEMENTLEARNINGCONTROLFORARADIANTHEATINGSYSTEM PARK 2023 121364 B BUNNING 2022 118491 F COSTANZO 2016 81 90 G CHEN 2020 119866 Y QIU 2020 110055 S CORACI 2023 117303 D SUTTON 1991 160 163 R GAO 2023 106852 C BLUM 2021 586 610 D PUTERMAN 1990 331 434 M STOCHASTICMODELS CHAPTER8MARKOVDECISIONPROCESSES SUTTON 2018 R REINFORCEMENTLEARNINGINTRODUCTION FILAR 1997 J COMPETITIVEMARKOVDECISIONPROCESSES KARNIADAKIS 2021 422 440 G SHUKLA 2022 e24 K MCCLENNY 2023 111722 L SAEED 2023 M PROCEEDINGS10THACMINTERNATIONALCONFERENCESYSTEMSFORENERGYEFFICIENTBUILDINGSCITIESTRANSPORTATION PHYSICSINFORMEDMODELBASEDREINFORCEMENTLEARNINGPIMBRLCONTROLBUILDINGHEATINGSYSTEMSINLOWTRAININGDATAREGIMES FAN 2020 486 489 J PROCEEDINGS2NDCONFERENCELEARNINGFORDYNAMICSCONTROL ATHEORETICALANALYSISDEEPQLEARNING BROCKMAN G SAEEDX2024X114879 SAEEDX2024X114879XM 2026-10-13T00:00:00.000Z 2026-10-13T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2024 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies. 2024-10-07T10:59:58.162Z http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/aggregated-refined KU Leuven http://data.elsevier.com/vocabulary/SciValFunders/501100004040 http://sws.geonames.org/2802361 FWO Research Foundation Flanders http://data.elsevier.com/vocabulary/SciValFunders/501100003130 http://sws.geonames.org/2802361 https://doi.org/10.15223/policy-017 https://doi.org/10.15223/policy-037 https://doi.org/10.15223/policy-012 https://doi.org/10.15223/policy-029 https://doi.org/10.15223/policy-004 item S0378-7788(24)00995-2 S0378778824009952 1-s2.0-S0378778824009952 10.1016/j.enbuild.2024.114879 271089 2025-08-18T02:07:55.950635Z 2024-12-01 1-s2.0-S0378778824009952-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/MAIN/application/pdf/b9137401d089801703f46d17b0f8d164/main.pdf main.pdf pdf true 2353102 MAIN 18 1-s2.0-S0378778824009952-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/PREVIEW/image/png/0545c10ece5f58495b386070608cc6e6/main_1.png main_1.png png 54365 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0378778824009952-gr009.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr009/DOWNSAMPLED/image/jpeg/dc1a34ffee5ae4104016c1fcd70dee5d/gr009.jpg gr009 gr009.jpg jpg 71208 259 806 IMAGE-DOWNSAMPLED 1-s2.0-S0378778824009952-gr007.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr007/DOWNSAMPLED/image/jpeg/dbb91407a74e24a27471c5f5fdfa7b21/gr007.jpg gr007 gr007.jpg jpg 5314 49 421 IMAGE-DOWNSAMPLED 1-s2.0-S0378778824009952-gr008.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr008/DOWNSAMPLED/image/jpeg/669a9246686b0c82d54af918d1aca0ed/gr008.jpg gr008 gr008.jpg jpg 22278 321 582 IMAGE-DOWNSAMPLED 1-s2.0-S0378778824009952-gr001.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr001/DOWNSAMPLED/image/jpeg/b4877e61152dd1d9941aef1918f828b2/gr001.jpg gr001 true gr001.jpg jpg 12571 200 258 IMAGE-DOWNSAMPLED 1-s2.0-S0378778824009952-gr012.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr012/DOWNSAMPLED/image/jpeg/c5d3224e3839e40098c1a890b31e90cc/gr012.jpg gr012 gr012.jpg jpg 135980 577 806 IMAGE-DOWNSAMPLED 1-s2.0-S0378778824009952-gr002.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr002/DOWNSAMPLED/image/jpeg/326d173546294f869abe78ae9c556a54/gr002.jpg gr002 gr002.jpg jpg 44540 498 642 IMAGE-DOWNSAMPLED 1-s2.0-S0378778824009952-gr013.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr013/DOWNSAMPLED/image/jpeg/6f74cf158b2478dc3f1e9da454760797/gr013.jpg gr013 gr013.jpg jpg 142217 576 803 IMAGE-DOWNSAMPLED 1-s2.0-S0378778824009952-gr010.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr010/DOWNSAMPLED/image/jpeg/2fa3e75a6eb08106f81e6c0791ee46c4/gr010.jpg gr010 gr010.jpg jpg 59313 687 807 IMAGE-DOWNSAMPLED 1-s2.0-S0378778824009952-gr011.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr011/DOWNSAMPLED/image/jpeg/2b1457e8e3238cbf37bc640e5195c5e8/gr011.jpg gr011 gr011.jpg jpg 23848 340 581 IMAGE-DOWNSAMPLED 1-s2.0-S0378778824009952-gr005.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr005/DOWNSAMPLED/image/jpeg/adfde217983d510a84f76ff5a8f7ddd2/gr005.jpg gr005 gr005.jpg jpg 38546 514 497 IMAGE-DOWNSAMPLED 1-s2.0-S0378778824009952-gr006.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr006/DOWNSAMPLED/image/jpeg/a1058c2bbedd45a0c5eed20675056df4/gr006.jpg gr006 gr006.jpg jpg 12951 161 467 IMAGE-DOWNSAMPLED 1-s2.0-S0378778824009952-gr003.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr003/DOWNSAMPLED/image/jpeg/404a405742cfb28d2a65e3646e1178f9/gr003.jpg gr003 gr003.jpg jpg 58206 341 816 IMAGE-DOWNSAMPLED 1-s2.0-S0378778824009952-gr014.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr014/DOWNSAMPLED/image/jpeg/64c801d654f6b509fa781b16dabc8519/gr014.jpg gr014 gr014.jpg jpg 56154 312 647 IMAGE-DOWNSAMPLED 1-s2.0-S0378778824009952-gr004.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr004/DOWNSAMPLED/image/jpeg/8659d2e8f9f1d712cb46fd4048a5749e/gr004.jpg gr004 gr004.jpg jpg 18750 223 495 IMAGE-DOWNSAMPLED 1-s2.0-S0378778824009952-gr015.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr015/DOWNSAMPLED/image/jpeg/82e66da74fa85dc201611daa4386f916/gr015.jpg gr015 gr015.jpg jpg 103523 601 807 IMAGE-DOWNSAMPLED 1-s2.0-S0378778824009952-gr009.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr009/THUMBNAIL/image/gif/dc3aa02a6d53dddf35a81fdc10931013/gr009.sml gr009 gr009.sml sml 9749 70 219 IMAGE-THUMBNAIL 1-s2.0-S0378778824009952-gr007.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr007/THUMBNAIL/image/gif/22ba9f880969b3a6a692eb6d716f8977/gr007.sml gr007 gr007.sml sml 1923 26 219 IMAGE-THUMBNAIL 1-s2.0-S0378778824009952-gr008.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr008/THUMBNAIL/image/gif/c195d6df3ecad86cf6ad5c8e5bd083c5/gr008.sml gr008 gr008.sml sml 4287 121 219 IMAGE-THUMBNAIL 1-s2.0-S0378778824009952-gr001.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr001/THUMBNAIL/image/gif/bfcfadfcdc8feb62b9ad079d87c0a382/gr001.sml gr001 true gr001.sml sml 6467 164 211 IMAGE-THUMBNAIL 1-s2.0-S0378778824009952-gr012.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr012/THUMBNAIL/image/gif/0a7d2f6243fec0c0d641a2612889fa53/gr012.sml gr012 gr012.sml sml 15246 157 219 IMAGE-THUMBNAIL 1-s2.0-S0378778824009952-gr002.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr002/THUMBNAIL/image/gif/74a50d31db0f0cc01867f02538464f93/gr002.sml gr002 gr002.sml sml 5353 164 211 IMAGE-THUMBNAIL 1-s2.0-S0378778824009952-gr013.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr013/THUMBNAIL/image/gif/e5d61fe7d3d1f5e26c53f8b3217a3131/gr013.sml gr013 gr013.sml sml 14471 157 219 IMAGE-THUMBNAIL 1-s2.0-S0378778824009952-gr010.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr010/THUMBNAIL/image/gif/a4028ba5f432f4e7f519aac13d4fa175/gr010.sml gr010 gr010.sml sml 4371 164 192 IMAGE-THUMBNAIL 1-s2.0-S0378778824009952-gr011.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr011/THUMBNAIL/image/gif/5077676ee586caf0a7b796d7bdb7ca0f/gr011.sml gr011 gr011.sml sml 3744 128 219 IMAGE-THUMBNAIL 1-s2.0-S0378778824009952-gr005.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr005/THUMBNAIL/image/gif/41721e9d6603a78db78e39d5baaa3183/gr005.sml gr005 gr005.sml sml 4367 163 158 IMAGE-THUMBNAIL 1-s2.0-S0378778824009952-gr006.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr006/THUMBNAIL/image/gif/9cc98eb520a91d0ba06014cd678ec098/gr006.sml gr006 gr006.sml sml 2636 76 219 IMAGE-THUMBNAIL 1-s2.0-S0378778824009952-gr003.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009794/gr003/THUMBNAIL/image/gif/824109f4ee2020b6d58d8421ff917033/gr003.sml gr003 gr003.sml sml 4239 91 219 IMAGE-THUMBNAIL 1-s2.0-S0378778824009952-gr014.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr014/THUMBNAIL/image/gif/22d0896287290d45f0013648ba81af74/gr014.sml gr014 gr014.sml sml 4321 106 219 IMAGE-THUMBNAIL 1-s2.0-S0378778824009952-gr004.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/gr004/THUMBNAIL/image/gif/e63f7008d9161e402249469e9e2ab1b3/gr004.sml gr004 gr004.sml sml 3489 99 219 IMAGE-THUMBNAIL 1-s2.0-S0378778824009952-gr015.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009794/gr015/THUMBNAIL/image/gif/c9f6d84f47af28d9f76ac94cebdcfa59/gr015.sml gr015 gr015.sml sml 13404 163 219 IMAGE-THUMBNAIL 1-s2.0-S0378778824009952-gr009_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/HIGHRES/image/jpeg/b5b80df23752e4dbd656fbe1fbb222d8/gr009_lrg.jpg gr009 gr009_lrg.jpg jpg 660064 1146 3570 IMAGE-HIGH-RES 1-s2.0-S0378778824009952-gr007_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/HIGHRES/image/jpeg/179b29aba7b90d013d8b89dd66769698/gr007_lrg.jpg gr007 gr007_lrg.jpg jpg 115865 437 3726 IMAGE-HIGH-RES 1-s2.0-S0378778824009952-gr008_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/HIGHRES/image/jpeg/ca003a36b64633c3db19399ebd18405e/gr008_lrg.jpg gr008 gr008_lrg.jpg jpg 83675 852 1546 IMAGE-HIGH-RES 1-s2.0-S0378778824009952-gr001_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/HIGHRES/image/jpeg/f1cf7f3eb7afb4b2aa9c97245cfbfb94/gr001_lrg.jpg gr001 true gr001_lrg.jpg jpg 52674 531 685 IMAGE-HIGH-RES 1-s2.0-S0378778824009952-gr012_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/HIGHRES/image/jpeg/c3ad4d5a2c66906ef78cb088e821be5c/gr012_lrg.jpg gr012 gr012_lrg.jpg jpg 1173483 2556 3568 IMAGE-HIGH-RES 1-s2.0-S0378778824009952-gr002_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/HIGHRES/image/jpeg/6854636ae803dfc72063beff4a2294f4/gr002_lrg.jpg gr002 gr002_lrg.jpg jpg 157291 1325 1707 IMAGE-HIGH-RES 1-s2.0-S0378778824009952-gr013_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/HIGHRES/image/jpeg/d56c7f534bd89ea0f4a0c56ed757a469/gr013_lrg.jpg gr013 gr013_lrg.jpg jpg 1258915 2548 3555 IMAGE-HIGH-RES 1-s2.0-S0378778824009952-gr010_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/HIGHRES/image/jpeg/cee544b8b5dd77057629022bd541b427/gr010_lrg.jpg gr010 gr010_lrg.jpg jpg 408949 3042 3571 IMAGE-HIGH-RES 1-s2.0-S0378778824009952-gr011_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/HIGHRES/image/jpeg/b6dea7caa8ccfefbf472668f3dc74a0b/gr011_lrg.jpg gr011 gr011_lrg.jpg jpg 186401 1505 2571 IMAGE-HIGH-RES 1-s2.0-S0378778824009952-gr005_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/HIGHRES/image/jpeg/29e7e3a6847e103048ce7c16a55c66c1/gr005_lrg.jpg gr005 gr005_lrg.jpg jpg 134171 1366 1322 IMAGE-HIGH-RES 1-s2.0-S0378778824009952-gr006_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/HIGHRES/image/jpeg/95910155179a2f61d5520df8917fb2fb/gr006_lrg.jpg gr006 gr006_lrg.jpg jpg 47773 429 1241 IMAGE-HIGH-RES 1-s2.0-S0378778824009952-gr003_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009794/HIGHRES/image/jpeg/67e90eeebe3de32b20c4f246f4fe64cb/gr003_lrg.jpg gr003 gr003_lrg.jpg jpg 1660171 3018 7224 IMAGE-HIGH-RES 1-s2.0-S0378778824009952-gr014_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1069938422000195/HIGHRES/image/jpeg/d85382ea69a105387ad8fec187deb7e4/gr014_lrg.jpg gr014 gr014_lrg.jpg jpg 535022 1383 2867 IMAGE-HIGH-RES 1-s2.0-S0378778824009952-gr004_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/HIGHRES/image/jpeg/0f21043eb73162dfb9b58c388dc56716/gr004_lrg.jpg gr004 gr004_lrg.jpg jpg 75224 592 1315 IMAGE-HIGH-RES 1-s2.0-S0378778824009952-gr015_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009794/HIGHRES/image/jpeg/708dbd98ce361ddfc9d20d853fd7b42f/gr015_lrg.jpg gr015 gr015_lrg.jpg jpg 819749 2662 3575 IMAGE-HIGH-RES 1-s2.0-S0378778824009952-si1.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/9541987f8b4a4f01bd65dbe0917546bb/si1.svg si1 si1.svg svg 5678 ALTIMG 1-s2.0-S0378778824009952-si10.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/0013d1f57ee4b38b2c02c8ff5863fc0a/si10.svg si10 si10.svg svg 5646 ALTIMG 1-s2.0-S0378778824009952-si11.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/b61c0275f316ba7b49165c3a6c56eae4/si11.svg si11 si11.svg svg 14331 ALTIMG 1-s2.0-S0378778824009952-si12.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/381b945bc6623116df99e4abc03885c0/si12.svg si12 si12.svg svg 1904 ALTIMG 1-s2.0-S0378778824009952-si13.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/774612a12ceb1d76f8944cdcf3d219b0/si13.svg si13 si13.svg svg 2119 ALTIMG 1-s2.0-S0378778824009952-si14.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/e77110fa448effaa864394e0c31304fd/si14.svg si14 si14.svg svg 28288 ALTIMG 1-s2.0-S0378778824009952-si15.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/b364cd507433334dc34dd7ca821136cb/si15.svg si15 si15.svg svg 13414 ALTIMG 1-s2.0-S0378778824009952-si16.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/8ebdedd1576da7ba299d38f1c2731f4d/si16.svg si16 si16.svg svg 6400 ALTIMG 1-s2.0-S0378778824009952-si17.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/6f9396d1b935fd114334ec32e190ac70/si17.svg si17 si17.svg svg 2938 ALTIMG 1-s2.0-S0378778824009952-si18.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/290d995617d9e33b5bbcabfefb276a25/si18.svg si18 si18.svg svg 1863 ALTIMG 1-s2.0-S0378778824009952-si19.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/bd9494099894b979930a3f71d4dea0f5/si19.svg si19 si19.svg svg 3569 ALTIMG 1-s2.0-S0378778824009952-si2.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/89a427c83df1a80ffbb4477b2adcdbfc/si2.svg si2 si2.svg svg 7621 ALTIMG 1-s2.0-S0378778824009952-si20.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/0dcfd54b2fc1784aeb3991fe52cab219/si20.svg si20 si20.svg svg 2404 ALTIMG 1-s2.0-S0378778824009952-si21.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/4204993deeb3194ce487c7400c254546/si21.svg si21 si21.svg svg 2201 ALTIMG 1-s2.0-S0378778824009952-si22.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/0beb5e2756f78653fee5f4a372e3c62d/si22.svg si22 si22.svg svg 2340 ALTIMG 1-s2.0-S0378778824009952-si23.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/c7e42135ebc59fc07c3ae80e78c597b3/si23.svg si23 si23.svg svg 2248 ALTIMG 1-s2.0-S0378778824009952-si24.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/733cb91d6d53f4ce7a8afaf9af6c2ea3/si24.svg si24 si24.svg svg 1974 ALTIMG 1-s2.0-S0378778824009952-si25.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/edd216f31f26b5513e5eb51f6f2dac5a/si25.svg si25 si25.svg svg 1879 ALTIMG 1-s2.0-S0378778824009952-si26.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/1e79b26468df34be072b1e3621c67de1/si26.svg si26 si26.svg svg 2027 ALTIMG 1-s2.0-S0378778824009952-si27.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/8d562a40b0bbce1fdcabeddc9c1a00ef/si27.svg si27 si27.svg svg 1935 ALTIMG 1-s2.0-S0378778824009952-si28.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/01ae1b8c1654f11988064e1d14966a59/si28.svg si28 si28.svg svg 5554 ALTIMG 1-s2.0-S0378778824009952-si29.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/07e646bedb592800e39664a0c814698c/si29.svg si29 si29.svg svg 2733 ALTIMG 1-s2.0-S0378778824009952-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/ecf372c12d02861d175465baf7ae5eb5/si3.svg si3 si3.svg svg 1894 ALTIMG 1-s2.0-S0378778824009952-si30.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/429b6c25518d34f54fe981f373f71c87/si30.svg si30 si30.svg svg 5551 ALTIMG 1-s2.0-S0378778824009952-si31.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/510c2829b9c078e4923ea853ae21c059/si31.svg si31 si31.svg svg 13375 ALTIMG 1-s2.0-S0378778824009952-si32.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/548d85417a7213ceeb2454163d0f1598/si32.svg si32 si32.svg svg 16415 ALTIMG 1-s2.0-S0378778824009952-si33.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/b88da3f8f7326c183432dd5932f7e366/si33.svg si33 si33.svg svg 8804 ALTIMG 1-s2.0-S0378778824009952-si34.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/1cd289973ff087dc4109c9451e0094dc/si34.svg si34 si34.svg svg 13551 ALTIMG 1-s2.0-S0378778824009952-si35.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/2b4c758abba27c293c0b91b7e7ce51e3/si35.svg si35 si35.svg svg 3520 ALTIMG 1-s2.0-S0378778824009952-si36.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/5ba11a353589972b05447812091b31d3/si36.svg si36 si36.svg svg 17981 ALTIMG 1-s2.0-S0378778824009952-si37.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/05c0ec958a40be2ecae06e2963496c14/si37.svg si37 si37.svg svg 10298 ALTIMG 1-s2.0-S0378778824009952-si38.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/db635f9beec8070d0e1fb97953a3c091/si38.svg si38 si38.svg svg 25517 ALTIMG 1-s2.0-S0378778824009952-si39.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/5dd856c4899589bd522eada3e30dc1cc/si39.svg si39 si39.svg svg 14269 ALTIMG 1-s2.0-S0378778824009952-si4.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/b2847e55f13110e6e1821ac7832b2518/si4.svg si4 si4.svg svg 6380 ALTIMG 1-s2.0-S0378778824009952-si40.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/a233cbfcc51966c34acdd1965256c126/si40.svg si40 si40.svg svg 18882 ALTIMG 1-s2.0-S0378778824009952-si41.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/d52a05e3a604d4ae6ccc3e53f0805ae5/si41.svg si41 si41.svg svg 5566 ALTIMG 1-s2.0-S0378778824009952-si42.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/e43f3c2e42f01f6eed5d24651702662e/si42.svg si42 si42.svg svg 26749 ALTIMG 1-s2.0-S0378778824009952-si43.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/50f8a253672a4c3c20fd0d8d025927b6/si43.svg si43 si43.svg svg 4744 ALTIMG 1-s2.0-S0378778824009952-si44.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/a4cf69bcc548b9044bc756692e946c86/si44.svg si44 si44.svg svg 5434 ALTIMG 1-s2.0-S0378778824009952-si45.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/80c4c7936dbf66f980710e0d7a04cd1e/si45.svg si45 si45.svg svg 21946 ALTIMG 1-s2.0-S0378778824009952-si46.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/f8da43352a8ba704c4b035190d1bb9d2/si46.svg si46 si46.svg svg 2027 ALTIMG 1-s2.0-S0378778824009952-si47.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/3b18fe503d03429983697747a4adedea/si47.svg si47 si47.svg svg 6520 ALTIMG 1-s2.0-S0378778824009952-si48.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/8c6306af4057d90457b466be2eb39833/si48.svg si48 si48.svg svg 6388 ALTIMG 1-s2.0-S0378778824009952-si49.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/94bde47e45dc2fec34cf251005425413/si49.svg si49 si49.svg svg 8415 ALTIMG 1-s2.0-S0378778824009952-si5.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/d49ec293591a5a081314042f175d9133/si5.svg si5 si5.svg svg 4353 ALTIMG 1-s2.0-S0378778824009952-si50.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/61336e548d1b59c6efee5cde95f7f8c1/si50.svg si50 si50.svg svg 7518 ALTIMG 1-s2.0-S0378778824009952-si51.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/02c328881c0d944f36765f9d873ac9d5/si51.svg si51 si51.svg svg 4936 ALTIMG 1-s2.0-S0378778824009952-si52.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/2c7ec6e3cbdcdec289a8154bda3be4ec/si52.svg si52 si52.svg svg 4262 ALTIMG 1-s2.0-S0378778824009952-si6.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/bb3436e8e1f9b28cfab43fd389733d4d/si6.svg si6 si6.svg svg 27119 ALTIMG 1-s2.0-S0378778824009952-si7.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/99daa899877fa284be7245504a4f8018/si7.svg si7 si7.svg svg 24672 ALTIMG 1-s2.0-S0378778824009952-si8.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/cb177e79c3c0143a96458abd8c458f01/si8.svg si8 si8.svg svg 22302 ALTIMG 1-s2.0-S0378778824009952-si9.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0378778824009952/image/svg+xml/f83941fa682215653cffce70b4ebb7e6/si9.svg si9 si9.svg svg 11313 ALTIMG 1-s2.0-S0378778824009952-am.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:10HQ4NNRGB9/MAIN/application/pdf/7de8de482badf869e38833db6289bca1/am.pdf am am.pdf pdf false 1969432 AAM-PDF ENB 114879 114879 S0378-7788(24)00995-2 10.1016/j.enbuild.2024.114879 Elsevier B.V. Fig. 1 Dyna-PINN Schematic. Fig. 1 Algorithm 1 Physics-Informed Deep Dyna-Q (Dyna-PINN) Approach. Algorithm 1 Fig. 2 NN-DDQ Schematic. Fig. 2 Fig. 3 RC-DDQ Schematic. Fig. 3 Fig. 4 Deep Q-Network Schematic. Fig. 4 Fig. 5 Schematic representation of the baseline controller. Fig. 5 Fig. 6 BESTEST Hydonic Heat Pump Use-case Schematic [38]. Fig. 6 Fig. 7 Plots of outdoor ambient temperature and direct normal solar irradiation during the training and testing periods of the controllers. (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.) Fig. 7 Fig. 8 Boxplots of the Mean Absolute Error (MAE) for PINN, NN, and RC models across 6-week and 27-week scenarios, with 50 and 100 planning steps. Plots (a) and (b) represent the 6-week scenarios with 50 and 100 planning steps, respectively, while plots (c) and (d) represent the 27-week scenarios for the same planning steps. The boxplots visualize the distribution of the MAE across episodes, providing insights into the prediction accuracy of the models. The PINN models generally show more consistent and lower MAE values across all scenarios, while the NN and RC models display larger variations, particularly in the longer prediction horizon (greater number of planning steps). Fig. 8 Fig. 9 Performance of rule-based-controller (RBC). Fig. 9 Fig. 10 Performance of RL controllers trained in low-diversity data regimes. Fig. 10 Fig. 11 Performance of RL controllers trained in high-diversity data regimes. Fig. 11 Fig. 12 Thermal Discomfort vs Total Energy Consumed for various RL agents and the RBC controller. The red dots represent agents trained on a 27-week observational period, while the blue dots represent agents trained on a 6-week observational period. The green dot corresponds to the RBC controller. The y-axis represents the thermal discomfort in Celsius hours on a logarithmic scale, while the x-axis shows the total energy consumption in kWh. Fig. 12 Fig. 13 Averaged prediction errors and standard deviations for PINN-6, NN-6, and RC-6 models across 50 and 100 planning steps. The plots on the left display the absolute averaged errors, while the plots on the right show the raw averaged errors. The top row corresponds to 50 planning steps, and the bottom row corresponds to 100 planning steps. The standard deviation regions (±2 standard deviations) are shaded, and the average error lines are outlined to improve visibility. These results illustrate the error dynamics of each model, highlighting their predictive performance over different prediction horizons in the 6-week scenario. Fig. 13 Table 1 MDP Components for Building Thermal Control. Table 1 MDP Component Description State (S) Indoor zone temperature (T in)Ambient temperature (T out)Solar irradiance (P sol)Price (P) - constant profile Lagged values: T in,t−1, T in,t−2, T in,t−3, T in,t−4, T out,t−1, T out,t−2, T out,t−3, T out,t−4, P sol,t−1, P sol,t−2, P sol,t−3, P sol,t−4 Input Dimensionality: 16 dimensions Action (A) Heat Pump control: ON, OFF Transition (P) Building dynamics, influenced by actions and environmental conditions Reward / Cost (R) −(α\u202f⋅\u202fEnergy used in heating\u202f+\u202fβ\u202f⋅\u202fIndoor Thermal Discomfort), where α and β are scaling factors to vary importance of each term. Table 2 Summary of Initial Configurations. Table 2 Configuration Setting Control Step 30 minutes Forecasting None RC Model Two-state RC model (2R2C) trained on BOPTEST use case Data Selection Random batch selection with shuffle=True (ensures diversity) Learning Rates (LR) LR Planning = LR DQN / planning _ steps Table 3 Training Data Regimes, Episodes, and Planning Steps. Table 3 Data Regime Data Observation Window Training Episodes Planning Steps Low-Diversity Data Regime January 1 to February 15 (6 weeks) 25 50, 100 50 50, 100 High-Diversity Data Regime January 1 to June 30 (27 weeks) 25 50, 100 50 50, 100 Table 4 Description of the Six Models used in DDQ Analysis. Table 4 Model Description Loss Function used for Training PINN-6 Physics-Informed Neural Network model trained on a data from 6-week observation window MSE(T in,NN)+λ\u202f⋅\u202fMSE(T in,RC) (Refer to Equation (1)) PINN-27 Physics-Informed Neural Network model trained on data from 27-week observation window MSE(T in,NN)+λ\u202f⋅\u202fMSE(T in,RC) NN-6 Neural Network model trained on data from 6-week observation window MSE(T in,NN) (Refer to Equation (2)) NN-27 Neural Network model trained on data from 27-week observation window MSE(T in,NN) RC-6 Resistance-Capacitance model trained on a 6-week observation window min \u2061 \u2016 T in, predicted − T in, actual \u2016 2 RC-27 Resistance-Capacitance model trained on a 27-week observation window min \u2061 \u2016 T in, predicted − T in, actual \u2016 2 Table A.5 Hyper-Parameter Sweep Ranges, and Initial Guesses for RL and RC Models. Table A.5 DQN Policy Network PINN & NN Model RC Model (2R2C) Learning Rate: [10−4,10−1] Discount Factor (γ): [0.9,0.99] Activation Function: Tanh, ReLU Optimizer: Adam, RMSProp Neurons per Layer: [8,16,32,64,128] Batch Size: [32,64,128] Learning Rate: [10−4,10−1] Epochs: [25,100] Activation Function: Tanh, ReLU Optimizer: Adam, SGD Neurons per Layer: [8,16,32,64,128] Batch Size: [32,64,128] Initial Guess for R 1: 0.01\u202fK/W Initial Guess for R 2: 0.01\u202fK/W Initial Guess for C 1: 1\u202f×\u202f106\u202fJ/K Initial Guess for C 2: 1\u202f×\u202f107\u202fJ/K Initial Guess for A i : 5\u202fm 2 Initial Guess for A e : 5\u202fm 2 Table B.6 Finalized Hyperparameters and Network Architectures for Model-Free DQN Policy Network, PINN Model, NN Model, and RC Model. Table B.6 DQN Policy Network PINN Model NN Model Learning Rate: 0.01 Discount Factor (γ): 0.99 Replay Buffer Size: 2\u202f×\u202f104 Exploration Strategy: ϵ-greedy Optimizer: Adam Batch Size: 64 Architecture: - Input: Dense, 16 inputs, 64 units - Hidden: Dense, 64 units, ReLU - Hidden: Dense, 8 units, ReLU - Output: Dense, 2 actions Learning Rate: 0.0091 Weight Decay: 1.05\u202f×\u202f10−5 Epochs: 50 Optimizer: Adam Batch Size: 64 Architecture: - Input: Dense, 17 inputs, 32 units - Hidden: Dense, 32 units - Layer Norm: 32 units - Activation: ReLU - Output: Dense, 1 unit Learning Rate: 0.0011 Weight Decay: 2.38\u202f×\u202f10−4 Epochs: 50 Optimizer: Adam Batch Size: 64 Architecture: - Input: Dense, 17 inputs, 128 units - Hidden: Dense, 128 units - Layer Norm: 128 units - Activation: ReLU - Output: Dense, 1 unit Dyna-PINN: Physics-informed deep dyna-q reinforcement learning for intelligent control of building heating system in low-diversity training data regimes Muhammad Hafeez Saeed Writing \u2013 original draft Visualization Software Methodology Investigation Formal analysis Conceptualization a b \u204e Hussain Kazmi Writing \u2013 review & editing Supervision Investigation Conceptualization a b Geert Deconinck Writing \u2013 review & editing Supervision Project administration Funding acquisition a b a Department of Electrical Engineering, KU Leuven, Belgium Department of Electrical Engineering KU Leuven Belgium Department of Electrical Engineering, KU Leuven, Belgium b EnergyVille, Thor Park, Genk, Belgium EnergyVille Thor Park Genk Belgium EnergyVille, Thor Park, Genk, Belgium \u204e Corresponding author. This paper introduces Dyna-PINN, a novel physics-informed Deep Dyna-Q (DDQ) reinforcement learning (RL) approach, designed to address the data-intensive training requirements and model-agnostic nature of the conventional model-free RL methods. The DDQ approach blends model-based and model-free elements to enhance both learning and decision-making processes. By utilizing a physics-informed neural network (PINN) based model, our method enriches the learning process with physical information, enhancing the agent's planning capabilities and leading to faster learning compared to conventional model-free RL methods like Deep Q-Network (DQN) in scenarios with low-diversity training data availability. Our results demonstrate that Dyna-PINN has 50% greater sample efficiency than DQN and outperforms rule-based control in terms of thermal discomfort. Due to physics incorporation, the Dyna-PINN implements a more logical and interpretable control policy. It shows consistently good performance compared to all control variants across low-diversity data scenarios, i.e., 6 weeks of building data, and in higher-diversity data regimes, i.e., 6 months of building energy data, demonstrating the value of physics incorporation into the RL training. Additionally, we present two other DDQ-based techniques, RC-DDQ and NN-DDQ, exploring the synergy between neural networks and physical data in intelligent control designs for building energy systems. Rigorous controller testing is performed using the Building Optimization and Testing Framework (BOPTEST), a high-fidelity simulator that closely represents a real building's operation. Through comprehensive comparisons and realistic simulations, our study underscores the effectiveness of incorporating physics-informed approaches into RL-based control strategies, paving the way for more efficient and robust building energy management systems. Graphical abstract Keywords Model-based reinforcement learning Physics-informed neural networks Buildings heating system Deep dyna-q approach Deep q-network Data availability Data will be made available on request. 1 Introduction Buildings account for 30% of global final energy use and 26% of global energy-related emissions, according to the Tracking Clean Energy Progress 2023 report by the IEA [1]. To mitigate this significant share of energy use and emissions, substantial measures for energy efficiency and the adoption of clean energy alternatives are necessary. One effective approach to enhance energy efficiency is through smart building heating control. 1.1 Classical and physics-based approaches for building heating control Traditional building control systems have typically relied on simple rule-based methods, such as using thermal comfort setpoints to control heat pump operation [2,3]. Although straightforward to implement, these techniques lack the ability to predict future scenarios or leverage historical data on energy use, indoor environments, and ambient conditions, limiting their energy saving potential [4]. To address these limitations, Physics-based control such as Model Predictive Control (MPC) was introduced, which uses a receding horizon optimization framework to account for future states of input features like weather and price data, optimizing for long-term energy saving benefits while maintaining thermal comfort [2,5]. However, MPC requires detailed physics-based models of buildings, making their performance highly dependent on accurate and comprehensive building data [6]. In many cases, access to such data is limited due to factors such as old buildings, lack of smart meters, or absence of building physics data [7]. 1.2 Reinforcement Learning for building heating control Data-driven control methods, such as Reinforcement learning (RL) offer a promising alternative by allowing control agents to learn optimal policies through interaction with the environment. These methods enable the control agent to generate its own data through interactions with the building environment, recording experiences that are later used for training the agent to maximize rewards. RL agents can be classified as either model-free and model-based. Model-free RL is oblivious to the underlying physical model of the system and relies entirely on collected experiences from the agent's interactions with the environment. Model-free RL, as explored by [8], enables agents to learn control strategies that maximize the expected cumulative returns without the need for a model of the Heating, Ventilation and Air Conditioning (HVAC) system of the building. This approach often faces challenges due to its requirement for enormous amounts of training data and a lack of knowledge about the physical properties of the building needed to learn a logical control policy [9], leading to sample inefficiency and the risk of sub-optimal actions during the learning phase [7]. To address these issues, policy-based methods such as actor-critic algorithms have been evaluated in simulated environments, highlighting their potential for maintaining thermal comfort while increasing energy efficiency. These methods combine the benefits of policy-based approaches with value-based methods, resulting in significant improvements over traditional controllers [10]. Some studies address the shortcomings of model-free RL by developing high-fidelity white-box environments to train the RL agents. In [11], a detailed physics-based simulated environment is designed to train a DQN agent for controlling an ice-based thermal energy system in commercial building cooling. However, despite the detailed simulated environment, training is conducted in a model-free manner, reducing sample efficiency, and extensive meta-data of the physical system is required, which is not readily available for all buildings. Techniques have been introduced that combine model-free RL and physics-based control techniques [12,13]. In [12], model-free RL-based value estimates are used for setting the parameters of the non-linear receding horizon problem in MPC. In [13], MPC is combined with model-free RL to generate local Q-function approximations. A parameter, λ, balances between MPC's control and RL's value estimates, reducing reliance on MPC as the RL model learns, enabling more efficient control for buildings. Conversely, Model-based RL (MBRL) uses an underlying model of the building to determine optimal control actions, offering an advantage by incorporating some understanding of the system into the training process increasing the sample efficiency of the training. Studies have highlighted the potential of MBRL for HVAC control, showing how these methods can improve the sample efficiency of the RL training process compared to model-free approaches [14,15]. Zhang et al. utilized an MBRL approach to learn a building's dynamics model for implementing MPC [14]. Chen et al. used a thermal dynamics model trained by supervised learning to implement a model-based RL method [15]. An et al. developed CLUE, an MBRL approach for HVAC control, designed to minimize human thermal comfort violations [16]. This method employs a Gaussian Process (GP) to model the building dynamics, leveraging a small dataset for training. The GP model quantifies uncertainty in the building dynamics and integrates this information into the safe HVAC control strategy [16]. There are three broad categories of models typically considered in model-based reinforcement learning (MBRL) for buildings. The first is purely data-driven, or \u2018black box\u2019 models, such as neural networks (NNs) and time series models like ARIMA [17]. The second category consists of hybrid models, which balance data-driven approaches with physics-based principles, examples being gray-box RC-models [18]. Lastly, there are physics-based white-box models, which rely on equation-based formulations tailored to specific applications [19]. Despite these advantages, the model's accuracy and type can greatly influence the effectiveness of the training [20,21], and the lack of visible information on incorporating physics in the methods explained in this sub-section further limits the performance of model-based RL approaches. 1.3 Physics-informed Reinforcement Learning Physics-informed reinforcement learning (PIRL) combines physical knowledge with reinforcement learning algorithms to enhance decision-making. This integration offers a transformative approach to control in various fields, enabling the exploitation of both data-driven insights and underlying physical principles. Applications of PIRL include complex flow-field reconstruction [22], musculoskeletal simulations [23], and power system stability [24], among others [25]. This hybrid method is particularly effective in building control scenarios, where both data insights and physical principles are crucial [26]. [27] presents PhysCon, a PINN-based model for modeling the thermal dynamics of buildings, demonstrating superior performance compared to purely data-driven models. However, it has not yet been applied in a control framework to demonstrate its efficacy in building heating control applications. Gokhale et al. demonstrated the application of physics-informed neural networks for thermal modeling in buildings, enhancing control accuracy and efficiency [18]. The PhysQ framework, as detailed by Gokhale et al. [28], incorporates the PINN model from [18] into the RL process, enabling more accurate predictions and efficient control strategies. However, this technique bases its RL training on a simplified two-state RC environment as a building simulator, which lacks the necessary detail for a real-world building use case. Moreover, the testing strategy is limited to the same environment and evaluated over a short 5-day window, which does not provide a robust test of the controller's performance. In another study, a purely physics-based model was employed to train a deep reinforcement learning algorithm using Asynchronous Advantage Actor-Critic (A3C) in a model-free manner [29], though a drawback of this approach is its sample inefficiency, requiring detailed building data to design the physics-based simulator. A more recent study developed climate-adapted reinforcement learning controllers by first learning an initial policy through imitation learning, followed by refining the policy using the policy-gradient REINFORCE algorithm to adapt to specific climatic conditions in U.S. cities [30]. However, this approach lacks any explicit incorporation of physics-based insights into the RL training process. In addition to the physics-informed RL approaches mentioned earlier, [31] presents an MPC approach that uses physics-informed Autoregressive\u2013Moving-Average with Exogenous Inputs (ARMAX) model for building heating and cooling control. The study demonstrates that the MPC using physics-informed ARMAX model outperforms MPC using conventional machine learning models, such as random forests and neural networks, highlighting the value of physics-informed machine learning in control systems. However, a robust physics-informed RL approach is not found in the literature which takes into account the benefits of model-based RL that is sample-efficient and uses a standardized testing approach for building heating control. 1.4 Challenges in training with low-diversity data regimes Both physics-based control and traditional reinforcement learning (RL) methods often face challenges in low-diversity data scenarios, which are common in building control applications. These challenges arise due to the high cost of real-world experimentation and the potential disruption to occupants' thermal comfort [32]. A known approach for addressing this in MPC-based heating and ventilation control is transfer learning, where a pre-trained neural network is fine-tuned with small amounts of measurement data from the target building [33]. In the context of RL, options include model-free RL, which demands extensive data, or model-based RL, which may inadequately capture physical dynamics, resulting in suboptimal performance in low-diversity data regimes [34]. For instance, in [35], an LSTM-based environment model trained on simulated data is used by the RL agent, but the model lacks physical insights into the building's thermal dynamics. Another technique is Dyna architecture, a special type of RL combining both model-free and model-based RL techniques enabling sample efficient RL learning with dynamic model-learning of the underlying system [36]. Gao and Wang [37] demonstrated that Dyna-style RL achieves higher sample efficiency and reduced training time compared to model-free alternatives for controlling building HVAC systems. However, in their study, the learned building model is purely data-driven, with no incorporation of physics, which limits its generalizability to unseen cases. This highlights the necessity for a physics-informed reinforcement learning (PIRL) approach that can address the challenges of low-diversity data regimes in building heating control, while leveraging both model-free and model-based RL techniques to provide a robust control solution. 1.5 Contribution To address the significant gaps in the state of the art, such as the sample inefficiency and absence of physics-based insights in model-free RL, the reliance of model-based RL on accurate but often difficult-to-obtain environmental models (particularly when these models are purely data-driven), and the lack of standardized testing environments (as discussed in Subsections 1.2, 1.3, and 1.4), this paper presents Dyna-PINN: a novel physics-aware, model-based reinforcement learning approach designed to tackle these key research gaps. By combining the strengths of model-free RL with the advantages of physics-informed, model-based RL, Dyna-PINN employs the Deep Dyna-Q (DDQ) framework to enhance both sample efficiency and decision-making in low-diversity data scenarios. The approach has been rigorously tested on a building model from the BOPTEST framework [38], which provides a standardized testing environment, where it demonstrated substantial improvements in generalization, physics-aware decision-making, and robust control for heating systems. The key contributions of this work are as follows: 1. We propose a novel reinforcement learning (RL) approach that integrates physics-informed neural networks (PINNs) within the Deep Dyna-Q (DDQ) framework, addressing challenges posed by low-diversity data regimes such as scenarios with limited real-world interactions or short observation windows (e.g., 6 weeks). This integration significantly enhances decision-making and control performance by incorporating physics-based insights into the RL process. To the best of our knowledge, this is the first exploration of a physics-informed DDQ framework for building heating control, bridging the gap between model-free RL, and model-based RL. 2. We rigorously evaluate the Dyna-PINN approach, along with two novel variants\u2014RC-DDQ (utilizing RC-based physics models) and NN-DDQ (using purely data-driven models)\u2014within the standardized BOPTEST framework. These approaches are benchmarked against existing methods, including model-free RL (Deep Q-Network, DQN) and rule-based control (RBC), to evaluate their performance on key metrics such as thermal comfort, energy efficiency, and error reduction, measured by mean absolute error (MAE) and residuals. Our evaluations demonstrate that the proposed Dyna-PINN approach outperforms these strong benchmarks across all key metrics, particularly in scenarios with low-diversity training data. 2 Methodology In this section, we explain the methodology of various techniques utilized in designing the DDQ-based controller variants. 2.1 Markov decision process (MDP) formulation In this study, the building's thermal system is modeled as a Markov Decision Process (MDP), a mathematical framework for modeling decision-making in situations where outcomes are partly under the control of an agent and partly stochastic. The MDP framework is crucial for the development of reinforcement learning-based control, as it allows us to model the interactions between the agent (the control system) and the environment (the building and its thermal conditions) over discrete time steps. An MDP is defined by a tuple ( S , A , P , R ) , where: \u2022 S is the set of states representing the environment's configurations (e.g., indoor temperature, outdoor temperature, etc). \u2022 A is the set of actions available to the agent (e.g., turning the heat pump on or off). \u2022 P : S × A × S \u2032 → [ 0 , 1 ] is the state transition probability function, which defines the probability of transitioning from state S to state S \u2032 after taking action A. \u2022 R : S × A × S \u2032 → R is the reward function, which assigns a scalar value representing the desirability of a transition from S to S \u2032 via action A. The objective of the reinforcement learning algorithm in this context is to derive an optimal policy π : S → A that maximizes the expected cumulative reward, also known as the return, over a specified time horizon. The policy π dictates the agent's actions based on the current state S, aiming to optimize the trade-off between minimizing energy consumption and maintaining thermal comfort for the building occupants [39,40]. The key MDP components for our building heating system are summarized in Table 1 . The formulation of the building's thermal control as an MDP allows for the application of advanced reinforcement learning techniques, such as the Physics-Informed Deep Dyna-Q (Dyna-PINN) method, which improves upon traditional model-free methods by integrating physical knowledge of the system into the learning process. Additionally, traditional approaches such as Deep Q-Network (DQN) can also leverage the MDP formulation for control decisions [39,41,40]. 2.2 Physics informed neural networks (PINNs) Physics-informed neural networks (PINNs) serve as versatile function approximators capable of incorporating physical laws during training, provided these laws can be expressed using ordinary/partial differential equations (PDEs). PINNs are particularly valuable in scenarios with limited training data, where conventional data-driven methods may lack robustness and generalizability [42,43]. In the context of building control, where data on energy usage is often limited and detailed building physics information may not be readily accessible, PINNs provide a potential solution. By representing the model of the building, PINNs generate physics-informed simulated experiences for model-based reinforcement learning agents to learn from. We define a PINN model as a neural network that is infused with physical information using building RC models. The order of the RC model used is not critical, as long as it adequately represents the building use case considered. In this analysis, we tune the RC model's parameters to represent a single-zone BOPTEST environment. We use non-linear least squares to fit a function to the available building energy use data, which includes measured heating input data, outdoor temperature, solar irradiation, and past indoor room temperatures, to predict the next time step's indoor room temperature. There are several ways to train a PINN model. We used the learning bias approach, where a custom loss function is defined as the regularized sum of errors in predicting indoor room temperature from both neural networks and RC models [42]. The objective is to train the neural network predictions to align with the pattern of predictions from the RC model, thus inherently embedding its physical laws into the neural network training process. The custom loss function is expressed as follows: (1) Custom Loss = MSE ( T in, NN ) + λ ⋅ MSE ( T in, RC ) (2) MSE ( T in, NN ) = 1 N ∑ i = 1 N ( T in, NN , i − T in, actual , i ) 2 (3) MSE ( T in, RC ) = 1 N ∑ i = 1 N ( T in, RC , i − T in, NN , i ) 2 Here, MSE denotes Mean Squared Error. The first term, MSE ( T in, NN ) , is the regression loss, which calculates the error between the predicted indoor room temperature T in, NN from the neural network and the actual values. The second term, MSE ( T in, RC ) , is the physics-based loss, which calculates the error between the predictions of the RC-model and the neural network. The parameter λ is a hyperparameter used for regularization and is tuned in a self-adaptive manner during training, controlling the weight of the physics-based loss term in the custom loss function (explained in detail by [44]). The reason we compute the error of the RC-model with the NN model predictions is to ensure that the neural network predictions are consistent with the physical dynamics captured by the RC-model. This allows us to calculate the gradient of the loss function, ensuring that the physics-based term influences the training process. This custom loss function incorporates the physical knowledge from the RC model, guiding the network to learn in a physics-consistent manner. 2.3 Deep Dyna-Q (DDQ) architecture: integrated learning, acting, and planning We adopted the DDQ framework to address challenges encountered in controlling building heating systems when faced with limited training data and incomplete physical information [36]. This framework allows us to integrate physics into reinforcement learning, thereby enabling a model-based RL approach. Additionally, DDQ framework facilitates the simultaneous handling of both model-free and model-based RL, through acting, learning, and planning within an integrated framework. This work is an extension of our previous work about physics-informed model-based RL in low data regimes [45]. DQN serves as the underlying reinforcement learning approach in this integrated framework, hence it is referred to as Deep Dyna-Q (DDQ). The DDQ framework consists of three main steps: \u2022 Direct Reinforcement Learning (RL): In the first step, the DDQ framework utilizes a model-free RL algorithm to train the building control agent based on real interactions within the BOPTEST environment. During this stage, the agent learns from its experiences without prior knowledge of the building's underlying model. To effectively handle the high-dimensional state space (16-dimensional state vector, refer to Table 1), we chose a neural network-based Q-function RL approximator called Deep Q-Network (DQN). The DQN focuses on maximizing cumulative rewards (or minimizing cumulative costs) by learning a policy that dictates actions based on expected rewards [46]. All real experiences are stored in a buffer, referred to as the real memory buffer ( D r ). \u2022 Model Learning: In this step, a model of the BOPTEST environment is learned using randomly sampled experiences stored in the memory buffer obtained during the direct RL phase. Three variants of the models are learned: data-driven, purely physics-based RC model, and PINN-based models. Each variant of the model when used within the DDQ framework is named accordingly, such as Dyna-PINN for the PINN-based model, NN-DDQ for the data-driven model, and RC-DDQ for the purely physics-driven model. Detailed descriptions of the three model learning variants are provided in their respective sub-sections. \u2022 Planning: In the third step of the DDQ approach, planning comes into play. Here, we leverage the learned model of the BOPTEST environment to generate additional state-action experiences, which we refer to as simulated experiences. These interactions are then stored in a dedicated memory buffer known as the simulated memory buffer ( D s ). The primary purpose of this buffer is to provide model-based experiences that are generated faster than the real experiences, and are enriched insights governing the underlying model's functionality. This step ensures model-based reinforcement learning, with the nature of the infused knowledge depending on the type of model employed: physics-informed for RC-based or PINN-based models, or purely data-driven in the case of NN-based models. By integrating learning, acting, and planning within the Deep Dyna-Q architecture, this approach seeks to address the challenges posed by limited data and the unavailability of physical information in building heating control. The three proposed DDQ-variants are given in the following sub-sections. 2.4 Physics-informed neural-network based deep Dyna-Q (dyna-PINN) control: In Dyna-PINN, we employ a Physics-informed Neural Network (PINN) as the model for the planning step. The PINN model is learned using randomly sampled real experiences and a custom loss function (as explained in sub-section 2.2), to minimize deviations between its predictions and those of the RC model, internalizing the physical principles of the building. We illustrate Dyna-PINN through a comprehensive schematic in Fig. 1 . There are three main steps as explained in sub-section 2.3. Firstly, in the direct RL phase, real interactions of the Dyna-PINN agent take place with the BOPTEST environment takes, and these interactions are stored in a real memory buffer ( D r ). Secondly, model learning takes place where a randomly sampled batch of interactions are used to learn a PINN model minimizing the custom loss function of the building (refer to Equation (1)), enabling the PINN model to embed the physical laws represented by RC model. Thirdly, in the planning phase, the trained PINN model is used to generate simulated experiences that are faster to generate compared to the real experiences, and are subsequentially stored in the simulated memory buffer ( D s ). Randomly sampled experiences from D r and D s are used to make Bellman's update to the policy function π of the underlying DQN agent which decides on the control actions of heating for the BOPTEST environment. A more clear, step-wise explanation of the Dyna-PINN approach has been provided in Algorithm 1 . In Algorithm 1, lines 6-12 represent the direct RL phase, lines 11 and 14 represent the model learning phase, and lines 15-20 represent the planning phase of the Dyna-PINN In this analysis, we chose two state RC-model (Fig. 1) to represent the BOPTEST single-zone building use case. The RC model, governed by the ordinary differential equations is provided below: (4) d d t [ T i n T e ] = ( − 1 R i C i 1 R i C i 1 R i C i − ( 1 R i C i + 1 R e C e ) ) [ T i n T e ] + ( 0 1 C i A i C i 1 R e C e 0 A e C e ) [ T o u t Φ h Φ s ] + σ d ω (5) d x ( t ) d t = A r c x ( t ) d t + B r c u ( t ) d t + σ θ d ω (6) y t = C θ x t + v t The indoor temperature T i n and the envelope temperature T e represent the temperatures within the building and its thermal envelope, respectively, while T o u t indicates the outdoor ambient temperature. The heating power Φ h (W) measures the rate of heat addition to the indoor space, and the solar irradiance Φ s (W/m2) quantifies the solar energy received. The thermal resistances R i and R e (both in K/W) describe the heat transfer impedances from the indoor space to the envelope and from the envelope to the external environment, respectively. The heat capacitances C i and C e (J/K) reflect the ability of the interior and the envelope to store thermal energy, and the coefficients A i and A e (m2) influence the solar gains for their respective areas. The stochastic elements are represented by σ, the standard deviation of the Wiener process dω, which accounts for the random fluctuations due to unmodeled inputs or noise in measurements. 2.5 Neural network based Deep-Dyna-Q (NN-DDQ) control Moving on to the second variant of DDQ control, NN-DDQ, this method utilizes a standard neural network-based model for policy updates. In contrast to Dyna-PINN, which incorporates physics-informed knowledge, NN-DDQ relies entirely on a data-driven neural network. This model is learned using randomly sampled examples from the real memory buffer ( D r ), employing a loss function that minimizes mean squared error to reduce prediction discrepancies. The NN-DDQ framework aligns with the process outlined in Algorithm 1, with a key distinction in step 11 where F pinn is replaced by the data-driven model F nn . For a detailed view of the NN-DDQ architecture, refer to Fig. 2 . 2.6 RC-model based Deep-Dyna-Q (RC-DDQ) control Moving forward, we delve into the third variant of DDQ control, RC-DDQ, which relies on a carefully tuned RC-model of BOPTEST environment for the planning phase. Similar to the previous variants, RC-DDQ maintains a clear distinction between real memory buffer and simulated memory buffer, and then randomly sampled mini-batches of experiences are used to update the policy function. The RC-DDQ framework can also be explained using Algorithm 1, but the only difference is that in step 11, instead of F pinn now we have purely physics-based RC-model F RC . We have used the same two state RC model for RC-DDQ as we used for the Dyna-PINN. It is important to note that unlike NN-DDQ, both Dyna-PINN and RC-DDQ can provide meaningful physical information to the DDQ control agent through the planning phase. However, RC-DDQ lacks the data-driven component present in Dyna-PINN, which may lead to bias toward the tuned RC model without real-time data inputs for policy updates. The schematic depiction of RC-DDQ is presented in Fig. 3 . 2.7 Baslines To benchmark the performance of our proposed DDQ controllers, we selected two baseline approaches: a model-free reinforcement learning (RL) alternative and a rule-based control system for managing the building's heating system. 2.7.1 Model-free RL - first baseline The Deep Q-Network (DQN) serves as a model-free reinforcement learning approach, where a convolutional neural network learns a state-action value function Q (Equation (7)). This function estimates the expected return if an RL agent were to take an action A in a given state S [46]. (7) Q : S × A → Return The objective of DQN is to train an RL agent to maximize the Q-function, thereby maximizing the cumulative rewards of the agent (Equation (8)). (8) π ( s ) = argmax a Q ( s , a ) In DQN, the agent interacts with the BOPTEST environment, generating real experiences stored in an experience replay buffer. The Q-network weights are then trained by randomly sampling mini-batches of experiences from this buffer, aiming to minimize the discrepancy between predicted and target action-values [46]. The Q-value of a state-action pair is computed using Bellman's equation (Equation (9)), where it equals the immediate reward of the action plus the discounted maximum return of the next state. (9) Q ( s , a ) = r + γ Q ( s \u2032 , π ( s \u2032 ) ) The learning objective is to minimize the difference between the current Q-value and the next state value of Q, denoted as δ (Equation (10)), and update the Q-network weights accordingly. (10) δ = Q ( s , a ) − ( r + γ max a \u2032 \u2061 Q ( s \u2032 , a \u2032 ) ) In summary, the Q-network outputs expected returns for each possible action given a state S. The control policy then chooses the action with the highest Q-value. A schematic explaining the methodology of DQN is given in Fig. 4 . 2.7.2 Rule-based control (RBC) - second baseline The second baseline controller is an RBC, implemented to regulate the zone operative temperature of the building, ensuring user-defined thermal comfort. It operates based on a set of rules, manipulating the heat pump (HP) operation to maintain the desired temperature set point. The controller processes the inputs of real zone temperature T in and the setpoint temperature range [ T setp _ low , T setp _ high ] to operate in a binary mode with only two states: fully ON (1) and fully OFF (0). Given this binary operation, modulation of the compressor frequency is not utilized, as the system relies on switching between these states to maintain the indoor temperature above 21°C ( T setp _ low ). The RBC's rules can be summarized as follows: { if T in < T setp _ low → HP ON if T in > T setp _ low → HP OFF The corresponding HP control action is determined based on the indoor room temperature. See Fig. 5 . The objective function of the baseline controller can be customized according to specific requirements. In this paper, the RBC is programmed to minimize the total thermal discomfort expressed in Celsius.hour. The reward function is defined as: (11) Reward = T dis (12) Objective = min \u2061 ( T dis ) where: \u2022 T dis represents the thermal discomfort, which is quantified by summing the products of the temperature deviations from the thermal comfort range (21\u201324°C) and the corresponding time intervals during which these deviations occur, measured in Celsius.hour. This provides a measure of the extent and duration of thermal conditions outside the comfort range. 3 Experimental setup 3.1 Building models: BOPTEST framework To evaluate the efficacy of our developed control algorithms, we employed the Building Optimization Testing Framework (BOPTEST) developed by [38]. This framework provides detailed physics-based white-box building models created using the Modelica language, offering a standardized environment for testing. BOPTEST ensures accurate modeling of buildings and their HVAC systems through physical equations, serving as a detailed simulator of building dynamics. Specifically, we focused on the \u201cBESTEST Hydronic Heat Pump\u201d test case, featuring a single-zone residential building equipped with a floor heating system [38]. Throughout this paper, the BOPTEST single-zone building use case is referred to as the \u201cBOPTEST environment\u201d. The floor heating system includes an air-to-water heat pump (air source heat pump - ASHP) and a floor heating emission system. Operating with a 15 kW heating capacity, the heat pump extracts energy from the surrounding air to heat the working fluid, water, within the floor heat emission system. Detailed parameters of the heat pump system can be found in Appendix C. The occupancy pattern of the BESTEST Hydronic Heat Pump model is tailored to simulate real-world scenarios: occupied by five (5) people during evening and night hours, vacant during typical office hours, and fully occupied on weekends. Internal loads are minimal, limited to the presence of occupants. Aligned with the philosophy of OpenAI gym environments [47], BOPTEST test cases offer standardized building configurations suitable for testing various control algorithms, including RL-based control and Model Predictive Control (MPC). This ensures consistent evaluation across different control methodologies. For a schematic diagram of the BOPTEST Hydronic Heat Pump test case, refer to Fig. 6 . 3.2 Initial configurations Table 2 summarizes the initial configurations for the experimental setup. 3.3 Evaluation criteria and success metrics This study addresses the challenge of limited building energy use data for training RL agents by incorporating physics. The results in the following section aim to answer these key questions: 1. How do the predictive performances of the three simulated building models (PINN, NN, and RC-model) compare? 2. Does the DDQ control outperform the DQN-based control and the RBC in low-diversity training data regimes? 3. What is the impact of incorporating physics into the DDQ architecture? Specifically, do Dyna-PINN and RC-DDQ-based controls perform better than NN-DDQ and DQN controls in low-diversity data settings? Does this improve sample efficiency and generalizability? 4. How does increasing the training data availability from 6 weeks to 27 weeks impact the benefits of incorporating physics into the RL approach compared to the model-free RL approach and the RBC approach? The results are evaluated using two main training data windows, as detailed in Table 3 . The low-diversity data regime captures a shorter observation window with two configurations: 25 and 50 training episodes. The high-diversity data regime encompasses a longer observation window, also with two configurations: 25 and 50 training episodes. Additionally, each regime includes two configurations of planning steps: 50 and 100 steps. The planning steps' configuration applies only to the DDQ control variants and not to the model-free DQN or the RBC. Planning steps refer to the model's capacity to predict 50 or 100 steps ahead of the present step and generate simulated experiences. This helps the RL agent learn from these simulated experiences, effectively addressing the low-diversity data regime with few training episodes. The hypothesis is that by incorporating these planning or forecasted states and actions during training without real interaction with the BOPTEST environment, the DDQ control variants can efficiently learn in low-diversity data regimes. The high-diversity data regime provides more diverse weather conditions. First, we assess the performance of each of the three models using box plots of the MAE to illustrate the prediction accuracy. Second, we compare the control performance of our novel DDQ-based control variants (Dyna-PINN, NN-DDQ, and RC-DDQ) against the RBC and model-free DQN control alternatives. The controller performance is evaluated using the cumulative cost of the trained controller over a fixed 2-month test window (November 1 to December 31). The RL agents were trained using episodes of 1 week in length, with a control step of 30 minutes, corresponding to 336 timesteps per episode. This cumulative cost (from Table 1) is the negative sum of the total energy use of the HP operation in kWh and the thermal discomfort measured in Celsius.hour during the testing period. For this experiment, α and β were both set to 1. The choice of a two-month testing window is key in this study, as it shows how well controllers trained with limited data can perform over longer periods. In contrast, many studies only use a few days for training and testing, which might miss important insights on how controllers perform under varied winter conditions over longer times. The evolution of the outdoor ambient temperature ( T out ) and the direct normal solar irradiation ( P sol ) during the training and testing periods is illustrated in Fig. 7 . During the training period, the low-diversity data regime (6-week data window) primarily captures winter weather conditions. In contrast, the high-diversity data regime includes winter, spring, and summer, allowing the controller to learn from diverse weather conditions. For the testing period, we evaluate the controller during November and December, which represent the typical winter season, requiring heating for most of the time. This testing window helps us assess the controller's performance over an extended duration. 3.4 System setup and hyperparameter tuning for RL architecture and DDQ models The hyperparameter sweep ranges and finalized values for tuning the RL architecture and the three DDQ model variants\u2014PINN, NN, and RC\u2014are provided in Appendix A and Appendix B. The experiments were conducted on a system with an Intel Core i9 processor, an NVIDIA RTX A2000 GPU, and 64 GB of RAM. The software environment consisted of Python 3.8, TensorFlow 2.5, and PyTorch 2.0.1 with CUDA 11.8 support. In terms of performance, the inference time for the PINN, NN, and RC models was approximately the same, averaging 2 ms per inference for each model. Training time per episode was similar across the models, with the PINN-based DDQ agent, the NN-based agent, and the RC-based agent all averaging around 50 seconds per episode. For comparison, the DQN policy network required 45 seconds per training episode. 4 Results and discussion This section aims to answer the four key questions outlined in Section 3.3. 4.1 Prediction performance of the models We evaluated the prediction performance of six models learned within the DDQ framework, tailored to different observation windows and framework types: PINN-6, PINN-27, NN-6, NN-27, RC-6, and RC-27. Fig. 8 showcases the models' Mean Absolute Error (MAE) through box plots, illustrating their performance at predicting 50 and 100 planning steps ahead in both 6-week and 27-week data regimes. This visualization provides a comparative analysis, highlighting the models' adaptability and learning progression under varying data conditions. The effectiveness of each model was verified by comparing their forecasts with the actual subsequent state transitions in the BOPTEST environment, thereby confirming their predictive accuracy. Table 4 provides a brief description of the six models used in this analysis. The PINN-6 model demonstrates the best predictive performance compared to the NN-6 and RC-6 models, with the lowest median MAE and only a few outliers in the 6-week observation window for both 50 and 100 planning steps, highlighting its suitability for low-diversity data regimes. In contrast, the PINN-27 model shows a relatively higher median MAE compared to the PINN-6, reflecting the training data complexity from a broader range of the 27-week observation window. However, among the 27-week model variants, it still achieves the lowest median MAE and tight interquartile ranges, underscoring the robustness and suitability of the PINN model for both low-diversity data (6-week) and high-diversity data (27-week) regimes. The NN-6 model demonstrates effective predictive performance within the 6-week observation window for both 50 and 100 planning steps. However, while its performance is superior to that of RC-27, it remains below the accuracy of PINN-6. NN-6 exhibits acceptable mean absolute error (MAE) with some variability, indicating its suitability for short-term predictions, particularly in low-diversity data scenarios. In the 27-week observation window with 50 planning steps, NN-27 shows a lower median MAE and fewer outliers compared to the RC-27 model but does not surpass the performance of PINN-27. When considering 100 planning steps in the 27-week observation window, NN-27 experiences a significant increase in prediction error with more outliers, highlighting its unsuitability for high-diversity data regimes with longer prediction horizons. As a result, NN-27 requires further refinement or additional training data to enhance its performance in high-diversity data scenarios. The RC-6 model demonstrates prediction performance comparable to the NN-6 model although it remains inferior to the performance of PINN-6 with 50 planning steps in the 6-week observation window, making it suitable for low-diversity data regimes. However, its prediction error worsens with an increase in planning steps, with the median MAE rising to 2.5°C. In high-diversity data regimes, the RC-27 model consistently exhibits poor predictive performance, with a median MAE of around 2°C, indicating its unsuitability for these scenarios. Overall, while the RC models show potential, particularly in low-diversity data regimes, the RC-27 model still requires improvement to be effective in high-diversity data contexts. 4.2 Control performance of the proposed DDQ-Controllers This section details the performance of the three DDQ-framework based controller variants, each utilizing a different underlying model. The controllers were trained under low and high-diversity data regimes with varying training episodes and planning steps, as summarized in Table 3. The trained DDQ control agents were tested over a fixed two-month period (November 1 to December 31), covering a major part of the winter season to ensure robust evaluation of the proposed controllers. The training profile of the various RL control agents is summarized in Section 3.3. To benchmark the performance of the DDQ controllers, the model-free RL DQN agent was trained using the same data regimes and episodes, but without planning steps. Additionally, a rule-based controller (RBC) was used as a second baseline to assess the performance of the DDQ control variants. The control performance of the RBC over the testing period is shown in Fig. 9 . The RBC operates reactively, switching on the heat pump whenever the measured indoor temperature is lower than the temperature threshold. In the first two weeks of November, some oscillations in the indoor temperature are observed, they can be attributed to the heat gains due to high solar irradiation coupled with high ambient temperatures (refer to Fig. 7 for weather profile). 4.2.1 Performance in low-diversity data regime In this scenario, we evaluated the controllers after 25 direct RL episodes of agent training using the BOPTEST environment (refer to the top two subplots of Fig. 10 ). In the low-diversity data regime, characterized by the 6-week observation window with 50 planning steps, the Dyna-PINN controller achieves the best performance in terms of thermal comfort, outperforming the model-free DQN control, which fails, as well as RBC and the remaining two DDQ control variants. The superior performance of Dyna-PINN can be attributed to the integration of physics with the limited data available, through the PINN-based model used as the underlying model in the DDQ controller. Although NN-DDQ shows reasonable performance, it remains inferior to Dyna-PINN-6. As the planning steps increase from 50 to 100, NN-DDQ's performance remains consistent but still does not surpass Dyna-PINN. RC-DDQ performs well only in the scenario with 100 planning steps, likely due to the fact that RC-DDQ requires more data and training time to function effectively, a consequence of its underlying physics-based RC-model. The next training configuration in this low-diversity data regime involves 50 direct RL episodes of agent training using the BOPTEST environment (refer to the bottom two subplots of Fig. 10). With 50 planning steps, Dyna-PINN again performs the best, surpassing all DDQ variants and RBC in terms of thermal discomfort, and matching the performance of DQN. NN-DDQ, however, operates close to the lower thermal comfort bound, with occasional deviations toward the upper thermal comfort bound. This indicates its poor generalization to testing data, which can primarily be attributed to the lower prediction accuracy of the underlying model used for planning compared to the PINN-6 model (refer to Fig. 8). RC-DDQ, on the other hand, performs entirely outside of the thermal comfort bounds, demonstrating poor performance due to the RC-27 model's inadequate prediction accuracy (refer to Fig. 8). For the scenario with 100 planning steps, the control performance of all three DDQ variants, as well as DQN, is generally acceptable, maintaining indoor temperature within thermal comfort bounds. The improved performance of RC-DDQ here is likely due to its requirement for more simulated experiences based on the RC-model, which are necessary for decent training. A similar behavior was observed in RC-DDQ with 25 direct RL episodes. It should be noted that in Fig. 10, the y-axis is limited to the range of 10°C to 35°C for better visual clarity. Any controller that produces an indoor temperature profile outside this window is considered to have failed, and its performance beyond this range is deemed irrelevant. The limitation in the y-axis is applied to simplify the visual comparison and highlight the controllers' performance within the acceptable thermal comfort range. 4.2.2 Performance in high-diversity data regime In this scenario, we trained the DDQ agents using the 27-week observation window and began evaluating the controllers after 25 direct RL episodes of agent training using the BOPTEST environment (refer to Fig. 11 ). The Dyna-PINN controller once again performs the best among the DDQ controllers and nearly matches the performance of the DQN controller, which is now able to learn the control policy to minimize thermal discomfort and energy use. The DQN's improved performance can be attributed to the extended observation window from 6 weeks to 27 weeks, providing more diverse training samples and facilitating faster learning. Conversely, RC-DDQ and NN-DDQ perform poorly, primarily because RC-DDQ lacks data insights and NN-DDQ lacks physics information, and neither has enough direct RL training episodes to learn an optimal control policy. This is consistent for both 50 and 100 planning steps. Subsequently, we evaluated the controllers with 50 direct RL episodes of agent training using the BOPTEST environment (refer to Fig. 11), further increasing the available training data from the 27-week observation window. With 50 planning steps, Dyna-PINN exhibited the best performance among all DDQ variants, adhering to the thermal comfort bounds, and matching the performance of DQN. This highlights the effectiveness of the physics-informed approach even in the high-diversity data regime. In contrast, both RC-DDQ and NN-DDQ failed to maintain indoor thermal comfort, indicating the poor performance of their respective planning models, NN-27 and RC-27. When the planning steps were increased from 50 to 100, all the DDQ control variants failed to learn an optimal control policy and severely diverged towards extreme high (NN-DDQ) and low temperatures (Dyna-PINN and RC-DDQ). This behavior can be attributed to the poor prediction accuracy of the three planning models, caused by error accumulation in 100-step ahead predictions, as confirmed in Fig. 8. As in the previous subsection, the y-axis in Fig. 11 is limited to the range of 10°C to 35°C for improved visual clarity. Any controller generating an indoor temperature profile outside this range is considered to have failed, and performance beyond this limit is considered irrelevant. 4.2.3 Complete overview of all controllers A more comprehensive scatter plot illustrating thermal discomfort versus energy usage for the eight RL controllers and the Rule-Based Control (RBC) discussed in this work is presented in Fig. 12 . This figure demonstrates that Dyna-PINN consistently outperforms all the other RL control variants as well as the RBC in low-diversity data regime of 6 weeks (blue dots), achieving superior results in terms of thermal comfort. In terms of energy consumed, it uses more energy than RBC, NN-DDQ, and v-DQN, however, the high thermal discomfort of v-DQN makes its energy consumption comparison less meaningful in this regime. Over the extended period of 27 weeks (the red dots on the plot), Dyna-PINN maintains better performance in reducing thermal discomfort compared to other DDQ variants and RBC. However, it falls slightly behind the DQN controller in this aspect. Regarding energy use, while Dyna-PINN lacks behind RBC and DQN, it surpasses the performance of other DDQ variants i.e., RC-DDQ and NN-DDQ, both of which exhibit significantly higher thermal discomfort, making their energy consumption comparisons less pertinent. This analysis indicates that given a large enough observation window and sufficient direct RL episodes with the BOPTEST environment, DQN could be a viable and simpler alternative to DDQ controllers. The Dyna-PINN controllers perform better both in terms of thermal discomfort and energy use in low-diversity data regimes because they do not aim to maintain a constant indoor room temperature profile like DQN. While maintaining a constant indoor temperature is a low-risk strategy, it does not exploit the allowable temperature range of 21 to 24°C. The DDQ controllers take better advantage of this range primarily due to their two-fold training approach i.e. direct reinforcement learning (RL) and planning, which enables them to learn thermal discomfort and energy-use-based costs more efficiently than DQN, which relies solely on direct RL. Consequently, DDQ controllers can improve overall energy efficiency while providing thermal comfort. 5 Discussion Our study aimed to evaluate the control performance of three dyna-based approaches: Dyna-PINN, RC-DDQ, and NN-DDQ. The Dyna-PINN and NN-DDQ controllers successfully demonstrated 50% higher sample efficiency in low-diversity data regimes (6-week observation window) compared to model-free RL control, requiring only 25 direct RL episodes to begin performing reasonably on two-month testing window, whereas DQN needed at least 50 episodes or training data from a 27-week observation window. Additionally, Dyna-PINN resulted in lower thermal discomfort compared to the three RL variants (DQN, NN-DDQ, and RC-DDQ) and rule-based control methods in low-diversity data regime. To address the first question from Section 3.3, we plotted the prediction error curves (refer to Fig. 13 ). The figure presents the averaged prediction errors and their corresponding standard deviations for the PINN, NN, and RC models across both 6-week and 27-week variants for two prediction horizons, i.e., 50 and 100 planning steps. As shown in the figure, the absolute averaged errors are displayed on the left, while the raw averaged errors, which depict the average bias of the models, are presented on the right. A vertical dotted line at the 50 planning steps separates the short- and long-term prediction horizons. For the 6-week variants, the PINN-6 model demonstrates stable performance across both prediction horizons, with mean errors close to zero and a balanced positive and negative standard deviation region, compared to the NN-6 and RC-6 models, which exhibit slightly skewed standard deviations. This stability indicates that the PINN-6 model has minimal bias in its predictions, as evidenced by the raw error plot, and its consistent accuracy over time suggests it is well-suited for planning tasks, especially in low-diversity data regimes. The NN-6 model displays a slight positive bias, indicative of underfitting, as its absolute average error is almost identical to the raw error, suggesting that additional data is required for improved fitting and generalization. Similarly, the RC-6 model exhibits the largest positive bias, particularly at longer planning steps, indicating a tendency to overpredict, with increasing error variability as the prediction horizon extends. For the 27-week variants, similar patterns emerge but with more pronounced differences among the models. The PINN-27 model maintains a raw average error line close to zero but shows higher variability compared to the PINN-6, particularly beyond the 50-planning step mark. While the PINN-27 model leverages both data-driven insights from high-diversity data and physics-based insights from the RC-27 model predictions, the accuracy of the RC model becomes crucial. Since the RC model is not a fully accurate representation of the building dynamics, it's slightly deviating predictions introduce error into the PINN-27 model while training, which can propagate over time, especially in high-diversity data regimes. However, the PINN-27 remains the most robust option, though more sensitive to extended prediction horizons compared to the 6-week variant. The NN-27 model exhibits a more significant positive bias than its 6-week counterpart, further indicating underfitting from the onset of planning, and its error deviations increase significantly after 50 planning steps, suggesting poor performance in high-diversity data regimes. The RC-27 model consistently underpredicts, showing a slightly negative bias, with the absolute average error exceeding the raw error, indicating a generalization issue. This suggests that the RC-27 model becomes increasingly unreliable for long-term predictions, highlighting the need for better tuning and potentially higher fidelity (e.g., third-order) RC models for improved performance. These findings are crucial for evaluating the predictive performance of these models in the context of DDQ control variants. The stability of the PINN-6 and PINN-27 models, particularly their lower raw average error up to 50 planning steps, and even over extended horizons for the 6-week variant, positions them as reliable choices for planning, especially in building heating control applications with limited or low-diversity data. In contrast, the larger biases and overprediction tendencies of the RC and NN models, especially in the 27-week dataset, highlight their underfitting issues and the need for more data and higher-fidelity physical models for effective integration into DDQ-based RL variants. These insights guide the selection and optimization of models for practical deployment in predictive control scenarios, with the Dyna-PINN variant emerging as the most robust across both short- and long-horizon predictions. For the second and third questions from Section 3.3 on whether the dyna-based RL control agents outperform the model-free RL variant and the rule-based control, we plotted and compared the control performance of the six variants against the DQN and RBC agents. As shown in Fig. 10 and Fig. 11, the Dyna-PINN agent outperformed all other agents in low-diversity data regime, while achieving almost similar performance as DQN in the 27-week observation window and beating the other control variants in terms of thermal discomfort performance. Moreover, when the planning steps are increased from 50 to 100 in the 6-week window and with 25 training episodes, all the DDQ variants show good performance in terms of thermal comfort, but in the 27-week window, only Dyna-PINN demonstrates slightly reasonable performance, while the other two DDQ variants diverge greatly towards extremely low temperatures, suggesting that increasing the planning steps beyond 50 is not useful for the DDQ controllers. NN-DDQ exhibited more oscillating performance, outperforming DQN in low-diversity data regimes but diverging to extremely low indoor temperatures in the 27-week observation window. This indicates unstable performance likely due to data mixing from the direct RL phase and planning phase without physical rules guiding it. RC-DDQ showed stable control performance in only two instances but underperformed compared to Dyna-PINN, suggesting its fixed RC-parameters model lacks data-driven insights. Overall, Dyna-PINN delivered the best performance in low-diversity data regimes highlighting its pivotal role in realistic building scenarios. Incorporating physics into the models clearly benefited control, generalizability, and sample efficiency of RL training. 6 Conclusion Our study highlights the substantial benefits of incorporating physics-informed models within the DDQ framework, particularly in low-diversity data regimes. The DDQ approaches consistently outperformed the model-free DQN in low-diversity data regimes, resulting in lower thermal discomfort and energy use. This was primarily due to the two-fold training approach of the DDQ controllers, where learning from real experiences was coupled with simulated experiences, thus alleviating the issue of low-diversity training data availability. Among the DDQ control variants, only Dyna-PINN could beat RBC in the low-diversity data regime and proved superior to both RC-DDQ and NN-DDQ. This superiority is attributable to the use of a physics-informed neural network (PINN) model, which encapsulates the benefits of both physics-based and data-driven techniques. Consequently, Dyna-PINN outperforms purely data-driven or purely physics-based control approaches. The accuracy of the model is crucial for the success of model-based RL approaches, emphasizing the importance of careful training and validation of the simulated building model. In examining the usefulness of physics-informed RL approaches (Dyna-PINN and RC-DDQ) with increased training data windows, we extended the observational data window from 6 weeks to 27 weeks (6 months). This experiment revealed that while Dyna-PINN still clearly outperformed the RC-DDQ controller, but performed slightly inferior to the model-free DQN-27 both in terms of discomfort and energy use. Therefore, the marginal benefits of using physics-informed control were not as pronounced in high-diversity data regimes. This suggests that physics-informed RL approaches hold a distinct advantage in low-diversity training data regimes. However, in high-diversity data regimes, a data-driven model-free RL control can perform even slightly better than Dyna-PINN if optimal RL training episodes are provided to the RL agent. Therefore, the choice of control technique can be intelligently tailored to the availability of data, the extent of physical insights, and the desired level of control precision. The improved performance of the Dyna-PINN controller indicates that hybrid models combining physics-based insights with data-driven approaches could be pivotal in advancing smart building heating control. This could lead to more energy-efficient and cost-effective building operations, contributing to sustainable urban development. While our study demonstrates the efficacy of the PINN-based model, it is important to note that our analysis was limited to a specific building and a fixed set of conditions. Future research should explore the scalability of these models across different building types and weather conditions. Additionally, future studies should investigate the integration of PINN-based controllers in hybrid models that incorporate other physical systems, such as PV panels and battery dynamics, to further enhance control performance. Declaration of the use of generative AI and AI-assisted technologies in writing During the preparation of this work the author(s) used ChatGPT 4 in order to improve the readability and language of the manuscript. After using this tool/service, the author(s) reviewed and edited the content as needed and take(s) full responsibility for the content of the publication. CRediT authorship contribution statement Muhammad Hafeez Saeed: Writing \u2013 original draft, Visualization, Software, Methodology, Investigation, Formal analysis, Conceptualization. Hussain Kazmi: Writing \u2013 review & editing, Supervision, Investigation, Conceptualization. Geert Deconinck: Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements \u2022 Muhammad Hafeez Saeed acknowledges support from Research Foundation \u2013 Flanders (FWO) (grant no. 1S66623N) \u2022 This work has been supported by KU Leuven through the TECHPED - C2 project (C24M/21/021). The TECHPED project investigates TECHnically feasible and effective solutions for Positive Energy Districts. Appendix A Hyper-parameter sweep ranges and initial guesses with network architectures This section outlines the hyper-parameter sweep ranges and initial guesses used to optimize the reinforcement learning models and the RC Model. For the DQN Policy Network, PINN Model, and NN Model, hyper-parameter sweeps were performed using the Python Optuna library, while the RC model parameters were fine-tuned using the SciPy library's curve_fit function. See Table A.5 . Appendix B Finalized hyperparameters and network architectures for DQN policy network and DDQ agent models The hyper-parameters for the DQN, PINN, and NN models are summarized in the table below, followed by the detailed parameters for the RC Model See Table B.6 . B.1 RC-model parameters The finalized parameters for the RC Model (2R2C) include thermal resistances R i and R o with values of 9.00 × 10 − 5 K / W and 3.20 × 10 − 3 K / W , respectively. The thermal capacitances are C i = 6.49 × 10 7 J / K and C e = 7.78 × 10 8 J / K . Additionally, the internal area A i is 28.94 m 2 , and the external area A e is 0.00066 m 2 . These values were obtained using training data from the BOPTEST use case. Appendix C Parameters of the heat pump Table C.7 Parameter Overview of the Heat Pump used in the BOPTEST BESTEST Hydronic Heat Pump Test Case. Table C.7 Variable Value and Unit Heat pump model Carrier Air-to-Water Heat Pump 30AW015 Medium model at condenser side PartialMedium Medium model at evaporator side PartialMedium Refrigerant model R410A Nominal mass flow rate on condenser side 1.42 kg/s Nominal mass flow rate on evaporator side 1.42 kg/s Pressure drop at nominal mass flow rate on condenser side 1000 Pa Pressure drop at nominal mass flow rate on evaporator side 1000 Pa Thermal conductance of condenser 12000 W/K Thermal conductance of evaporator 12000 W/K Built-in volume ratio 2.0 Refrigerant volume flow rate at suction 0.009 m3/s Leakage coefficient 0.03 kg/s Electro-mechanical efficiency of the compressor 0.696 Constant part of compressor power losses 500.0 W Superheating at compressor suction 10.0 K Source: https://ibpsa.github.io/project1-boptest/docs-testcases/bestest_hydronic_heat_pump/index.html. References [1] I.E.A. Tracking Clean Energy Progress 2023 https://www.iea.org/reports/tracking-clean-energy-progress-2023 2023 licence: CC BY 4.0 IEA, Tracking Clean Energy Progress 2023, https://www.iea.org/reports/tracking-clean-energy-progress-2023, licence: CC BY 4.0 (2023). [2] J.-M. Dussault M. Sourbron L. Gosselin Reduced energy consumption and enhanced comfort with smart windows: comparison between quasi-optimal, predictive and rule-based control strategies Energy Build. 127 2016 680 691 10.1016/j.enbuild.2016.06.024 J.-M. Dussault, M. Sourbron, L. Gosselin, Reduced energy consumption and enhanced comfort with smart windows: Comparison between quasi-optimal, predictive and rule-based control strategies, Energy and Buildings 127 (2016) 680\u2013691. doi:https://doi.org/10.1016/j.enbuild.2016.06.024. [3] J. Salpakari P. Lund Optimal and rule-based control strategies for energy flexibility in buildings with PV Appl. Energy 161 2016 425 436 10.1016/j.apenergy.2015.10.036 J. Salpakari, P. Lund, Optimal and rule-based control strategies for energy flexibility in buildings with PV, Applied Energy 161 (2016) 425\u2013436. doi:10.1016/j.apenergy.2015.10.036. [4] B. Svetozarevic C. Baumann S. Muntwiler L. Di Natale M. Zeilinger P. Heer Data-driven control of room temperature and bidirectional ev charging using deep reinforcement learning: simulations and experiments Appl. Energy 307 2022 118127 10.1016/j.apenergy.2021.118127 B. Svetozarevic, C. Baumann, S. Muntwiler, L. Di Natale, M. Zeilinger, P. Heer, Data-driven control of room temperature and bidirectional ev charging using deep reinforcement learning: Simulations and experiments, Applied Energy 307 (2022) 118127. doi:https://doi.org/10.1016/j.apenergy.2021.118127. [5] J. Drgoňa J. Arroyo I.C. Figueroa D. Blum K. Arendt D. Kim E.P. Ollé J. Oravec M. Wetter D.L. Vrabie L. Helsen All you need to know about model predictive control for buildings Annu. Rev. Control 50 2020 190 232 10.1016/j.arcontrol.2020.09.001 J. Drgoňa, J. Arroyo, I. C. Figueroa, D. Blum, K. Arendt, D. Kim, E. P. Ollé, J. Oravec, M. Wetter, D. L. Vrabie, L. Helsen, All you need to know about model predictive control for buildings, Annual Reviews in Control 50 (2020) 190\u2013232. doi:10.1016/j.arcontrol.2020.09.001. [6] M. Killian M. Kozek Ten questions concerning model predictive control for energy efficient buildings Build. Environ. 105 2016 403 412 10.1016/j.buildenv.2016.05.034 M. Killian, M. Kozek, Ten questions concerning model predictive control for energy efficient buildings, Building and Environment 105 (2016) 403\u2013412. doi:10.1016/j.buildenv.2016.05.034. [7] Z. Nagy G. Henze S. Dey J. Arroyo L. Helsen X. Zhang B. Chen K. Amasyali K. Kurte A. Zamzam H. Zandi J. Drgoňa M. Quintana S. McCullogh J.Y. Park H. Li T. Hong S. Brandi G. Pinto A. Capozzoli D. Vrabie M. Bergés K. Nweye T. Marzullo A. Bernstein Ten questions concerning reinforcement learning for building energy management Build. Environ. 241 2023 110435 10.1016/j.buildenv.2023.110435 Z. Nagy, G. Henze, S. Dey, J. Arroyo, L. Helsen, X. Zhang, B. Chen, K. Amasyali, K. Kurte, A. Zamzam, H. Zandi, J. Drgoňa, M. Quintana, S. McCullogh, J. Y. Park, H. Li, T. Hong, S. Brandi, G. Pinto, A. Capozzoli, D. Vrabie, M. Bergés, K. Nweye, T. Marzullo, A. Bernstein, Ten questions concerning reinforcement learning for building energy management, Building and Environment 241 (2023) 110435. doi:https://doi.org/10.1016/j.buildenv.2023.110435. [8] B. Huchuk S. Sanner W. O'Brien Development and evaluation of data-driven controls for residential smart thermostats Energy Build. 249 2021 10.1016/j.enbuild.2021.111201 (Oct 15) B. Huchuk, S. Sanner, W. O'Brien, Development and evaluation of data-driven controls for residential smart thermostats, Energy and Buildings 249 (Oct 15 2021). doi:10.1016/j.enbuild.2021.111201. [9] Z. Wang T. Hong Reinforcement learning for building controls: the opportunities and challenges Appl. Energy 269 2020 115036 10.1016/j.apenergy.2020.115036 Z. Wang, T. Hong, Reinforcement learning for building controls: The opportunities and challenges, Applied Energy 269 (2020) 115036. doi:https://doi.org/10.1016/j.apenergy.2020.115036. [10] M. Biemann F. Scheller X. Liu L. Huang Experimental evaluation of model-free reinforcement learning algorithms for continuous HVAC control Appl. Energy 298 2021 10.1016/j.apenergy.2021.117164 M. Biemann, F. Scheller, X. Liu, L. Huang, Experimental evaluation of model-free reinforcement learning algorithms for continuous HVAC control, Applied Energy 298 (2021). doi:10.1016/j.apenergy.2021.117164. [11] X. Wang X. Kang J. An H. Chen D. Yan Reinforcement learning approach for optimal control of ice-based thermal energy storage (tes) systems in commercial buildings Energy Build. 301 2023 113696 10.1016/j.enbuild.2023.113696 X. Wang, X. Kang, J. An, H. Chen, D. Yan, Reinforcement learning approach for optimal control of ice-based thermal energy storage (tes) systems in commercial buildings, Energy and Buildings 301 (2023) 113696. doi:https://doi.org/10.1016/j.enbuild.2023.113696. [12] J. Arroyo C. Manna F. Spiessens L. Helsen Reinforced model predictive control (rl-mpc) for building energy management Appl. Energy 309 2022 118346 10.1016/j.apenergy.2021.118346 J. Arroyo, C. Manna, F. Spiessens, L. Helsen, Reinforced model predictive control (rl-mpc) for building energy management, Applied Energy 309 (2022) 118346. doi:https://doi.org/10.1016/j.apenergy.2021.118346. [13] M. Bhardwaj S. Choudhury B. Boots Blending MPC & value function approximation for efficient reinforcement learning CoRR arXiv:2012.05909 [abs] 2020 https://arxiv.org/abs/2012.05909 M. Bhardwaj, S. Choudhury, B. Boots, Blending MPC & value function approximation for efficient reinforcement learning, CoRR abs/2012.05909 (2020). URL https://arxiv.org/abs/2012.05909 [14] C. Zhang S.R. Kuppannagari R. Kannan V.K. Prasanna Building HVAC scheduling using reinforcement learning via neural network based model approximation Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation BuildSys \u201819 2019 Association for Computing Machinery New York, NY, USA 287 296 10.1145/3360322.3360861 C. Zhang, S. R. Kuppannagari, R. Kannan, V. K. Prasanna, Building HVAC scheduling using reinforcement learning via neural network based model approximation, in: Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, BuildSys \u201819, Association for Computing Machinery, New York, NY, USA, 2019, p. 287\u2013296. doi:10.1145/3360322.3360861. [15] L. Chen F. Meng Y. Zhang MBRL-MC: An HVAC control approach via combining model-based deep reinforcement learning and model predictive control IEEE Int. Things J. 9 19 2022 19160 19173 10.1109/JIOT.2022.3164023 L. Chen, F. Meng, Y. Zhang, MBRL-MC: An HVAC Control Approach via Combining Model-Based Deep Reinforcement Learning and Model Predictive Control, IEEE Internet of Things Journal 9 (19) (2022) 19160\u201319173. doi:10.1109/JIOT.2022.3164023. [16] Z. An X. Ding A. Rathee W. Du CLUE: safe model-based RL HVAC control using epistemic uncertainty estimation Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, BuildSys 2023, Assoc Comp Machinery, ACM Special Interest Grp Energy Syst & Informat Istanbul, Turkey, November 15-16, 2023. 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation (BuildSys) 2023 149 158 10.1145/3600100.3623742 Z. An, X. Ding, A. Rathee, W. Du, CLUE: Safe Model-Based RL HVAC Control using Epistemic Uncertainty Estimation, in: Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, BuildSys 2023, Assoc Comp Machinery, ACM Special Interest Grp Energy Syst & Informat, 2023, pp. 149\u2013158, 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation (BuildSys), Istanbul, Turkey, November 15-16, 2023. doi:10.1145/3600100.3623742. [17] Z. Afroz G. Shafiullah T. Urmee G. Higgins Modeling techniques used in building hvac control systems: a review Renew. Sustain. Energy Rev. 83 2018 64 84 10.1016/j.rser.2017.10.044 Z. Afroz, G. Shafiullah, T. Urmee, G. Higgins, Modeling techniques used in building hvac control systems: A review, Renewable and Sustainable Energy Reviews 83 (2018) 64\u201384. doi:https://doi.org/10.1016/j.rser.2017.10.044. [18] G. Gokhale B. Claessens C. Develder Physics informed neural networks for control oriented thermal modeling of buildings Appl. Energy 314 5 2022 10.1016/j.apenergy.2022.118852 G. Gokhale, B. Claessens, C. Develder, Physics informed neural networks for control oriented thermal modeling of buildings, Applied Energy 314 (5 2022). doi:10.1016/j.apenergy.2022.118852. [19] L. Di Natale B. Svetozarevic P. Heer C. Jones Physically consistent neural networks for building thermal modeling: theory and analysis Appl. Energy 325 2022 119806 10.1016/j.apenergy.2022.119806 L. Di Natale, B. Svetozarevic, P. Heer, C. Jones, Physically consistent neural networks for building thermal modeling: Theory and analysis, Applied Energy 325 (2022) 119806. doi:https://doi.org/10.1016/j.apenergy.2022.119806. [20] K. Al Sayed A. Boodi R. Sadeghian Broujeny K. Beddiar Reinforcement learning for HVAC control in intelligent buildings: a technical and conceptual review J. Build. Eng. 95 2024 110085 10.1016/j.jobe.2024.110085 K. Al Sayed, A. Boodi, R. Sadeghian Broujeny, K. Beddiar, Reinforcement learning for HVAC control in intelligent buildings: A technical and conceptual review, Journal of Building Engineering 95 (2024) 110085. doi:https://doi.org/10.1016/j.jobe.2024.110085. [21] P. Westermann R. Evins Surrogate modelling for sustainable building design \u2013 a review Energy Build. 198 2019 170 186 10.1016/j.enbuild.2019.05.057 P. Westermann, R. Evins, Surrogate modelling for sustainable building design \u2013 a review, Energy and Buildings 198 (2019) 170\u2013186. doi:https://doi.org/10.1016/j.enbuild.2019.05.057. [22] M.Z. Yousif M. Zhang Y. Yang H. Zhou L. Yu H. Lim Physics-guided deep reinforcement learning for flow field denoising arXiv preprint arXiv:2302.09559 2023 M. Z. Yousif, M. Zhang, Y. Yang, H. Zhou, L. Yu, H. Lim, Physics-guided deep reinforcement learning for flow field denoising, arXiv preprint arXiv:2302.09559 (2023). [23] S. Korivand N. Jalili J. Gong Inertia-constrained reinforcement learning to enhance human motor control modeling Sensors 23 5 2023 10.3390/s23052698 S. Korivand, N. Jalili, J. Gong, Inertia-constrained reinforcement learning to enhance human motor control modeling, Sensors 23 (5) (2023). doi:10.3390/s23052698. [24] T. Zhao J. Wang M. Yue A barrier-certificated reinforcement learning approach for enhancing power system transient stability IEEE Trans. Power Syst. 38 6 2023 5356 5366 10.1109/TPWRS.2022.3233770 T. Zhao, J. Wang, M. Yue, A barrier-certificated reinforcement learning approach for enhancing power system transient stability, IEEE Transactions on Power Systems 38 (6) (2023) 5356\u20135366. doi:10.1109/TPWRS.2022.3233770. [25] C. Banerjee K. Nguyen C. Fookes M. Raissi A survey on physics informed reinforcement learning: review and open problems arXiv preprint arXiv:2309.01909 2023 C. Banerjee, K. Nguyen, C. Fookes, M. Raissi, A survey on physics informed reinforcement learning: Review and open problems, arXiv preprint arXiv:2309.01909 (2023). [26] L. von Rueden S. Mayer K. Beckh B. Georgiev S. Giesselbach R. Heese B. Kirsch J. Pfrommer A. Pick R. Ramamurthy M. Walczak J. Garcke C. Bauckhage J. Schuecker Informed machine learning \u2013 a taxonomy and survey of integrating prior knowledge into learning systems IEEE Trans. Knowl. Data Eng. 35 1 2023 614 633 10.1109/TKDE.2021.3079836 L. von Rueden, S. Mayer, K. Beckh, B. Georgiev, S. Giesselbach, R. Heese, B. Kirsch, J. Pfrommer, A. Pick, R. Ramamurthy, M. Walczak, J. Garcke, C. Bauckhage, J. Schuecker, Informed machine learning \u2013 a taxonomy and survey of integrating prior knowledge into learning systems, IEEE Transactions on Knowledge and Data Engineering 35 (1) (2023) 614\u2013633. doi:10.1109/TKDE.2021.3079836. [27] Y. Chen Q. Yang Z. Chen C. Yan S. Zeng M. Dai Physics-informed neural networks for building thermal modeling and demand response control Build. Environ. 234 2023 110149 10.1016/j.buildenv.2023.110149 Y. Chen, Q. Yang, Z. Chen, C. Yan, S. Zeng, M. Dai, Physics-informed neural networks for building thermal modeling and demand response control, Building and Environment 234 (2023) 110149. doi:https://doi.org/10.1016/j.buildenv.2023.110149. [28] G. Gokhale B. Claessens C. Develder Physq: a physics informed reinforcement learning framework for building control arXiv preprint arXiv:2211.11830 2022 G. Gokhale, B. Claessens, C. Develder, Physq: a physics informed reinforcement learning framework for building control, arXiv preprint arXiv:2211.11830 (2022). [29] Z. Zhang K.P. Lam Practical implementation and evaluation of deep reinforcement learning control for a radiant heating system Proceedings of the 5th Conference on Systems for Built Environments BuildSys \u201818 2018 Association for Computing Machinery New York, NY, USA 148 157 10.1145/3276774.3276775 Z. Zhang, K. P. Lam, Practical implementation and evaluation of deep reinforcement learning control for a radiant heating system, in: Proceedings of the 5th Conference on Systems for Built Environments, BuildSys \u201818, Association for Computing Machinery, New York, NY, USA, 2018, p. 148\u2013157. doi:10.1145/3276774.3276775. [30] B. Park A.R. Rempel S. Mishra Performance, robustness, and portability of imitation-assisted reinforcement learning policies for shading and natural ventilation control Appl. Energy 347 2023 121364 10.1016/j.apenergy.2023.121364 B. Park, A. R. Rempel, S. Mishra, Performance, robustness, and portability of imitation-assisted reinforcement learning policies for shading and natural ventilation control, Applied Energy 347 (2023) 121364. doi:https://doi.org/10.1016/j.apenergy.2023.121364. [31] F. Bünning B. Huber A. Schalbetter A. Aboudonia M. Hudoba de Badyn P. Heer R.S. Smith J. Lygeros Physics-informed linear regression is competitive with two machine learning methods in residential building mpc Appl. Energy 310 2022 118491 10.1016/j.apenergy.2021.118491 F. Bünning, B. Huber, A. Schalbetter, A. Aboudonia, M. Hudoba de Badyn, P. Heer, R. S. Smith, J. Lygeros, Physics-informed linear regression is competitive with two machine learning methods in residential building mpc, Applied Energy 310 (2022) 118491. doi:https://doi.org/10.1016/j.apenergy.2021.118491. [32] G. Costanzo S. Iacovella F. Ruelens T. Leurs B. Claessens Experimental analysis of data-driven control for a building heating system Sustain. Energy Grids Netw. 6 2016 81 90 10.1016/j.segan.2016.02.002 G. Costanzo, S. Iacovella, F. Ruelens, T. Leurs, B. Claessens, Experimental analysis of data-driven control for a building heating system, Sustainable Energy, Grids and Networks 6 (2016) 81\u201390. doi:https://doi.org/10.1016/j.segan.2016.02.002. [33] Y. Chen Z. Tong Y. Zheng H. Samuelson L. Norford Transfer learning with deep neural networks for model predictive control of hvac and natural ventilation in smart buildings J. Clean. Prod. 254 2020 119866 10.1016/j.jclepro.2019.119866 Y. Chen, Z. Tong, Y. Zheng, H. Samuelson, L. Norford, Transfer learning with deep neural networks for model predictive control of hvac and natural ventilation in smart buildings, Journal of Cleaner Production 254 (2020) 119866. doi:https://doi.org/10.1016/j.jclepro.2019.119866. [34] S. Qiu Z. Li Z. Li J. Li S. Long X. Li Model-free control method based on reinforcement learning for building cooling water systems: validation by measured data-based simulation Energy Build. 218 2020 110055 10.1016/j.enbuild.2020.110055 S. Qiu, Z. Li, Z. Li, J. Li, S. Long, X. Li, Model-free control method based on reinforcement learning for building cooling water systems: Validation by measured data-based simulation, Energy and Buildings 218 (2020) 110055. doi:https://doi.org/10.1016/j.enbuild.2020.110055. [35] D. Coraci S. Brandi A. Capozzoli Effective pre-training of a deep reinforcement learning agent by means of long short-term memory models for thermal energy management in buildings Energy Convers. Manag. 291 2023 117303 10.1016/j.enconman.2023.117303 D. Coraci, S. Brandi, A. Capozzoli, Effective pre-training of a deep reinforcement learning agent by means of long short-term memory models for thermal energy management in buildings, Energy Conversion and Management 291 (2023) 117303. doi:https://doi.org/10.1016/j.enconman.2023.117303. [36] R.S. Sutton Dyna, an integrated architecture for learning, planning, and reacting SIGART Bull. 2 4 1991 160 163 10.1145/122344.122377 R. S. Sutton, Dyna, an integrated architecture for learning, planning, and reacting, SIGART Bull. 2 (4) (1991) 160\u2013163. doi:10.1145/122344.122377. [37] C. Gao D. Wang Comparative study of model-based and model-free reinforcement learning control performance in HVAC systems J. Build. Eng. 74 2023 106852 10.1016/j.jobe.2023.106852 C. Gao, D. Wang, Comparative study of model-based and model-free reinforcement learning control performance in HVAC systems, Journal of Building Engineering 74 (2023) 106852. doi:https://doi.org/10.1016/j.jobe.2023.106852. [38] D. Blum J. Arroyo S. Huang J. Drgoňa F. Jorissen H.T. Walnum Y. Chen K. Benne D. Vrabie M. Wetter L. Helsen Building optimization testing framework (BOPTEST) for simulation-based benchmarking of control strategies in buildings J. Build. Perform. Simul. 14 5 2021 586 610 10.1080/19401493.2021.1986574 D. Blum, J. Arroyo, S. Huang, J. Drgoňa, F. Jorissen, H. T. Walnum, Y. Chen, K. Benne, D. Vrabie, M. Wetter, L. Helsen, Building optimization testing framework (BOPTEST) for simulation-based benchmarking of control strategies in buildings, Journal of Building Performance Simulation 14 (5) (2021) 586\u2013610. doi:10.1080/19401493.2021.1986574. [39] M.L. Puterman Chapter 8 Markov decision processes Stochastic Models Handbooks in Operations Research and Management Science vol. 2 1990 Elsevier 331 434 10.1016/S0927-0507(05)80172-0 M. L. Puterman, Chapter 8 markov decision processes, in: Stochastic Models, Vol. 2 of Handbooks in Operations Research and Management Science, Elsevier, 1990, pp. 331\u2013434. doi:https://doi.org/10.1016/S0927-0507(05)80172-0. [40] R.S. Sutton A.G. Barto Reinforcement Learning: An Introduction 2nd edition 2018 MIT Press R. S. Sutton, A. G. Barto, Reinforcement Learning: An Introduction, 2nd Edition, MIT Press, 2018. [41] J.A. Filar K. Vrieze Competitive Markov Decision Processes 1st Edition, Springer-Verlag, New York 1997 Inc. 10.1007/978-1-4612-4054-9 J. A. Filar, K. Vrieze, Competitive Markov Decision Processes, 1st Edition, Springer-Verlag New York, Inc., 1997. doi:10.1007/978-1-4612-4054-9. [42] G.E. Karniadakis I.G. Kevrekidis L. Lu P. Perdikaris S. Wang L. Yang Physics-informed machine learning Nat. Rev. Phys. 3 6 2021 422 440 10.1038/s42254-021-00314-5 G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, L. Yang, Physics-informed machine learning, Nature Reviews Physics 3 (6) (2021) 422\u2013440. doi:10.1038/s42254-021-00314-5. [43] K. Shukla M. Xu N. Trask G.E. Karniadakis Scalable algorithms for physics-informed neural and graph networks Data-Cent. Eng. 3 2022 e24 10.1017/dce.2022.24 K. Shukla, M. Xu, N. Trask, G. E. Karniadakis, Scalable algorithms for physics-informed neural and graph networks, Data-Centric Engineering 3 (2022) e24. doi:10.1017/dce.2022.24. [44] L.D. McClenny U.M. Braga-Neto Self-adaptive physics-informed neural networks J. Comput. Phys. 474 2023 111722 10.1016/j.jcp.2022.111722 L. D. McClenny, U. M. Braga-Neto, Self-adaptive physics-informed neural networks, Journal of Computational Physics 474 (2023) 111722. doi:https://doi.org/10.1016/j.jcp.2022.111722. [45] M.H. Saeed H. Kazmi G. Deconinck Physics-informed model-based reinforcement learning (pi-mbrl) to control building heating systems in low training data regimes Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation BuildSys \u201823 2023 Association for Computing Machinery New York, NY, USA 10.1145/3600100.3626270 M. H. Saeed, H. Kazmi, G. Deconinck, Physics-informed model-based reinforcement learning (pi-mbrl) to control building heating systems in low training data regimes, in: Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, BuildSys \u201823, Association for Computing Machinery, New York, NY, USA, 2023. doi:10.1145/3600100.3626270. [46] J. Fan Z. Wang Y. Xie Z. Yang A theoretical analysis of deep q-learning Proceedings of the 2nd Conference on Learning for Dynamics and Control PMLR Proceedings of Machine Learning Research vol. 120 2020 486 489 https://proceedings.mlr.press/v120/yang20a.html J. Fan, Z. Wang, Y. Xie, Z. Yang, A theoretical analysis of deep q-learning, in: Proceedings of the 2nd Conference on Learning for Dynamics and Control, Vol. 120 of Proceedings of Machine Learning Research, PMLR, 2020, pp. 486\u2013489. URL https://proceedings.mlr.press/v120/yang20a.html [47] G. Brockman V. Cheung L. Pettersson J. Schneider J. Schulman J. Tang W. Zaremba http://arxiv.org/abs/1606.01540 June 2016 OpenAI Gym G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, W. Zaremba, OpenAI Gym (6 2016). URL http://arxiv.org/abs/1606.01540",
    "scopus-id": "85206186834",
    "coredata": {
        "eid": "1-s2.0-S0378778824009952",
        "dc:description": "This paper introduces Dyna-PINN, a novel physics-informed Deep Dyna-Q (DDQ) reinforcement learning (RL) approach, designed to address the data-intensive training requirements and model-agnostic nature of the conventional model-free RL methods. The DDQ approach blends model-based and model-free elements to enhance both learning and decision-making processes. By utilizing a physics-informed neural network (PINN) based model, our method enriches the learning process with physical information, enhancing the agent's planning capabilities and leading to faster learning compared to conventional model-free RL methods like Deep Q-Network (DQN) in scenarios with low-diversity training data availability. Our results demonstrate that Dyna-PINN has 50% greater sample efficiency than DQN and outperforms rule-based control in terms of thermal discomfort. Due to physics incorporation, the Dyna-PINN implements a more logical and interpretable control policy. It shows consistently good performance compared to all control variants across low-diversity data scenarios, i.e., 6 weeks of building data, and in higher-diversity data regimes, i.e., 6 months of building energy data, demonstrating the value of physics incorporation into the RL training. Additionally, we present two other DDQ-based techniques, RC-DDQ and NN-DDQ, exploring the synergy between neural networks and physical data in intelligent control designs for building energy systems. Rigorous controller testing is performed using the Building Optimization and Testing Framework (BOPTEST), a high-fidelity simulator that closely represents a real building's operation. Through comprehensive comparisons and realistic simulations, our study underscores the effectiveness of incorporating physics-informed approaches into RL-based control strategies, paving the way for more efficient and robust building energy management systems.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2024-12-01",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S0378778824009952",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Saeed, Muhammad Hafeez"
            },
            {
                "@_fa": "true",
                "$": "Kazmi, Hussain"
            },
            {
                "@_fa": "true",
                "$": "Deconinck, Geert"
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S0378778824009952"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S0378778824009952"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S0378-7788(24)00995-2",
        "prism:volume": "324",
        "articleNumber": "114879",
        "prism:publisher": "Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
        "dc:title": "Dyna-PINN: Physics-informed deep dyna-q reinforcement learning for intelligent control of building heating system in low-diversity training data regimes",
        "prism:copyright": "© 2024 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
        "openaccess": "0",
        "prism:issn": "03787788",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "Model-based reinforcement learning"
            },
            {
                "@_fa": "true",
                "$": "Physics-informed neural networks"
            },
            {
                "@_fa": "true",
                "$": "Buildings heating system"
            },
            {
                "@_fa": "true",
                "$": "Deep dyna-q approach"
            },
            {
                "@_fa": "true",
                "$": "Deep q-network"
            }
        ],
        "openaccessArticle": "false",
        "prism:publicationName": "Energy and Buildings",
        "openaccessSponsorType": null,
        "prism:pageRange": "114879",
        "pubType": "fla",
        "prism:coverDisplayDate": "1 December 2024",
        "prism:doi": "10.1016/j.enbuild.2024.114879",
        "prism:startingPage": "114879",
        "dc:identifier": "doi:10.1016/j.enbuild.2024.114879",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "259",
            "@width": "806",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr009.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "71208",
            "@ref": "gr009",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "49",
            "@width": "421",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr007.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "5314",
            "@ref": "gr007",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "321",
            "@width": "582",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr008.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "22278",
            "@ref": "gr008",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "200",
            "@width": "258",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr001.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "12571",
            "@ref": "gr001",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "577",
            "@width": "806",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr012.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "135980",
            "@ref": "gr012",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "498",
            "@width": "642",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr002.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "44540",
            "@ref": "gr002",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "576",
            "@width": "803",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr013.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "142217",
            "@ref": "gr013",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "687",
            "@width": "807",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr010.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "59313",
            "@ref": "gr010",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "340",
            "@width": "581",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr011.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "23848",
            "@ref": "gr011",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "514",
            "@width": "497",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr005.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "38546",
            "@ref": "gr005",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "161",
            "@width": "467",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr006.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "12951",
            "@ref": "gr006",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "341",
            "@width": "816",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr003.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "58206",
            "@ref": "gr003",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "312",
            "@width": "647",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr014.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "56154",
            "@ref": "gr014",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "223",
            "@width": "495",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr004.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "18750",
            "@ref": "gr004",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "601",
            "@width": "807",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr015.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "103523",
            "@ref": "gr015",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "70",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr009.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "9749",
            "@ref": "gr009",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "26",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr007.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "1923",
            "@ref": "gr007",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "121",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr008.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "4287",
            "@ref": "gr008",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "211",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr001.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "6467",
            "@ref": "gr001",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "157",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr012.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "15246",
            "@ref": "gr012",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "211",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr002.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "5353",
            "@ref": "gr002",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "157",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr013.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "14471",
            "@ref": "gr013",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "192",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr010.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "4371",
            "@ref": "gr010",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "128",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr011.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "3744",
            "@ref": "gr011",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "158",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr005.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "4367",
            "@ref": "gr005",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "76",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr006.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "2636",
            "@ref": "gr006",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "91",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr003.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "4239",
            "@ref": "gr003",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "106",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr014.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "4321",
            "@ref": "gr014",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "99",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr004.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "3489",
            "@ref": "gr004",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr015.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "13404",
            "@ref": "gr015",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "1146",
            "@width": "3570",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr009_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "660064",
            "@ref": "gr009",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "437",
            "@width": "3726",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr007_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "115865",
            "@ref": "gr007",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "852",
            "@width": "1546",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr008_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "83675",
            "@ref": "gr008",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "531",
            "@width": "685",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr001_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "52674",
            "@ref": "gr001",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2556",
            "@width": "3568",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr012_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1173483",
            "@ref": "gr012",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1325",
            "@width": "1707",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr002_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "157291",
            "@ref": "gr002",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2548",
            "@width": "3555",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr013_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1258915",
            "@ref": "gr013",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "3042",
            "@width": "3571",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr010_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "408949",
            "@ref": "gr010",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1505",
            "@width": "2571",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr011_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "186401",
            "@ref": "gr011",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1366",
            "@width": "1322",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr005_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "134171",
            "@ref": "gr005",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "429",
            "@width": "1241",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr006_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "47773",
            "@ref": "gr006",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "3018",
            "@width": "7224",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr003_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1660171",
            "@ref": "gr003",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1383",
            "@width": "2867",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr014_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "535022",
            "@ref": "gr014",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "592",
            "@width": "1315",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr004_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "75224",
            "@ref": "gr004",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2662",
            "@width": "3575",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-gr015_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "819749",
            "@ref": "gr015",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si1.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5678",
            "@ref": "si1",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si10.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5646",
            "@ref": "si10",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si11.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "14331",
            "@ref": "si11",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si12.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1904",
            "@ref": "si12",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si13.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2119",
            "@ref": "si13",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si14.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "28288",
            "@ref": "si14",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si15.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "13414",
            "@ref": "si15",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si16.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6400",
            "@ref": "si16",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si17.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2938",
            "@ref": "si17",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si18.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1863",
            "@ref": "si18",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si19.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3569",
            "@ref": "si19",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si2.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7621",
            "@ref": "si2",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si20.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2404",
            "@ref": "si20",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si21.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2201",
            "@ref": "si21",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si22.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2340",
            "@ref": "si22",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si23.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2248",
            "@ref": "si23",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si24.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1974",
            "@ref": "si24",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si25.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1879",
            "@ref": "si25",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si26.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2027",
            "@ref": "si26",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si27.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1935",
            "@ref": "si27",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si28.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5554",
            "@ref": "si28",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si29.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2733",
            "@ref": "si29",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si3.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1894",
            "@ref": "si3",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si30.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5551",
            "@ref": "si30",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si31.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "13375",
            "@ref": "si31",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si32.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "16415",
            "@ref": "si32",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si33.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8804",
            "@ref": "si33",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si34.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "13551",
            "@ref": "si34",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si35.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3520",
            "@ref": "si35",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si36.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "17981",
            "@ref": "si36",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si37.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "10298",
            "@ref": "si37",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si38.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "25517",
            "@ref": "si38",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si39.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "14269",
            "@ref": "si39",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si4.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6380",
            "@ref": "si4",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si40.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "18882",
            "@ref": "si40",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si41.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5566",
            "@ref": "si41",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si42.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "26749",
            "@ref": "si42",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si43.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4744",
            "@ref": "si43",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si44.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5434",
            "@ref": "si44",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si45.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "21946",
            "@ref": "si45",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si46.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2027",
            "@ref": "si46",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si47.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6520",
            "@ref": "si47",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si48.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6388",
            "@ref": "si48",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si49.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8415",
            "@ref": "si49",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si5.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4353",
            "@ref": "si5",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si50.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7518",
            "@ref": "si50",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si51.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4936",
            "@ref": "si51",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si52.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4262",
            "@ref": "si52",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si6.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "27119",
            "@ref": "si6",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si7.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "24672",
            "@ref": "si7",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si8.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "22302",
            "@ref": "si8",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-si9.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11313",
            "@ref": "si9",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0378778824009952-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "1969432",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85206186834"
    }
}}