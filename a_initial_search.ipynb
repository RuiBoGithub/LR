{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "440061b0-f4e4-4452-9bf6-29055ef43b94",
   "metadata": {},
   "source": [
    "## Elsevier API Documentation\n",
    "https://dev.elsevier.com/documentation/ScienceDirectSearchAPI.wadl\n",
    "\n",
    "### API Key Request\n",
    "\n",
    "*   Navigate to the [Elsevier Developer Portal](https://dev.elsevier.com).\n",
    "*   Sign in and request an API key. This generates a string that provides the value for the `els_apikey` variable in `credentials.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112ae783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import xmltodict\n",
    "import json\n",
    "from _credentials import keys"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91a60815",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def GENboolean_query(path: str, quote_multiword: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    To read a 2-column CSV (Category, Keywords).\n",
    "    Within category: join terms with OR.\n",
    "    Across categories: join groups with AND.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    groups = []\n",
    "    for _, row in df.iterrows():\n",
    "        kws = [k.strip() for k in str(row[\"Keywords\"]).split(\",\") if str(k).strip()]\n",
    "        terms = []\n",
    "        for k in kws:\n",
    "            terms.append(f'\"{k}\"' if quote_multiword and len(k.split()) > 1 else k)\n",
    "        if terms:\n",
    "            groups.append(f'({\" OR \".join(terms)})')\n",
    "    return \" AND \".join(groups)\n",
    "\n",
    "csv_keywords_path = \"csv/_keywords.csv\"\n",
    "base_query = GENboolean_query(csv_keywords_path)\n",
    "# sanity check\n",
    "print(base_query)\n",
    "\n",
    "if not os.path.exists('csv_output'):\n",
    "    os.makedirs('csv_output')\n",
    "csv_file_path = os.path.join('csv_output', '_fetching.csv')\n",
    "url = 'https://api.elsevier.com/content/search/sciencedirect'\n",
    "headers = {\n",
    "    'Accept': 'application/json',\n",
    "    \"X-ELS-APIKey\": keys[\"els-apikey\"]\n",
    "    # \"X-ELS-Insttoken\": keys[\"els-inst-token\"]\n",
    "}\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['DOI', 'Year', 'Title', 'Journal'])\n",
    "    for year in range(2015, 2025):\n",
    "        start = 0\n",
    "        while True:\n",
    "            date_range = f'{year}'\n",
    "            params = {\n",
    "                'query': base_query,\n",
    "                'date': date_range,\n",
    "                'count': 100,  # 100 is typically maximum allowed by API\n",
    "                'start': start\n",
    "            }\n",
    "\n",
    "            # Send the GET request\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                results = response.json()\n",
    "                articles = results['search-results']['entry']\n",
    "                total_fetched = len(articles)\n",
    "\n",
    "                for article in articles:\n",
    "                    if 'dc:identifier' not in article: # Skip articles without a DOI\n",
    "                        continue\n",
    "                    \n",
    "                    doi = article['dc:identifier'].replace('DOI:', '')\n",
    "                    year = article.get('prism:coverDate', '')[0:4]  # Extract publication year\n",
    "                    title = article.get('dc:title', 'Title not available')\n",
    "                    journal = article.get('prism:publicationName', 'Journal not available')\n",
    "                    writer.writerow([doi, year, title, journal])\n",
    "\n",
    "                start += total_fetched\n",
    "\n",
    "                if total_fetched < 100:\n",
    "                    break\n",
    "            else:\n",
    "                print(f'Error fetching data for {year}: {response.status_code}')\n",
    "                break\n",
    "\n",
    "print(f'Data has been successfully saved to {csv_file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cfb889",
   "metadata": {},
   "source": [
    "### Article filter\n",
    "* Filtered for unique DOIs, restricted to Elsevier journal articles on the designated whitelist, and excluded review-like items (review/survey/overview).\n",
    "* Applied a topic filter that excluded terms such as LCA and BIM unless “operation” or “control” appeared, followed by a hard filter for terms such as heat pump/exchanger, with triggers recorded.\n",
    "* Executed a final inclusion filter retaining titles matching curated Model/Data/Application/Target keywords, and exported a strictly formatted file (DOI, Year, Title, Journal) with audit summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3d13af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 48100\n",
      "Deduplication: 47106\n",
      "* After filtering books/conference papers: 45342\n",
      "(Excluded: 1764)\n",
      "* After filtering those are not published in designated journals: 23247\n",
      "(Excluded: 22095)\n",
      "* After removing reviews: 21816\n",
      "(Excluded: 1431)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s2589602\\AppData\\Local\\Temp\\ipykernel_17580\\2184806677.py:200: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  hit = titles.str.contains(pat, flags=re.IGNORECASE, na=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* After soft topic exclusion: 13881\n",
      "(Excluded: 7935)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s2589602\\AppData\\Local\\Temp\\ipykernel_17580\\2184806677.py:211: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  hit = titles.str.contains(pat, flags=re.IGNORECASE, na=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* After hard topic exclusion: 7749\n",
      "(Excluded: 6132)\n",
      "* After keyword inclusion: 4177\n",
      "...Final Saved... \n",
      "csv_output\\99_final\\elsevier_final_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import os, re, csv, sys, json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# ======================= CONFIG =======================\n",
    "OUT_ROOT = \"csv_output\"\n",
    "RAW_DIR          = os.path.join(OUT_ROOT, \"00_raw\")\n",
    "DEDUP_DIR        = os.path.join(OUT_ROOT, \"01_dedup\")\n",
    "JDOI_DIR         = os.path.join(OUT_ROOT, \"02_journal_only\")\n",
    "DESIG_DIR        = os.path.join(OUT_ROOT, \"03_designated\")\n",
    "REVIEW_DIR       = os.path.join(OUT_ROOT, \"04_reviews\")\n",
    "TOPIC_SOFT_DIR   = os.path.join(OUT_ROOT, \"05_topic_soft\")\n",
    "TOPIC_HARD_DIR   = os.path.join(OUT_ROOT, \"06_topic_hard\")\n",
    "KEYWORDS_DIR     = os.path.join(OUT_ROOT, \"07_keywords\")\n",
    "FINAL_DIR        = os.path.join(OUT_ROOT, \"99_final\")\n",
    "\n",
    "JOURNALS_LIST    = \"csv/_journals.csv\"\n",
    "KEYWORDS_CSV     = \"csv/_keywords-reduced.csv\"\n",
    "\n",
    "# Try these inputs in order\n",
    "CANDIDATE_INPUTS = [\n",
    "    os.path.join(OUT_ROOT, \"fetching_1.csv\"),\n",
    "    os.path.join(OUT_ROOT, \"elsevier_search_results_cleaned.csv\"),\n",
    "    \"elsevier_search_results_cleaned.csv\",\n",
    "]\n",
    "\n",
    "# Hard-exclusion terms (always drop; ignore allow pattern)\n",
    "HARD_TERMS = [\n",
    "    (\"life cycle\", r\"\\blife*cycle?\\b\"),\n",
    "    (\"district heating\", r\"\\bdistrict\\s*heat(ing)?\\b\"),\n",
    "    (\"district cooling\", r\"\\bdistrict\\s*cool(ing)?\\b\"),\n",
    "    (\"district\",        r\"\\bdistrict(s)?\\b\"),\n",
    "    (\"stock\",        r\"\\bstock(s)?\\b\"),\n",
    "    (\"city\",        r\"\\bcity(s)?\\b\"),\n",
    "    (\"standard\",        r\"\\bstandards\\b\"),\n",
    "    (\"grid\",        r\"\\bgrid(s)?\\b\"),\n",
    "    (\"credit\",        r\"\\bcredit(s)?\\b\"),\n",
    "    (\"dc\",        r\"\\bdc(s)?\\b\"),\n",
    "    (\"ac\",        r\"\\bac(s)?\\b\"),\n",
    "    (\"wave\",        r\"\\bwave(s)?\\b\"),\n",
    "    (\"tide\",        r\"\\btide(s)?\\b\"),\n",
    "    (\"tidal\",        r\"\\btidal\\b\"),\n",
    "    (\"offshore\",        r\"\\boff[-\\s]?shore\\b\"),\n",
    "    (\"wind\",        r\"\\bwind(s)?\\b\"),\n",
    "    (\"hydro\",        r\"\\bhydro\\b\"),\n",
    "    (\"daylight\",        r\"\\bdaylight(s)?\\b\"),\n",
    "    (\"lighting\",        r\"\\blight(ing)?\\b\"),\n",
    "    (\"solar thermal\",   r\"\\bsolar[-\\s]?thermal\\b\"),\n",
    "    (\"microgrid\", r\"\\bmicro[-\\s]?grid(s)?\\b\"), # grid\n",
    "    (\"grid\", r\"\\bgrid(s)?\\b\"), #\n",
    "    (\"embodied\",        r\"\\bembodied?\\b\"),\n",
    "    (\"storage\",        r\"\\bstorage(s)?\\b\"),\n",
    "    (\"material\",            r\"\\bmaterial(s)?\\b\"),\n",
    "    (\"window\",       r\"\\bwindow(s)?\\b\"),\n",
    "    (\"roof\",         r\"\\broof(s)?\\b\"),\n",
    "    (\"ceiling\",         r\"\\bceiling(s)?\\b\"),\n",
    "    (\"wall\",         r\"\\bwall(s)?\\b\"),\n",
    "    (\"pcm\",             r\"\\bpcm\\b\"),\n",
    "    (\"housing\",       r\"\\bhousing?\\b\"),\n",
    "    (\"borehole\",        r\"\\bborehole(s)?\\b\"),\n",
    "    (\"geothermal\",      r\"\\bgeo[-\\s]?thermal\\b\"),\n",
    "    (\"envelope\",     r\"\\benvelope(s)?\\b\"),\n",
    "    (\"heat pump\",        r\"\\bheat[-\\s]?pump(s)?\\b\"),\n",
    "    (\"heat exchanger\",   r\"\\bheat[-\\s]?exchanger(s)?\\b\"),\n",
    "    (\"industry\",         r\"\\bindustr(y|ies)\\b\"),\n",
    "    (\"glass\",            r\"\\bglass(es)?\\b\"),\n",
    "    (\"glaze\",            r\"\\bglaz(ing)?\\b\"),\n",
    "    (\"urban\",            r\"\\burban?\\b\"),\n",
    "    (\"York\",            r\"\\bYork?\\b\"),\n",
    "    (\"façade\",            r\"\\bfaçade?\\b\"),\n",
    "    (\"ev\",            r\"\\bev?\\b\"),\n",
    "    (\"electric vehicle\",  r\"\\belectric[-\\s]?vehicle(s)?\\b\"),\n",
    "    (\"mobility\",         r\"\\bmobilit(y|ies)\\b\"),\n",
    "    (\"aircraft\",        r\"\\baircraft(s)?\\b\"),\n",
    "    (\"transport\",        r\"\\btransport(s|ation)?\\b\"),\n",
    "    (\"vehicle\",          r\"\\bvehicle(s)?\\b\"),\n",
    "    (\"boiler\",             r\"\\bboiler(s)\\b\"),\n",
    "    (\"chp\",               r\"\\bchp\\b\"),\n",
    "    (\"cogeneration\",      r\"\\bcogeneration\\b\"),\n",
    "    (\"trigeneration\",     r\"\\btrigeneration\\b\"),\n",
    "    (\"cogen\",            r\"\\bcogen\\b\"),\n",
    "    (\"trigen\",           r\"\\btrigen\\b\"),\n",
    "    (\"fuel cell\",        r\"\\bfuel[-\\s]?cell(s)?\\b\"),\n",
    "    (\"combustion\",       r\"\\bcombustion\\b\"),\n",
    "    (\"turbine\",          r\"\\bturbine(s)?\\b\"),\n",
    "    (\"generator\",        r\"\\bgenerator(s)?\\b\"),\n",
    "    (\"chiller\",               r\"\\bchiller(s)?\\b\"),\n",
    "    (\"plant\",                r\"\\bplant(s)\\b\"),\n",
    "    (\"tunnel\",               r\"\\btunnel\\b\"),\n",
    "    (\"pv\",               r\"\\bpv\\b\"),\n",
    "    (\"solar\",            r\"\\bsolar\\b\"),\n",
    "    (\"indicators\",            r\"\\bindicator(s)?\\b\"),\n",
    "    (\"evaluation\",  r\"\\bevaluation\\b\"),\n",
    "    (\"construction\", \"construction\"),\n",
    "]\n",
    "\n",
    "# Soft topic exclusion (unless allow_pattern hits)\n",
    "SOFT_KEYWORDS = [\n",
    "    (\"roof\", \"roof(s)?\"),\n",
    "    (\"wall\", \"wall(s)?\"),\n",
    "    (\"envelope\", \"envelope(s)?\"),\n",
    "    (\"design\", \"design(s)?\"),\n",
    "    (\"compliance\", \"compliance\"),\n",
    "    (\"retrofit\", \"retrofit(s|ting)?\"),\n",
    "    (\"benchmark\", \"benchmark(ing|s)?\"), \n",
    "    (\"renovation\", \"renovation\"),\n",
    "    (\"predict\", \"predict(ing)?\"),\n",
    "    (\"forecast\", \"forecast\"),\n",
    "    (\"lca\", \"lca\"),\n",
    "    (\"life cycle assessment\", r\"life\\s*cycle\\s+assessment\"),\n",
    "    (\"life cycle\", r\"life\\s*cycle\\s+\"),\n",
    "    (\"emission\", \"emission(s)?\"),\n",
    "    (\"embodied\", \"embodied\"),\n",
    "    (\"sustainability\", r\"sustainab(le|ility)\"),\n",
    "    (\"green\", \"green\"),\n",
    "    (\"simulation\", \"simulation\"),\n",
    "    (\"calibration\", r\"calibration\"),\n",
    "    (\"roadmap\", \"roadmap\"),\n",
    "    (\"annex\", r\"\\bannex\\b\"),\n",
    "    (\"iea\", r\"\\biea\\b\"),\n",
    "    (\"policy\", \"policy\"),\n",
    "    (\"bim\", \"bim\"),\n",
    "    (\"thermal comfort\", r\"thermal\\s*comfort\"),\n",
    "    (\"indoor air\", r\"indoor\\s*air|indoor\\s*air\\s*quality|\\biaq\\b\"),\n",
    "]\n",
    "ALLOW_PATTERN = re.compile(\n",
    "    r\"\\b(?:operation|operations|operational|operate|operating|control|controls|controlled|controlling)\\b\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "REVIEW_LIKE = re.compile(\n",
    "    r\"\\b(?:review|surveys?|overviews?|advance?|limit?|meta[-\\s]?analysis)\\b\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "# ======================= UTILS ========================\n",
    "def ensure_dirs(*paths: str) -> None:\n",
    "    for p in paths:\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def write_csv(df: pd.DataFrame, path: str) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def find_input() -> str:\n",
    "    for p in CANDIDATE_INPUTS:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"No input CSV found in: {CANDIDATE_INPUTS}\")\n",
    "\n",
    "def load_df(path: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> Dict[str, Optional[str]]:\n",
    "    if \"DOI\" not in df.columns:\n",
    "        dc = next((c for c in df.columns if c.lower() in [\"dc:identifier\",\"identifier\"]), None)\n",
    "        if not dc: raise ValueError(\"No DOI column (expected 'DOI' or 'dc:identifier').\")\n",
    "        df[\"DOI\"] = df[dc].astype(str).str.replace(\"DOI:\", \"\", regex=False).str.strip()\n",
    "    title_col = next((c for c in [\"Title\",\"dc:title\",\"title\"] if c in df.columns), None)\n",
    "    journal_col = next((c for c in [\"Journal\",\"prism:publicationName\",\"journal\",\"publicationName\"] if c in df.columns), None)\n",
    "    if not journal_col:\n",
    "        raise ValueError(\"No journal column (e.g., 'Journal' or 'prism:publicationName').\")\n",
    "    return {\"title_col\": title_col, \"journal_col\": journal_col}\n",
    "\n",
    "def _norm_name(s) -> str:\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.replace(\"&\", \"and\")\n",
    "    return s\n",
    "\n",
    "def load_journal_whitelist(path: str) -> set:\n",
    "    jl = pd.read_csv(path)\n",
    "    possible = [\"Journal\",\"journal\",\"Title\",\"title\",\"publication\",\"Publication\"]\n",
    "    col = next((c for c in possible if c in jl.columns), jl.columns[0])\n",
    "    return set(jl[col].dropna().map(_norm_name).drop_duplicates().tolist())\n",
    "\n",
    "def is_review_flag(df: pd.DataFrame) -> pd.Series:\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    review = pd.Series(False, index=df.index)\n",
    "    if \"subtype\" in cols_lower:\n",
    "        st_col = cols_lower[\"subtype\"]\n",
    "        st = df[st_col].astype(str).str.lower()\n",
    "        review |= st.eq(\"re\") | df[st_col].astype(str).str.contains(REVIEW_LIKE, na=False)\n",
    "    if \"subtypedescription\" in cols_lower:\n",
    "        sd_col = cols_lower[\"subtypedescription\"]\n",
    "        review |= df[sd_col].astype(str).str.contains(REVIEW_LIKE, na=False)\n",
    "    title_col = cols_lower.get(\"title\", cols_lower.get(\"dc:title\"))\n",
    "    if title_col is not None:\n",
    "        undecided = ~review\n",
    "        if undecided.any():\n",
    "            t = df.loc[undecided, title_col].astype(str)\n",
    "            review.loc[undecided] = t.str.contains(REVIEW_LIKE, na=False)\n",
    "    return review\n",
    "\n",
    "def soft_topic_flags(titles: pd.Series):\n",
    "    matched_lists = []\n",
    "    for name, pat in SOFT_KEYWORDS:\n",
    "        hit = titles.str.contains(pat, flags=re.IGNORECASE, na=False)\n",
    "        matched_lists.append(hit.map(lambda x: name if x else \"\"))\n",
    "    reasons = pd.Series([\",\".join(filter(None, row)) for row in zip(*matched_lists)], index=titles.index)\n",
    "    has_kw = reasons.astype(bool)\n",
    "    has_allow = titles.str.contains(ALLOW_PATTERN, na=False)\n",
    "    topic_exclude = has_kw & (~has_allow)\n",
    "    return topic_exclude, reasons\n",
    "\n",
    "def hard_topic_flags(titles: pd.Series):\n",
    "    matched_lists = []\n",
    "    for name, pat in HARD_TERMS:\n",
    "        hit = titles.str.contains(pat, flags=re.IGNORECASE, na=False)\n",
    "        matched_lists.append(hit.map(lambda x: name if x else \"\"))\n",
    "    reasons = pd.Series([\",\".join(filter(None, row)) for row in zip(*matched_lists)], index=titles.index)\n",
    "    return reasons.astype(bool), reasons\n",
    "\n",
    "def phrase_to_regex(phrase: str):\n",
    "    tokens = re.split(r\"[-\\s]+\", phrase.strip())\n",
    "    tokens = [t for t in tokens if t]\n",
    "    if not tokens: return None\n",
    "    parts = [re.escape(tok) + r\"\\w*\" for tok in tokens]  # allow plurals/affixes & embedded\n",
    "    return re.compile(r\"(?:%s)\" % r\"[-\\s]*\".join(parts), flags=re.IGNORECASE)\n",
    "\n",
    "def split_terms(s: str) -> List[str]:\n",
    "    if pd.isna(s): return []\n",
    "    return [x.strip() for x in re.split(r\"[;,]\", str(s)) if x and x.strip()]\n",
    "\n",
    "def keyword_inclusion(df: pd.DataFrame, title_col: Optional[str]):\n",
    "    kw_df = pd.read_csv(KEYWORDS_CSV)\n",
    "    cmap = {c.lower(): c for c in kw_df.columns}\n",
    "    cat_col = cmap.get(\"category\"); keys_col = cmap.get(\"keywords\"); ref_col = cmap.get(\"ref\")\n",
    "    if not cat_col or not keys_col:\n",
    "        raise ValueError(\"`csv/_keywords.csv` must have 'Category' and 'Keywords'.\")\n",
    "\n",
    "    compiled = []\n",
    "    for _, row in kw_df.iterrows():\n",
    "        cat = str(row[cat_col]).strip()\n",
    "        base_terms = split_terms(row[keys_col])\n",
    "        ref_terms = split_terms(row[ref_col]) if (ref_col and pd.notna(row[ref_col])) else []\n",
    "        for term in base_terms + ref_terms:\n",
    "            rx = phrase_to_regex(term)\n",
    "            if rx is not None:\n",
    "                compiled.append({\"category\": cat, \"term\": term, \"regex\": rx})\n",
    "\n",
    "    if not title_col or not compiled:\n",
    "        df[\"_include_terms\"] = \"\"; df[\"_include_categories\"] = \"\"; df[\"_include_any\"] = False\n",
    "        counts_df = pd.DataFrame(columns=[\"Category\",\"Keyword\",\"Match_Count\"])\n",
    "        audit_df = df[[]]\n",
    "        return df, audit_df, counts_df\n",
    "\n",
    "    t = df[title_col].astype(str)\n",
    "    term_names = [it[\"term\"] for it in compiled]\n",
    "    term_cats  = [it[\"category\"] for it in compiled]\n",
    "    term_series = [t.str.contains(it[\"regex\"], na=False) for it in compiled]\n",
    "\n",
    "    matched_term_names = []\n",
    "    matched_categories = []\n",
    "    for row_bools in zip(*term_series):\n",
    "        names = [n for n, hit in zip(term_names, row_bools) if hit]\n",
    "        cats  = {c for c, hit in zip(term_cats,  row_bools) if hit}\n",
    "        matched_term_names.append(\",\".join(names))\n",
    "        matched_categories.append(\",\".join(sorted(cats)))\n",
    "\n",
    "    df[\"_include_terms\"] = pd.Series(matched_term_names, index=df.index)\n",
    "    df[\"_include_categories\"] = pd.Series(matched_categories, index=df.index)\n",
    "    df[\"_include_any\"] = df[\"_include_terms\"].astype(bool)\n",
    "\n",
    "    counts = [{\"Category\": it[\"category\"], \"Keyword\": it[\"term\"], \"Match_Count\": int(s.sum())}\n",
    "              for it, s in zip(compiled, term_series)]\n",
    "    counts_df = pd.DataFrame(counts).groupby([\"Category\",\"Keyword\"], as_index=False)[\"Match_Count\"].sum()\n",
    "\n",
    "    audit_cols = [\"DOI\", \"Year\"] + ([title_col] if title_col else []) + [\"_include_terms\",\"_include_categories\"]\n",
    "    audit_df = df[audit_cols].copy()\n",
    "    return df, audit_df, counts_df\n",
    "\n",
    "def tidy(df: pd.DataFrame, title_col: Optional[str], journal_col: str) -> pd.DataFrame:\n",
    "    cols = [\"DOI\", \"Year\"] + ([title_col] if title_col else []) + [journal_col]\n",
    "    exist = [c for c in cols if c in df.columns]\n",
    "    rest = [c for c in df.columns if c not in exist and not c.startswith(\"_\")]\n",
    "    return df[exist + rest]\n",
    "\n",
    "def save_final_exact(df: pd.DataFrame, title_col: Optional[str], journal_col: str, path: str):\n",
    "    out = df.copy()\n",
    "    if title_col and \"Title\" not in out.columns:\n",
    "        out[\"Title\"] = out[title_col]\n",
    "    if \"Journal\" not in out.columns:\n",
    "        out[\"Journal\"] = out[journal_col]\n",
    "    out = out[[\"DOI\",\"Year\",\"Title\",\"Journal\"]]\n",
    "    write_csv(out, path)\n",
    "    return out\n",
    "\n",
    "def stage_summary_line(name: str, kept: int, excl: Optional[int] = None) -> str:\n",
    "    return f\"{name}: {kept}\" + (f\"\\n(Excluded: {excl})\" if excl is not None else \"\")\n",
    "\n",
    "def contains_ci(s: pd.Series, pat):\n",
    "    if isinstance(pat, re.Pattern):          # already compiled with flags\n",
    "        return s.str.contains(pat, na=False, regex=True)\n",
    "    return s.str.contains(pat, na=False, regex=True, case=False)\n",
    "\n",
    "# ===================== STAGE-ONLY RUN =====================\n",
    "def run_pipeline_stage_only():\n",
    "    ensure_dirs(RAW_DIR, DEDUP_DIR, JDOI_DIR, DESIG_DIR, REVIEW_DIR, TOPIC_SOFT_DIR, TOPIC_HARD_DIR, KEYWORDS_DIR, FINAL_DIR)\n",
    "\n",
    "    input_csv = find_input()\n",
    "    df = load_df(input_csv)\n",
    "    mapping = normalize_columns(df)\n",
    "    title_col, journal_col = mapping[\"title_col\"], mapping[\"journal_col\"]\n",
    "\n",
    "    write_csv(df, os.path.join(RAW_DIR, os.path.basename(input_csv)))\n",
    "    print(\"Original:\", len(df))\n",
    "\n",
    "    # 1) De-dup DOI\n",
    "    df = df.dropna(subset=[\"DOI\"]).drop_duplicates(subset=[\"DOI\"])\n",
    "    write_csv(tidy(df, title_col, journal_col), os.path.join(DEDUP_DIR, \"dedup.csv\"))\n",
    "    print(stage_summary_line(\"Deduplication\", len(df)))\n",
    "\n",
    "    # 2) Journal DOIs only (/j.)\n",
    "    is_journal = df[\"DOI\"].astype(str).str.contains(r\"/j\\.\", case=False, na=False)\n",
    "    books_df = tidy(df.loc[~is_journal].copy(), title_col, journal_col)\n",
    "    df = df.loc[is_journal].copy()\n",
    "    write_csv(books_df, os.path.join(JDOI_DIR, \"excluded_books.csv\"))\n",
    "    write_csv(tidy(df, title_col, journal_col), os.path.join(JDOI_DIR, \"journal_only.csv\"))\n",
    "    print(stage_summary_line(\"* After filtering books/conference papers\", len(df), excl=len(books_df)))\n",
    "\n",
    "    # 3) Designated journals\n",
    "    whitelist = load_journal_whitelist(JOURNALS_LIST)\n",
    "    df[\"_journal_norm\"] = df[journal_col].map(_norm_name)\n",
    "    in_designated = df[\"_journal_norm\"].isin(whitelist)\n",
    "    not_designated_df = tidy(df.loc[~in_designated].copy(), title_col, journal_col)\n",
    "    df = df.loc[in_designated].copy()\n",
    "    write_csv(not_designated_df, os.path.join(DESIG_DIR, \"excluded_not_designated.csv\"))\n",
    "    write_csv(tidy(df, title_col, journal_col), os.path.join(DESIG_DIR, \"designated.csv\"))\n",
    "    print(stage_summary_line(\"* After filtering those are not published in designated journals\", len(df), excl=len(not_designated_df)))\n",
    "\n",
    "    # 4) Remove reviews (review/survey/overview/etc.)\n",
    "    df[\"_is_review\"] = is_review_flag(df)  # keep your function but ensure it uses non-capturing groups internally\n",
    "\n",
    "    reviews_df = tidy(df.loc[df[\"_is_review\"]].copy(), title_col, journal_col)\n",
    "    df = df.loc[~df[\"_is_review\"]].copy()\n",
    "\n",
    "    write_csv(reviews_df, os.path.join(REVIEW_DIR, \"excluded_reviews.csv\"))\n",
    "    write_csv(tidy(df, title_col, journal_col), os.path.join(REVIEW_DIR, \"no_reviews.csv\"))\n",
    "    print(stage_summary_line(\"* After removing reviews\", len(df), excl=len(reviews_df)))\n",
    "\n",
    "    # 5) Soft topic exclusion (unless allow pattern)\n",
    "    if title_col:\n",
    "        titles = df[title_col].astype(str).str.lower()\n",
    "        topic_excl, reasons = soft_topic_flags(titles)\n",
    "        df[\"_topic_exclude\"] = topic_excl\n",
    "        df[\"_exclude_reason\"] = reasons\n",
    "    else:\n",
    "        df[\"_topic_exclude\"] = False\n",
    "        df[\"_exclude_reason\"] = \"\"\n",
    "    topic_excl_df = tidy(df.loc[df[\"_topic_exclude\"]].copy(), title_col, journal_col)\n",
    "    df = df.loc[~df[\"_topic_exclude\"]].copy()\n",
    "    write_csv(topic_excl_df, os.path.join(TOPIC_SOFT_DIR, \"excluded_topic_keywords.csv\"))\n",
    "    write_csv(tidy(df, title_col, journal_col), os.path.join(TOPIC_SOFT_DIR, \"passed_soft_topic.csv\"))\n",
    "    print(stage_summary_line(\"* After soft topic exclusion\", len(df), excl=len(topic_excl_df)))\n",
    "\n",
    "    # 6) Hard topic exclusion (always drop)\n",
    "    if title_col:\n",
    "        titles = df[title_col].astype(str).str.lower()\n",
    "        hard_mask, hard_reasons = hard_topic_flags(titles)\n",
    "        df[\"_hard_topic_exclude\"] = hard_mask\n",
    "        df[\"_hard_exclude_reason\"] = hard_reasons\n",
    "    else:\n",
    "        df[\"_hard_topic_exclude\"] = False\n",
    "        df[\"_hard_exclude_reason\"] = \"\"\n",
    "    hard_df = tidy(df.loc[df[\"_hard_topic_exclude\"]].copy(), title_col, journal_col)\n",
    "    df = df.loc[~df[\"_hard_topic_exclude\"]].copy()\n",
    "    write_csv(hard_df, os.path.join(TOPIC_HARD_DIR, \"excluded_hard_topic_keywords.csv\"))\n",
    "    write_csv(tidy(df, title_col, journal_col), os.path.join(TOPIC_HARD_DIR, \"passed_hard_topic.csv\"))\n",
    "    print(stage_summary_line(\"* After hard topic exclusion\", len(df), excl=len(hard_df)))\n",
    "\n",
    "    # 7) Keyword-driven inclusion (keeps only titles matching keywords from csv/_keywords.csv)\n",
    "    df, audit_df, counts_df = keyword_inclusion(df, title_col)\n",
    "    df = df.loc[df[\"_include_any\"]].copy()\n",
    "    audit_df.to_csv(os.path.join(KEYWORDS_DIR, \"included_keyword_matches.csv\"), index=False)\n",
    "    counts_df.to_csv(os.path.join(KEYWORDS_DIR, \"keyword_match_counts.csv\"), index=False)\n",
    "    print(stage_summary_line(\"* After keyword inclusion\", len(df)))\n",
    "\n",
    "    # FINAL (exact columns only)\n",
    "    final_path = os.path.join(FINAL_DIR, \"elsevier_final_clean.csv\")\n",
    "    final_out = save_final_exact(df, title_col, journal_col, final_path)\n",
    "    print(f\"...Final Saved... \\n{final_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline_stage_only()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eplus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
