{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8de0aac",
   "metadata": {},
   "source": [
    "### Documentation here https://dev.elsevier.com/documentation/ArticleRetrievalAPI.wadl\n",
    "https://api.elsevier.com/content/article/doi/{doi}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609ef54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import xmltodict\n",
    "import json\n",
    "from _credentials import keys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30257ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 554\n",
      "Number of rows after removing duplicates: 549\n",
      "Cleaned data saved to elsevier_search_results_cleaned.csv\n",
      "Downloaded: 548/549\n",
      "All articles processed.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = \"csv_output\\papers_included_537.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Print the original number of rows for comparison\n",
    "print(f\"Original number of rows: {len(df)}\")\n",
    "\n",
    "# Remove duplicates based on the 'DOI' column\n",
    "df_cleaned = df.drop_duplicates(subset=['DOI'])\n",
    "\n",
    "# Print the number of rows after removing duplicates\n",
    "print(f\"Number of rows after removing duplicates: {len(df_cleaned)}\")\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "cleaned_file_path = 'elsevier_search_results_cleaned.csv'\n",
    "df_cleaned.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {cleaned_file_path}\")\n",
    "\n",
    "\n",
    "df = pd.read_csv('elsevier_search_results_cleaned.csv')\n",
    "\n",
    "# Creating a folder to save the downloaded articles\n",
    "download_dir = 'papers_json'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Getting the total number of articles to download\n",
    "total_articles = len(df)\n",
    "\n",
    "# Initializing the counter for downloaded articles\n",
    "downloaded_articles = 0\n",
    "\n",
    "# Iterating through the list of DOIs\n",
    "for index, row in df.iterrows():\n",
    "    doi = row['DOI']  # Assuming there's a 'doi' column in the CSV file\n",
    "    url = f\"https://api.elsevier.com/content/article/doi/{doi}\"\n",
    "    \n",
    "    # Sending the request\n",
    "    response = requests.get(url, headers={\n",
    "        \"X-ELS-APIKey\": keys[\"els-apikey\"],\n",
    "\n",
    "        \"Accept\": \"application/json\"\n",
    "    })\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Increment the counter if the download is successful\n",
    "        downloaded_articles += 1\n",
    "        article_data = response.json()\n",
    "        filename = doi.replace('/', '_') + '.json'\n",
    "        file_path = os.path.join(download_dir, filename)\n",
    "        \n",
    "        # Saving the article data to a file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "    # Use '\\r' to return to the start of the line and 'end=\"\"' to prevent new line. Flush to ensure it's displayed immediately.\n",
    "    print(f\"\\rDownloaded: {downloaded_articles}/{total_articles}\", end='', flush=True)\n",
    "\n",
    "# Adding a new line at the end of the process to ensure the command prompt appears correctly after the script finishes.\n",
    "print(\"\\nAll articles processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64f1b07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted Error Files:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "directory = 'papers_json'\n",
    "\n",
    "def process_json_files(directory):\n",
    "    error_files = []\n",
    "\n",
    "    # Iterate through each JSON file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    data = json.load(file)\n",
    "                \n",
    "                # Extract the original text from the JSON structure\n",
    "                text = data['full-text-retrieval-response']['originalText']\n",
    "                # Attempt to convert to lowercase to check for errors\n",
    "                _ = text.lower()\n",
    "            except AttributeError:\n",
    "                error_files.append(filename)\n",
    "                # Delete the file\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted {filename} due to an AttributeError.\")\n",
    "            except KeyError:\n",
    "                print(f\"KeyError: 'originalText' not found in {filename}\")\n",
    "    \n",
    "    return error_files\n",
    "\n",
    "\n",
    "\n",
    "# Run the processing function\n",
    "error_files = process_json_files(directory)\n",
    "\n",
    "# Print the problematic files\n",
    "print(\"Deleted Error Files:\")\n",
    "for file in error_files:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddaec18",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b18f4e",
   "metadata": {},
   "source": [
    "IF it is the first time using this code, please download dependencies:\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "950fa389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary has been saved to _cpwords.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s2589602\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\s2589602\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "def preprocess_keyword(keyword):\n",
    "    # Convert to lowercase\n",
    "    keyword = keyword.lower()\n",
    "    # Remove parentheses and their contents\n",
    "    keyword = re.sub(r'\\s*\\([^)]*\\)', '', keyword)\n",
    "    # Tokenize words\n",
    "    processed_words = nltk.word_tokenize(keyword)\n",
    "\n",
    "    return processed_words\n",
    "\n",
    "def extract_compound_keywords_from_json(folder_path):\n",
    "    compound_keywords_dict = {}\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                # Extract keywords from the JSON structure\n",
    "                keywords_data = data.get('full-text-retrieval-response', {}).get('coredata', {}).get('dcterms:subject', [])\n",
    "                keywords = [kw['$'] for kw in keywords_data if '$' in kw]\n",
    "                \n",
    "                for keyword in keywords:\n",
    "                    processed_words = preprocess_keyword(keyword)\n",
    "                    if len(processed_words) > 1:\n",
    "                        compound_keyword = '_'.join(processed_words)\n",
    "                        normal_keyword = ' '.join(processed_words)\n",
    "                        compound_keywords_dict[normal_keyword] = compound_keyword\n",
    "                    \n",
    "    return compound_keywords_dict\n",
    "\n",
    "def save_dict_as_py(dict_obj, output_file):\n",
    "    # Convert dictionary to string and add import statement\n",
    "    dict_content = f\"compound_keywords = {dict_obj}\\n\"\n",
    "    # Write to a .py file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(dict_content)\n",
    "        \n",
    "# Define the directory path that contains your JSON files\n",
    "folder_path = 'json_papers'\n",
    "compound_keywords = extract_compound_keywords_from_json(folder_path)\n",
    "\n",
    "# Save dictionary to a .py file\n",
    "output_file = '_cpwords.py'\n",
    "save_dict_as_py(compound_keywords, output_file)\n",
    "print(f\"Dictionary has been saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eplus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
