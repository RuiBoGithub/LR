{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8de0aac",
   "metadata": {},
   "source": [
    "### Documentation here https://dev.elsevier.com/documentation/ArticleRetrievalAPI.wadl\n",
    "https://api.elsevier.com/content/article/doi/{doi}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e6eb76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting notebook\n",
      "  Downloading notebook-6.5.7-py3-none-any.whl (529 kB)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (6.16.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.1 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from notebook) (4.12.0)\n",
      "Collecting nbformat\n",
      "  Downloading nbformat-5.8.0-py3-none-any.whl (77 kB)\n",
      "Collecting ipython-genutils\n",
      "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting argon2-cffi\n",
      "  Using cached argon2_cffi-23.1.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from notebook) (6.2)\n",
      "Collecting nbconvert>=5\n",
      "  Downloading nbconvert-7.6.0-py3-none-any.whl (290 kB)\n",
      "Collecting nbclassic>=0.4.7\n",
      "  Downloading nbclassic-1.2.0-py3-none-any.whl (10.0 MB)\n",
      "Collecting terminado>=0.8.3\n",
      "  Downloading terminado-0.17.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from notebook) (1.6.0)\n",
      "Collecting prometheus-client\n",
      "  Downloading prometheus_client-0.17.1-py3-none-any.whl (60 kB)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from notebook) (26.2.1)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from notebook) (5.9.0)\n",
      "Collecting Send2Trash>=1.8.0\n",
      "  Using cached Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Requirement already satisfied: jupyter-client<8,>=5.3.4 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from notebook) (7.4.9)\n",
      "Requirement already satisfied: packaging in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from ipykernel) (24.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from ipykernel) (7.0.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from ipykernel) (7.34.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from ipykernel) (0.1.6)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from ipykernel) (1.7.0)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" and platform_python_implementation != \"PyPy\" in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-core>=4.6.1->notebook) (308)\n",
      "Collecting fastjsonschema\n",
      "  Downloading fastjsonschema-2.21.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: importlib-metadata>=3.6; python_version < \"3.8\" in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from nbformat->notebook) (6.7.0)\n",
      "Collecting jsonschema>=2.6\n",
      "  Using cached jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from argon2-cffi->notebook) (4.7.1)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl (30 kB)\n",
      "Collecting mistune<4,>=2.0.3\n",
      "  Using cached mistune-3.0.2-py3-none-any.whl (47 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting markupsafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.5-cp37-cp37m-win_amd64.whl (17 kB)\n",
      "Collecting tinycss2\n",
      "  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting bleach!=5.0.0\n",
      "  Downloading bleach-6.0.0-py3-none-any.whl (162 kB)\n",
      "Collecting defusedxml\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.5-py3-none-any.whl (105 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from nbconvert>=5->notebook) (2.17.2)\n",
      "Collecting nbclient>=0.5.0\n",
      "  Downloading nbclient-0.7.4-py3-none-any.whl (73 kB)\n",
      "Collecting notebook-shim>=0.2.3\n",
      "  Using cached notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Collecting pywinpty>=1.1.0; os_name == \"nt\"\n",
      "  Downloading pywinpty-2.0.10-cp37-none-win_amd64.whl (1.4 MB)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-client<8,>=5.3.4->notebook) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-client<8,>=5.3.4->notebook) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.48)\n",
      "Requirement already satisfied: backcall in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\program files (x86)\\microsoft visual studio\\shared\\python37_64\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (47.1.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata>=3.6; python_version < \"3.8\"->nbformat->notebook) (3.15.0)\n",
      "Collecting pkgutil-resolve-name>=1.3.10; python_version < \"3.9\"\n",
      "  Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
      "Collecting attrs>=17.4.0\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
      "  Downloading pyrsistent-0.19.3-cp37-cp37m-win_amd64.whl (62 kB)\n",
      "Collecting importlib-resources>=1.4.0; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting cffi>=1.0.1\n",
      "  Downloading cffi-1.15.1-cp37-cp37m-win_amd64.whl (179 kB)\n",
      "Collecting webencodings>=0.4\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from bleach!=5.0.0->nbconvert>=5->notebook) (1.17.0)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
      "Collecting jupyter-server<3,>=1.8\n",
      "  Downloading jupyter_server-1.24.0-py3-none-any.whl (347 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.5)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting anyio<4,>=3.1.0\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Collecting websocket-client\n",
      "  Downloading websocket_client-1.6.1-py3-none-any.whl (56 kB)\n",
      "Collecting idna>=2.8\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Requirement already satisfied: exceptiongroup; python_version < \"3.11\" in c:\\users\\s2589602\\appdata\\roaming\\python\\python37\\site-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook) (1.3.0)\n",
      "Collecting sniffio>=1.1\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: fastjsonschema, pkgutil-resolve-name, attrs, pyrsistent, importlib-resources, jsonschema, nbformat, ipython-genutils, pycparser, cffi, argon2-cffi-bindings, argon2-cffi, mistune, pandocfilters, markupsafe, webencodings, tinycss2, bleach, defusedxml, soupsieve, beautifulsoup4, jupyterlab-pygments, jinja2, nbclient, nbconvert, idna, sniffio, anyio, pywinpty, terminado, prometheus-client, Send2Trash, websocket-client, jupyter-server, notebook-shim, nbclassic, notebook\n",
      "Successfully installed Send2Trash-1.8.3 anyio-3.7.1 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 attrs-24.2.0 beautifulsoup4-4.13.5 bleach-6.0.0 cffi-1.15.1 defusedxml-0.7.1 fastjsonschema-2.21.2 idna-3.10 importlib-resources-5.12.0 ipython-genutils-0.2.0 jinja2-3.1.6 jsonschema-4.17.3 jupyter-server-1.24.0 jupyterlab-pygments-0.2.2 markupsafe-2.1.5 mistune-3.0.2 nbclassic-1.2.0 nbclient-0.7.4 nbconvert-7.6.0 nbformat-5.8.0 notebook-6.5.7 notebook-shim-0.2.4 pandocfilters-1.5.1 pkgutil-resolve-name-1.3.10 prometheus-client-0.17.1 pycparser-2.21 pyrsistent-0.19.3 pywinpty-2.0.10 sniffio-1.3.1 soupsieve-2.4.1 terminado-0.17.1 tinycss2-1.2.1 webencodings-0.5.1 websocket-client-1.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install notebook ipykernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609ef54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import xmltodict\n",
    "import json\n",
    "from credentials import keys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30257ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 1137\n",
      "Number of rows after removing duplicates: 1137\n",
      "Cleaned data saved to elsevier_search_results_cleaned.csv\n",
      "Downloaded: 1137/1137\n",
      "All articles processed.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = \"csv_output/101_sorted_manual_check/by_reason/papers_KEEP_operational_EE_control.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Print the original number of rows for comparison\n",
    "print(f\"Original number of rows: {len(df)}\")\n",
    "\n",
    "# Remove duplicates based on the 'DOI' column\n",
    "df_cleaned = df.drop_duplicates(subset=['DOI'])\n",
    "\n",
    "# Print the number of rows after removing duplicates\n",
    "print(f\"Number of rows after removing duplicates: {len(df_cleaned)}\")\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "cleaned_file_path = 'elsevier_search_results_cleaned.csv'\n",
    "df_cleaned.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {cleaned_file_path}\")\n",
    "\n",
    "\n",
    "df = pd.read_csv('elsevier_search_results_cleaned.csv')\n",
    "\n",
    "# Creating a folder to save the downloaded articles\n",
    "download_dir = 'downloaded_articles'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Getting the total number of articles to download\n",
    "total_articles = len(df)\n",
    "\n",
    "# Initializing the counter for downloaded articles\n",
    "downloaded_articles = 0\n",
    "\n",
    "# Iterating through the list of DOIs\n",
    "for index, row in df.iterrows():\n",
    "    doi = row['DOI']  # Assuming there's a 'doi' column in the CSV file\n",
    "    url = f\"https://api.elsevier.com/content/article/doi/{doi}\"\n",
    "    \n",
    "    # Sending the request\n",
    "    response = requests.get(url, headers={\n",
    "        \"X-ELS-APIKey\": keys[\"els-apikey\"],\n",
    "\n",
    "        \"Accept\": \"application/json\"\n",
    "    })\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Increment the counter if the download is successful\n",
    "        downloaded_articles += 1\n",
    "        article_data = response.json()\n",
    "        filename = doi.replace('/', '_') + '.json'\n",
    "        file_path = os.path.join(download_dir, filename)\n",
    "        \n",
    "        # Saving the article data to a file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "    # Use '\\r' to return to the start of the line and 'end=\"\"' to prevent new line. Flush to ensure it's displayed immediately.\n",
    "    print(f\"\\rDownloaded: {downloaded_articles}/{total_articles}\", end='', flush=True)\n",
    "\n",
    "# Adding a new line at the end of the process to ensure the command prompt appears correctly after the script finishes.\n",
    "print(\"\\nAll articles processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1b07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted Error Files:\n"
     ]
    }
   ],
   "source": [
    "# Delete file without 'originalText'\n",
    "import os\n",
    "import json\n",
    "\n",
    "def process_json_files(directory):\n",
    "    error_files = []\n",
    "\n",
    "    # Iterate through each JSON file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    data = json.load(file)\n",
    "                \n",
    "                # Extract the original text from the JSON structure\n",
    "                text = data['full-text-retrieval-response']['originalText']\n",
    "                # Attempt to convert to lowercase to check for errors\n",
    "                _ = text.lower()\n",
    "            except AttributeError:\n",
    "                error_files.append(filename)\n",
    "                # Delete the file\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted {filename} due to an AttributeError.\")\n",
    "            except KeyError:\n",
    "                print(f\"KeyError: 'originalText' not found in {filename}\")\n",
    "    \n",
    "    return error_files\n",
    "\n",
    "# Define the directory path that contains your JSON files\n",
    "directory = 'papers_json'\n",
    "\n",
    "# Run the processing function\n",
    "error_files = process_json_files(directory)\n",
    "\n",
    "# Print the problematic files\n",
    "print(\"Deleted Error Files:\")\n",
    "for file in error_files:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddaec18",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b18f4e",
   "metadata": {},
   "source": [
    "IF it is the first time using this code, please download dependencies:\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950fa389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s2589602\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\s2589602\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary has been saved to compound_keywords.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "def preprocess_keyword(keyword):\n",
    "    # Convert to lowercase\n",
    "    keyword = keyword.lower()\n",
    "    # Remove parentheses and their contents\n",
    "    keyword = re.sub(r'\\s*\\([^)]*\\)', '', keyword)\n",
    "    # Tokenize words\n",
    "    processed_words = nltk.word_tokenize(keyword)\n",
    "\n",
    "    return processed_words\n",
    "\n",
    "def extract_compound_keywords_from_json(folder_path):\n",
    "    compound_keywords_dict = {}\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                # Extract keywords from the JSON structure\n",
    "                keywords_data = data.get('full-text-retrieval-response', {}).get('coredata', {}).get('dcterms:subject', [])\n",
    "                keywords = [kw['$'] for kw in keywords_data if '$' in kw]\n",
    "                \n",
    "                for keyword in keywords:\n",
    "                    processed_words = preprocess_keyword(keyword)\n",
    "                    if len(processed_words) > 1:\n",
    "                        compound_keyword = '_'.join(processed_words)\n",
    "                        normal_keyword = ' '.join(processed_words)\n",
    "                        compound_keywords_dict[normal_keyword] = compound_keyword\n",
    "                    \n",
    "    return compound_keywords_dict\n",
    "\n",
    "def save_dict_as_py(dict_obj, output_file):\n",
    "    # Convert dictionary to string and add import statement\n",
    "    dict_content = f\"compound_keywords = {dict_obj}\\n\"\n",
    "    # Write to a .py file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(dict_content)\n",
    "        \n",
    "# Define the directory path that contains your JSON files\n",
    "folder_path = 'downloaded_articles'\n",
    "compound_keywords = extract_compound_keywords_from_json(folder_path)\n",
    "\n",
    "# Save dictionary to a .py file\n",
    "output_file = '_cpwords.py'\n",
    "save_dict_as_py(compound_keywords, output_file)\n",
    "print(f\"Dictionary has been saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
