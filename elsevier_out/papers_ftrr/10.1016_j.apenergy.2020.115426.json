{
  "full-text-retrieval-response": {
    "originalText": "Abstract\nMost existing methods for controlling the energy consumption of air conditioning (AC), focus on either scheduling the switching (on/off) of compressors or optimizing the overall energy consumption of AC system of an entire building. Unlike commercial buildings, residential apartments typically house separate ACs in individual rooms occupied by people with different thermal comfort preferences. Fortunately, the advancement of Internet-of-Things (IoT) technology has enabled the exploitation of sensory data to intelligently control the set-point temperature of ACs in individual rooms based on environmental conditions and occupant’s preferences, improving the energy efficiency of residential buildings. Indeed, control decisions based on sensory data may suffer from uncertainties due to error in data measurement and contribute to model uncertainty. This work proposes a data-driven uncertainty-aware approach to control split-type inverter ACs of residential buildings. First, information from similar AC and residential units are aggregated to reduce data imbalances, and Bayesian-Convolutional-Neural-Networks (BCNNs) are utilized to model the performance and uncertainty of the ACs from the aggregated data. Second, a Q-learning based reinforcement learning algorithm for set-point decision making is designed for setpoint optimization with transitions sampled from the BCNN models. Third, a case study is simulated based on such a framework to show that the control actions taken by the uncertainty-aware agent perform better in terms of discomfort management and energy savings compared to the uncertainty unaware agent. Further, the agent could also be adjusted to capture the trade-off between energy savings and comfort levels for varying degrees of energy and discomfort savings.\n\nMethods\n2.2 The proposed framework The general idea of this paper is to obtain uncertainty-aware power and temperature models for each room, before planning the optimal SP for each room using reinforcement learning. The general modelling framework is shown in Fig. 1 . While looking at the AC consumption pattern of a single room, we find that the consumers typically keep to a narrow set of setpoints, as illustrated in Fig. 2 . For example, the residents in Room 5 has been using their AC at mostly 23 °C for the entirety of the observation period. If we build a black-box model for each room based on the limited and incomplete dataset for each room, we risk a high chance of generating erroneous predictions P as the model might not have an idea on the AC behavior for other setpoints outside of the data it has seen. The first step in our methodology is to aggregate the data from all residences and combined the AC data with weather data. This increases the chances of us obtaining a more complete model for room power and temperature prediction, since each consumer is using slightly different preference for their AC setpoint. In the second step, we train an overall Room Power and Room Temperature model based on the aggregated data, before retraining the models with specific room data so that the model can fit more closely to the room dynamics, while retaining information on other setpoints outside the limited dataset of a specific room. The choice of model used is the Bayesian Convolutional Neural Network (BCNN), to be further explained in Section 4 , which allows the neural network to express its uncertainty if the data is scarce or wildly fluctuating. This will result in having in N different Room Power models and N different Room Temperature models for N different rooms. Finally, with the Room Power and Room Temperature models ready, a Q-learning agent will be developed for each room. Together with historical weather data, the Room Power and Room Temperature model serve as a virtual environment, where the Q-learning agent can sample transitions from. After repeated exploration of this virtual environment, the Q-learning agent will be able to learn and choose the best action at any given state based on a specific reward function. The Q-learning algorithm for this purpose is described in detail in Section 5 . Room 5 and Room 8 were randomly chosen to serve as case studies.\n\nConclusion\n6 Conclusion and future work In this work, we have achieved our goal of creating an automated data driven AC controller. We have shown that by aggregating data from multiple rooms in a neural network training framework, we are able to reduce the problems of overfitting and data imbalances. The Bayesian Convolutional Neural Networks (BCNN) used for Room Power and Room Temperature Modelling are able to express uncertainty with regards to different states. Using these models as a simulation environment, the Q-learning agents trained with a uncertainty-aware reward function have shown potential in reducing energy consumption of ACs while preserving human comfort, with flexibility towards energy savings or discomfort reduction by changing the parameter a in the reward function. A potential limitation of our framework is the large data pool required for successful implementation. However, with the increasing popularity of IoT, as well as more cities in the world subscribing to smart cities initiative, it is easier to collect data. Cities in Europe [40] and Singapore [41] have the ambition to push for zero energy buildings through the installation of smart meters and other related infrastructure, and this could be a potential source of information for our framework. Another advantage of using a neural network based approach is that once new data is available from a real-world setup, we could conduct transfer learning to retrain the model to adapt them to real-world conditions. Our framework is also easily transferable between setup to setup, due to the basic parameters that we use for our AC Power and Temperature models, which are common across different types of ACs. As future work, we would like to extend our framework to another real-world setup for further testing, if the opportunity arises. Currently, this work is consumer-centric in the sense that it optimizes energy and comfort for the benefit of the consumer. However, the Q-learning agents can be incorporated with dynamic pricing for the ACs to participate in grid level Demand Response [42] . Also, with knowledge of the Room Power and Room Temperature models of each individual residence, the Grid Operator can plan for direct control for ACs or various demand response incentives to benefit the various stakeholders.\n"
  }
}