{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85122543142",
    "originalText": "serial JL 271429 291210 291702 291731 291787 291877 291878 291881 31 90 Applied Energy APPLIEDENERGY 2022-01-07 2022-01-07 2022-01-07 2022-01-07 2022-01-28T18:59:56 1-s2.0-S0306261921015932 S0306-2619(21)01593-2 S0306261921015932 10.1016/j.apenergy.2021.118346 S300 S300.1 FULL-TEXT 1-s2.0-S0306261921X0027X 2024-01-01T13:28:09.020216Z 0 0 20220301 2022 2022-01-07T18:20:03.017308Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref 0306-2619 03062619 UNLIMITED NONE true 309 309 C Volume 309 18 118346 118346 118346 20220301 1 March 2022 2022-03-01 2022 Research Papers article fla © 2022 The Authors. Published by Elsevier Ltd. REINFORCEDMODELPREDICTIVECONTROLRLMPCFORBUILDINGENERGYMANAGEMENT ARROYO J 1 Introduction 2 Related work 3 Disentangling the differences between MPC and RL 3.1 Approach 3.2 Terminology 3.3 Solution method 3.4 Optimality 3.5 Computational effort 3.6 Prediction horizon 3.7 Use of models 3.8 Partial observability 4 Reinforced model predictive control (RL-MPC) 4.1 Intuitive description 4.2 Formal description 5 Implementation 5.1 The control problem in BOPTEST 5.2 System identification 5.3 MPC implementation 5.4 RL implementation 5.5 RL-MPC implementation 6 Simulation results 7 Discussion 8 Conclusion and future work CRediT authorship contribution statement Acknowledgment References DECONINCK 2016 653 665 R ARROYO 2018 J FLEXIBILITYQUANTIFICATIONINCONTEXTFLEXIBLEHEATPOWERFORBUILDINGS MBUWIR 2019 1 11 B SCHWARM 1999 1743 1752 A BEMPORAD 1999 207 226 A ROBUSTNESSINIDENTIFICATIONCONTROL ROBUSTMODELPREDICTIVECONTROLASURVEY BACCI 2020 E PROBABILISTICGUARANTEESFORSAFEDEEPREINFORCEMENTLEARNING KOLLER 2019 T LEARNINGBASEDMODELPREDICTIVECONTROLFORSAFEEXPLORATIONREINFORCEMENTLEARNING ERNST 2009 517 529 D NEGENBORN 2005 354 359 R IFACPROCEEDINGSVOLUMES16THIFACWORLDCONGRESS LEARNINGBASEDMODELPREDICTIVECONTROLFORMARKOVDECISIONPROCESSES RECHT 2018 B ATOURREINFORCEMENTLEARNINGVIEWCONTINUOUSCONTROL GORGES 2017 4920 4928 D DULACARNOLD 2019 G CHALLENGESREALWORLDREINFORCEMENTLEARNING OJAND 2021 1 12 K BLUM 2021 586 610 D STURZENEGGER 2016 1 12 D MASON 2019 300 312 K DRGONA 2020 190 232 J JORISSEN 2018 180 192 F WETTER 2014 253 270 M JORISSEN 2018 669 688 F CONINCK 2016 288 303 R ATAM 2016 86 111 E ARROYO 2020 472 486 J MNIH 2015 529 533 V VAZQUEZCANTELI 2019 1072 1089 J VAZQUEZCANTELI 2019 243 257 J PEIRELINCK 2018 1 6 T PROCEEDINGSIEEEINTERNATIONALENERGYCONFERENCEENERGYCON USINGREINFORCEMENTLEARNINGFOROPTIMIZINGHEATPUMPCONTROLINABUILDINGMODELINMODELICA NAGY 2018 A DEEPREINFORCEMENTLEARNINGFOROPTIMALCONTROLSPACEHEATING PICARD 2017 739 751 D LIU 2006 148 161 S SCHARNHORST 2021 P ABADI 2015 M TENSORFLOWLARGESCALEMACHINELEARNINGHETEROGENEOUSSYSTEMSSOFTWAREAVAILABLETENSORFLOWORG HEWING 2020 269 296 L LIU 2006 142 147 S AMOS 2019 B DIFFERENTIABLEMPCFORENDTOENDPLANNINGCONTROL DRGONA 2021 J DEEPLEARNINGEXPLICITDIFFERENTIABLEPREDICTIVECONTROLLAWSFORBUILDINGS GROS 2020 636 648 S GROS 2020 S REINFORCEMENTLEARNINGFORMIXEDINTEGERPROBLEMSBASEDMPC GROS 2021 1947 1952 S 2021AMERICANCONTROLCONFERENCE REINFORCEMENTLEARNINGBASEDMPCSTOCHASTICPOLICYGRADIENTMETHOD ZANON 2021 3638 3652 M GROS 2020 S SAFEREINFORCEMENTLEARNINGSTABILITYSAFETYGUARANTEESUSINGROBUSTMPC KAMTHE 2017 S ZHANG 2020 17987 17999 H BUSONIUL 2010 B INTERACTIVECOLLABORATIVEINFORMATIONSYSTEMSSTUDIESINCOMPUTATIONALINTELLIGENCE APPROXIMATEDYNAMICPROGRAMMINGREINFORCEMENTLEARNINGTECHNICALREPORT DRGONA 2018 199 216 J KELLY 2017 M TRANSCRIPTIONMETHODSFORTRAJECTORYOPTIMIZATIONABEGINNERSTUTORIAL FLETCHER 1987 R PRACTICALMETHODSOPTIMIZATION CHUA 2018 K DEEPREINFORCEMENTLEARNINGINAHANDFULTRIALSUSINGPROBABILISTICDYNAMICSMODELS BHARDWAJ 2020 M INFORMATIONTHEORETICMODELPREDICTIVEQLEARNING MNIH 2013 V PLAYINGATARIDEEPREINFORCEMENTLEARNING SILVER 2017 D MASTERINGCHESSSHOGIBYSELFPLAYAGENERALREINFORCEMENTLEARNINGALGORITHM WEBER 2017 T IMAGINATIONAUGMENTEDAGENTSFORDEEPREINFORCEMENTLEARNING AKESSON 2010 1737 1749 J ANDERSSON 2019 1 36 J BROCKMAN 2016 G OPENAIGYM VANHASSELT 2015 H DEEPREINFORCEMENTLEARNINGDOUBLEQLEARNING TECHNICALCOMMITTEECENTCVENTILATIONFORBUILDINGS 2019 H EN16798ENERGYPERFORMANCEBUILDINGSVENTILATIONFORBUILDINGSPART2INTERPRETATIONREQUIREMENTSINEN167981GUIDELINEFORUSINGINDOORENVIRONMENTALINPUTPARAMETERSFORDESIGNASSESSMENTENERGYPERFORMANCEBUILDINGS BARBATO 2016 398 A ARROYOX2022X118346 ARROYOX2022X118346XJ Full 2022-01-07T10:04:27Z Author http://creativecommons.org/licenses/by/4.0/ 2024-01-07T00:00:00.000Z 2024-01-07T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ This is an open access article under the CC BY license. © 2022 The Authors. Published by Elsevier Ltd. 2022-06-07T18:54:55.411Z http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/aggregated-refined International Building Performance Simulation Association Modelica Framework for building and community energy system design and operation Iago Cupeiro Figueroa VITO 1710754 VITO Vlaamse Instelling voor Technologisch Onderzoek http://data.elsevier.com/vocabulary/SciValFunders/501100007610 http://sws.geonames.org/2802361/ This work emerged from the IBPSA Project 1, an international project conducted under the umbrella of the International Building Performance Simulation Association (IBPSA). Project 1 will develop and demonstrate a BIM/GIS and Modelica Framework for building and community energy system design and operation. The work of Javier Arroyo is financed by VITO, Belgium through a PhD Fellowship (grant number 1710754). Finally, the authors wish to thank to Brida V. Mbuwir, Ján Drgoňa, and Iago Cupeiro Figueroa for kindly reviewing the paper. This work emerged from the IBPSA Project 1, an international project conducted under the umbrella of the International Building Performance Simulation Association (IBPSA). Project 1 will develop and demonstrate a BIM/GIS and Modelica Framework for building and community energy system design and operation. The work of Javier Arroyo is financed by VITO, Belgium through a PhD Fellowship (grant number 1710754) . Finally, the authors wish to thank to Brida V. Mbuwir, Ján Drgoňa, and Iago Cupeiro Figueroa for kindly reviewing the paper. 0 item S0306-2619(21)01593-2 S0306261921015932 1-s2.0-S0306261921015932 10.1016/j.apenergy.2021.118346 271429 2024-01-01T13:28:09.020216Z 2022-03-01 UNLIMITED NONE 1-s2.0-S0306261921015932-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/MAIN/application/pdf/100be2d5de3648c43b2db43776577963/main.pdf main.pdf pdf true 1526788 MAIN 16 1-s2.0-S0306261921015932-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/PREVIEW/image/png/baa1d3c19da420e2cba4c54851cc910d/main_1.png main_1.png png 59731 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0306261921015932-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr1/DOWNSAMPLED/image/jpeg/9651f8289edecfa641ddec8ef05ad7c3/gr1.jpg gr1 gr1.jpg jpg 213597 572 1054 IMAGE-DOWNSAMPLED 1-s2.0-S0306261921015932-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr6/DOWNSAMPLED/image/jpeg/f184a58123aa91a12fde32e883525628/gr6.jpg gr6 gr6.jpg jpg 206468 512 620 IMAGE-DOWNSAMPLED 1-s2.0-S0306261921015932-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr7/DOWNSAMPLED/image/jpeg/6e23ac1f800ef3da3db2cb31627cd12d/gr7.jpg gr7 gr7.jpg jpg 95502 289 366 IMAGE-DOWNSAMPLED 1-s2.0-S0306261921015932-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr8/DOWNSAMPLED/image/jpeg/753a7231a43c7b4a74c4cc8b310bc59b/gr8.jpg gr8 gr8.jpg jpg 218152 606 785 IMAGE-DOWNSAMPLED 1-s2.0-S0306261921015932-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr9/DOWNSAMPLED/image/jpeg/a2cea29a5ddd0dcf9c81410556a873d9/gr9.jpg gr9 gr9.jpg jpg 119659 259 496 IMAGE-DOWNSAMPLED 1-s2.0-S0306261921015932-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr2/DOWNSAMPLED/image/jpeg/47af61c3d936735cde5ca72f457986e0/gr2.jpg gr2 gr2.jpg jpg 96009 277 783 IMAGE-DOWNSAMPLED 1-s2.0-S0306261921015932-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr3/DOWNSAMPLED/image/jpeg/5a5d504092881a22214e4f3c8609a148/gr3.jpg gr3 gr3.jpg jpg 92489 236 647 IMAGE-DOWNSAMPLED 1-s2.0-S0306261921015932-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr4/DOWNSAMPLED/image/jpeg/248fe2476b0c76a38d578a5956c7fb4e/gr4.jpg gr4 gr4.jpg jpg 92358 271 591 IMAGE-DOWNSAMPLED 1-s2.0-S0306261921015932-fx1001.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/fx1001/DOWNSAMPLED/image/jpeg/4a0703709693ba27f0ba88d4472db3c2/fx1001.jpg fx1001 fx1001.jpg jpg 93703 196 817 IMAGE-DOWNSAMPLED 1-s2.0-S0306261921015932-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr5/DOWNSAMPLED/image/jpeg/bc7e2e209cba8771d7ab1d268d94a324/gr5.jpg gr5 gr5.jpg jpg 121597 418 663 IMAGE-DOWNSAMPLED 1-s2.0-S0306261921015932-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr1/THUMBNAIL/image/gif/dd46e5c86ec07c33b89314a37bcdbf8a/gr1.sml gr1 gr1.sml sml 76469 119 219 IMAGE-THUMBNAIL 1-s2.0-S0306261921015932-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr6/THUMBNAIL/image/gif/03d4633a2f5e7994ae59a8f1eb002d36/gr6.sml gr6 gr6.sml sml 83998 164 198 IMAGE-THUMBNAIL 1-s2.0-S0306261921015932-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr7/THUMBNAIL/image/gif/a4e225db8c27c795acb701dd5ee8e560/gr7.sml gr7 gr7.sml sml 72444 163 207 IMAGE-THUMBNAIL 1-s2.0-S0306261921015932-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr8/THUMBNAIL/image/gif/3aadbf448384afe123a46197480811c3/gr8.sml gr8 gr8.sml sml 80774 164 212 IMAGE-THUMBNAIL 1-s2.0-S0306261921015932-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr9/THUMBNAIL/image/gif/9a8daa399aa5d064020527c39f178f6d/gr9.sml gr9 gr9.sml sml 75556 114 219 IMAGE-THUMBNAIL 1-s2.0-S0306261921015932-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr2/THUMBNAIL/image/gif/f97ac70b8438f1521884d683565c86ff/gr2.sml gr2 gr2.sml sml 69094 77 219 IMAGE-THUMBNAIL 1-s2.0-S0306261921015932-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr3/THUMBNAIL/image/gif/eb0f36833aae3c3a3529fe006199184e/gr3.sml gr3 gr3.sml sml 70850 80 219 IMAGE-THUMBNAIL 1-s2.0-S0306261921015932-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr4/THUMBNAIL/image/gif/5035bec711c8205f2b80be46ed14abbd/gr4.sml gr4 gr4.sml sml 70423 100 219 IMAGE-THUMBNAIL 1-s2.0-S0306261921015932-fx1001.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/fx1001/THUMBNAIL/image/gif/7400e8a603aa142429ee47cfc8b674ba/fx1001.sml fx1001 fx1001.sml sml 67824 53 219 IMAGE-THUMBNAIL 1-s2.0-S0306261921015932-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr5/THUMBNAIL/image/gif/de232d2702b3c2af01577845965df11c/gr5.sml gr5 gr5.sml sml 72525 138 219 IMAGE-THUMBNAIL 1-s2.0-S0306261921015932-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr1/HIGHRES/image/jpeg/752a3f221836e1317b9d5d20cd547ad4/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 1179629 2531 4667 IMAGE-HIGH-RES 1-s2.0-S0306261921015932-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr6/HIGHRES/image/jpeg/5038b2cb50a653e66892a7b1d05a4e4a/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 1068041 2269 2745 IMAGE-HIGH-RES 1-s2.0-S0306261921015932-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr7/HIGHRES/image/jpeg/475ee7137fd9125e46064ed3560a7fab/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 230726 1281 1622 IMAGE-HIGH-RES 1-s2.0-S0306261921015932-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr8/HIGHRES/image/jpeg/435a1dd268986fdaf1874025e3f323bb/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 1254829 2680 3474 IMAGE-HIGH-RES 1-s2.0-S0306261921015932-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr9/HIGHRES/image/jpeg/92b31d4303115e487031036b68c43885/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 375970 1147 2197 IMAGE-HIGH-RES 1-s2.0-S0306261921015932-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr2/HIGHRES/image/jpeg/2a56ca292731dd5904a0af53106275df/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 299700 1224 3465 IMAGE-HIGH-RES 1-s2.0-S0306261921015932-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr3/HIGHRES/image/jpeg/4dc8623a75cb3fe8ee57d219644f47b6/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 271790 1046 2863 IMAGE-HIGH-RES 1-s2.0-S0306261921015932-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr4/HIGHRES/image/jpeg/a7a50fbe8f2b12faf2bcb6ccaa76667c/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 255720 1201 2618 IMAGE-HIGH-RES 1-s2.0-S0306261921015932-fx1001_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/fx1001/HIGHRES/image/jpeg/8e9ecad15815140d7319e5609fc7e01f/fx1001_lrg.jpg fx1001 fx1001_lrg.jpg jpg 265242 868 3618 IMAGE-HIGH-RES 1-s2.0-S0306261921015932-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/gr5/HIGHRES/image/jpeg/4c61f01895570f67cb26a8a961c0131a/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 465363 1850 2934 IMAGE-HIGH-RES 1-s2.0-S0306261921015932-si1.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/901889f330c6e3b43d398948c577cdd5/si1.svg si1 si1.svg svg 1050 ALTIMG 1-s2.0-S0306261921015932-si10.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/265f0b9b9a85ac85f54cbafcbbfe3b80/si10.svg si10 si10.svg svg 3206 ALTIMG 1-s2.0-S0306261921015932-si102.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/0a612b52d2f7527eda4c3194e5334cc7/si102.svg si102 si102.svg svg 1323 ALTIMG 1-s2.0-S0306261921015932-si104.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/5e24e25d3b0823a1d38682ae41037cdf/si104.svg si104 si104.svg svg 3249 ALTIMG 1-s2.0-S0306261921015932-si105.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/92466b4ae68be7aeb15d26aa20208d07/si105.svg si105 si105.svg svg 2458 ALTIMG 1-s2.0-S0306261921015932-si111.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/0885f687fb848902a7e50ab97c775f9c/si111.svg si111 si111.svg svg 1389 ALTIMG 1-s2.0-S0306261921015932-si112.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/270240cb4f53e740d2769562b726f61e/si112.svg si112 si112.svg svg 2626 ALTIMG 1-s2.0-S0306261921015932-si113.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/357b338973434ac59f4d214f9f7561cc/si113.svg si113 si113.svg svg 7382 ALTIMG 1-s2.0-S0306261921015932-si116.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/f55ad7bdcd7adb63dcb08a81fba040a7/si116.svg si116 si116.svg svg 3959 ALTIMG 1-s2.0-S0306261921015932-si12.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/bef265ce3955ae24cb4fb498cbed7c64/si12.svg si12 si12.svg svg 1441 ALTIMG 1-s2.0-S0306261921015932-si121.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/4e196448a5736b5b55a112c53e14afc6/si121.svg si121 si121.svg svg 2743 ALTIMG 1-s2.0-S0306261921015932-si122.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/57af12a567e6b4c160130a02fc9b9dbe/si122.svg si122 si122.svg svg 3792 ALTIMG 1-s2.0-S0306261921015932-si123.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/f9b95f4198f92d16d6a5c5b332917d4e/si123.svg si123 si123.svg svg 2381 ALTIMG 1-s2.0-S0306261921015932-si124.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/212d7bc8284623bb417e13cc08ae7591/si124.svg si124 si124.svg svg 5129 ALTIMG 1-s2.0-S0306261921015932-si125.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/1873e6f8a3bfe27de24a71c591f0f8dc/si125.svg si125 si125.svg svg 3498 ALTIMG 1-s2.0-S0306261921015932-si126.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/227b3e8e5bc9a837922cfd37e7e24ea5/si126.svg si126 si126.svg svg 1423 ALTIMG 1-s2.0-S0306261921015932-si127.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/dd21a015115ec74a8b27b634949f0932/si127.svg si127 si127.svg svg 34873 ALTIMG 1-s2.0-S0306261921015932-si128.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/2de104ba8549acdfcb44887b2fb31f77/si128.svg si128 si128.svg svg 25676 ALTIMG 1-s2.0-S0306261921015932-si129.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/e0e174a4a36c4d66b37761558f15bc55/si129.svg si129 si129.svg svg 7602 ALTIMG 1-s2.0-S0306261921015932-si13.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/61159ec029583398cca0bd03ba5bdfee/si13.svg si13 si13.svg svg 2694 ALTIMG 1-s2.0-S0306261921015932-si130.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/0be11ad1e1d4a8b70e53c8367d2344cf/si130.svg si130 si130.svg svg 4507 ALTIMG 1-s2.0-S0306261921015932-si131.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/57d14ff7c81f990f34432787188ab04e/si131.svg si131 si131.svg svg 6371 ALTIMG 1-s2.0-S0306261921015932-si133.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/8c6db6edecdcfab86db51965b6f7f56e/si133.svg si133 si133.svg svg 1663 ALTIMG 1-s2.0-S0306261921015932-si134.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/f9ed942b19fb961dd100a94ed41a600f/si134.svg si134 si134.svg svg 3217 ALTIMG 1-s2.0-S0306261921015932-si135.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/77cae55b5b47515b7d47488fc33826cf/si135.svg si135 si135.svg svg 2375 ALTIMG 1-s2.0-S0306261921015932-si137.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/7222770bc6c88895e5a62fa2d44b69c7/si137.svg si137 si137.svg svg 5136 ALTIMG 1-s2.0-S0306261921015932-si138.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/d09a9cb8f64f645a41d0348529738296/si138.svg si138 si138.svg svg 5442 ALTIMG 1-s2.0-S0306261921015932-si139.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/2bb18d130a0938d57002205beef7bd10/si139.svg si139 si139.svg svg 1129 ALTIMG 1-s2.0-S0306261921015932-si14.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/c5f8b81b787efc94e55fdccca520444d/si14.svg si14 si14.svg svg 1231 ALTIMG 1-s2.0-S0306261921015932-si142.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/68e1c172eadab33552c62546fa5b0201/si142.svg si142 si142.svg svg 2396 ALTIMG 1-s2.0-S0306261921015932-si143.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/e6d9d4fa611a12a5f914415cd4dbc2e7/si143.svg si143 si143.svg svg 2395 ALTIMG 1-s2.0-S0306261921015932-si146.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/eb802fc819f2b1a45e287cebc58768a2/si146.svg si146 si146.svg svg 4576 ALTIMG 1-s2.0-S0306261921015932-si147.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/99bee133620d938bc9d6a54c3570e243/si147.svg si147 si147.svg svg 4290 ALTIMG 1-s2.0-S0306261921015932-si157.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/7a719ef8f70636d1e71c4ec8589f4fe6/si157.svg si157 si157.svg svg 3750 ALTIMG 1-s2.0-S0306261921015932-si158.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/ae482e42d9ea48562ec80eb8827747f7/si158.svg si158 si158.svg svg 4889 ALTIMG 1-s2.0-S0306261921015932-si159.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/334ec1194ee8b6bfcb82d90114fd1281/si159.svg si159 si159.svg svg 2797 ALTIMG 1-s2.0-S0306261921015932-si16.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/3c8449771af514dca823331bbbcf6030/si16.svg si16 si16.svg svg 1678 ALTIMG 1-s2.0-S0306261921015932-si160.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/674ae449fec3212d3249e7b1140f8f08/si160.svg si160 si160.svg svg 7094 ALTIMG 1-s2.0-S0306261921015932-si161.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/151c950da3aca3e5873822004d383b06/si161.svg si161 si161.svg svg 11176 ALTIMG 1-s2.0-S0306261921015932-si162.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/513133887f0650c2321a164c4abae4fe/si162.svg si162 si162.svg svg 2756 ALTIMG 1-s2.0-S0306261921015932-si163.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/7ea523337109b901afdcf8e9bfc9281f/si163.svg si163 si163.svg svg 7755 ALTIMG 1-s2.0-S0306261921015932-si164.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/954b462e2d5cf8c40ebbd43e6306c631/si164.svg si164 si164.svg svg 1244 ALTIMG 1-s2.0-S0306261921015932-si169.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/4f2682798b9664c87a4eb0934e3eebad/si169.svg si169 si169.svg svg 1425 ALTIMG 1-s2.0-S0306261921015932-si17.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/507016c6e874def047ed1179fec55b17/si17.svg si17 si17.svg svg 1360 ALTIMG 1-s2.0-S0306261921015932-si171.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/d7a037379f50c582d0ce65f1c7ad5c37/si171.svg si171 si171.svg svg 2184 ALTIMG 1-s2.0-S0306261921015932-si176.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/dc7624df6de97da5177e5872cfe447d2/si176.svg si176 si176.svg svg 5756 ALTIMG 1-s2.0-S0306261921015932-si18.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/c0cce5966c5f46a5c6a7d2e03243564b/si18.svg si18 si18.svg svg 1256 ALTIMG 1-s2.0-S0306261921015932-si180.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/f926e6a13cdaef27558ea368eba8d195/si180.svg si180 si180.svg svg 2599 ALTIMG 1-s2.0-S0306261921015932-si181.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/cd79aa3c77c70273d70b2fcb014be9eb/si181.svg si181 si181.svg svg 2700 ALTIMG 1-s2.0-S0306261921015932-si185.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/a80aa5882ae1ec6491427417af871e5d/si185.svg si185 si185.svg svg 2181 ALTIMG 1-s2.0-S0306261921015932-si186.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/3da74782f50fbc168a32a9e85da4efa0/si186.svg si186 si186.svg svg 2305 ALTIMG 1-s2.0-S0306261921015932-si19.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/e1645d8af9020cd8783636fb96ae705f/si19.svg si19 si19.svg svg 1603 ALTIMG 1-s2.0-S0306261921015932-si2.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/3caf4ed8bbd7c721f08240f05b33e027/si2.svg si2 si2.svg svg 1625 ALTIMG 1-s2.0-S0306261921015932-si20.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/64d70a4c02cdcb40c8b5ecf841137d28/si20.svg si20 si20.svg svg 1471 ALTIMG 1-s2.0-S0306261921015932-si21.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/0242298764901c2f1452a33167ca0725/si21.svg si21 si21.svg svg 7396 ALTIMG 1-s2.0-S0306261921015932-si22.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/cd9d0ab467df8be7bba879573057514e/si22.svg si22 si22.svg svg 1083 ALTIMG 1-s2.0-S0306261921015932-si23.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/cfe7a671e8d7812f80c473d1e13027b3/si23.svg si23 si23.svg svg 22942 ALTIMG 1-s2.0-S0306261921015932-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/7bc89d911910d5d311bdbd9ae014c37d/si3.svg si3 si3.svg svg 1690 ALTIMG 1-s2.0-S0306261921015932-si30.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/4b4e59b51d9b628169730c327aff0874/si30.svg si30 si30.svg svg 1276 ALTIMG 1-s2.0-S0306261921015932-si31.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/569e437fe1151ce67c6b53b5390e4855/si31.svg si31 si31.svg svg 1173 ALTIMG 1-s2.0-S0306261921015932-si32.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/d552a62e4fb6d3e308da0615e05335ac/si32.svg si32 si32.svg svg 4304 ALTIMG 1-s2.0-S0306261921015932-si33.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/40278663f5c4daa55523edf3d0cb00f2/si33.svg si33 si33.svg svg 1541 ALTIMG 1-s2.0-S0306261921015932-si37.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/1d57b9f3c342c66fc232912443ee84aa/si37.svg si37 si37.svg svg 4321 ALTIMG 1-s2.0-S0306261921015932-si38.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/5ad67e5bd01f12c35a20168cf9631ad6/si38.svg si38 si38.svg svg 4582 ALTIMG 1-s2.0-S0306261921015932-si39.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/583c5678c7c87108a414cedee5b21a6f/si39.svg si39 si39.svg svg 3886 ALTIMG 1-s2.0-S0306261921015932-si4.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/66159d986ac2f6db18d61d59690c0bc2/si4.svg si4 si4.svg svg 1136 ALTIMG 1-s2.0-S0306261921015932-si40.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/fbe83abfda77f5475b5ac42d7df45b69/si40.svg si40 si40.svg svg 6801 ALTIMG 1-s2.0-S0306261921015932-si41.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/86118fe6dbd4b7f3c2cafa6671e1e232/si41.svg si41 si41.svg svg 4929 ALTIMG 1-s2.0-S0306261921015932-si42.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/76f33b7b5bbd8ec43d7c1df20ec12589/si42.svg si42 si42.svg svg 1453 ALTIMG 1-s2.0-S0306261921015932-si43.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/d43a12792274ed3880b047e8f22276a4/si43.svg si43 si43.svg svg 4561 ALTIMG 1-s2.0-S0306261921015932-si44.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/6e907a55231580175c96583aeb03f0f3/si44.svg si44 si44.svg svg 15257 ALTIMG 1-s2.0-S0306261921015932-si45.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/0004a9e2f061273221f853ec4e950772/si45.svg si45 si45.svg svg 4956 ALTIMG 1-s2.0-S0306261921015932-si47.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/d25c4f7fa8980b5cad6ac9056aeed86d/si47.svg si47 si47.svg svg 1173 ALTIMG 1-s2.0-S0306261921015932-si5.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/83f19e053b4e362b2f7961c4a4cd50bd/si5.svg si5 si5.svg svg 1530 ALTIMG 1-s2.0-S0306261921015932-si50.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/e817f5648426c4077f4a5bab15200124/si50.svg si50 si50.svg svg 3294 ALTIMG 1-s2.0-S0306261921015932-si51.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/1d182cbfe69441d8a0660abe9d4c5540/si51.svg si51 si51.svg svg 6427 ALTIMG 1-s2.0-S0306261921015932-si53.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/527ab26b9c0842e7120f7ca0b07aadcd/si53.svg si53 si53.svg svg 1507 ALTIMG 1-s2.0-S0306261921015932-si58.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/ae91eb424ec158a2fa7c7c87abdc0b38/si58.svg si58 si58.svg svg 5572 ALTIMG 1-s2.0-S0306261921015932-si59.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/4c36775bd221c85b7a332defa5d7812b/si59.svg si59 si59.svg svg 3270 ALTIMG 1-s2.0-S0306261921015932-si6.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/421baaa1ae66e0b6861e6c39ef71e76c/si6.svg si6 si6.svg svg 1132 ALTIMG 1-s2.0-S0306261921015932-si60.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/117bbf78127454e25a33a58a60da991a/si60.svg si60 si60.svg svg 6293 ALTIMG 1-s2.0-S0306261921015932-si62.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/018c6494adb7ea12248af9c022750745/si62.svg si62 si62.svg svg 2445 ALTIMG 1-s2.0-S0306261921015932-si63.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/aa4da4536a0d49140b57533c119a3594/si63.svg si63 si63.svg svg 1978 ALTIMG 1-s2.0-S0306261921015932-si64.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/703d3c13623640c52c783c48f16e29fc/si64.svg si64 si64.svg svg 1615 ALTIMG 1-s2.0-S0306261921015932-si66.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/a74fcfc16d36de4cc64012830be0c9f3/si66.svg si66 si66.svg svg 1405 ALTIMG 1-s2.0-S0306261921015932-si67.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/a6a73b3942f91967a785201cffded69e/si67.svg si67 si67.svg svg 5284 ALTIMG 1-s2.0-S0306261921015932-si69.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/8180bf14ea2f2581c0a52dfd43516a97/si69.svg si69 si69.svg svg 1378 ALTIMG 1-s2.0-S0306261921015932-si7.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/7cc03c4e1145605b50572fe83c920d05/si7.svg si7 si7.svg svg 1425 ALTIMG 1-s2.0-S0306261921015932-si72.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/fb12d8b173e75da8aaf7f0840a4273c4/si72.svg si72 si72.svg svg 1416 ALTIMG 1-s2.0-S0306261921015932-si73.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/d32eec5de5def939a4c5356f29ca350b/si73.svg si73 si73.svg svg 1098 ALTIMG 1-s2.0-S0306261921015932-si74.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/0714333ab63957f76713bf50fa7fe89c/si74.svg si74 si74.svg svg 1992 ALTIMG 1-s2.0-S0306261921015932-si76.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/47e2a42c093a3a2882e66274ec7fc9ed/si76.svg si76 si76.svg svg 6476 ALTIMG 1-s2.0-S0306261921015932-si8.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/9ed669bab5113e7152cc54d8890526f7/si8.svg si8 si8.svg svg 1347 ALTIMG 1-s2.0-S0306261921015932-si80.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/a53b326446d66cf94506bfee01e74e09/si80.svg si80 si80.svg svg 3175 ALTIMG 1-s2.0-S0306261921015932-si81.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/ebfe2429f437ca3d32063f7e624e287d/si81.svg si81 si81.svg svg 20889 ALTIMG 1-s2.0-S0306261921015932-si82.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/26f5254b656e8b24028b458179b68697/si82.svg si82 si82.svg svg 3216 ALTIMG 1-s2.0-S0306261921015932-si83.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/8b553a4c009293b5ccf4ce74fc00b72a/si83.svg si83 si83.svg svg 1458 ALTIMG 1-s2.0-S0306261921015932-si87.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/16828df25f53221ffd9aa0da99b3e6ef/si87.svg si87 si87.svg svg 19600 ALTIMG 1-s2.0-S0306261921015932-si88.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/f913d92ed1292c751502a686b2fdc49b/si88.svg si88 si88.svg svg 4691 ALTIMG 1-s2.0-S0306261921015932-si9.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/f46653838f8781327449786a04662d0a/si9.svg si9 si9.svg svg 2948 ALTIMG 1-s2.0-S0306261921015932-si91.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/b9ad3db93a6d3eadfdc803a7f1831b54/si91.svg si91 si91.svg svg 18396 ALTIMG 1-s2.0-S0306261921015932-si92.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/eb77b42ffcaff42532759b7381c84df9/si92.svg si92 si92.svg svg 14374 ALTIMG 1-s2.0-S0306261921015932-si95.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/969d501f78c3e6401521bb5aa696034d/si95.svg si95 si95.svg svg 7040 ALTIMG 1-s2.0-S0306261921015932-si97.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/827a1135eb4ac2183f53a45068ce39b4/si97.svg si97 si97.svg svg 17059 ALTIMG 1-s2.0-S0306261921015932-si98.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/fcce5fd98ce3b4395bf88ed22df7fb78/si98.svg si98 si98.svg svg 36001 ALTIMG 1-s2.0-S0306261921015932-si99.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261921015932/STRIPIN/image/svg+xml/3ab83cd667138c2dd9aacc06e0e61268/si99.svg si99 si99.svg svg 29684 ALTIMG 1-s2.0-S0306261921015932-am.pdf am am.pdf pdf 1157835 AAM-PDF https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:10Q65PDCMR6/MAIN/application/pdf/94c977eb9d6441bfd87ab2a6da81be92/am.pdf APEN 118346 118346 S0306-2619(21)01593-2 10.1016/j.apenergy.2021.118346 Fig. 1 The (non exhaustive) taxonomy of optimal control. An algorithm shall be characterized by tracing a path from the highest abstraction level to the lowest. Ramifications may arise for the same algorithm. Fig. 2 Block diagrams of MPC (left) and MDP (right). Note: ( ⋅ ) k = Δ ( ⋅ ) ( t k ) . Fig. 3 Schematic overview of solution methods for optimal control. In the MPC approach, the objective function is inferred every control step based on the controller model. In the value-based approach, a value function is constructed and cached to derive the most performant action from a specific state. Finally, the policy-based approach parametrizes a policy to map states to actions directly. Fig. 4 Block diagram showing a high-level introduction of RL-MPC. Every control step, RL-MPC decides an action a k by optimizing the sum of the one step ahead non-linear program of the MPC and the value function from the estimated following state. Note: ( ⋅ ) k = Δ ( ⋅ ) ( t k ) . Fig. 5 Implementation methodology. Fig. 6 Training and validation periods for estimating the controller model F . The training period is at the left of the vertical gray dashed line and the validation period at the right. The inputs are presented in the bottom two plots, and the simulated outputs are presented in the top two plots and compared with the measured data. The gray lines in the first plot represent the comfort constraints. Fig. 7 Average episodic return for the DDQN agents trained with the simulation E F and the actual E f environments. Both environments expose the same observations and control actions to the agent, but the simulation environment is configured with the RC model, while the actual environment is configured with the BOPTEST emulator. Fig. 8 Simulation results for all controllers in each of the BOPTEST testing periods for the envisaged building. The plots at the top show the pricing signal, the comfort constraints, and the evolution of the zone operative temperature T z for each control strategy. The plots at the bottom show the main system disturbances, namely the ambient temperature and the direct solar irradiation. Fig. 9 KPIs for each controller and the two testing periods as obtained from the BOPTEST framework. Each of the testing periods lasts for two weeks. The horizontal lines serve as a reference of reasonable thermal discomfort levels when following the recommended criteria for acceptable deviations of standard EN16798-2 [75]. Reinforced model predictive control (RL-MPC) for building energy management Javier Arroyo Conceptualization Methodology Software Formal analysis Writing \u2013 original draft Writing \u2013 review & editing Visualization Funding acquisition a b c \u204e Carlo Manna Methodology Writing \u2013 original draft Writing \u2013 review & editing b c Fred Spiessens Conceptualization Methodology Writing \u2013 original draft Writing \u2013 review & editing Supervision Funding acquisition b c Lieve Helsen Conceptualization Methodology Writing \u2013 original draft Writing \u2013 review & editing Supervision Funding acquisition a b a Department of Mechanical Engineering, KU Leuven, Heverlee, Belgium Department of Mechanical Engineering, KU Leuven Heverlee Belgium Department of Mechanical Engineering, KU Leuven, Heverlee, Belgium b EnergyVille, Thor Park, Waterschei, Belgium EnergyVille, Thor Park, Waterschei Belgium EnergyVille, Thor Park, Waterschei, Belgium c Flemish Institute for Technological Research (VITO), Mol, Belgium Flemish Institute for Technological Research (VITO), Mol Belgium Flemish Institute for Technological Research (VITO), Mol, Belgium \u204e Corresponding author at: Department of Mechanical Engineering, KU Leuven, Heverlee, Belgium. Department of Mechanical Engineering, KU Leuven Heverlee Belgium Buildings need advanced control for the efficient and climate-neutral use of their energy systems. Model predictive control (MPC) and reinforcement learning (RL) arise as two powerful control techniques that have been extensively investigated in the literature for their application to building energy management. These methods show complementary qualities in terms of constraint satisfaction, computational demand, adaptability, and intelligibility, but usually a choice is made between both approaches. This paper compares both control approaches and proposes a novel algorithm called reinforced predictive control (RL-MPC) that merges their relative merits. First, the complementarity between RL and MPC is emphasized on a conceptual level by commenting on the main aspects of each method. Second, the RL-MPC algorithm is described that effectively combines features from each approach, namely state estimation, dynamic optimization, and learning. Finally, MPC, RL, and RL-MPC are implemented and evaluated in BOPTEST, a standardized simulation framework for the assessment of advanced control algorithms in buildings. The results indicate that pure RL cannot provide constraint satisfaction when using a control formulation equivalent to MPC and the same controller model for learning. The new RL-MPC algorithm can meet constraints and provide similar performance to MPC while enabling continuous learning and the possibility to deal with uncertain environments. Keywords Model predictive control Reinforcement learning Reinforced model predictive control Building automation BOPTEST 1 Introduction To achieve a net-zero carbon building stock by 2050, the International Energy Agency estimates that direct building CO 2 emissions need to decrease by 50% and indirect building sector emissions need to decline 60% by 2030 [1]. Optimal control in buildings has gained popularity in recent years because of the substantial energy savings potential in the building sector. Building energy automation systems that use optimal predictive controllers can improve indoor comfort while lowering energy use locally and enabling demand response strategies to offer flexibility from the buildings to the electric grid [2\u20134]. Model predictive control (MPC) and reinforcement learning (RL) are two approaches for optimal control that pursue the same goal but follow different strategies. Both approaches show promising potential for their implementation in buildings but come along with relevant challenges. The MPC approach is mainly studied within the field of control theory, and it is well known for its robustness and sample efficiency, but falls short in terms of adaptability. It usually requires a significant engineering effort to implement and configure a controller model suitable for optimization. Particularly, large and complex systems, like buildings, can easily lead to models comprising several variables and equations that give rise to non-convex optimization problems that are challenging to solve. Even if a solution to the optimization problem can be found, the controller model suffers from unavoidable model mismatch, and the system is usually exposed to forecast uncertainty. The latter can be addressed using stochastic MPC like robust or chance-constrained MPC, but these approaches require the description of the process uncertainties, which is usually hard to provide [5,6]. RL is mainly studied within the machine learning community. Contrarily to MPC, this advanced control approach is well known for its adaptability, but it has difficulties dealing with constraints, it needs extensive training, and it lacks intelligibility. The curse of dimensionality has always threatened the usability of RL. Nevertheless, the recent developments in deep learning have opened up the possibilities of these algorithms to applications as complex as building automation systems. Still, constraint satisfaction remains a relevant challenge in RL and is a research topic intensively investigated [7,8]. In the building sector, RL has been mainly used at a supervisory level with a few set-points as controllable actions and with low dimensional observation spaces. Usually, authors use either one approach or another based on their expertise and justify their choice by highlighting the strengths of their approach and stressing the weaknesses of the other. Both control families have advantages and disadvantages, and the complementarity between them is clear. On the one hand, MPC struggles with uncertainties, system complexity and long term prediction horizons while deep RL can naturally deal with the uncertainty of complex systems and tackle infinite prediction horizons. On the other hand, RL can difficultly satisfy constraints and lacks interpretability, while MPC can provide safety guarantees and intelligibility. Although there is a clear potential for synergy between both families of methods, minimal efforts have been made to merge them and combine their relative merits. This research gap is not only seen in the application of building energy management. The control and machine learning communities keep evolving independently with a radically different notation natively adopted to formulate the same problem. Despite the parallel developments, some authors have pointed out that potential benefits may arise from their collaboration [9\u201312]. Combining these methodologies is a powerful way to integrate robust methods from the control theory with machine learning approaches that can exploit further information from real-time data [12,13]. The motivation for the work presented in this paper revolves around the question how RL and MPC can work together for the application of building energy management. While there appears to be universal agreement that benefits may arise from their combination, little has been done to develop methods that involve both algorithms working together. Moreover, the works that have investigated how these controllers can collaborate either have the algorithms working at different control levels e.g. in [14], or tested their collaborative methods with a low-dimensional state\u2013action space [15]. There is no prior work that compares and merges MPC and RL for the same optimal control problem formulation for building energy management. The aim of this paper is twofold: First, it is of interest to compare RL and MPC for their application to building energy management. This comparison begins on a conceptual level to underline the main similitudes and differences between the optimal control approaches. Then, the algorithms are tested when implemented separately. In the comparison, the MPC uses a gray-box model and the RL agent uses a value-based algorithm implemented by an equivalent control formulation and for the same building test case taken from the Building Optimization Testing (BOPTEST) framework [16]. The BOPTEST framework is a new environment meant to evaluate the performance of advanced controllers for building energy management systems in simulation. BOPTEST provides a menu of detailed building emulators and standardizes the way controllers are evaluated. Moreover, a BOPTEST-Gym wrapper [17] has been developed that facilitates the implementation of RL algorithms. The second aim of this paper is to propose a novel method named Reinforced Model Predictive Control (RL-MPC) that synergistically combines RL and MPC. The conceptual and practical comparison of MPC and RL motivates the development and formulation of this new RL-MPC algorithm. Differently to similar approaches proposed in the literature like [15], our method combines the MPC objective function with the RL agent value function while using a nonlinear controller model encoded from domain knowledge. This practice ensures the interoperability between both methods and enables truncation of the MPC optimization problem, which can become very complex even for fairly simple buildings. Finally, this new algorithm is explained and tested in the same BOPTEST building case. Therefore, the main novelty of this paper is the introduction of RL-MPC, a control algorithm that combines methods from the control theory and the machine learning communities. This algorithm is tested in BOPTEST, a new standardized framework for advanced control of building energy management systems. Furthermore, the proposed RL-MPC algorithm can be used in different applications and domains e.g. complex industrial processes or energy markets. This study also brings control theory and machine learning closer by comparing and disentangling the principal differences between MPC and RL. The main limitations of this paper relate to the lack of theoretical guarantees of RL-MPC, which has only been tested empirically. Additionally, the tests are performed in a deterministic setting since the BOPTEST framework does not yet provide the functionality to emulate uncertainties. Although RL-MPC is expected to excel in stochastic settings, this study shows that it can achieve similar performance levels to MPC in the deterministic case. This paper uses the following notation: scalar-valued variables, scalar-valued functions, and scalar-valued sets are denoted by regular italic symbols as in x . Vector-valued variables, vector-valued functions, and vector-valued sets are denoted by bold italic symbols as in x . Deterministic variables in the abstract domain are denoted by lower-case letters as in r , whereas stochastic variables in the abstract domain are denoted by capital letters as in R . The latter does not apply to the physical domain where a variable like the temperature or electric power may be denoted by capital letters e.g. by T , or P . The outline of the paper is as follows: Section 2 summarizes related work; Section 3 performs a conceptual comparison between MPC and RL; Section 4 describes the RL-MPC algorithm. Section 5 describes the implementation details of MPC, RL, and RL-MPC for their assessment in simulation. Section 6 presents the simulation results obtained from the implementation of each control algorithm in the same simulation test case building. Finally, Section 8 draws the main conclusions. 2 Related work Extensive scientific literature exists that explores the implementation of either MPC or RL in buildings, e.g. [18] or [19]. The increased attention to MPC for this application during the last years is remarkable. An elaborate review on the application of MPC in buildings is offered by [20]. This advanced control approach has been proven not only at the building supervisory level as a reference governor for temperature set-point control [21], but also at the local loop level, where MPC directly decides on the actuator signals that drive the heating, ventilation, and air conditioning (HVAC) systems in buildings [22]. It is natural that MPC has dominated the control of individual buildings because buildings are usually large continuous systems with well-known dynamics. Hence, analytical models can be derived from domain knowledge to optimize the system in a receding horizon approach. Multiple libraries and tools exist that facilitate the modeling task, e.g. [23\u201327]. These models can also help practitioners in making decisions for building energy commissioning. However, the complexity of the optimization problems subject to these models (representing system constraints) remains a major bottleneck for the widespread adoption of MPC in practice. That is why the configuration of control-oriented models is a central research topic in the field [28,29]. On the other hand, the rapid emergence and sophistication of general function approximation techniques like neural networks or random forests have leveraged the development of powerful RL algorithms that can be implemented for various applications [9,30]. The latter has gained the attention of the building sector, and recent studies arise as well where this optimal control technique is implemented in buildings, usually to harness the flexibility of buildings in a demand response setting [4]. A review of previous implementations of RL for demand response can be found in [31]. Despite the substantial progress in RL, this control approach is still generally implemented at a supervisory level, which allows only indirect control of the building HVAC systems. Examples of this indirect control can be found in [32] or in [33]. Even when direct control is implemented, the RL agent usually controls low-dimensional action spaces like on/off operation [34,35]. Many simulation case studies have claimed that model-free RL agents can learn policies for building climate control from only a few days of operation, but the agents are usually tested using reduced-order models [4,36\u201338]. This practice directly conflicts the findings of Picard et al. [39], who emphasized the relevance of using a detailed and reliable plant model when evaluating the building controllers in order not to overestimate their performance. Real implementations of RL for building HVAC control like [15,40,41] never use model-free RL techniques directly and rely on simulation-based off-line learning or model-based RL. To the best of the authors\u2019 knowledge, only Peirelinck et al. [34] has evaluated a model-free RL agent in a simulation environment using a detailed emulator building model. However, they used indirect control and a low-dimensional action space. By using indirect control the agent does not need to deal with constraint satisfaction. Contrarily, it only suggests actions that can be overwritten by the backup controller of the emulator, which is ultimately responsible for satisfying constraints. Note that satisfying constraints is of utmost importance for every controlled system because it avoids unwanted behavior, performance degradation, and the violation safe-critical conditions. This paper uses a detailed and high-fidelity building model from the BOPTEST framework to evaluate the controllers. Similar setups to BOPTEST are the EnerGym [42] environment or the CityLearn challenge [43], although these use reduced-order building models and focus on algorithms coordinating demand response strategies in districts rather than control strategies for the energy management of individual buildings. The CityLearn setup was extended in [32] with a Q-learning agent based on TensorFlow [44] to compare a rule-based controller with a single- and a multi-agent RL controller. Despite using function approximations, the state and action spaces were limited to only four states and one action. Moreover, the indoor temperature was not included in the state space, making it impossible to exploit the thermal flexibility of the building\u2019s indoor air. We use a high-order model from the BOPTEST framework to test the advanced control algorithms, which contrasts with previous related studies that used first or second order emulator models, e.g. in [4,36\u201338,42]. Most of the studies mentioned above consider either MPC or RL independently, and only a few of them compare both methods for the same building test cases. One example is described in [36], where a model-based and a model-free RL algorithm was implemented to control building space heating. They also compared this algorithm in simulations against an idealized MPC and a rule-based controller. However, the MPC served as an upper performance bound since it assumed perfect knowledge of the system dynamics. Mbuwir et al. [4] also compared two RL techniques to MPC, where again the MPC was assuming full knowledge of the system parameters. Cost savings of a simulation-based RL agent were shown in [41] during an experimental study. This approach was then compared to other approaches using a calibrated model for the evaluation, and it was indicated that a model-based optimization approach could significantly outperform the RL agent. The agent was shown to be highly affected by the training model accuracy used in the off-line learning phase. Ernst et al. [9] directly compared both control paradigms, showing that RL may be competitive with MPC. However, they implemented such comparison in a simple case without any disturbances. Even less common are the studies that investigate the possibilities of merging MPC and RL approaches to exploit the merits of each method. Learning-based MPC is a research field that automates control design and tuning from monitoring data to enhance performance. Although the field of learning-based and data-driven MPC approaches is vast, only a few studies investigate the possibilities of merging MPC and RL to exploit the strengths of each method. Hewing et al. [45] present a general review about learning-based MPC and classifies these methods into three main categories: learning the system dynamics, learning the controller design, and MPC for safe learning. An overview of learning-based MPC with a major focus on RL can be found in [11]. For the application of collaborative methods in buildings, we highlight the work of Ojand et al. [14] who present a two-level MPC integrating Q-learning for the control of a residential community using an energy management system aggregator. The MPC computes optimal planning for day-ahead operation at the aggregator level and Q-learning is used for the real-time control of each thermostatically controlled load in the community. Although MPC and RL collaborate in this case study, they work at two different levels, with each agent being independent of the other. It is also worth noting the work of Liu et al. [46]. They laid the foundations of a theoretical framework for implementing RL in an actual building and classified the solution of sequential decision making problems into four main categories: pure planning (MPC), classical RL, simulation-based RL and model-based RL. They present simulation- and model-based RL as hybrid approaches between both optimal control families. While it is true that model- and simulation-based approaches combine learning and planning, they do not combine the machinery of MPC and RL. On the contrary, these approaches learn a model that serves an agent to update its policy from simulated experience. The recently developed Differentiable MPC [47] provides a methodology to combine planning and control. This algorithm solves the linear quadratic regulator problem formulated as a classical MPC problem in a forward pass. In a backward pass, the parameters of the model are estimated. Differentiable MPC has been adopted by Chen et al. [15] to develop a new algorithm named Gnu-RL that allows end-to-end planning and control of HVAC systems. A quadratic cost function with linear constraints is formulated as in MPC, and the parameters of the system dynamics are estimated from historical data. Remarkably, the objective function parameters are also estimated to obtain the advantage function of their gradient-based RL algorithm, which constitutes their core novelty and bridges the gap between MPC and RL. On a similar note, Drgoňa et al. [48] propose to use differentiable predictive control combined with neural networks for optimal control without requiring the supervision of an expert controller. Their approach is successfully tested in a MIMO building system. Although Differentiable MPC and Gnu-RL suppose a significant step forward in merging both optimal control families, the implementation of Chen et al. only used a low-dimensional state\u2013action space and a prediction horizon of only 1.5 h. Moreover, they formulate an objective function that balances thermal discomfort and energy use, but do not encode any building system dynamics in their optimization problem. Contrarily, the system dynamics are introduced as state-space matrices randomly initialized with parameters that lack any physical meaning. More generally, RL has been proposed to collaborate with MPC for applications other than building energy management. In [49\u201351], and in [52] it is proposed to use the MPC as a function approximation for RL, in the same spirit as Differentiable MPC. Formal theory for this approach is presented in [53]. Kamthe et al. [54] use dynamic programming combined with Pontryagin\u2019s maximum principle to solve the constrained open-loop optimization of an MPC. In the latter scheme, Gaussian process models are learned instead of a parametrized policy or value function. Negenborn et al. [10] propose to use an explicit representation of the system dynamics to optimize in the short term (e.g., for one step ahead) and to lean on a value function to account for the long-term value of an action. The idea is to use value functions to help the MPC in dealing with suboptimality, finite horizons, and computational requirements. In this approach, the MPC provides robustness and decision making over the short term, and RL provides adaptation and decision making over the long term. The proposal of Negenborn et al. is probably the prior work that resembles the RL-MPC algorithm of this paper the most, although they never implemented their method in practice. On the contrary, Zhang et al. [55] implemented a similar concept in the Q-learning-based model predictive control using the Lyapunov technique (Q-LMPC) for continuous nonlinear systems with complicated dynamics. They analyzed the convergence of such a scheme and showed that the control policy approximated by the algorithm ensures the stability of the closed-loop system. The algorithm was successfully tested in simulations for a well-mixed, non-isothermal continuous stirred tank reactor with two states and two inputs. The simulation results showed that when using an inaccurate system model to pretrain Q-LMPC can improve the performance of the closed-loop system and that a well-trained Q-LMPC can achieve performances similar to those of LMPC. This paper elaborates on the approach proposed by Negenborn et al. [10], which is also similar to Q-LMPC. The approach is extended by encoding domain knowledge with a pretrained physics-based controller model and using a state observer to estimate the initial states of the model every control step. Additionally, the algorithm is empirically tested in a standardized framework for the assessment of building controls and compared against classical MPC and RL approaches. 3 Disentangling the differences between MPC and RL Optimal control solves a sequential decision making problem to determine the actions that optimize a performance objective. The previous section has underlined the need of comparing and the potential of merging the two main approaches for optimal control applied to building energy management: MPC and RL. Both methods share common components, while other components are more controller-specific. These differences in formulation make it difficult to compare and merge both approaches, and motivates an analysis at a conceptual level. Fig. 1 classifies the solution methods for optimal control. The arrows indicate the dependencies of each element on a lower abstraction level, such that a single algorithm can be characterized by tracing a downward path. Ramifications may arise, especially for Markov Decision Processes (MDP), where the solution methods for learning an optimal policy are characterized as the combination of several features, rather than exclusive attributes leading to a single thread. It is important to note that the taxonomy presented in Fig. 1 is not exhaustive because of the wide variety of existing approaches. However, it helps in locating the main methods for optimal control and sets a common ground for a complete classification. The following subsections report the main aspects of these control methods. 3.1 Approach Starting from the highest level of abstraction at the top of Fig. 1, there exist mainly two ways of approaching an optimal control problem: using the receding horizon principle inherent to MPC, or formalizing the problem as a MDP. In MPC, at every time step k a vector of measurements m k is taken from the plant, and an observer estimates the state vector x ˆ k that fully characterizes the controller model at current time. Then, the future state x and input u trajectories are optimized for a finite prediction horizon Δ t h according to the explicit representation of an objective function J and a controller model F . The set of constraints H are also explicitly introduced in the optimization problem. The objective function, model and constraints may also depend on the model outputs y , algebraic variables z , disturbances d , and time-invariant parameters p . The forecast of the disturbances along the prediction horizon d ( t k , t k + Δ t h ) are provided as external data to the optimization. The objective function is usually defined as the integral of a function l of the model variables along the prediction horizon, as indicated by Eq. (1). Only the first control input from the optimized trajectory is implemented. The complete MPC process is shown in Fig. 2(a). (1) J k = ∫ t = t k t k + Δ t h l ( x ̇ ( t ) , x ( t ) , u ( t ) , y ( t ) , z ( t ) , d ( t ) , p ) d t In the application of MPC to building energy management, the state vector x typically represents building temperatures like the zone operative, floor, or wall temperatures, and the model outputs y are usually a subset of these, e.g. the zone operative temperature only. The vector of algebraic variables z represents the variables dependent on the states, e.g. the heat flows through the building envelope, and the vector of disturbances d comprises the main uncontrollable variables affecting the building thermal behavior. Typically, the ambient temperature, solar irradiation, and the occupancy are included in d . The set of time invariant parameters p may or may not represent physical properties of the building depending on whether the controller model uses or does not use physical insights, respectively. In an MDP, the decision making model is defined by the state-space S , the action space A , the rewards R ⊂ R , and the transition function of the environment f . Notice that f represents the ground truth of the system dynamics, contrarily to the controller model F used in MPC, which is a simplified representation of the system. In RL, the agent interacts with the environment during a sequence of discrete-time steps. Every step k , the RL agent receives an observation of the state-space S k ∈ S and a reward R k ∈ R that reflects the goodness of the action taken. In turn, the agent computes its control logic and sends a new action A k ∈ A to the environment. The environment is defined by E : S × A → S × R , and the objective of RL is to infer an optimal control policy π : S → A that maximizes the expected cumulative return G when the agent acts according to it. The cumulative return is defined as some function of the rewards sequence, and a typical definition is to discount the rewards with a factor γ ∈ [ 0 , 1 ] as shown in Eq. (2). The process is summarized in Fig. 2(b). (2) G k = R k + 1 + γ R k + 2 + γ 2 R k + 3 + ⋯ = ∑ i = 0 ∞ γ i R k + i + 1 3.2 Terminology By analyzing the control processes from Figs. 2(a) and 2(b) it is possible to identify several expressions with a total or partial equivalence between both approaches. Controller and agent, plant and environment, control input u and action A , can all be considered as equivalent elements. Notice, however, that slight differences arise from these elements. For instance, the plant is limited to the representation of the system process, while the environment is extended to provide the states and rewards as perceived by the agent. Others, like the objective and return, or the state\u2019s definition, can be seen as analogous elements only. The control community usually minimizes an objective function J whereas the machine learning community maximizes a cumulative return G . The relation between both can be formalized through the immediate reward. In the deterministic setting, the scalar immediate reward r k + 1 can be related to the objective function as follows: (3) r k + 1 = − ( J k + 1 − J k ) Special attention should be paid to the definition of the state. The control community tends to use the state x to designate only internal properties of the system. On the contrary, the machine learning community tends to use the state S to refer to the environment\u2019s condition, which may include internal and external system variables like the forecast of the disturbances, the previous measurements, or some notion of time. Therefore, the state in machine learning may acquire a different meaning than in control theory, especially for controllers using a physics-based model where the state is a strict representation of a system variable. This terminology is conventionally used by each community separately. However, it is common to find studies that maintain their native notation while borrowing concepts from the other community, or the other way around, e.g. in [12] or in [56]. While this is a valid practice as far as consistency is maintained, it makes the cooperation of both approaches difficult because it obscures the precise meaning of each term. For the sake of clarity, this work strictly respects the notation typically adopted by each community as presented above. Therefore, a state x has a different meaning than S , and the controller model F is also different from the ground truth dynamics represented with f . 3.3 Solution method MPC can either be solved implicitly by performing state estimation, forecasting and resolving a dynamic optimization problem at every time step, or it can be solved explicitly by learning a control policy from data produced from an implicit MPC with any kind of function approximation. Therefore, implicit MPC has larger online computational cost since it requires to estimate the states and perform dynamic optimization every control step. Contrarily, explicit MPC can deliver the optimal input just at the cost of a function evaluation, but it has a larger offline computational cost. The reason is that explicit MPC requires the configuration and implementation of an implicit MPC to learn the policy function using any supervised learning method. The main advantage of an explicit MPC formulation stems from its easy implementation even on low-level hardware [57]. The outcome of an explicit MPC is similar to the outcome of any algorithm solving an MDP since the function approximation of explicit MPC can be interpreted as a policy that decides actions from system observations. The solution of the dynamic optimization problem is the core of the implicit MPC approach. Three main solution methods can be identified to resolve dynamic optimization problems: dynamic programming (DP), direct methods, and indirect methods [58]. Direct and indirect methods differ in when discretization takes place, but both require discretization. The process of transforming the infinite-dimensional optimization problem to the finite-dimensional optimization problem through discretization is called transcription. Single-shooting, multiple-shooting, and collocation are the most widely used transcription methods. These methods differ on how the system variables are discretized and how the system dynamic are imposed. After transcription, the solution boils down to solving a (Mixed Integer) \u2014 (Non) Linear Program ((MI)-(N)LP) with methods that should be selected based on the properties of the resulting problem. Examples are quadratic programming, line search, or the Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) algorithm [59]. The solution method used for an MDP depends on the information accessible to the agent and the approximations needed to derive a control policy. When perfect knowledge of the MDP is available, DP algorithms can obtain an exact solution. If the MDP contains infinite elements, as in continue state\u2013action spaces, approximations are required, and Approximate Dynamic Programming (ADP) is used. When the agent cannot reproduce the actual environment and needs to learn from samples of experience, RL methods are needed instead. In the latter case, the agent learns from empirical experience, either from historical data or direct interaction with the environment. A trade-off should be found between off-line and on-line learning. Fig. 1 shows the main aspects that may characterize the learning process of the solution method of an MDP, i.e.: the period when learning takes place, the approach used to decide actions, the exploration method, the use of a system model, the way of representing the targets for training, the method used for policy updates, and the agent\u2019s architecture. 3.4 Optimality In MPC, the quality of the optimization solution is subject to the accuracy of the controller model, which is often simplified for computational reasons. Moreover, direct optimization methods are based on Karush\u2013Kuhn\u2013Tucker conditions, and indirect methods are based on Pontryagin\u2019s maximum principle. Both can only provide necessary conditions for optimality unless specific convexity properties are met for the objective function and the constraints. Stability and feasibility are inherently guaranteed for MPC, while there is only immature theory for these matters in RL [12]. The lack of safety guarantees in RL stems from the constraints not being directly imposed in the formulation of the solution method. On the other hand, DP-based methods rely on Bellman\u2019s principle and can provide sufficient conditions for global optimality. As a downside, these methods are hampered by the curse of dimensionality, i.e., an exponential complexity growth with the size of the state\u2013action space. Pure DP can be used only when the state space is countable and the MDP is perfectly known. Algorithms approximating any element of the MDP, like ADP or RL, can only guarantee optimality under certain conditions, namely when using linear function approximations or Monte Carlo learning. Despite these challenges, RL can target real complex problems where exact methods may become infeasible [13], though optimality is not guaranteed in these cases. Additionally, the same RL algorithms may extrapolate to very different types of environments. 3.5 Computational effort A main drawback of implicit MPC is the burden of solving an optimization problem on-line that can be complex and involve a large number of optimization variables. This is why controller models for MPC are commonly simplified at the cost of optimality loss and why efficiency gains in optimization solvers are highly wanted. Additionally, state estimation and forecasting need to be performed at every control step. Explicit MPC aims to overcome this burden by developing a control policy through behavior cloning, but an implicit MPC needs to be developed beforehand. The off-line complexity of MPC relates to the engineering effort required to develop and identify a controller model that is accurate and suitable for optimization. While the latter is a challenging task, its computational cost is limited. DP-based algorithms, on the other hand, typically work mostly off-line, producing a policy that is then used to control the process [56]. Large computational efforts are expected to train policies even for fairly simple systems. In RL, a model of the environment may be used for off-line simulation-based learning. This approach provides a separate training environment that does not limit exploration to obtain a high variance data-set. Other off-line methods to learn a policy are behavior cloning or Fitted-Q Iteration (FQI) [60]. Once the policy is developed, it directly provides the estimated optimal action given the state and at the cost of a policy evaluation only, which may be expressed by a neural network or other mathematical functions. While learning may continue on-line, the agent shall be more limited when exploring new actions to improve its policy. This is because deviations from the learned policy may result in performance degradation or unexpected behavior [13]. The sampling efficiency is defined as the capacity of an RL agent to learn a reliable policy from the least possible amount of interactions with the environment, and it is an actively studied topic within the machine learning community, especially in the context of model-based RL approaches [13,61]. 3.6 Prediction horizon Both MPC and RL are predictive controllers, independently of whether they integrate disturbances forecast in their control logic. MPC uses explicit optimization along a finite prediction horizon, and RL learns actions to optimize the sum of the immediate and the discounted future rewards. A shortcoming of MPC is the finite horizon, which is especially pronounced in tasks with sparse rewards where a short horizon can make the agent extremely myopic [62]. Additionally, MPC struggles with longer prediction horizons because these increase the number of state and input variables in the optimization. Only single shooting methods are independent of the number of states, but they are known for their instability, especially for stiff systems. On the contrary, the machine learning community usually works with infinite discounted returns, although finite returns are sometimes considered as well. The discount factor represents the extent of credit given to future rewards and bounds the infinite sum. Notice that policies built with finite returns require time as an argument to use the policy function associated with the passed time. An effective horizon can be defined as the number of steps after which rewards are negligible for policies with infinite prediction horizons. It is determined by the discount factor using the convergence of a geometric series, as shown in Eq. (4). (4) 1 + γ + γ 2 + ⋯ = 1 1 − γ The above equation indicates that the rewards for time steps after 1 / ( 1 − γ ) are ignored in the total return. Hence, it is possible to relate the discount factor of an RL agent with the finite prediction time horizon of an MPC as follows. (5) Δ t h = Δ t s 1 1 − γ Where Δ t h is the finite prediction horizon of the MPC and Δ t s is the control step period. Although infinite prediction horizons may benefit from improved behavior in the long term, it should be noted that RL algorithms make less efficient use of the forecast information. The reason is that all forecast data is flattened and brought to the agent\u2019s observations with the time dependency removed from it. The agent needs to figure out the time dependency that is not explicitly described in the problem definition during the learning process. On the contrary, MPC naturally preserves the time dependency (and thus the chronology) by taking into account the system dynamics computed in the controller model. A potential synergetic approach may use short prediction horizons to solve an optimization problem for MPC while benefitting from the infinite horizons that characterize RL. 3.7 Use of models Models and function approximations are used differently in MPC and RL. In MPC, the model used to represent the system is called the controller model. These models are obtained through domain knowledge, system identification or supervised learning from historical monitoring data. Commonly, the controller models are qualified as white-, gray-, or black-box depending on whether physical insights and/or monitoring data are used for their configuration. The optimization problem in MPC imposes severe restrictions on the controller models, which are often simplified to guarantee convergence at the cost of performance loss. The same controller model (or a derivation) is commonly used for state estimation as well. In RL, models approximate the policy or the value function, which does not directly represent the system dynamics. A particularity of training policies and value functions is that the training data targets are continuously obtained from experience samples, in contrast to those used for supervised learning, which are fixed before learning begins [63]. This characteristic poses a significant challenge to many machine learning modeling approaches that are based on independent and identically distributed data. RL may also use a system model to pretrain a policy in simulation-based RL [41], or to alternate real and modeled experience in model-based RL [64,65]. An important advantage of using system models to train RL agents is that their analytical form is not required, such that they are not restricted in complexity. 3.8 Partial observability Since MPC uses a system model to optimize actions for every control step, it needs to know the present value of the state vector, which may not be entirely determined from the system measurements. Hence, state estimation is required to estimate the hidden states of the controller model, i.e., those states that are not measured during operation. A few observers used for state estimation are listed in the scheme of Fig. 1. A Luenberger observer is probably the most straightforward and intuitive type. Instead, moving horizon estimation is a complex and computationally expensive approach since it requires optimization for each state estimation, but allows nonlinear dynamics. Extended Kalman filter methods are the non-linear version of the conventional stationary Kalman filter and the time-varying Kalman filter. Finally, the unscented Kalman filter uses a minimal set of carefully chosen sample points to represent the state distribution. It uses the unscented transform to propagate through the true non-linear system empirically. Comparative studies of different state estimators for their application to building systems can be found in [66,67]. Similarly, an MDP is partially observable when some of the elements of the state-space remain hidden to the agent\u2019s observation. The Markov property is violated in these scenarios since the future behavior cannot be entirely determined from the present state. To amend this, an arbitrary selection of past features is introduced in the observation space, e.g. in [34], to allow the agent to extract the hidden features. 4 Reinforced model predictive control (RL-MPC) This section introduces the details of the novel proposed RL-MPC algorithm. The goal is to learn from the environment while ensuring constraint satisfaction. With that aim, elements from the control and machine learning communities are effectively combined, namely state estimation, dynamic optimization, and learning. First, Section 4.1 provides a high-level introduction on how MPC and RL are logically merged. Then, Section 4.2 provides a formal description of the RL-MPC algorithm. 4.1 Intuitive description To understand the intuition behind RL-MPC it is first required to realize the differences between the non-linear program used to solve the online optimization problems in MPC and the two main learning methods of RL: value-based and policy-based methods. Fig. 3(a) shows a schematic representation of the non-linear program obtained from an MPC. The solution method of MPC takes as an input the union of the estimated states x ˆ and the forecast of the disturbances d . In this approach, the objective function J is inferred every control step based on the controller model. Fig. 3 also depicts the value-based approach (Fig. 3(b)) and the policy-based approach (Fig. 3(c)) for uni-dimensional state and action spaces in an equivalent optimal control problem. These RL approaches take as an input the agent\u2019s observation s . Value-based methods characterize an action-value function q ( s , a ) as the expected total cumulative return from state s when taking action a . Policy-based methods directly map a state s to an action a with a policy function π parametrized with the parameter set θ . In RL-MPC, the MPC non-linear program is truncated with the expected value of the state one step ahead s \u2032 as estimated by the value-based RL approach. Hence, RL-MPC uses value-based RL to estimate the value of being in a specific state s \u2032 as obtained by the MPC when using a prediction horizon of only one control step. Therefore, the main components of the MPC remain active in RL-MPC, namely the state estimator, forecaster and optimizer, but the value function is used to shorten the non-linear program and to enable learning. Fig. 4 shows a high-level diagram with the interaction of the main components of RL-MPC. Fig. 4 intuitively illustrates how MPC and RL are merged in the RL-MPC algorithm. 4.2 Formal description In the deterministic setting of an MDP, the action-value function q π ( s , a ) is defined as the expected return when taking action a from state s and following policy π hereafter. The objective is to find a policy π ∗ that maximizes the sum of the expected cumulative returns. Implementing Bellman\u2019s principle of optimality, the following expression is obtained: (6) q ∗ ( s , a ) = max π q π ( s , a ) = r k + 1 + γ max a \u2032 q ∗ ( s \u2032 , a \u2032 ) Where q ∗ represents the optimal q function and s \u2032 is the reached state when applying action a from state s . Using the equivalence from Eq. (3), the previous expression can be rewritten as Eq. (7). (7) q ∗ ( s , a ) = − ( J k + 1 − J k ) + γ max a \u2032 q ∗ ( s \u2032 , a \u2032 ) This can be simplified by defining the state-value function v π ( s ) , which is the expected return from state s and following policy π hereafter. The state-value function can be related to the action-value function as shown in Eq. (8). (8) v ∗ ( s ) = max a ∈ A q π ∗ ( s , a ) This allows reformulating the action-value function as follows. (9) q ∗ ( s , a ) = − ( J k + 1 − J k ) + γ v ∗ ( s \u2032 ) Notice that Eq. (7) exposes the objective function from a classical MPC formulation along the first prediction step. Hence, given a state s , the optimal action a can be obtained by optimizing q ∗ ( s , a ) while explicitly imposing system constraints, as typically done in the MPC approach. Assuming that a controller model F is available, the policy followed by RL-MPC is defined in the set of Eqs. (10). (10a) π ∗ ( s ) = arg max a q ∗ ( s , a ) = (10b) arg max a − ∫ t = t k t k + 1 l ( x ̇ ( t ) , x ( t ) , a ( t ) , y ( t ) , z ( t ) , d ( t ) , p ) d t + γ v ∗ ( s \u2032 ) (10) s . t . 0 = F ( x ̇ ( t ) , x ( t ) , a ( t ) , y ( t ) , z ( t ) , d ( t ) , p ) (c) 0 ≤ H ( x ̇ ( t ) , x ( t ) , a ( t ) , y ( t ) , z ( t ) , d ( t ) , p ) (d) The main advantage of utilizing the formulation presented by Eqs. (10) is that it imposes safety constraints in the short term while enabling continuous learning from empirical experience. Moreover, shortening the prediction horizon of the dynamic optimization problem considerably eases the complexity of the resulting non-linear program. The long-term behavior is still accounted for with the state-value function v . It is important to note that both terms from Eq. (10b) should be jointly optimized, such that state s \u2032 needs to be related to the expected optimization variables in t k + 1 . This leads to a lower overhead than optimizing with longer prediction horizons that need discretization over time. Additionally, the reward needs to be shaped in alignment with the design of the objective function and the constraints to encourage cooperation between both terms in the action-value function. This means that the first and second elements of the objective function as shown in Eq. (10b) should not compete. Contrarily, they should be defined to be complementary. It is also worth noting that domain knowledge is encoded in the controller model F for optimization and state estimation, providing intelligibility to the algorithm. The controller model F can also be used to configure a separate simulation environment E F to pretrain q ( s , a ) from simulated experience and expedite learning. The process is summarized in Algorithm 1, where α is known as the learning rate. Note that lines 1\u20133 of Algorithm 1 constitute the off-line learning part, while lines 4\u201310 constitute the deployment of the algorithm in the actual building environment. 5 Implementation This section describes the implementation details of MPC, RL, and RL-MPC to the same test building included in the BOPTEST framework. Each controller uses equivalent features to enable a fair comparison. An explanatory diagram is shown in Fig. 5 that summarizes the implementation methodology. 5.1 The control problem in BOPTEST A standardized simulation environment is required to ensure a fair evaluation of the algorithms. BOPTEST is an open-source framework that provides a menu of high-fidelity emulator building models and the standardization of test cases to assess control algorithms. In this framework, the building models are containerized with the Docker technology and considered as the ground truth plant to be controlled. The functionality is enabled through a clear API to select a test scenario, advance a simulation, and get data like measurements, forecast, or Key Performance Indicators (KPIs) at every control step. The framework is freely accessible in https://github.com/ibpsa/project1-boptest. The building of interest in this study is the single-zone residential hydronic case as in version v0.1.0 of the framework. The building represents a residential dwelling of 192 m 2 for a family of five members located in Belgium. The family inhabits the building before 7:00 h and after 20:00 h every weekday and full-time during weekends. The HVAC system consists of an air-to-water modulating heat pump of 15 kW thermal power coupled to a floor heating system. The control signal is the heat pump modulation signal for compressor frequency u h p ∈ [ 0 , 1 ] that controls the heat delivered to the floor heating. In turn, this heat is released to the building zone by a water emission circuit with a hydronic pump that is activated when the heat pump is working. The controlled signal is the zone operative temperature T z that should remain within the temperature range 21\u201324 °C during occupied hours and within 15\u201330 °C otherwise. No cooling is considered in this test case. All BOPTEST cases come with one year of boundary condition data and predefined testing periods of two weeks. The cases can be initialized at any arbitrary time of the year, although special attention should be paid to not use any of the predefined testing periods for training. The envisaged building offers a total of six testing scenarios from the combination of two heating periods: peak and typical, and three electricity price profiles: constant, dynamic, and highly dynamic. This work considers the two heating periods with the highly dynamic electricity pricing for testing. This highly dynamic price profile is obtained from the sum of (1) historical day-ahead energy prices as determined by the Belpex wholesale electricity market in 2019, and (2) constant transmission fees and taxes representative for the same location. The final objective of the controller is to guarantee thermal comfort while minimizing operational cost. From the control perspective, the presented problem is particularly challenging for different reasons. First, the controlled variable, T z , has a significant delay with respect to the control variable, u h p , because of the considerable thermal inertia of the system. Second, the data used for system identification is limited and has low variance because it is generated with the baseline controller that comes along with the test case and that uses a regular thermostatic control. Long training periods or extra-excitations are not considered when identifying the controller model F to emulate a realistic scenario where the controller model works with typical operational data. Third, the controller can only access a few of the signals that influence the plant dynamics, as is the case in reality. Precisely, only the operative zone temperature, out of the 63 continuous-time states present in the emulator model, is measured. Moreover, only 6 out of the 31 disturbances are provided through the forecast. These are weather variables like ambient temperature and solar irradiation or others like energy pricing or comfort setpoint bounds. All control algorithms in this work assume perfect deterministic forecast provided by BOPTEST. 5.2 System identification A controller model F is required by the MPC, and to configure the environment E F used to pretrain the policies in the RL and RL-MPC algorithms. The same gray-box model is utilized in all cases, consisting of a thermal resistance\u2013capacitance (RC) architecture constructed from basic physical principles and without using any system metadata like building geometry or material properties. A model of order five is decided from a forward-selection procedure. The model has a total of 20 lumped parameters that need to be estimated. Two weeks of data are generated by the emulator with the baseline controller for training, and two weeks are used for validation of the model. The fitting variables are the zone operative temperature T z , the heat pump condenser thermal power Q ̇ c and the heat pump electrical power use P h p . The last two variables determine the coefficient of performance of the heat pump. The model further consists of three inputs: ambient temperature T a , solar irradiation Q ̇ r a d , and occupancy gains Q ̇ o c c . Fig. 6 shows an overview of the training and validation periods that have been selected not to overlap any of the testing periods. 5.3 MPC implementation The MPC of this study heavily relies on JModelica [68], a framework for dynamic simulation and optimization of Modelica models. Particularly, the non-linear JModelica MPC module developed in [69] is used here and has been extended to enable mutable external data. The JModelica MPC module utilizes the direct collocation scheme and relies on CasADi [70] for algorithmic differentiation. This approach is well known for its versatility and robustness. The optimization problem that needs to be solved at every control step is formulated in Eqs. (11e). (11a) min u H P J k = min u H P ∫ t = t k t k + Δ t h ( p e ( P h p + P f a n + P p u m ) + w δ T z ) d t (11b) T ̇ z , P h p , P f a n , P p u m = F ( u h p , Q ̇ r a d , Q ̇ o c c , T a , T z , T , p ) (11c) T ̲ z − δ T z ≤ T z ≤ T ¯ z + δ T z (11d) δ T z ≥ 0 (11e) 0 ≤ u h p ≤ 1 . In Eqs. (11e), the time dependency has been omitted for clarity because all variables are time-dependent except the vector of model parameters p , and the weighting factor w , the latter being used to account for the different orders of magnitude between energy cost and discomfort δ T z . The energy cost is the first term of the objective function where all elements accounting for electrical power are summed and multiplied by the electricity price p e . These are the heat pump power P h p , the evaporator fan power P f a n , and the circulation pump power P p u m . T represents the vector of non-measured temperature states of the controller model. The discomfort δ T z is defined as the deviations of T z out of the comfort range bounded by T ̲ z and T ¯ z . The weighting factor w is tuned to penalize more heavily the instantaneous discomfort than the instantaneous energy cost, such that the thermal discomfort is treated as a soft constraint in the optimization. The MPC module is combined with the unscented Kalman filter of the JModelica toolbox for state estimation. The specific state estimation algorithm is the non-augmented version described in [71], and the sigma points are chosen according to [72]. The controller model F described in the previous section is directly used for optimization and state estimation with a control step of Δ t s = 15 min and a prediction horizon of Δ t h = 24 h. 5.4 RL implementation The BOPTEST-Gym wrapper developed in [17] accommodates the BOPTEST API to the Gym standard [73] to assemble the testing environment E f , which we call the actual environment henceforth. Similarly, the E F is developed by replacing the ground truth building model f by the simpler lower-order controller model F explained in the previous sections. We call E F the simulation environment. Both environments are wrapped following the Gym standard, which provides a convenient setting for implementing any RL algorithm. Additionally, both environments expose the same observations and actions to the RL agent, but their underlying dynamics are different: the actual environment uses a detailed emulator model representing reality, while the simulation environment uses a simple RC model. The motivation for the development of E F is that it represents an environment that could be obtained in practice to train an agent with simulation-based RL and without any restriction on the actions taken. Contrarily, E f represents an actual building and thus, only a limited set of interactions may be permitted. In both environments, the state\u2013action space is designed to be analogous to the MPC formulation described in the previous section. Specifically, the control step is also set to Δ t s = 15 min, and the agent\u2019s observations include forecast over a prediction horizon of Δ t h = 24 h with the same interval period. In this configuration, a discount factor of γ = 0 . 99 accounts for an effective horizon of approximately one day according to Eq. (5). The reward function is defined as the negated increment of the MPC objective integrand cost as shown in Eq. (3). The state observer is also replicated by including past measurements of the zone operative temperature and the time of the week in the agent\u2019s observation. The agent shall use these observations to infer the current building state and to deal with the partial observability of the environment. As a result, the state space observed by the agent has a total dimension of | S | = 608 with all continuous variables that are normalized between [ − 1 , 1 ] to facilitate learning. The heat pump modulation signal is discretized in 10 uniform intervals such that the action space is A = { u h p } , with u H P ∈ { 0 , 0 . 1 , 0 . 2 , \u2026 , 1 } , and | A | = 11 . The large state\u2013action space requires the use of function approximations and a suitable RL algorithm for training. We use the Double Deep Q-Network (DDQN) algorithm as proposed in [63] and in [74], which enables a neural network as a function approximation and offers a natural extension towards the implementation of RL-MPC, as explained in the following section. Specifically, a double network is implemented to avoid the overoptimism inherent to Q-learning for large-scale problems [74]. DDQN is an off-policy algorithm that updates network weights following a stochastic gradient descent scheme. Tuples of the form ( s k , a k , r k , s k + 1 ) are stored in a replay memory D and served in random batches during training in order to alleviate the problem of correlated data. We set the batches to sample one week of transition data. A multi-layer perceptron (MLP) neural network is configured with TensorFlow for the state\u2013action value function. The network contains two hidden layers of 64 neurons each and a rectified linear activation function for the nodes. In a first learning step, the agent is pretrained with behavior cloning during the month of data used for system identification. Then, the agent interacts with the simulation environment E F for one million steps. Notice that this is still off-line learning since E F is a simulation environment. During this learning process, E F is configured to launch episodes of experience that last for one week and that are randomly initialized throughout the year. The episodes never overlap the periods that will be used for testing later on with E f , such that the boundary condition data used for testing is not available during training. The agent is configured to follow an ε -greedy exploration scheme with a linear schedule that goes from 10% to 1% of random exploration probability along the learning process. The reason to limit the initial exploration to 10% is that the prior behavior cloning forces the agent to explore the most interesting region of the state\u2013action space already from the beginning. 5.5 RL-MPC implementation The RL-MPC algorithm inherits all attributes and hyperparameters described in the MPC and the RL implementations. This means that the same control step, prediction horizon, and state estimator than the MPC is used, as well as the same pretrained q function than the RL agent. Since both RL and MPC have been designed with analogous hyperparameters, they are expected to be complementary in the RL-MPC implementation. The method jointly optimizes the sum of the immediate rewards (negated objective function increase) obtained from the MPC and the value function obtained from the RL agent. At every time step t k , the same state observer as the one used by the MPC provides an estimate of the vector of states x ˆ k to initialize the E F environment. Then, this environment evaluates the immediate reward of taking every possible action. The advantage of using exhaustive search for the optimization of the first step in the horizon is that the environment E F also delivers the expected next state s \u2032 , such that it can directly evaluate v ∗ ( s \u2032 ) and sum it to the immediate reward. This approach would be prohibitive for larger action spaces that would require a function mapping between x and s . We prefer the exhaustive search approach from the convenience of having E F available. 6 Simulation results Fig. 7 shows the evolution of the average episodic return during the off-line learning process, i.e., during the one million steps of interaction with the simulation environment E F . The steady increase of the expected episodic return indicates that DDQN can properly learn from experience. For the sake of benchmarking, the same algorithm is separately trained with the actual environment, E f , and using the same random seed for exploration. Although unrealistic, the latter scenario provides an upper performance bound for testing. The comparison of both learning curves in Fig. 7 reveals an overestimation of the rewards when the simulation environment is used for training. The reason is the less dynamic behavior of the RC model that leads to a more gentle response of the zone operative temperature to the random actions of the agent. The agent is thus overoptimistic when trained with simulated experience and thinks that it can easily maintain the indoor temperature within the comfort bounds. This effect has a catastrophic consequence during testing, as is clear from Fig. 8 which shows that the agent trained in the simulation environment cannot maintain the controlled variable within bounds during testing. Contrarily, the agent trained in the actual environment can successfully keep the temperature within bounds. It is concluded that the false cues received from E F mislead the agent leading to poor behavior during testing. A similar observation of simulation-based RL in buildings was already pointed out in [41]. Interestingly, the MPC results of Fig. 8 reveal that MPC uses the controller model more effectively than the DDQN algorithm because it respects the constraints when using the same controller model and an equivalent control configuration. This performance difference stems from the different machinery of each algorithm: MPC performs state estimation and dynamic optimization every control step, which leads to a higher on-line computational intensity, but allows to decide on best actions based on an accurate representation of the current system state at all times; conversely, RL only requires a function evaluation to decide best actions on-line (which is computationally very efficient), but relies on a parametrized value function that has been trained off-line and that can hardly infer the current system state. The poor constraint satisfaction of the DDQN algorithm is eliminated by the implementation of RL-MPC, which incorporates a state estimator to accurately track the state of the environment and an optimizer to evaluate actions during the first horizon step. The long-term behavior is accounted for with the same value function trained with the DDQN algorithm in the simulation environment. To emphasize the effect of the value function in RL-MPC, the same algorithm using γ = 0 has been implemented. This represents a controller with a one-step-ahead prediction horizon that disregards posterior behavior since the value function is disabled in the objective. It is apparent from Fig. 8 that the state estimator and the one-step-ahead optimizer substantially aid the RL-MPC agent to satisfy constraints when compared to the DDQN agent. Moreover, the value function positively contributes to the controller performance, which can be seen from the comparison with RL-MPC when γ = 0 . The inability to predict beyond one time step period leads to constraint violations with the myopic agent at occupancy setbacks. This effect is most evident during the peak heating test because of the increased heat demand. The KPIs obtained with each controller at the end of the two-week testing periods are summarized in Fig. 9. These KPIs, among others, are directly obtained from the BOPTEST framework and have been selected based on the designed control objective. The horizontal lines serve as a reference of reasonable thermal discomfort levels when following the recommended criteria for acceptable deviations of standard EN16798-2 [75]. Particularly, these horizontal lines are obtained when using weighting factors of 1, 2, and 3 K during the short time deviations allowed in this standard. In our case, the temperature limits are established by the predefined BOPTEST temperature comfort range. Note that there exist other environmental factors that should be addressed when providing comfort for occupants, like thermal radiation, humidity, and air speed, or personal factors like activity and clothing. For the sake of clarity, our comfort assessment is based on temperature only since it is the main factor influencing the thermal comfort perceived by the occupants in a building [76]. The results of the DDQN agent trained in E F are located at higher values and thus omitted in Fig. 9 to preserve the scale. The DDQN agent trained in the actual environment maintains a low thermal discomfort but at a higher operational cost than the MPC strategy. The DDQN algorithm trained in E f achieves slightly lower discomfort during the peak heating period than MPC, that penalizes thermal discomfort more heavily than the energy cost. However, this does not hold for the typical heating period, where the MPC clearly outperforms DDQN, even when the latter has been trained in the actual environment and the former only uses a simplified controller model. The use of RL-MPC substantially improves the results and leads the agent to achieve similar performance levels as the MPC even for the deterministic setting of this example. The importance of the value function in the objective is underlined by the comparison with the myopic DDQN agent with γ = 0 that incurs substantially higher discomfort, especially during the peak testing period. 7 Discussion There are two main ways to merge MPC and RL: truncating the objective function or using the MPC as the function approximator of the learning agent. The former is the approach adopted by RL-MPC and described in this work, and the latter is the approach proposed by other recently developed algorithms, namely Differentiable MPC and its associated Gnu-RL. RL-MPC constrains the actions explicitly (using the controller model) during the first control step, and implicitly (using the value function) along the rest of the prediction horizon. Differentiable MPC constrains the actions explicitly throughout the whole prediction horizon and uses the rewards to tune the weights of the objective function. These are just two ways of constraining the agent to improve operational safety and it is still an open question which approach is preferred over the other. Probably, Differentiable MPC is more conservative as it imposes the system constraints throughout the prediction horizon. This may lead to safer operation but offers less degrees of freedom to learn from the environment. Both approaches should be compared in BOPTEST to find out which one is preferred for the application in building climate control. It is also important to note that, although BOPTEST is a simulation framework, it uses high-fidelity emulator building models. Uncertainties (e.g. related to weather predictions, measurements, model mismatch) are not yet included, but the provided emulators constitute a detailed representation of the system dynamics and only expose a realistic set of measurements and setpoint variables. Therefore, it is expected that the results obtained in the BOPTEST framework are representative of what would be obtained in real implementations. In this work, the implementation of RL-MPC in BOPTEST has clearly outperformed classical RL. RL-MPC has also shown similar performance to the MPC in a deterministic setting, but RL-MPC incorporates the important advantage of learning as in a classical RL approach: RL-MPC learns the value function from rewards that are directly retrieved from the environment, and an analytical form is therefore not required. This opens the path towards learning perceived comfort from occupants e.g. through an App where people could indicate their thermal sensation from different levels as defined by ANSI/ASHRAE Standard 55-2010, or towards accounting long-term dynamics that cannot be captured by the typical MPC horizons e.g. in thermal systems with very large thermal inertia like geothermal borefields. Moreover, RL-MPC is expected to excel in uncertain environments where the uncertainty distribution could be learned by the agent and accounted for during operation. Confirmation of this hypothesis is a topic for future research. This paper has focused on the definition and empirical demonstration of RL-MPC, but the current implementation of this new algorithm can be further improved. A major challenge for the implementation of RL-MPC relates to setting up an optimization framework comprising the software dependencies required for both: MPC and RL. Dedicated frameworks exist for either one or the other, but, to the best of our knowledge, there is no unified framework facilitating the configuration and implementation of both algorithms together. Moreover, the truncation of the RL-MPC algorithm\u2019s prediction horizon requires estimating the next state and its associated value (expected return) at each iteration of the optimization. Therefore, another major challenge is posed in the implementation of RL-MPC to incorporate the value function in the MPC optimization. The current implementation uses exhaustive search: the controller model simulates every possible action from the state vector obtained with the state observer. The simulation environment is used for this, such that the expected reward and the next expected state are returned from the simulation of each action. The sum of the immediate reward and the value of the expected next state is evaluated for each possible action to decide the action that leads to the highest expected value of this sum. This is obviously not computationally efficient. The most efficient way of implementing RL-MPC would be to extend the controller model with the equations that define the value function. Both could be optimized together using more effective optimization techniques. 8 Conclusion and future work Buildings require advanced control algorithms for the efficient use of their energy systems. The control theory and machine learning communities are both working on solutions using MPC and RL, respectively, and prior literature has expressed interest in merging both methods. The complementarity is underlined from a conceptual reflection on their principal aspects like their approach, optimality, or computational complexity. This paper presents and assesses reinforced model predictive control RL-MPC, an algorithm that effectively combines elements from RL and MPC like state estimation, dynamic optimization, and learning. The BOPTEST framework is a standardized environment for the assessment and benchmarking of control algorithms in buildings. This simulation environment allows consistent and repeatable testing for the same building and boundary conditions. The results for one test case building of the BOPTEST framework reveal that MPC makes effective use of the controller model while RL incurs severe constraint violations using an equivalent formulation of the control problem. Both implementations are combined in the RL-MPC algorithm, which can satisfactorily meet the constraints. The new algorithm obtains performance results similar to the MPC in a deterministic setting even when using an imperfect value function. It also enables learning as in a classical RL approach which allows to naturally deal with uncertain environments or complex rewards without requiring their analytical form. Future research should compare RL-MPC with other methods merging MPC and RL like Differentiable MPC. Additionally, the current implementation of RL-MPC may be further improved by extending the controller model with the equations that define the value function to use efficient optimization techniques and enable scalability of the algorithm. An unified and open-source framework for optimal control may facilitate this task. CRediT authorship contribution statement Javier Arroyo: Conceptualization, Methodology, Software, Formal analysis, Data Curation, Writing \u2013 original draft, Writing \u2013 review & editing, Visualization, Funding acquisition. Carlo Manna: Methodology, Writing \u2013 original draft, Writing \u2013 review & editing. Fred Spiessens: Conceptualization, Methodology, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Funding acquisition. Lieve Helsen: Conceptualization, Methodology, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Funding acquisition. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgment This work emerged from the IBPSA Project 1, an international project conducted under the umbrella of the International Building Performance Simulation Association (IBPSA). Project 1 will develop and demonstrate a BIM/GIS and Modelica Framework for building and community energy system design and operation. The work of Javier Arroyo is financed by VITO, Belgium through a PhD Fellowship (grant number 1710754). Finally, the authors wish to thank to Brida V. Mbuwir, Ján Drgoňa, and Iago Cupeiro Figueroa for kindly reviewing the paper. References [1] IEA, GlobalABC, UN Environmental Programme. Global status report for buildings and construction: Towards a zero-emissions, efficient and resilient buildings and construction sector. 2020, https://wedocs.unep.org/handle/20.500.11822/34572. IEA, GlobalABC, and UN Environmental Programme. Global Status Report for Buildings and Construction: Towards a zero-emissions, efficient and resilient buildings and construction sector. Technical report, 2020. https://wedocs.unep.org/handle/20.500.11822/34572. [2] De Coninck R. Helsen L. Quantification of flexibility in buildings by cost curves \u2013 Methodology and application Appl Energy 162 2016 653 665 R. De Coninck and L. Helsen. Quantification of flexibility in buildings by cost curves \u2013 methodology and application. Applied Energy, 162:653\u2013665, 2016. https://doi.org/10.1016/j.apenergy.2015.10.114. [3] Arroyo J. Gowri S. De Ridder F. Helsen L. Flexibility quantification in the context of flexible heat and power for buildings 2018 Proceedings of the Federation of European Heating, Ventilation and Air Conditioning associations conference, Brussels, Belgium J. Arroyo, S. Gowri, F. De Ridder, and L. Helsen. Flexibility quantification in the context of flexible heat and power for buildings. In Proceedings of the Federation of European Heating, Ventilation and Air Conditioning associations conference, Brussels, Belgium, April 2018. [4] Mbuwir B.V. Geysen D. Spiessens F. Deconinck G. Reinforcement learning for control of flexibility providers in a residential microgrid IET Smart Grid 3 1 2019 1 11 B. V. Mbuwir, D. Geysen, F. Spiessens, and G. Deconinck. Reinforcement learning for control of flexibility providers in a residential microgrid. IET Smart Grid, 3(1):1\u201311, 2019. https://doi.org/10.1049/iet-stg.2019.0196. [5] Schwarm A.T. Nikolaou M. Chance-constrained model predictive control AIChE J 45 8 1999 1743 1752 A. T. Schwarm and M. Nikolaou. Chance-constrained model predictive control. AIChE Journal, 45(8):1743\u20131752, 1999. https://doi.org/10.1002/aic.690450811. [6] Bemporad A. Morari M. Robust model predictive control: A survey Garulli A. Tesi A. Robustness in identification and control Lecture notes in control and information sciences vol. 245 1999 Springer London 207 226 A. Bemporad and M. Morari. Robust model predictive control: A survey. In: Garulli A., Tesi A. (eds) Robustness in identification and control. Lecture Notes in Control and Information Sciences. volume 245, pages 207\u2013226. Springer London, 1999. https://doi.org/10.1007/BFb0109870. [7] Bacci E. Parker D. Probabilistic guarantees for safe deep reinforcement learning 2020 arXiv, https://arXiv.org/abs/2005.07073 E. Bacci and D. Parker. Probabilistic guarantees for safe deep reinforcement learning. arXiv, 2020. https://arXiv.org/abs/2005.07073. [8] Koller T. Berkenkamp F. Turchetta M. Boedecker J. Krause A. Learning-based model predictive control for safe exploration and reinforcement learning 2019 arXiv, https://arxiv.org/abs/1906.12189 T. Koller, F. Berkenkamp, M. Turchetta, J. Boedecker, and A. Krause. Learning-based model predictive control for safe exploration and reinforcement learning. arXiv, 2019. https://arXiv.org/abs/1906.12189. [9] Ernst D. Glavic M. Capitanescu F. Wehenkel L. Reinforcement learning versus model predictive control: A comparison on a power system problem IEEE Trans Syst Man Cybern B 39 2 2009 517 529 D. Ernst, M. Glavic, F. Capitanescu, and L. Wehenkel. Reinforcement learning versus model predictive control: A comparison on a power system problem. IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 39(2):517\u2013529, 2009. https://doi.org/10.1109/TSMCB.2008.2007630. [10] Negenborn R.R. De Schutter B. Wiering M.A. Hellendoorn H. Learning-based model predictive control for Markov decision processes IFAC Proceedings volumes of the 16th IFAC world congress vol. 38 2005 354 359 R. R. Negenborn, B. De Schutter, M. A. Wiering, and H. Hellendoorn. Learning-based model predictive control for Markov decision processes. IFAC Proceedings Volumes of the 16th IFAC World Congress, 38(1):354\u2013359, 2005. https://doi.org/10.3182/20050703-6-CZ-1902.00280. [11] Recht B. A tour of reinforcement learning: The view from continuous control 2018 arXiv, http://arxiv.org/abs/1806.09460 B. Recht. A Tour of Reinforcement Learning: The View from Continuous Control. arXiv, 2018. http://arxiv.org/abs/1806.09460. [12] Görges D. Relations between model predictive control and reinforcement learning IFAC-PapersOnLine 50 1 2017 4920 4928 10.1016/j.ifacol.2017.08.747 20th IFAC World Congress D. Görges. Relations between Model Predictive Control and Reinforcement Learning. IFAC-PapersOnLine. 20th IFAC World Congress, 50(1):4920\u20134928, 2017. https://doi.org/10.1016/j.ifacol.2017.08.747. [13] Dulac-Arnold G. Mankowitz D. Hester T. Challenges of real-world reinforcement learning 2019 arXiv, https://arxiv.org/abs/1904.12901 G. Dulac-Arnold, D. Mankowitz, and T. Hester. Challenges of Real-World Reinforcement Learning. arXiv, 2019. https://arXiv.org/abs/1904.12901. [14] Ojand K. Dagdougui H. Q-learning-based model predictive control for energy management in residential aggregator IEEE Trans Autom Sci Eng 2021 1 12 K. Ojand and H. Dagdougui. Q-Learning-Based Model Predictive Control for Energy Management in Residential Aggregator. IEEE Transactions on Automation Science and Engineering, pages 1\u201312, 2021. https://doi.org/10.1109/TASE.2021.3091334. [15] Chen B, Cai Z, Bergés M. Gnu-RL: A precocial reinforcement learning solution for building HVAC control using a differentiable MPC policy. In: Proceedings of the 6th ACM international conference on systems for energy-efficient buildings, cities, and transportation. New York, New York, USA; 2019. p. 316\u2013325. [16] Blum D. Arroyo J. Huang S. Drgoňa J. Jorissen F. Taxt Walnum H. Building optimization testing framework (BOPTEST) for simulation-based benchmarking of control strategies in buildings J Build Perform Simul 14 5 2021 586 610 D. Blum, J. Arroyo, S. Huang, J. Drgoňa, F. Jorissen, H. Taxt Walnum, C. Yan, K. Benne, D. Vrabie, M. Wetter, and L. Helsen. Building Optimization Testing Framework (BOPTEST) for Simulation-Based Benchmarking of Control Strategies in Buildings. Journal of Building Performance Simulation, 2021. Accepted, in production. [17] Arroyo J, Manna C, Spiessens F, Helsen L. An OpenAI-Gym environment for the Building Optimization Testing (BOPTEST) framework. In: Proceedings of the 17th IBPSA conference. Bruges, Belgium. September 2021. [18] Sturzenegger D. Gyalistras D. Morari M. Smith R.S. Model predictive climate control of a Swiss office building: Implementation, results, and cost-benefit analysis IEEE Trans Control Syst Technol 24 2016 1 12 D. Sturzenegger, D. Gyalistras, M. Morari, and R. S. Smith. Model Predictive Climate Control of a Swiss Office Building: Implementation, Results, and Cost-Benefit Analysis. IEEE Transactions on Control Systems Technology, 24:1\u201312, 1 2016. https://doi.org/10.1109/TCST.2015.2415411. [19] Mason K. Grijalva S. A review of reinforcement learning for autonomous building energy management Comput Electr Eng 78 2019 300 312 K. Mason and S. Grijalva. A Review of Reinforcement Learning for Autonomous Building Energy Management. Computers & Electrical Engineering, 78:300\u2013312, sep 2019. [20] Drgoňa J. Arroyo J. Cupeiro Figueroa I. Blum D. Arendt K. Kim D. All you need to know about model predictive control for buildings Annu Rev Control 50 2020 190 232 Published on-line https://doi.org/10.1016/j.arcontrol.2020.09.001 J. Drgoňa, J. Arroyo, I. Cupeiro Figueroa, D. Blum, K. Arendt, D. Kim, E. P. Ollé, J. Oravec, M. Wetter, D. L. Vrabie, and L. Helsen. All you need to know about model predictive control for buildings. Annual Reviews in Control, 50:190\u2013232, 2020. Published on-line https://doi.org/10.1016/j.arcontrol.2020.09.001. [21] Drgoňa J, Klaučo M, Kvasnica M. MPC-based reference governors for thermostatically controlled residential buildings. In: Proceedings of the 54th IEEE conference on decision and control. Osaka, Japan; 2015. p. 1334\u20139. http://dx.doi.org/10.1109/CDC.2015.7402396. [22] Jorissen F. Boydens W. Helsen L. TACO, an automated toolchain for model predictive control of building systems: Implementation and verification J Build Perfor Simul 12 2 2018 180 192 F. Jorissen, W. Boydens, and L. Helsen. TACO, an automated toolchain for model predictive control of building systems: implementation and verification. Journal of Building Performance Simulation, 12(2):180\u2013192, 2018. https://doi.org/10.1080/19401493.2018.1498537. [23] Wetter M. Zuo W. Nouidui T. Pang X. Modelica buildings library J Build Perfor Simul 7 4 2014 253 270 M. Wetter, W. Zuo, T. Nouidui, and X. Pang. Modelica buildings library. Journal of Building Performance Simulation, 7(4):253\u2013270, 2014. https://doi.org/10.1080/19401493.2013.765506. [24] Jorissen F. Reynders G. Baetens R. Picard D. Saelens D. Helsen L. Implementation and verification of the IDEAS building energy simulation library J Build Perfor Simul 11 6 2018 669 688 F. Jorissen, G. Reynders, R. Baetens, D. Picard, D. Saelens, and L. Helsen. Implementation and verification of the IDEAS building energy simulation library. Journal of Building Performance Simulation, 11(6):669\u2013688, 2018. https://doi.org/10.1080/19401493.2018.1428361. [25] Müller D, Lauster M, Constantin A, Fuchs M, Remmen P. AixLib \u2013 An open-source modelica library within the IEA-EBC annex 60 framework. In: Proceedings of the BauSIM IBPSA conference. Dresden, Germany; 2016. p. 3\u20139. [26] Nytsch-Geusen C, Banhardt C, Inderfurth A, Mucha K, Möckel J, Rädler J et al. Buildingsystems - Eine modular hierarchische Modell-Bibliothek zur energetischen Gebäude und Anlagensimulation. In: Proceedings of the BauSIM IBPSA conference. Dresden, Germany; 2016. p. 473\u2013480. [27] Coninck R.D. Magnusson F. Åkesson J. Helsen L. Toolbox for development and validation of grey-box building models for forecasting and control J Buil Perform Simul 9 3 2016 288 303 R. D. Coninck, F. Magnusson, J. Åkesson, and L. Helsen. Toolbox for development and validation of grey-box building models for forecasting and control. Journal of Building Performance Simulation, 9(3):288\u2013303, 2016. https://doi.org/10.1080/19401493.2015.1046933. [28] Atam E. Helsen L. Control-oriented thermal modeling of multizone buildings: Methods and issues: Intelligent control of a building system IEEE Control Syst Mag 36 3 2016 86 111 E. Atam and L. Helsen. Control-oriented thermal modeling of multizone buildings: Methods and issues: Intelligent control of a building system. IEEE Control Systems Magazine, 36(3):86\u2013111, June 2016. https://doi.org/10.1109/MCS.2016.2535913. [29] Arroyo J. Spiessens F. Helsen L. Identification of multi-zone grey-box building models for use in model predictive control J Build Perform Simul 13 4 2020 472 486 J. Arroyo, F. Spiessens, and L. Helsen. Identification of multi-zone grey-box building models for use in model predictive control. Journal of Building Performance Simulation, 13(4):472\u2013486, 2020. https://doi.org/10.1080/19401493.2020.1770861. [30] Mnih V. Kavukcuoglu K. Silver D. Rusu A. Veness J. Bellemare M. Human-level control through deep reinforcement learning Nature 518 2015 529 533 V. Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness, M. Bellemare, A. Graves, M. Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529\u201333, 2015. https://doi.org/10.1038/nature14236. [31] Vázquez-Canteli J.R. Nagy Z. Reinforcement learning for demand response: A review of algorithms and modeling techniques Appl Energy 235 2019 1072 1089 J. R. Vázquez-Canteli and Z. Nagy. Reinforcement learning for demand response: A review of algorithms and modeling techniques. Applied Energy, 235:1072\u20131089, 2019. https://doi.org/10.1016/j.apenergy.2018.11.002. [32] Vázquez-Canteli J.R. Ulyanin S. Kämpf J. Nagy Z. Fusing TensorFlow with building energy simulation for intelligent energy management in smart cities Sustainable Cities Soc 45 2019 243 257 J. R. Vázquez-Canteli, S. Ulyanin, J. Kämpf, and Z. Nagy. Fusing TensorFlow with building energy simulation for intelligent energy management in smart cities. Sustainable Cities and Society, 45:243\u2013257, 2019. https://doi.org/10.1016/j.scs.2018.11.021. [33] Nagai T. Dynamic optimization technique for control of HVAC system utilizing building thermal storage. In: Proceedings of the 6th building simulation conference, vol. I. Kyoto, Japan; 1999. p. 1311\u20137. [34] Peirelinck T. Ruelens F. Decnoninck G. Using reinforcement learning for optimizing heat pump control in a building model in modelica Proceedings of the IEEE international energy conference, ENERGYCON 2018 1 6 T. Peirelinck, F. Ruelens, and G. Decnoninck. Using reinforcement learning for optimizing heat pump control in a building model in Modelica. In Proceedings of the IEEE International Energy Conference, ENERGYCON, pages 1\u20136, June 2018. [35] An energy-efficient predictive control for HVAC systems applied to tertiary buildings based on regression techniques. Energy Build 2017;152:409\u201317. [36] Nagy A. Kazmi H. Cheaib F. Driesen J. Deep reinforcement learning for optimal control of space heating 2018 arXiv, https://arxiv.org/abs/1805.03777 A. Nagy, H. Kazmi, F. Cheaib, and J. Driesen. Deep Reinforcement Learning for Optimal Control of Space Heating. ArXiv, 2018. https://arXiv.org/abs/1805.03777. [37] Mbuwir BV, Spiessens F, Deconinck G. Benchmarking regression methods for function approximation in reinforcement learning: Heat pump control. In: Proceedings of the IEEE PES innovative smart grid technologies Europe. Bucharest, Romania; 2019. p. 1\u20135. [38] Patyn C, Ruelens F, Deconinck G. Comparing neural architectures for demand response through model-free reinforcement learning for heat pump control. In: Proceedings of the IEEE international energy conference. Limassol, Cyprus; 2018. p. 1\u20136. http://dx.doi.org/10.1109/ENERGYCON.2018.8398836. [39] Picard D. Drgoňa J. Kvasnica M. Helsen L. Impact of the controller model complexity on model predictive control performance for buildings Energy Build 152 2017 739 751 D. Picard, J. Drgoňa, M. Kvasnica, and L. Helsen. Impact of the controller model complexity on model predictive control performance for buildings. Energy and Buildings, 152:739\u2013751, 2017. https://doi.org/10.1016/j.enbuild.2017.07.027. [40] Zhang Z, Lam K. Practical implementation and evaluation of deep reinforcement learning control for a radiant heating system. In: Proceedings of the 5th ACM international conference on systems for energy-efficient buildings, cities, and transportation. Shenzen, China; 2018. p. 148\u2013157. http://dx.doi.org/10.1145/3276774.3276775. [41] Liu S. Henze G.P. Experimental analysis of simulated reinforcement learning control for active and passive building thermal storage inventory: Part 2: Results and analysis Energy Build 38 2 2006 148 161 S. Liu and G. P. Henze. Experimental analysis of simulated reinforcement learning control for active and passive building thermal storage inventory: Part 2: Results and analysis. Energy and Buildings, 38(2):148\u2013161, 2006. https://doi.org/10.1016/j.enbuild.2005.06.001. [42] Scharnhorst P. Schubnel B. Fernández Bandera C. Salom J. Taddeo P. Boegli M. Energym: A building model library for controller benchmarking Appl Sci 11 8 2021 10.3390/app11083518 P. Scharnhorst, B. Schubnel, C. Fernández Bandera, J. Salom, P. Taddeo, M. Boegli, T. Gorecki, Y. Stauffer, A. Peppas, and C. Politi. Energym: A building model library for controller benchmarking. Applied Sciences, 11(8), 2021. https://doi.org/10.3390/app11083518. [43] Vázquez-Canteli JR, Kämpf J, Henze G, Nagy Z. CityLearn v1.0: An OpenAI gym environment for demand response with deep reinforcement learning. In: Proceedings of the 6th ACM international conference on systems for energy-efficient buildings, cities, and transportation. New York, New York, USA; 2019. p. 356\u20137. http://dx.doi.org/10.1145/3360322.3360998. [44] Abadi M. Agarwal A. Barham P. Brevdo E. Chen Z. Citro C. TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org 2015 arXiv, https://arXiv.org/abs/1603.04467v2 M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org. arXiv, 2015. https://arXiv.org/abs/1603.04467v2. [45] Hewing L. Wabersich K.P. Menner M. Zeilinger M.N. Learning-based model predictive control: Toward safe learning in control Annu Rev Control Robot Auton Syst 3 1 2020 269 296 L. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger. Learning-based model predictive control: Toward safe learning in control. Annual Review of Control, Robotics, and Autonomous Systems, 3(1):269\u2013296, 2020. https://doi.org/10.1146/annurev-control-090419-075625. [46] Liu S. Henze G.P. Experimental analysis of simulated reinforcement learning control for active and passive building thermal storage inventory: Part 1. Theoretical foundation Energy Build 38 2 2006 142 147 S. Liu and G. P. Henze. Experimental analysis of simulated reinforcement learning control for active and passive building thermal storage inventory: Part 1. theoretical foundation. Energy and Buildings, 38(2):142\u2013147, 2006. [47] Amos B. Rodriguez I.D.J. Sacks J. Boots B. Kolter J.Z. Differentiable MPC for end-to-end planning and control 2019 arXiv, arXiv:1810.13400 B. Amos, I. D. J. Rodriguez, J. Sacks, B. Boots, and J. Z. Kolter. Differentiable MPC for end-to-end planning and control. arXiv, page arXiv:1810.13400, 2019. [48] Drgoňa J. Tuor A. Skomski E. Vasisht S. Vrabie D. Deep learning explicit differentiable predictive control laws for buildings 2021 arXiv, arXiv:abs/2107.11843v1 J. Drgoňa, A. Tuor, E. Skomski, S. Vasisht, and D. Vrabie. Deep learning explicit differentiable predictive control laws for buildings. arXiv, page arXiv:2107.11843v1, June 2021. [49] Gros S. Zanon M. Data-driven economic NMPC using reinforcement learning IEEE Trans Automat Control 65 2 2020 636 648 S. Gros and M. Zanon. Data-Driven Economic NMPC Using Reinforcement Learning. IEEE Transactions on Automatic Control, 65(2):636\u2013648, February 2020. [50] Gros S. Zanon M. Reinforcement learning for mixed-integer problems based on MPC 2020 arXiv, arXiv:2004.01430 S. Gros and M. Zanon. Reinforcement Learning for Mixed-Integer Problems Based on MPC. arXiv, page arXiv:2004.01430, 2020. [51] Gros S. Zanon M. Reinforcement learning based on MPC and the stochastic policy gradient method 2021 American control conference 2021 1947 1952 S. Gros and M. Zanon. Reinforcement Learning based on MPC and the Stochastic Policy Gradient Method. In 2021 American Control Conference (ACC), pages 1947\u20131952, 2021. [52] Zanon M. Gros S. Safe reinforcement learning using robust MPC IEEE Trans Automat Control 66 8 2021 3638 3652 M. Zanon and S. Gros. Safe Reinforcement Learning Using Robust MPC. IEEE Transactions on Automatic Control, 66(8):3638\u20133652, August 2021. [53] Gros S. Zanon M. Safe reinforcement learning with stability & safety guarantees using robust MPC 2020 arXiv, arXiv:2012.07369 S. Gros and M. Zanon. Safe Reinforcement Learning with Stability & Safety Guarantees Using Robust MPC. arXiv, page arXiv:2012.07369, 2020. [54] Kamthe S. Deisenroth M.P. Data-efficient reinforcement learning with probabilistic model predictive control CoRR abs/1706.06491 2017 S. Kamthe and M. P. Deisenroth. Data-efficient reinforcement learning with probabilistic model predictive control. CoRR, abs/1706.06491, 2017. [55] Zhang H. Li S. Zheng Y. Q-learning-based model predictive control for nonlinear continuous-time systems Ind Eng Chem Res 59 40 2020 17987 17999 H. Zhang, S. Li, and Y. Zheng. Q-Learning-Based Model Predictive Control for Nonlinear Continuous-Time Systems. Industrial & Engineering Chemistry Research, 59(40):17987\u201317999, 2020. [56] Buşoniu L. B.R. Approximate dynamic programming and reinforcement learning. Technical report Interactive collaborative information systems. Studies in computational intelligence vol. 281 2010 Springer Berlin, Heidelberg B. R. Buşoniu L., De Schutter B. Approximate dynamic programming and reinforcement learning. Technical report, Interactive Collaborative Information Systems. Studies in Computational Intelligence, Berlin, Heidelberg, 2010. [57] Drgoňa J. Picard D. Kvasnica M. Helsen L. Approximate model predictive building control via machine learning Appl Energy 218 2018 199 216 J. Drgoňa, D. Picard, M. Kvasnica, and L. Helsen. Approximate model predictive building control via machine learning. Applied Energy, 218:199\u2013216, 05 2018. 10.1016/j.apenergy.2018.02.156. [58] Kelly M.P. Transcription methods for trajectory optimization: A beginners tutorial 2017 arXiv, https://arxiv.org/abs/1707.00284 M. P. Kelly. Transcription methods for trajectory optimization: a beginners tutorial. arXiv, 2017. https://arxiv.org/abs/1707.00284. [59] Fletcher R. Practical methods of optimization 2nd ed. 1987 Wiley-Interscience https://dl.acm.org/doi/book/10.5555/39857 R. Fletcher. Practical Methods of Optimization. Wiley-Interscience, second edition, 1987. https://dl.acm.org/doi/book/10.5555/39857. [60] Ernst D, Geurts P, Wehenkel L. Iteratively extending time horizon reinforcement learning. In: Lavrač N, Gamberger D, Blockeel H, Todorovski L, editors. Proceedings of the 14th European conference on machine learning. Dubrovnik, Croatia; 2003. p. 96\u2013107. [61] Chua K. Calandra R. McAllister R. Levine S. Deep reinforcement learning in a handful of trials using probabilistic dynamics models 2018 arXiv, http://arxiv.org/abs/1805.12114 K. Chua, R. Calandra, R. McAllister, and S. Levine. Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models. arXiv, 2018. http://arxiv.org/abs/1805.12114. [62] Bhardwaj M. Handa A. Fox D. Boots B. Information theoretic model predictive Q-learning 2020 arXiv, https://arXiv.org/abs/2001.02153 M. Bhardwaj, A. Handa, D. Fox, and B. Boots. Information Theoretic Model Predictive Q-Learning. arXiv, 2020. https://arXiv.org/abs/2001.02153. [63] Mnih V. Kavukcuoglu K. Silver D. Graves A. Antonoglou I. Wierstra D. Playing atari with deep reinforcement learning 2013 arXiv, http://arxiv.org/abs/1312.5602 V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. A. Riedmiller. Playing Atari with Deep Reinforcement Learning. arXiv, 2013. http://arxiv.org/abs/1312.5602. [64] Silver D. Hubert T. Schrittwieser J. Antonoglou I. Lai M. Guez A. Mastering Chess and Shogi by self-play with a general reinforcement learning algorithm 2017 arXiv, http://arxiv.org/abs/1712.01815 D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm. arXiv, 2017. http://arxiv.org/abs/1712.01815. [65] Weber T. Racanière S. Reichert D.P. Buesing L. Guez A. Rezende D.J. Imagination-augmented agents for deep reinforcement learning 2017 arXiv, http://arxiv.org/abs/1707.06203 T. Weber, S. Racanière, D. P. Reichert, L. Buesing, A. Guez, D. J. Rezende, A. P. Badia, O. Vinyals, N. Heess, Y. Li, R. Pascanu, P. W. Battaglia, D. Silver, and D. Wierstra. Imagination-augmented agents for deep reinforcement learning. arXiv, 2017. http://arxiv.org/abs/1707.06203. [66] Cupeiro Figueroa I, Drgoňa J, Helsen L. State estimators applied to a linear white-box geothermal borefield controller model. In: Proceedings of the 16th international conference of IBPSA. 2019. [67] Vande Cavey M, Bonvini M, Helsen L. Comparison and application of different state estimation techniques for control in buildings. In: Workshop on optimal control of thermal systems in buildings using modelica. Freiburg, Germany; 2015. [68] Åkesson J. Årzén K.E. Gäfvert M. Bergdahl T. Tummescheit H. Modeling and optimization with optimica and JModelica.org-Languages and tools for solving large-scale dynamic optimization problems Comput Chem Eng 34 2010 1737 1749 J. Åkesson, K. E. Årzén, M. Gäfvert, T. Bergdahl, and H. Tummescheit. Modeling and optimization with Optimica and JModelica.org-Languages and tools for solving large-scale dynamic optimization problems. Computers and Chemical Engineering, 34:1737\u20131749, 11 2010. https://doi.org/10.1016/j.compchemeng.2009.11.011. [69] Axelsson M, Magnusson F, Henningsson T. A framework for nonlinear model predictive control in JModelica.org. In: Proceedings of the 11th international modelica conference, vol. 118. Versailles, France. 2015. p. 301\u201310. [70] Andersson J. Gillis J. Horn G. Rawlings J. Diehl M. CasADi \u2013 A software framework for nonlinear optimization and optimal control Math Program Comput 11 1 2019 1 36 J. Andersson, J. Gillis, G. Horn, J. Rawlings, and M. Diehl. CasADi \u2013 a software framework for nonlinear optimization and optimal control. Mathematical Programming Computation, 11(1):1\u201336, 2019. https://doi.org/10.1007/s12532-018-0139-4. [71] Sun F, Li G, Wang J. Unscented Kalman Filter using augmented state in the presence of additive noise. In: Proceedings of the IITA international conference on control, automation and systems engineering. Zhangjiajie, China; 2009, p. 379\u201382. F. Sun, G. Li, and J. Wang. Unscented Kalman Filter using augmented state in the presence of additive noise. In Proceedings of the IITA International Conference on Control, Automation and Systems Engineering, pages 379\u2013382, Zhangjiajie, China, 2009. [72] Wan EA, Merwe RVD. The unscented Kalman filter for nonlinear estimation. In: Proceedings of the IEEE adaptive systems for signal processing, communications, and control symposium. Alberta, Canada; 2000. p. 153\u2013158. [73] Brockman G. Cheung V. Pettersson L. Schneider J. Schulman J. Tang J. OpenAI Gym 2016 arXiv, http://arxiv.org/abs/1606.01540 G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI Gym. arXiv, 2016. http://arxiv.org/abs/1606.01540. [74] van Hasselt H. Guez A. Silver D. Deep reinforcement learning with double Q-learning 2015 arXiv, http://arxiv.org/abs/1509.06461 H. van Hasselt, A. Guez, and D. Silver. Deep Reinforcement Learning with Double Q-learning. arXiv, 2015. http://arxiv.org/abs/1509.06461. [75] Technical Committee CENTC 156 \u201cVentilation for Buildings\u201d H. EN16798. Energy performance of buildings - Ventilation for buildings - Part 2: Interpretation of the requirements in EN 16798-1 - Guideline for using indoor environmental input parameters for the design and assessment of energy performance of buildings 2019 BSI London, BSI Technical Committee CENTC 156 \u201cVentilation for Buildings\u201d. EN16798. Energy performance of buildings - Ventilation for buildings - Part 2: Interpretation of the requirements in EN 16798-1 - Guideline for using indoor environmental input parameters for the design and assessment of energy performance of buildings. Technical report, BSI, London, BSI, 2019. [76] Barbato A. Bolchini C. Geronazzo A. Quintarelli E. Palamarciuc A. Pitì A. Energy optimization and management of demand response interactions in a smart campus Energies 9 2016 398 A. Barbato, C. Bolchini, A. Geronazzo, E. Quintarelli, A. Palamarciuc, A. Pitì, C. Rottondi, and G. Verticale. Energy Optimization and Management of Demand Response Interactions in a Smart Campus. Energies, 9:398, 05 2016. https://doi.org/10.3390/en9060398.",
    "scopus-id": "85122543142",
    "coredata": {
        "eid": "1-s2.0-S0306261921015932",
        "dc:description": "Buildings need advanced control for the efficient and climate-neutral use of their energy systems. Model predictive control (MPC) and reinforcement learning (RL) arise as two powerful control techniques that have been extensively investigated in the literature for their application to building energy management. These methods show complementary qualities in terms of constraint satisfaction, computational demand, adaptability, and intelligibility, but usually a choice is made between both approaches. This paper compares both control approaches and proposes a novel algorithm called reinforced predictive control (RL-MPC) that merges their relative merits. First, the complementarity between RL and MPC is emphasized on a conceptual level by commenting on the main aspects of each method. Second, the RL-MPC algorithm is described that effectively combines features from each approach, namely state estimation, dynamic optimization, and learning. Finally, MPC, RL, and RL-MPC are implemented and evaluated in BOPTEST, a standardized simulation framework for the assessment of advanced control algorithms in buildings. The results indicate that pure RL cannot provide constraint satisfaction when using a control formulation equivalent to MPC and the same controller model for learning. The new RL-MPC algorithm can meet constraints and provide similar performance to MPC while enabling continuous learning and the possibility to deal with uncertain environments.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2022-03-01",
        "openaccessUserLicense": "http://creativecommons.org/licenses/by/4.0/",
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S0306261921015932",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Arroyo, Javier"
            },
            {
                "@_fa": "true",
                "$": "Manna, Carlo"
            },
            {
                "@_fa": "true",
                "$": "Spiessens, Fred"
            },
            {
                "@_fa": "true",
                "$": "Helsen, Lieve"
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S0306261921015932"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S0306261921015932"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": "Full",
        "pii": "S0306-2619(21)01593-2",
        "prism:volume": "309",
        "articleNumber": "118346",
        "prism:publisher": "The Authors. Published by Elsevier Ltd.",
        "dc:title": "Reinforced model predictive control (RL-MPC) for building energy management",
        "prism:copyright": "© 2022 The Authors. Published by Elsevier Ltd.",
        "openaccess": "1",
        "prism:issn": "03062619",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "Model predictive control"
            },
            {
                "@_fa": "true",
                "$": "Reinforcement learning"
            },
            {
                "@_fa": "true",
                "$": "Reinforced model predictive control"
            },
            {
                "@_fa": "true",
                "$": "Building automation"
            },
            {
                "@_fa": "true",
                "$": "BOPTEST"
            }
        ],
        "openaccessArticle": "true",
        "prism:publicationName": "Applied Energy",
        "openaccessSponsorType": "Author",
        "prism:pageRange": "118346",
        "pubType": "fla",
        "prism:coverDisplayDate": "1 March 2022",
        "prism:doi": "10.1016/j.apenergy.2021.118346",
        "prism:startingPage": "118346",
        "dc:identifier": "doi:10.1016/j.apenergy.2021.118346",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "572",
            "@width": "1054",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "213597",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "512",
            "@width": "620",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "206468",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "289",
            "@width": "366",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "95502",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "606",
            "@width": "785",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr8.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "218152",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "259",
            "@width": "496",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr9.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "119659",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "277",
            "@width": "783",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "96009",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "236",
            "@width": "647",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "92489",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "271",
            "@width": "591",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "92358",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "196",
            "@width": "817",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-fx1001.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "93703",
            "@ref": "fx1001",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "418",
            "@width": "663",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "121597",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "119",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "76469",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "198",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "83998",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "207",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "72444",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "212",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr8.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "80774",
            "@ref": "gr8",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "114",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr9.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "75556",
            "@ref": "gr9",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "77",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "69094",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "80",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "70850",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "100",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "70423",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "53",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-fx1001.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "67824",
            "@ref": "fx1001",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "138",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "72525",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "2531",
            "@width": "4667",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1179629",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2269",
            "@width": "2745",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1068041",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1281",
            "@width": "1622",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "230726",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2680",
            "@width": "3474",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr8_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1254829",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1147",
            "@width": "2197",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr9_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "375970",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1224",
            "@width": "3465",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "299700",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1046",
            "@width": "2863",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "271790",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1201",
            "@width": "2618",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "255720",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "868",
            "@width": "3618",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-fx1001_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "265242",
            "@ref": "fx1001",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1850",
            "@width": "2934",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "465363",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si1.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1050",
            "@ref": "si1",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si10.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3206",
            "@ref": "si10",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si102.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1323",
            "@ref": "si102",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si104.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3249",
            "@ref": "si104",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si105.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2458",
            "@ref": "si105",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si111.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1389",
            "@ref": "si111",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si112.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2626",
            "@ref": "si112",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si113.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7382",
            "@ref": "si113",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si116.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3959",
            "@ref": "si116",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si12.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1441",
            "@ref": "si12",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si121.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2743",
            "@ref": "si121",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si122.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3792",
            "@ref": "si122",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si123.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2381",
            "@ref": "si123",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si124.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5129",
            "@ref": "si124",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si125.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3498",
            "@ref": "si125",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si126.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1423",
            "@ref": "si126",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si127.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "34873",
            "@ref": "si127",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si128.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "25676",
            "@ref": "si128",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si129.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7602",
            "@ref": "si129",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si13.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2694",
            "@ref": "si13",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si130.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4507",
            "@ref": "si130",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si131.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6371",
            "@ref": "si131",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si133.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1663",
            "@ref": "si133",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si134.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3217",
            "@ref": "si134",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si135.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2375",
            "@ref": "si135",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si137.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5136",
            "@ref": "si137",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si138.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5442",
            "@ref": "si138",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si139.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1129",
            "@ref": "si139",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si14.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1231",
            "@ref": "si14",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si142.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2396",
            "@ref": "si142",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si143.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2395",
            "@ref": "si143",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si146.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4576",
            "@ref": "si146",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si147.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4290",
            "@ref": "si147",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si157.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3750",
            "@ref": "si157",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si158.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4889",
            "@ref": "si158",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si159.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2797",
            "@ref": "si159",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si16.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1678",
            "@ref": "si16",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si160.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7094",
            "@ref": "si160",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si161.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11176",
            "@ref": "si161",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si162.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2756",
            "@ref": "si162",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si163.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7755",
            "@ref": "si163",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si164.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1244",
            "@ref": "si164",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si169.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1425",
            "@ref": "si169",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si17.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1360",
            "@ref": "si17",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si171.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2184",
            "@ref": "si171",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si176.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5756",
            "@ref": "si176",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si18.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1256",
            "@ref": "si18",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si180.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2599",
            "@ref": "si180",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si181.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2700",
            "@ref": "si181",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si185.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2181",
            "@ref": "si185",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si186.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2305",
            "@ref": "si186",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si19.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1603",
            "@ref": "si19",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si2.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1625",
            "@ref": "si2",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si20.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1471",
            "@ref": "si20",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si21.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7396",
            "@ref": "si21",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si22.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1083",
            "@ref": "si22",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si23.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "22942",
            "@ref": "si23",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si3.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1690",
            "@ref": "si3",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si30.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1276",
            "@ref": "si30",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si31.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1173",
            "@ref": "si31",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si32.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4304",
            "@ref": "si32",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si33.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1541",
            "@ref": "si33",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si37.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4321",
            "@ref": "si37",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si38.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4582",
            "@ref": "si38",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si39.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3886",
            "@ref": "si39",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si4.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1136",
            "@ref": "si4",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si40.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6801",
            "@ref": "si40",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si41.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4929",
            "@ref": "si41",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si42.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1453",
            "@ref": "si42",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si43.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4561",
            "@ref": "si43",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si44.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "15257",
            "@ref": "si44",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si45.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4956",
            "@ref": "si45",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si47.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1173",
            "@ref": "si47",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si5.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1530",
            "@ref": "si5",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si50.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3294",
            "@ref": "si50",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si51.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6427",
            "@ref": "si51",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si53.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1507",
            "@ref": "si53",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si58.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5572",
            "@ref": "si58",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si59.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3270",
            "@ref": "si59",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si6.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1132",
            "@ref": "si6",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si60.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6293",
            "@ref": "si60",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si62.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2445",
            "@ref": "si62",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si63.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1978",
            "@ref": "si63",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si64.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1615",
            "@ref": "si64",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si66.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1405",
            "@ref": "si66",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si67.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5284",
            "@ref": "si67",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si69.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1378",
            "@ref": "si69",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si7.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1425",
            "@ref": "si7",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si72.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1416",
            "@ref": "si72",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si73.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1098",
            "@ref": "si73",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si74.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1992",
            "@ref": "si74",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si76.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6476",
            "@ref": "si76",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si8.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1347",
            "@ref": "si8",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si80.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3175",
            "@ref": "si80",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si81.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "20889",
            "@ref": "si81",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si82.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3216",
            "@ref": "si82",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si83.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1458",
            "@ref": "si83",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si87.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "19600",
            "@ref": "si87",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si88.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4691",
            "@ref": "si88",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si9.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2948",
            "@ref": "si9",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si91.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "18396",
            "@ref": "si91",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si92.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "14374",
            "@ref": "si92",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si95.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7040",
            "@ref": "si95",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si97.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "17059",
            "@ref": "si97",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si98.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "36001",
            "@ref": "si98",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-si99.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "29684",
            "@ref": "si99",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261921015932-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "1157835",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85122543142"
    }
}}