{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85126650177",
    "originalText": "serial JL 271429 291210 291702 291731 291787 291877 291878 291881 31 90 Applied Energy APPLIEDENERGY 2022-03-22 2022-03-22 2022-03-22 2022-03-22 2022-11-21T15:00:06 1-s2.0-S0306261922002689 S0306-2619(22)00268-9 S0306261922002689 10.1016/j.apenergy.2022.118825 S300 S300.2 FULL-TEXT 1-s2.0-S0306261922X00068 2024-01-01T13:30:21.133674Z 0 0 20220515 2022 2022-03-22T11:48:11.338616Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table e-component body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor highlightsabst orcid primabst ref 0306-2619 03062619 UNLIMITED JISC2022 true 314 314 C Volume 314 38 118825 118825 118825 20220515 15 May 2022 2022-05-15 2022 Research Papers article fla © 2022 The Authors. Published by Elsevier Ltd. SCALABLEMULTIAGENTREINFORCEMENTLEARNINGFORDISTRIBUTEDCONTROLRESIDENTIALENERGYFLEXIBILITY CHARBONNIER F 1 Introduction 2 MARL-based energy coordination: literature review and gap analysis 3 Local system description 3.1 Variables 3.2 Objective function 3.3 Constraints 4 Reinforcement learning methodology 4.1 Q-learning 4.2 Agent state 4.3 Agent action 4.4 Variations of the learning method 4.4.1 Experience sources 4.4.2 MARL structures 4.4.3 Reward definitions 5 Input data 5.1 Data selection and pre-processing 5.2 Markov chain 6 Case study results and discussion 6.1 Set-up 6.2 Results 6.2.1 Environment exploration-based learning 6.2.2 Optimisation-based learning 7 Conclusion CRediT authorship contribution statement Acknowledgement Appendix A Nomenclature Appendix B Case study input data Appendix C Supplementary data References MATIGNON 2012 1 31 L MASSONDELMOTTE 2018 V GLOBALWARMING15CIPCCSPECIALREPORTIMPACTSGLOBALWARMING15CABOVEPREINDUSTRIALLEVELSRELATEDGLOBALGREENHOUSEGASEMISSIONPATHWAYSINCONTEXTSTRENGTHENINGGLOBALRESPONSETHREATCLIMATECHANGE BOSE 2019 29 45 S SMARTGRIDCONTROLPOWERELECEDITION EMERGINGCHALLENGESINELECTRICITYMARKETS LEAUTIER 2019 T IMPERFECTMARKETSIMPERFECTREGULATIONINTRODUCTIONMICROECONOMICSPOLITICALECONOMYPOWERMARKETS VAZQUEZCANTELI 2019 1072 1089 J PUMPHREY 2020 101603 K DEPARTMENTFORBUSINESSENERGYANDINDUSTRIALSTRATEGY 2021 ENERGYCONSUMPTIONINUK AGENCY 2018 I RENEWABLEPOWERGENERATIONCOSTSIN2018 BLOOMBERNEF 2019 2019BATTERYPRICESURVEY CHARLESRIVERASSOCIATES 2017 ASSESSMENTECONOMICVALUEDEMANDSIDEPARTICIPATIONINBALANCINGMECHANISMEVALUATIONOPTIONSIMPROVEACCESS CHEN 2019 4338 4348 T BUGDEN 2019 137 145 D MORET 2019 3994 4004 F BOYD 2009 25 S CONVEXOPTIMIZATIONTHEORY DASGUPTA 2016 S COMPUTERSCIENCEASHORTINTRODUCTION RECHT 2018 B FRANCCCOISLAVET 2017 V CONTRIBUTIONSDEEPREINFORCEMENTLEARNINGAPPLICATIONSINSMARTGRIDS SEN 1994 426 431 S RUELENS 2017 2149 2159 F WOOLDRIDGE 2002 M INTELLIGENTAGENTSKEYCONCEPTS MORSTYN 2018 94 101 T SUTTON 1998 R REINFORCEMENTLEARNINGINTRODUCTIONELECTRONICRESOURCEADAPTIVECOMPUTATIONMACHINELEARNING ANTONOPOULOS 2020 109899 I SCHELLENBERG 2020 109966 C CHARBONNIER 2022 F COORDINATIONRESOURCESEDGEELECTRICITYGRIDSYSTEMATICREVIEWTAXONOMY ONEILL 2010 409 414 D 2010FIRSTIEEEINTERNATIONALCONFERENCESMARTGRIDCOMMUNICATIONS RESIDENTIALDEMANDRESPONSEUSINGREINFORCEMENTLEARNING DARBY 2020 111573 S POWELL 2011 W APPROXIMATEDYNAMICPROGRAMMINGSOLVINGCURSESDIMENSIONALITY LU 2019 937 949 R KIM 2016 2187 2198 B BABAR 2018 6118 6127 M YE 2020 1343 1355 Y DAUER 2013 102 107 D PROCEEDINGS2013IEEEWICACMINTERNATIONALCONFERENCEINTELLIGENTAGENTTECHNOLOGYIAT2013VOL2 MARKETBASEDEVCHARGINGCOORDINATION SUN 2015 2912 2917 Y PROCEEDINGSAMERICANCONTROLCONFERENCE2015 LEARNINGBASEDBIDDINGSTRATEGYFORHVACSYSTEMSINDOUBLEAUCTIONRETAILENERGYMARKETS KIM 2020 J CLAESSENS 2013 1 5 B 20134THIEEEPESINNOVATIVESMARTGRIDTECHNOLOGIESEUROPEISGTEUROPE2013 PEAKSHAVINGAHETEROGENEOUSCLUSTERRESIDENTIALFLEXIBILITYCARRIERSUSINGREINFORCEMENTLEARNING ZHANG 2017 348 365 X DUSPARIC 2015 I 2015IEEE1STINTERNATIONALSMARTCITIESCONFERENCE MAXIMIZINGRENEWABLEENERGYUSEDECENTRALIZEDRESIDENTIALDEMANDRESPONSE DUSPARIC 2013 90 96 I 20131STIEEECONFERENCETECHNOLOGIESFORSUSTAINABILITY MULTIAGENTRESIDENTIALDEMANDRESPONSEBASEDLOADFORECASTING HURTADO 2018 127 136 L MORSTYN 2020 T ANALYTICSFORSHARINGECONOMYMATHEMATICSENGINEERINGBUSINESSPERSPECTIVESMARCH PEERTOPEERENERGYTRADING TAYLOR 2014 2298 2305 A HERBERT 1982 S MODELSBOUNDEDRATIONALITY GUERRERO 2020 J CAO 2019 1 9 J YANG 2019 630 636 Y LARGESCALEHOMEENERGYMANAGEMENTUSINGENTROPYBASEDCOLLECTIVEMULTIAGENTDEEPREINFORCEMENTLEARNINGFRAMEWORK CROZIER 2018 474 481 C ROZADA 2020 0 4 S KRAEMER 2016 82 94 L BUSONIU 2008 156 172 L PARRY 2007 M PUBLISHEDFORINTERGOVERNMENTALPANELCLIMATECHANGEBY CLIMATECHANGE2007IMPACTSADAPTATIONVULNERABILITY MORSTYN 2019 4005 4014 T COFFRIN 2012 1 8 C MORSTYN 2020 3095 3106 T DUFOLOPEZ 2014 242 253 R ISO 2007 CALCULATIONENERGYUSEFORSPACEHEATINGCOOLINGISOFDIS137902007E SACHS 2012 O FIELDEVALUATIONPROGRAMMABLETHERMOSTATS MATIGNON 2007 64 69 L PROCEEDINGS2007IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMS HYSTERETICQLEARNINGALGORITHMFORDECENTRALIZEDREINFORCEMENTLEARNINGINCOOPERATIVEMULTIAGENTTEAMS VINYALS 2019 O WOLPERT 2002 D FOERSTER 2018 2974 2982 J 32NDAAAICONFERENCEARTIFICIALINTELLIGENCEAAAI2018 COUNTERFACTUALMULTIAGENTPOLICYGRADIENTS WARDLE 2014 R DATASETTC1ABASICPROFILINGDOMESTICSMARTMETERCUSTOMERS WARDLE 2014 R DATASETTC5ENHANCEDPROFILINGDOMESTICCUSTOMERSSOLARPHOTOVOLTAICSPV DEPARTMENTFORTRANSPORT 2019 NATIONALTRAVELSURVEY20022017 CROZIER 2018 1 7 C 20THPOWERSYSTEMSCOMPUTATIONCONFERENCE NUMERICALANALYSISNATIONALTRAVELDATAASSESSIMPACTUKFLEETELECTRIFICATION LLOYD 1982 129 137 S HIRST 2018 D COMMONSBRIEFINGPAPERSNO5927CARBONPRICEFLOORCPFPRICESUPPORTMECHANISM WEATHERWUNDERGROUND 2020 LONDONCITYAIRPORTWEATHERHISTORY OCTOPUSENERGY 2019 OCTOPUSENERGYAPI NATIONALGRIDESO 2020 ENVIRONMENTALDEFENSEFUNDEUROPE TAN 1993 M MULTIAGENTREINFORCEMENTLEARNINGINDEPENDENTVSCOOPERATIVEAGENTS RASHID 2020 T HOMERENERGY 2020 HOMERPRO314USERMANUAL SCHRAM 2020 W SEST20203RDINTERNATIONALCONFERENCESMARTENERGYSYSTEMSTECHNOLOGIESOCTOBER EMPIRICALEVALUATIONV2GROUNDTRIPEFFICIENCY BECKER 2018 V BRE 2014 SAP2012992GOVERNMENTSSTANDARDASSESSMENTPROCEDUREFORENERGYRATINGDWELLINGS BRITISHSTANDARDS 2009 1 89 HEATINGSYSTEMSINBUILDINGSMETHODFORCALCULATIONDESIGNHEATLOAD CHARBONNIERX2022X118825 CHARBONNIERX2022X118825XF Full 2022-05-05T09:45:15Z FundingBody JISC UK 2022: Hybrid journals http://creativecommons.org/licenses/by/4.0/ 2023-03-22T00:00:00.000Z 2023-03-22T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ This is an open access article under the CC BY license. © 2022 The Authors. Published by Elsevier Ltd. 2022-05-05T09:40:20.571Z http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/aggregated-refined 0 item S0306-2619(22)00268-9 S0306261922002689 1-s2.0-S0306261922002689 10.1016/j.apenergy.2022.118825 271429 2024-01-01T13:30:21.133674Z 2022-05-15 UNLIMITED JISC2022 1-s2.0-S0306261922002689-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/MAIN/application/pdf/8b145621f913a9efe4d0e6c2b77265f8/main.pdf main.pdf pdf true 1030305 MAIN 12 1-s2.0-S0306261922002689-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/PREVIEW/image/png/2e238f7f722ebfa87bb63d3f8831f13e/main_1.png main_1.png png 59405 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0306261922002689-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/gr2/DOWNSAMPLED/image/jpeg/7e98bd46bf59dc39ad408fb64dc17b91/gr2.jpg gr2 gr2.jpg jpg 108438 200 489 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922002689-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/gr3/DOWNSAMPLED/image/jpeg/c4cd82553be3a41e3e112e799f0626e1/gr3.jpg gr3 gr3.jpg jpg 105692 211 523 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922002689-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/gr1/DOWNSAMPLED/image/jpeg/b4106a7b3e424d8a08454a6c07e110c8/gr1.jpg gr1 gr1.jpg jpg 113073 222 516 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922002689-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/gr4/DOWNSAMPLED/image/jpeg/3ee9cae9b391023b858c8787e97c8c3e/gr4.jpg gr4 gr4.jpg jpg 169088 330 697 IMAGE-DOWNSAMPLED 1-s2.0-S0306261922002689-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/gr2/THUMBNAIL/image/gif/79e90a4cb0ae9a1a4e7aec510c2ec1d9/gr2.sml gr2 gr2.sml sml 74916 90 219 IMAGE-THUMBNAIL 1-s2.0-S0306261922002689-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/gr3/THUMBNAIL/image/gif/87e94389fd40c6a4c6657dda9b5b955b/gr3.sml gr3 gr3.sml sml 75631 88 219 IMAGE-THUMBNAIL 1-s2.0-S0306261922002689-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/gr1/THUMBNAIL/image/gif/00ae31280bf3b95747bbb335588316c1/gr1.sml gr1 gr1.sml sml 73347 94 219 IMAGE-THUMBNAIL 1-s2.0-S0306261922002689-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/gr4/THUMBNAIL/image/gif/d3c87f3edee25f618483f2fd976be4d0/gr4.sml gr4 gr4.sml sml 78536 104 219 IMAGE-THUMBNAIL 1-s2.0-S0306261922002689-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/gr2/HIGHRES/image/jpeg/c17c8e805237e417e1a95f41fff934b7/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 341351 887 2167 IMAGE-HIGH-RES 1-s2.0-S0306261922002689-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/gr3/HIGHRES/image/jpeg/0193592512add6bd6e10261d67b3e952/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 368946 935 2316 IMAGE-HIGH-RES 1-s2.0-S0306261922002689-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/gr1/HIGHRES/image/jpeg/6d2cd8553f549e15f87b60c5641070df/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 337601 983 2285 IMAGE-HIGH-RES 1-s2.0-S0306261922002689-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/gr4/HIGHRES/image/jpeg/45000e2fd22182940083b44b968b8174/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 807015 1460 3085 IMAGE-HIGH-RES 1-s2.0-S0306261922002689-mmc2.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/mmc2/MAIN/application/pdf/c2ebfe14b7b7ed64d72ec12f33353060/mmc2.pdf mmc2 mmc2.pdf pdf false 326845 APPLICATION 1-s2.0-S0306261922002689-mmc1.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261922002689/mmc1/MAIN/application/pdf/b1e0e29f7feb91b283834341736da2d3/mmc1.pdf mmc1 mmc1.pdf pdf false 347752 APPLICATION 1-s2.0-S0306261922002689-am.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:108XGHJ5G6C/MAIN/application/pdf/24f1cc1ecf3441cc2256792c8cd4bf2e/am.pdf am am.pdf pdf false 1223508 AAM-PDF APEN 118825 118825 S0306-2619(22)00268-9 10.1016/j.apenergy.2022.118825 The Authors Fig. 1 Local system model. Red dotted lines denote energy balances. Fig. 2 Decision variable ψ . Sections 1\u20135 denote the trade-off regimes described in Section 4.3. At each step, the fixed requirements for loads, heat and upcoming EV trips are first met. The ψ decision then applies to the remaining flexibility, from maximal energy exports (full use of flexibility) at ψ = 0 , to maximal energy imports (no use of flexibility) at ψ = 1 . d tot and d fixed are the sum of household and heating loads with and without their flexible component. If fixed loads cannot be fully met by PV energy, the residual is met by storage and imports (2). If there is additional PV energy after meeting all loads, it can be stored or exported (4). Fig. 3 Scaling factors for normalised profiles (i.e. total daily loads in kWh) in subsequent days. Linear correlation can be observed for the load profiles, while more complex patterns are exhibited for EV consumption. ρ is the Pearson correlation coefficient. Fig. 4 The left-hand side plot shows the five-epoch moving average of evaluation rewards relative to baseline rewards for a single prosumer. The right-hand side plot shows the mean of the final 10 evaluations against the number of prosumers. Lines show median values and shaded areas the 25th and 75th percentiles over the 10 repetitions. The best-performing MARL structure is displayed for each exploration source and reward definition pair. The performance of the baseline MARL algorithm (TE, orange) drops as the number of concurrently learning agents in the stochastic environment increases; the best-performing alternative algorithm proposed (MO, purple) maintains high performance at scale. Table 1 Markov chain mechanism for selecting behaviour clusters, profiles and scaling factors for input data in subsequent days. Normalised profile Scaling factor PV Randomly selected from current month bank b t + 1 = ( m ) Computed as λ t + 1 = λ t + x , where x ∼ Γ α ( b t , b t + 1 ) , β ( b t , b t + 1 ) Load Cluster selected based on transition probability p ( k t + 1 | k t , w t , w t + 1 ) Normalised profile randomly selected from bank b t + 1 = ( k t + 1 , w t + 1 ) EV Random variable from discrete distribution p ( λ t + 1 | λ t , b t , b t + 1 ) Table A.2 Nomenclature. Acronyms AE MARL with advantage rewards and exploration-based learning AO MARL using advantage rewards and optimisation-based learning AI Artificial intelligence CLNR Customer-led network revolution CO MARL using count rewards and optimisation-based learning ME MARL using marginal rewards and exploration-based learning MO MARL using marginal rewards and optimisation-based learning Dec-POMDP Decentralised partially observable Markov decision process DER Distributed energy resource DR Demand response EV Electric vehicle MARL Multi-agent reinforcement learning NTS National travel survey PV Photovoltaic RL Reinforcement learning TE MARL using total rewards and exploration-based learning TO MARL using total rewards and optimisation-based learning UK United Kingdom Variables b in Charge into the battery [kWh] b in ¯ Maximum charge into the battery [kWh] b out Discharge out of the battery [kWh] c Household consumption [kWh] c ˆ Partial consumption for load type and time demanded [kWh] c d Distribution cost [£] C d Distribution charge [£/kWh] c g Grid cost [£] C g Grid cost coefficient [£/kWh] c s Storage cost [£] C s Battery depreciation coefficient [£/kWh] d Household demand [kWh] d EV Electric vehicle demand [kWh] d fixed Sum of non-flexible household and heating loads [kWh] d tot Sum of household all heating loads [kWh] E Battery energy level [kWh] E 0 Initial battery energy level [kWh] E ̲ Minimum battery energy level [kWh] E ¯ Maximum battery energy level [kWh] f Flexibility boolean F Objective function [£] F ˆ Share of objective function for given time step g Total grid import to the group of prosumers [kWh] h Heating energy consumption [kWh] k Behaviour cluster for transport or household consumption profile p Prosumer import [kWh] p PV PV generation [kWh] Q Q value [£] Q ˆ Q value estimate [£] r Global reward [£] R Average resistance between the main grid and the prosumers [ Ω ] T e External temperature [ o C] T m Building mass temperature [ o C] T air Indoor air temperature [ o C] T ̲ Minimum indoor air temperature [ o C] T ¯ Maximum indoor air temperature [ o C] U Uniform distribution function V Nominal root mean square grid voltage [V] V ˆ State-value estimate [£] Greek letters α 0 Base learning rate [\u2013] α Learning rate [\u2013] β Hysteretic learning rate reduction factor [\u2013] γ Discount factor [\u2013] δ Loss [£] ε ch Battery charging losses [kWh] ε dis Battery discharging losses [kWh] ε Share of random action selection during exploration [\u2013] η ch Battery charging efficiency [\u2013] η dis Battery discharging efficiency [\u2013] κ Matrix of heating model coefficients λ Scaling factor for transport or household consumption profile [kWh] μ Electric vehicle availability boolean π Policy ϕ Solar heat flow rate [J s−1] ψ Local flexibility use decision variable [\u2013] Indexes a Action i Prosumer s State t Time step t C Consumption time step t D Demand time step w Day type (week day or weekend day) Sets A Set of actions T Set of time steps P Set of prosumers S Set of states Scalable multi-agent reinforcement learning for distributed control of residential energy flexibility Flora Charbonnier Data curation Formal analysis Investigation Methodology Writing \u2013 original draft a \u204e Thomas Morstyn Supervision Validation Writing \u2013 review & editing b Malcolm D. McCulloch Supervision Validation Writing \u2013 review & editing a a Department of Engineering Science, University of Oxford, UK Department of Engineering Science, University of Oxford UK Department of Engineering Science, University of Oxford, UK b School of Engineering, University of Edinburgh, UK School of Engineering, University of Edinburgh UK School of Engineering, University of Edinburgh, UK \u204e Corresponding author. This paper proposes a novel scalable type of multi-agent reinforcement learning-based coordination for distributed residential energy. Cooperating agents learn to control the flexibility offered by electric vehicles, space heating and flexible loads in a partially observable stochastic environment. In the standard independent Q-learning approach, the coordination performance of agents under partial observability drops at scale in stochastic environments. Here, the novel combination of learning from off-line convex optimisations on historical data and isolating marginal contributions to total rewards in reward signals increases stability and performance at scale. Using fixed-size Q-tables, prosumers are able to assess their marginal impact on total system objectives without sharing personal data either with each other or with a central coordinator. Case studies are used to assess the fitness of different combinations of exploration sources, reward definitions, and multi-agent learning frameworks. It is demonstrated that the proposed strategies create value at individual and system levels thanks to reductions in the costs of energy imports, losses, distribution network congestion, battery depreciation and greenhouse gas emissions. Keywords Energy management system Multi-agent reinforcement learning Demand-side response Peer-to-peer Prosumer Smart grid 1 Introduction This paper addresses the scalability issue of distributed domestic energy flexibility coordination in a cost-efficient and privacy-preserving manner. A novel class of coordination strategies using optimisation-based multi-agent reinforcement learning (MARL 1 1 A full nomenclature is available in Appendix A. ) with fixed Q-table size is proposed for household-level decision-making, tackling the challenge of scalability for simultaneously learning independent agents under partial observability in a stochastic environment [1]. Multiple versions of the novel strategy are assessed to maximise the statistical expectation of system-wide benefits, including local battery costs, grid costs and greenhouse gas emissions. Widespread electrification of primary energy provision and decarbonisation of the power sector are two vital prerequisites for limiting anthropogenic global warming to 1.5 °C above pre-industrial levels. To reduce risks of climate-related impacts on health, livelihood, security and economic growth, intermittent renewable power supplies could be required to supply 70% to 85% of electricity by 2050 [2]. However, this poses the challenges of the intermittency and limited controllability of resources [3]. Therefore, a robust, decarbonised power system will rely on two structural features: decentralisation and demand response (DR) [4]. The coordination of distributed flexible energy resources can help reduce costs for transmission, storage, peaking plants and capacity reserves, improve grid stability, align demand with decarbonised energy provision, promote energy independence and security, and lower household energy bills [5,6]. Residential sites constitute a significant share of potential DR, representing for example 38.5% of the 2019 UK electricity demand, and 56.4% of energy consumption if including transport and heat, which are both undergoing electrification [7]. Increasing ownership of EVs and PV panels has been facilitated by regulatory changes, with many countries committing to internal combustion car phase-outs in the near future, and by plummeting costs, with an 82% and 87% levelised cost drop between 2010 and 2019 for EVs and PV panels [8,9]. This potential is so far underexploited, as DR primarily focuses on larger well-known industrial and commercial actors that require less coordination and data management [10], with most customers still limited to trade with utility companies [11]. The primary hurdles to unlocking residential flexibility are the high capital cost of communication and control infrastructure as the domestic potential is highly fragmented [4], concerns about privacy and hindrance of activities [6,12], and computational challenges for real-time control at scale [13]. Traditionally, convex optimisation would be used to maximise global coordination objectives in convex problems with variables known ahead of time. Techniques such as least-squares and linear programming have been well-studied for over a century [14]. However, residential energy coordination presents challenges to its application. Firstly, optimisations that are centralised are hindered by privacy, acceptance, and communication constraints, and present exponential time complexity at the scale of millions of homes [15]. Secondly, standard optimisation methods cannot be used without full knowledge of the system\u2019s inputs and dynamics [16]. In residential energy, agents only have partial observability of the system due to both the stochasticity and uncertainty of environment variables such as individual residential consumption and generation profiles, and to the privacy and infrastructure cost constraints that hinder communication between agents during implementation [17]. Not relying on shared information may also improve the robustness of the solutions to failure of other agents, communication delays, and unreliable information, and improve adaptability to changing environments [18]. Finally, the real-life complex electricity grid environment may not be amenable to a convex model representation. Due to the heterogeneity of users and behaviours needing different parameters and models, the large-scale use of model-based controllers is cumbersome [19]. A model-free approach instead avoids modelling non-trivial interactions of parameters, including private information [15]. Given these challenges to residential energy flexibility coordination, and the specific constraints of the problem at play which renders traditional approaches unsuitable, we seek to develop a novel coordination mechanism which satisfies the following criteria, as tested in real-life scenarios: \u2022 Computational scalability: minimal and constant computation burden during implementation as the system size increases; \u2022 Performance scalability: no drop in coordination performance as the system size increases, measured in savings obtained per hour and per agent; \u2022 Acceptability: local control of appliances, no communication of personal data, thermal discomfort, or hindrance/delay of activities. The rest of this paper is organised as follows. In Section 2 we motivate the novel MARL approach with a literature review and a gap analysis. In Section 3, a system model is presented that includes household-level modelling of EVs, space heating, flexible loads and PV generation. Section 4 lays out the MARL methodology, with various methodological options for independent agents to learn to cooperate. In Section 5, the input data used to populate the model is presented. In Section 6, the performance of different MARL strategies is compared to lower and upper bounds in case studies. Finally, we conclude in Section 7. 2 MARL-based energy coordination: literature review and gap analysis Reinforcement learning (RL) can overcome the constraints faced by centralised convex optimisation for residential energy coordination, by allowing for decentralised and model-free decision-making based on partial knowledge. RL is an artificial intelligence (AI) framework for goal-oriented agents 2 2 Here agents are independent computer systems acting on behalf of prosumers [20]. Prosumers are proactive consumers with distributed energy resources actively managing their consumption, production and storage of energy [21]. to learn sequential decision-making by interacting with an uncertain environment [22]. As an increasing wealth of data is collected in local electricity systems, RL is of growing interest for the real-time coordination of distributed energy resources (DERs) [5,23]. Instead of optimising based on inherently uncertain data, RL more realistically searches for statistically optimal sequential decisions given partial observation and uncertainty, with no a priori knowledge [16]. Approximate learning methods may be more computationally scalable, more efficient in exploring high-dimensional state spaces and therefore more scalable than exact global optimisation with exponential time complexity [15,24]. As classified in [25], numerous RL-based coordination methods have been proposed in the literature for residential energy coordination, though with remaining limitations in terms of scalability and privacy protection. On the one hand, in RL-based direct control strategies, a central controller directly controls individual units, and households directly forfeit their data and control to a central RL-based scheduler [26]. While most existing AI-based DR research thus assumes fully observable tasks [23], direct controllability of resources from different owners with different objectives and resources and subject to privacy, comfort and security concerns is challenging [27]. Moreover, centralised policies do not scale due to the curse of dimensionality as the state and action spaces grow exponentially with the system size [28]. On the other hand, RL-based indirect control strategies consider decision-making at the prosumer level, entering the realm of MARL. This can be achieved using different communication structures, with either centralised, bilateral, or no sharing of personal information, as presented below. Firstly, agents may share information with a central entity, which in turn broadcasts signals based on a complete picture of the coordination problem. For example, the central entity may send unidirectional price signals to customers based on information such as prosumers\u2019 costs, constraints and day-ahead forecasts. RL can inform both the dynamic price signal [29,30], and the prosumer response to price signals [30,31]. The central entity may also collect competitive bids and set trades and match prosumers centrally, where RL algorithms are used to refine individual bidding strategies [32\u201336] or to dictate the auction market clearing [11,37]. Units may also use RL to cooperate towards common objectives with the mediation of a central entity that redistributes centralised personal information [38\u201341]. However, information centralisation also raises costs, security, privacy and scalability of computation issues. Biased information may lead to inefficient or even infeasible decisions [42]. Secondly, RL-based coordination has been proposed where prosumers only communicate information bilaterally without a central authority. For example, in [43] agents use transfer learning with distributed W-learning to achieve local and system objectives. Bilateral peer-to-peer communication offers autonomy and expression of individual preferences, though with remaining risks around privacy and bounded rationality [44]. There is greater robustness to communication failures compared situations with a single point of failure. However, as the system size increases, the number of communication iterations until algorithmic convergence increases, requiring adequate computational resources and limited communication network latency for feasibility [45]. The safe way of implementing distributed transactions to ensure data protection is an ongoing subject of research [25]. Finally, in RL-based implicit coordination strategies, prosumers rely solely on local information to make decisions. For example, in [46,47], competitive agents in isolation maximise their profits in RL-based energy arbitrage, though they do not consider the impacts of individual actions on the rest of the system, with potential negative impacts for the grid. For example, a concern is that all loads receive the same incentive, the natural diversity on which the grid relies may be diminished [48], and the peak potentially merely displaced, with overloads on upstream transformers. Implicit cooperation, which keeps personal information at the local level while encouraging cooperation towards global objectives, has been thus far under-researched beyond frequency control. In [49], agents learn the optimal way of acting and interacting with the environment to restore frequency using local information only. This is a promising approach for decentralised control. However, the applicability in more complex scenarios with residential electric vehicles and smart heating load scheduling problems has not been considered. Moreover, the convergence slows down for increasing number of agents, and scalability beyond 8 agents has not been investigated. Indeed, fundamental challenges to the coordination of simultaneously learning independent agents at scale under partial observability in a stochastic environment have been identified when using traditional RL algorithms [1]: independent learners may reach individual policy equilibriums that are incompatible with a global Pareto optimal, the non-stationarity of the environment due to other concurrently learning agents affects convergence, and the stochasticity of the environment prevents agents from discriminating between their own contribution to global rewards and noise from other agents or the environment. Novel methods are therefore needed to develop this approach. We seek to bridge this gap, using implicit coordination to unlock the so-far largely untapped value from residential energy flexibility to provide both individual and system benefits. We propose a new class of MARL-based implicit cooperation strategies for residential DR, to make the best use of the flexibility offered by increasingly accessible assets such as photovoltaic (PV) panels, electric vehicle (EV) batteries, smart heating and flexible loads. Agents learn RL policies using a data-based, model-free statistical approach by exploring a shared environment and interacting with decentralised partially observable Markov decision processes (Dec-POMDPs), either through random exploration or learning from convex optimisation results. In the first rehearsal phase [50] with full understanding of the system, they learn to cooperate to reach system-wide benefits by assessing the global impact of their individual actions, searching for trade-offs between local, grid and social objectives. The pre-learned policies are then used to make decisions under uncertainty given limited local information only. This approach satifies the computational scalability, coordination scalability and acceptance criteria set out in this paper. Firstly, the real-time control method is computationally scalable thanks to fixed-size Q-tables which avoid the curse of dimensionality, and there is only minimal, constant local computation required to implement the pre-learned policies during implementation. No further communication is required for implementation. This increases robustness to communication issues and data inaccuracy relative to when relying on centralised and bilateral communication, and cuts the costs of household computation and two-way communication infrastructure. Secondly, we address the outstanding MARL coordination performance scalability issue for agents with partial observability in a stochastic environment seeking to maximise rewards which also depend on other concurrently learning agents [1,51]. The case studies in this paper show that allowing agents to learn from omniscient, stable, and consistent optimisation solutions can successfully act as an equilibrium-selection mechanism, while the use of marginal rewards improves learnability 3 3 \u201cthe sensitivity of an agent\u2019s utility to its own actions as opposed to actions of others, which is often low in fully cooperative Markov games [1]\u201d. by isolating individual contributions to global rewards. This novel methodological combination offers significant improvements on MARL scalability and convergence issues, with high coordination performance maintained as the number of agents increases, where that of standard MARL drops at scale. Finally, this method tackles acceptability issues, with no interference in personal comfort nor communication of personal data. The specific novel contributions of this paper are (a) a novel class of decentralised flexibility coordination strategies, MARL-based implicit cooperation, with no communication and fixed-size Q-tables to mitigate the curse of dimensionality; (b) a novel MARL exploration strategy for agents under partial observability to learn from omniscient, convex optimisations prior to implementation for convergence to robust cooperation at scale; and (c) the design and testing with large banks of real-world data of combinations of reward definitions, exploration strategies and multi-agent learning frameworks for assessing individual impacts on global energy, grid and storage costs. Methodologies are identified which outperform a baseline with increasing numbers of agents despite uncertainty. 3 Local system description In this section, the variables, objective function and constraints of the problem are described. This sets the frame for the application of the RL algorithms presented in Section 4. 3.1 Variables We consider a set of time steps t ∈ T = { t 0 , \u2026 , t end } and a set of prosumers i ∈ P = { 1 , \u2026 , n } . Decision variables are italicised and input data are written in roman. Energy units are used unless specified otherwise. Participants have an EV, a PV panel, electric space heating and generic flexible loads. The EV at-home availability μ i t (1 if available, 0 otherwise), EV demand for required trips d EV , i t , household electric demand d i t , PV production p PV , i t , external temperature T e t and solar heat flow rate ϕ t are specified as inputs for t ∈ T and i ∈ P . The local decisions by prosumers are the energy flows in and out of the battery b in , i t and b out , i t , the electric heating consumption h i t and the prosumer consumption c i t . These have both local and system impacts (Fig. 1). Local impacts include battery energy levels E i t , losses ε ch , i t and ε dis , i t , prosumer import p i t , building mass temperature T m , i t and indoor air temperature T air , i t . System impacts arise through the costs of total grid import g t and distribution network trading. Distribution network losses and reactive power flows are not included. 3.2 Objective function Prosumers cooperate to minimise system costs consisting of grid ( c g t ), distribution ( c d t ) and storage ( c s t ) costs. This objective function will be maximised both in convex optimisations off-line \u2013 to provide an upper bound for the achievable objective function, and in some cases to provide information to the learners during the simulated learning phase \u2013 and in the learning of MARL policies for decentralised online implementation. (1) max F = ∑ ∀ t ∈ T F ˆ t = ∑ ∀ t ∈ T − ( c g t + c d t + c s t ) (2) c g t = C g t g t + ε g Where losses incurred by imports and exports from and to the main grid are approximated as (3) ε g = R V 2 g t 2 The grid cost coefficient C g t is the sum of the grid electricity price and the product of the carbon intensity of the generation mix at time t and the Social Cost of Carbon which reflects the long-term societal cost of emitting greenhouse gases [52]. The impacts of local decisions on upstream energy prices are neglected. Grid losses are approximated using the nominal root mean square grid voltage V and the average resistance between the main grid and the distribution network R [53], based on the assumption of small network voltage drops and relatively low reactive power flows [54]. The second-order dependency disincentivises large power imports and exports, which helps ensure interactions of transmission and distribution networks do not reduce system stability. (4) c d t = C d ∑ i ∈ P max − p i t , 0 Distribution costs c d t are proportional to the distribution charge C d on exports. The resulting price spread between individual imports and exports decreases risks of network constraints violation by incentivising the use of local flexibility first [55]. Distribution network losses due to power flows between prosumers are neglected so there is no second-order dependency. (5) c s t = C s ∑ i ∈ P b in , i t + b out , i t Storage battery depreciation costs c s t are assumed to be proportional to throughput using the depreciation coefficient C s , assuming a uniform energy throughput degradation rate [56]. 3.3 Constraints Let E 0 , E ̲ and E ¯ be the initial, minimum and maximum battery energy levels, η ch and η dis the charge and discharge efficiencies, and b in ¯ the maximum charge per time step. Demand d i , k t D is met by the sum of loads consumed c ˆ i , k , t C , t D at time t C by prosumer i for load of type k (fixed or flexible) demanded at t D . The flexibility boolean f i , k , t C , t D indicates if time t C lies within the acceptable range to meet d i , k t D . A Crank\u2013Nicholson scheme [57] is employed to model heating, with κ a 2 × 5 matrix of temperature coefficients, and T ̲ i t and T ¯ i t lower and upper temperature bounds. System constraints for steps ∀ t ∈ T and prosumers ∀ i ∈ P are: \u2022 Prosumer and substation energy balance (see Fig. 1) (6) p i t = c i t + h i t + b in , i t η ch − η dis b out , i t − p PV , i t (7) ∑ i ∈ P p i t = g t \u2022 Battery energy balance (8) E i t + 1 = E i t + b in , i t − b out , i t − d EV , i t \u2022 Battery charge and discharge constraints (9) E 0 = E i t 0 = E i t end + b in , i t end − b out , i t end − d EV , i t end (10) μ i t E ̲ i ≤ E i t ≤ E ¯ i (11) b in , i t ≤ μ i t b in ¯ (12) b out , i t ≤ μ i t E ¯ i \u2022 Consumption flexibility \u2014 the demand of type k at time t D by prosumer i must be met by the sum of partial consumptions c ˆ i , k , t C , t D at times t C . . . t C + n flex within the time frame n flex specified by the flexibility of each type of demand in matrix f i , k , t C , t D (13) ∑ t C ∈ T c ˆ i , k , t C , t D f i , k , t C , t D = d i , k t D \u2022 Consumption \u2014 the total consumption at time t C is the sum of all partial consumptions c ˆ i , k , t C , t D meeting parts of demands from current and previous time steps t D : (14) ∑ t D ∈ N c ˆ i , k , t C , t D = c i , k t C \u2022 Heating \u2014 the workings to obtain this equation are included in the supplementary material item \u201cHeating model\u201d: (15) T m , i t + 1 T air , i t + 1 = κ 1 , T m , i t , T e t , ϕ t , h i t ⊺ (16) T ̲ i t ≤ T air , i t ≤ T ¯ i t \u2022 Non-negativity constraints (17) c i t , h i t , E i t , b in , i t , b out , i t , c ˆ i , l , t C , t D ≥ 0 While the proposed framework could accommodate the use of idiosyncratic satisfaction functions to perform trade-offs between flexibility use and users\u2019 comfort, no such trade-offs are considered in this paper, with comfort requirements for temperature and EV usage always being met. Field evaluations have shown that programmes that do not maintain thermal comfort are consistently overridden, increasing overall energy use and costs [58], while interference in consumption patterns and temperature set-points cause dissatisfaction [5]. Meeting fixed domestic loads, ensuring sufficient charge for EV trips, and maintaining comfortable temperatures are therefore set constraints. 4 Reinforcement learning methodology The MARL approach is now presented in which independent prosumers learn to make individual decisions which together maximise the statistical expectation of the objective function in Section 3. At time step t ∈ T , each agent is in a state s i t ∈ S corresponding to accessible observations (here the time-varying grid cost), and selects an action a i t ∈ A as defined in Section 4.3. This action dictates the decision variables in Section 3.1 b in , i t , b out , i t , h i t and c i t . The environment then produces a reward r t ∈ R which corresponds to the share F ˆ t of the system objective function presented in Section 3.2 and agents transition to a state s i t + 1 . Agents learn individual policies π i by interacting with the environment using individual, decentralised fixed-size Q-tables. We first introduce the Q-learning methodology. Then, the mapping between the RL agent action and the decision variables in Section 3.1 is presented. Finally, we propose variations on the learning method, with different experience sources, multi-agent structures and reward definitions. 4.1 Q-learning While any reinforcement learning methodology could be used with the framework proposed in this paper, here we focus on Q-learning, a model-free, off-policy RL methodology. Its simplicity and proof of convergence make it suited to developing novel learning methodologies in newly defined environments [5]. State\u2013actions values Q ( s , a ) represent the expected value of all future rewards r t ∀ t ∈ T when taking action a in state s according to policy π : (18) Q ( s , a ) ≜ E π [ r t + γ r t + 1 + γ 2 r t + 2 . . . | s t = s , a t = a ] where γ is the discount factor setting the relative importance of future rewards. Estimates are refined incrementally as (19) Q ˆ ( s , a ) ← Q ˆ ( s , a ) + α δ where δ is the temporal-difference error, (20) δ = r t + γ V ˆ ( s next ) − Q ˆ ( s , a ) V ˆ is the state-value function estimate, (21) V ˆ ( s ) = max a ∗ ∈ A ( s ) Q ˆ ( s , a ∗ ) and α is the learning rate. In this work we use hysteretic learners, i.e. chiefly optimistic learners that use an increase rate superior to the decrease rate in order to reduce oscillations in the learned policy due to actions chosen by other agents [1,59]. For β < 1 : (22) α = α 0 if δ > 0 α 0 β otherwise Agents follow an ε -greedy policy to balance exploration of different state\u2013action pairs and knowledge exploitation. The greedy action with highest estimated rewards is selected with probability 1 − ε and random actions otherwise. (23) a ∗ = arg max a ∗ ∈ A Q ˆ ( s , a ∗ ) if x ∼ U ( 0 , 1 ) > ε a ∼ p ( a ) = 1 | A | ∀ a ∈ A otherwise Henceforth, we refer to the estimates Q ˆ and V ˆ as Q and V to reduce the amount of notation. 4.2 Agent state The agent state is defined by the time-dependent grid cost coefficient C g t , i.e. the sum of the grid electricity price and the product of the carbon intensity of the generation mix at time t and the social cost of carbon. To convert the RL policy action into local decisions, the agent also requires information on their current PV generation, battery level, flexible loads and indoor air temperature, as described below in Section 4.3. 4.3 Agent action Large action spaces compound the curse of dimensionality in Q-learning and waste exploration resources [28]. At each time step, the decision variables in Section 3 controlling the flows in and out of the battery b in , i t and b out , i t , the electric heating consumption h i t and the prosumer consumption c i t for household i are therefore synthesised into a single variable ψ ∈ [ 0 , 1 ] controlling the use of available local flexibility. Fig. 2 shows how consumption (for domestic loads and heat), imports and storage change with ψ . At each step, the fixed requirements for loads, heat and upcoming EV trips are first met. The ψ decision then applies to the remaining flexibility. In conditions deemed optimal for energy exports ψ = 0 , all initial storage and residual PV generation is exported and flexible loads are delayed. On the other end, a passive agent does not utilise its flexibility and uses the default action ψ = 1 , maximising imports with EVs charged when plugged in and no flexible loads delayed. Intermediate imports trade-offs are mapped on Fig. 2: 1. From exporting all to none of the initial storage E i t 2. From meeting fixed loads d i , fixed t with the energy stored to importing the required amount 3. From no to maximum flexible consumption d i , tot t 4. From exporting to storing PV energy p PV , i t remaining after meeting loads 5. From importing no additional energy to filling up the battery to capacity E ¯ i Costlier actions incurring battery depreciation, losses and export costs are towards either ψ extreme, only used in highly beneficial situations (convex local costs function in the lower plot of Fig. 2). Ranking actions consistently ensures agents do not waste resources trialling sub-optimal combinations of decisions. For example, it is more cost-efficient to first absorb energy imports by consuming flexible loads, and only use the battery (incurring costs) if imports are large. Note that although this action space is continuous, it can be discretised into intervals for implementation in Q-learning. 4.4 Variations of the learning method Different experience sources, reward definitions and MARL structures are proposed within the MARL approach. The performance of these combinations of algorithmic possibilities will be assessed in Section 6 to inform effective model design. 4.4.1 Experience sources In data-driven strategies, the learning is determined by the collected experience. \u2022 Environment exploration. Traditionally, agents collect experience by interacting with an environment [22]. \u2022 Optimisations. A novel approach collects experience from optimisations. Learning from entities with more knowledge or using knowledge more effectively than randomly exploring agents has previously been proposed, as with agents \u201cmimicking\u201d humans playing video games [60]. Similarly, agents learn from convex \u201comniscient\u201d optimisations on historical data with perfect knowledge of current and future variables. This experience is then used under partial observability and control for stable coordination between prosumers at scale. Note in this case that, although the MARL learning and implementation are model-free, a model of the system is used to run the convex optimisation and produce experience to learn from. A standard convex optimiser uses the same data that would be used to populate the environment explorations but solves over the whole day-horizon with perfect knowledge of all variables using the problem description in Section 3. Then, at each time step, the system variables are translated into equivalent RL { s t , a t , r t , s t + 1 } tuples for each agent, which are used to update the policies in the same way as for standard Q-learning as presented below. 4.4.2 MARL structures Both the centralised and decentralised structures proposed use fixed-size | S | × | A | Q-tables corresponding to individual state\u2013action pairs. The size of a global Q-table referencing all possible combinations of states and actions would grow exponentially with the number of agents. This would limit scalability due to memory limitations and exploration time requirements. Moreover, as strategies proposed in this paper are privacy-preserving, only local state\u2013action pairs are used for individual action selection, wasting the level of detail of a global Q-table. \u2022 Distributed learning. Each agent i learns its Q i table with its own experience. No information is shared between agents. \u2022 Centralised learning. A single table Q c uses experience from all agents during pre-learning. All agents use the centrally learned policy for decentralised implementation. 4.4.3 Reward definitions The reward definition is central to learning as its maximisation forms the basis for incrementally altering the policy [22]. Assessing the impact of individual actions on global rewards accurately is key to the effective coordination of a large number of prosumers. In the following, the Q-tables Q 0 , Q diff , Q A and Q count may be either agent-specific Q i or centralised Q c based on the MARL structure. We proposed four variations of the Q-table update rule for each experience step tuple collected ( s i t , a i t , r t , s i t + 1 ) . (24) Q ( s i t , a i t ) ← Q ( s i t , a i t ) + α δ \u2022 Total reward. The instantaneous total system reward r t = F ˆ t is used to update the Q-table Q 0 . (25) δ = r t + γ V 0 ( s i t + 1 ) − Q 0 ( s i t , a i t ) \u2022 Marginal reward. The difference in total instant rewards r t between that if agent i selects the greedy action and that if it selects the default action is used to update Q diff [61]. The default action a default corresponds to ψ = 1 , where no flexibility is used. The default reward r a i = a default t , where all agents perform their greedy action apart from agent i which performs the default action, is obtained by an additional simulation. (26) δ = r t − r a i = a default t + γ V diff ( s i t + 1 ) − Q diff ( s i t , a i t ) \u2022 Advantage reward. The post difference between Q 0 values when i performs the greedy and the default action is used. This corresponds to the estimated increase in rewards not just instantaneously but over all future states, analogously to in [62]. No additional simulations are required as the Q-table values are refined over the normal course of explorations. (27) δ = Q 0 ( s i t , a i t ) − Q 0 ( s i t , a a i = a default ) − Q A ( s i t , a i t ) \u2022 Count. The Q-table stores the number of times each state\u2013action pair is selected by the optimiser. (28) α δ = 1 5 Input data This section presents the data that is fed into the model presented in Section 3. Interaction with this data will shape the policies learned through RL [22] and should reflect resource intermittency and uncertainty to maximise the expectation of rewards in a robust way without over-fitting. EV demand d EV , i t and availability μ i t , PV production p PV , i t and electricity consumption d i t are drawn from large representative datasets. 5.1 Data selection and pre-processing Load and PV generation profiles are obtained from the Customer Led Network Revolution (CLNR), a UK-based smart grid demonstration project [63,64], and mobility data from the English National Travel Survey (NTS) [65]. The NTS does not focus on EVs only and offers a less biased view into the general population\u2019s travel pattern than small-scale EV trials data, both due to the smaller volume of data available compared to for generic cars and because the self-selected EV early trial participants may not be representative of patterns once EVs become widely adopted. It is implicitly assumed that electrification will not affect transport patterns [66]. NTS data from 82,455 households from 2002 to 2017 results in 1,272,834 full days of travel profiles. Load and PV data from 11,907 customers between 2011 and 2014 yields 620,702 and 22,670 full days of data, respectively. Profiles are converted to hourly resolution and single missing points replaced with the figure from the same time the day or week before or after which has the lowest sum of squares of differences between the previous and subsequent point. Tested with available data, this yields absolute errors with mean 0.13 and 0.08 kWh and 99th percentile 1.09 and 0.81 kWh for PV and load data. PV sources have nominal capacities between 1.35 and 2.02 kWp. The at home-availability of the vehicles is inferred from the recorded journeys\u2019 origin and destination. EV energy consumption profiles are obtained using representative consumption factors from a tank-to-wheel model proposed in [66], dependent on travel speed and type (rural, urban, motorway). 5.2 Markov chain During learning, agents continuously receive experience to learn from. However, numerous subsequent days of data are not available for single agents. We design a Markov chain mechanism to feed consistent profiles for successive days, using both consistent scaling factors and behaviour clusters. Daily profiles for load and travel are normalised such that ∑ t = 0 . . 24 x t = 1 , and clustered using K-means, minimising the within-cluster sum-of-squares [67] in four clusters for both weekday and weekend data (with one for no travel). The features used for load profiles clustering are normalised peak magnitude and time and normalised values over critical time windows, and those for travel are normalised values between 6 am and 10 pm. PV profiles were grouped per month. Probabilistic Markov chain transition rules are shown in Table 1. Transition probabilities for clusters k and scaling factors λ are obtained from available transitions between subsequent days in the datasets for each week day type w (week day or weekend day). Fig. 3 shows that subsequent PV and load scaling factors follow strong linear correlation, with the residuals of the perfect correlation following gamma distributions with zero mean, whereas EV load scaling factors follow more complex patterns, so transitions probabilities are computed between 50 discrete intervals. 6 Case study results and discussion This section compares the performance of the residential flexibility coordination strategies presented in Section 4 to baseline and upper bound scenarios for increasing numbers of prosumers. The performance of traditionally used MARL strategies drops at scale, while that of the novel optimisation-based methodology using marginal rewards is maintained. 6.1 Set-up The MARL algorithm is trained in off-line simulations using historical data prior to online implementation. This means agents do not trial unsuccessful actions with real-life impacts during learning. Moreover, the computation burden is taken prior to implementation, while prosumers only apply pre-learned policies, avoiding the computational challenges of large-scale real-time control. The learning occurs over 50 epochs consisting of an exploration, an update and an evaluation phase. First, the environment is explored over two training episodes of duration | T | = 24 hours. Learning in batches of multiple episodes helps stabilise learning in the stochastic environment. Then, Q-tables are updated based on the rules presented in Section 4.4. Finally, an evaluation is performed using a deterministic greedy policy on new evaluation data. Ten repetitions are performed such that the learning may be assessed over different trajectories. The Social Cost of Carbon is set at 70 £/tCO 2 , consistent with the UK 2030 target [68]. Weather [69], electricity time-of-use prices [70] and grid carbon intensity [71] are from January 2020, where relevant specified for London, UK. The low solar heat gains in January are neglected [72]. Other relevant parameters for the case studies are listed in Appendix B. As performed on a Intel(R) Core(TM) i7-9800X CPU @ 3.80 GHz, computation time for a learning trajectory is 2 \u2032 4 5 \u2033 for one agent and 9 7 \u2032 5 \u2033 for 30 agents, including evaluation points. The policy can then be directly applied at the household level during operation. Case study results using different experience sources, reward definitions and MARL structures are presented in Fig. 4. Acronyms for each strategy are tabulated in the legend. Positive values denote savings relative to a baseline scenario where all agents are passive, i.e. not using their flexibility with EVs charged immediately and no flexible loads delayed. As the Q-learning policies are first initialised with zero values, in the first epoch of learning completely random action values are chosen, which provides rewards far below the baseline. As agents collect experience and update their policies at each epoch, improved policies are learned, some of which are able to outperform the baseline. An upper bound is provided by results from \u201comniscient\u201d convex optimisations, which are however not achievable in practice for three main reasons. Firstly, they use perfect knowledge of all the environment variables in the present and future, despite uncertainty in renewable generation, mix of the grid, and customer behaviour. Optimisation with inaccurate data would lead to suboptimal results. Secondly, prosumers may not be willing to yield their data and direct control to an external entity. Finally, central optimisations become computationally expensive for real-time control of large numbers of prosumers. 6.2 Results Results presented in Fig. 4 show that only the algorithms learning from optimisations maintained stable coordination performance at scale, while the performance of traditionally used MARL algorithms would drop in this context of stochasticity and partial observation. The optimisation-based algorithm which uses marginal rewards (MO) performed best. We further elaborate on the results in the subsections below. 6.2.1 Environment exploration-based learning The centralised MARL structure is favoured for environment exploration-based learning (continuous lines in Fig. 4). A single policy uses experience collected by all agents, rather than each agent learning from their own experience only. Fig. 4 shows that environment exploration-based MARL using total rewards (TE, orange), the baseline MARL framework, exhibits a high performance for a single agent. However, savings drop as the number of cooperating agents increases, down to around zero from ten agents. Coordination challenges arise for independent learners to isolate the contribution of their actions to total rewards from the stochasticity of the environment, compounded by other simultaneously learning agents\u2019 random explorations, and the non-stationarity of their on-policy behaviour [1]. Using advantage rewards (AE, grey), based on estimates of the long-term value of actions relative to that of the baseline action, yields superior results beyond two agents. However, as AE uses the total reward Q 0 -table as an intermediary step, results similarly drops for increasing numbers of agents. Using marginal rewards (ME, dark green), the value of each agent\u2019s action relative to the baseline action is singled out immediately by an additional simulation and used as a reward at each time step. This improves the performance relative to TE and AE for five agents and more, though still with declining performance as the number of agents increases. 6.2.2 Optimisation-based learning Optimisation-based learning generally favours the distributed MARL structure, with agents able to converge to distinct compatible policies (dashed lines in Fig. 4). Comparing trajectories in Fig. 4, learning from the total rewards obtained by an optimiser (TO, light blue) yields lower savings than when using environment explorations (TE). The learned policies yield negative savings, i.e. would provide worse outcomes than inflexible agents. The omniscient optimiser takes precise, extreme decisions thanks to its perfect knowledge of all current and future system variables, importing at very high ψ values when it is optimal to do so. RL algorithms on the other hand are used under partial observability, aiming for actions that statistically perform well under uncertainty. Agents independently picking TO-based decisive actions in a stochastic environment do not yield optimal outcomes. Assessing the long-term advantage of actions from optimisations (AO, dark blue) follows a similar trend, whilst providing marginally superior savings relative to TO. Optimisation-based learning using marginal rewards (MO, purple) offers the highest savings as the additional baseline simulations are best able to isolate the contribution of individual actions from variations caused by both the environment and other agents. When increasing the number of agents, the strategy is able to learn from optimal, stable, consistently behaving agents. Savings of 6.18p per agent per hour, or £45.11 per agent per month are obtained on average for 30 agents, corresponding to a 33.7% reduction from baseline costs. 65.9% of savings stem from reduced battery depreciation, 20.32% from distribution grid congestion, 11.1% from grid energy, and 2.7% from greenhouse gas emissions. The count-based strategy learning from optimisations (CO, light green) seeks to reproduce the state\u2013action patterns of the omniscient optimiser with perfect knowledge of system variables and perfect control of agents for local decision-making under partial observability. It provides results lower than the high performances of MO, though with a stable performance at scale. Savings of £21.09 per agent per month on average for 30 agents are obtained. The battery and distribution grid costs increase by an equivalent of 6.0% and 7.7% of total savings respectively, while grid energy and greenhouse gas emissions costs reductions represent 59.7% and 54.0% of total savings. Both the MO and CO strategies exhibit stable performance at scale, though converging to different types of policy. The MO policy saves more by smoothing out the charging and distribution grid utilisation profiles despite smaller savings in imports and emissions costs, while CO derives a larger advantage from the grid price differentials in grid imports, though with higher battery and distribution grid costs. The weight applied to each of those competing objectives in the objective function directly impacts the policies that are learned. Examples of how the individual home energy management system decision variables (heating, energy consumption, battery charging) vary based on the controller are illustrated in the supplementary data item \u201cResidential energy management: commented illustrative day\u201d. Overall, the new class of optimisation-based learning performs significantly better across different numbers of prosumers, with higher savings and lower inter-quartile range than environment-based learning at scale. This superior performance requires computations to run optimisations on historical data, and to perform baseline simulations to compute marginal rewards, though computational time for pre-learning is not strictly a limiting factor as it is performed off-line ahead of implementation. A fundamental challenge in MARL has been the trade-off between fully centralised value functions, which are impractical for more than a handful of agents, or, in a more straightforward approach, independent learning of individual action-value functions by each agent in independent Q-learning (IQL) [73]. However, an ongoing issue with this approach has been that of convergence at scale, as agents do not have explicit representations of interactions between agents, and each agent\u2019s learning is confounded by the learning and exploration of others [74]. As shown in Fig. 4, the Pareto selection, non-stationarity and stochasticity issues presented in Section 2 have prevented environment exploration-based learners from achieving successful MARL cooperation at scale for agents under partial observability in a stochastic environment. This case study of coordinated residential energy management shows that the novel combination of marginal rewards, which help agents isolate their marginal contribution to total rewards, and the learning from results of convex optimisations, where agents learn successful policy equilibriums from omniscient, stable, and consistent solutions, offer significant improvements on these scalability and convergence issues. 7 Conclusion In this paper, a novel class of strategies has addressed the scalability issue of residential energy flexibility coordination in a cost-efficient and privacy-preserving manner. The combination of off-line optimisations with multi-agent reinforcement learning provides high, stable coordination performance at scale. We identified in the literature that the concept of RL-based implicit energy coordination, where energy prosumers cooperate towards global objectives based on local information only, had been under-researched beyond frequency droop control with limited number of agents. The scalability of such methods was identified as a key gap that we have sought to bridge. The novel coordination mechanism proposed in this paper thus satisfies the criteria for successful residential energy coordination set out in the introduction, as tested with large banks of real data in the case studies: \u2022 Computational scalability: The scalability of traditional learning algorithms is significantly improved thanks to fixed-size Q-tables to avoid the curse of dimensionality, so that policies can be learned for larger number of agents. The proposed method does not require expensive communication and control appliances at the prosumer level, as pre-learned policies are directly applied with no further communication and no exponential time real-time optimisations needed. This is a crucial benefit for applications with physical limitations in hardware availability and processing time. \u2022 Performance scalability: The coordination performance remains high for increasing numbers of prosumers despite the challenges of partial observability, environment stochasticity and concurrently learning of agents, thanks to learning from the results of global omniscient optimisations on historical data, and to rewards signals that isolate individual contributions to global rewards. Significant value of £45.11 per agent per month was obtained in the presented case study for 30 agents, thanks to savings in energy, prosumer storage and societal greenhouse gas emissions-related costs. Those savings do not drop with increasing number of agents, as opposed to with standard MARL approaches. \u2022 Acceptability: The approach does not rely on sharing of personal data, thermal discomfort, or hindrance/delay of activities, and the appliances are controlled locally. This cost-efficient and privacy-preserving implicit coordination approach could help integrate distributed energy resources such as residential energy, otherwise excluded from energy systems\u2019 flexibility management. Important future work is a more detailed assessment of the impacts of the coordination strategies on power flows, as well as an evaluation of the generalisation and adaptability potential of policies when used by other households or if household characteristics change over time. Moreover, while all agents readily reduce individual costs through participation in the framework, further game-theoretic tools could be used to design a post-operation reward scheme. CRediT authorship contribution statement Flora Charbonnier: Conceptualisation, Data curation, Formal analysis, Investigation, Methodology, Visualisation, Writing \u2013 original draft. Thomas Morstyn: Supervision, Validation, Writing \u2013 review & editing. Malcolm D. McCulloch: Supervision, Validation, Writing \u2013 review & editing. Acknowledgement This work was supported by the Saven European Scholarship and by the UK Research and Innovation and the Engineering and Physical Sciences Research Council (award references EP/S000887/1, EP/S031901/1, and EP/T028564/1). Appendix A Nomenclature See Table A.2. Appendix B Case study input data \u2022 Learning parameters: The depreciation, learning and exploration rates are γ = 0 . 99 , α 0 = 0 . 01 and ε = 0 . 5 . The hysteretic learning rate reduction parameter for negative errors is β = 0 . 5 . The states are defined by three uniform grid cost intervals for each day. The action space is discretised in 10 equal ψ intervals. \u2022 Battery: η ch = η dis = η round trip [75], where η round trip = 0 . 87 [76], capacity E ¯ = 75 kWh, max. charging rate b in ¯ = 22 kW, depreciation C s = 20 USD/MWh-throughput [53], initial and min. charge E 0 = 0 . 5 E ¯ and E ̲ = 0 . 1 E ¯ . \u2022 Grid: nominal voltage V = 415 [V], average resistance to prosumers R = 0 . 084 [ Ω ] [53]. \u2022 Flexible loads: 10% deferrable for up to n flex = 5 hours. \u2022 Heating: housing of 76 m 2 , 2.4 m height. Comfort temperature 20 °C between 7\u201310 am and 5−10 pm, setback 16 °C. Variations of 3 °C acceptable. U-values from [77], other heating inputs from [57,78,79]. Pre-heating up to five hours in advance. Coefficients after re-arranging: (29) κ = 6 . 84 e − 2 , 9 . 08 e − 1 , 9 . 15 e − 2 , 2 . 62 e − 4 , 2 . 52 e − 1 2 . 40 e − 1 , 8 . 80 e − 1 , 1 . 20 e − 1 , 3 . 46 e − 4 , 1 . 46 \u2022 EV consumption factors [kWh/10 km]: 2.25 for motorway, 1.62 for urban and 1.36 for rural travel [66]. \u2022 Distribution network export charge: 0.01 £/kWh. Appendix C Supplementary data Supplementary material related to this article can be found online at https://doi.org/10.1016/j.apenergy.2022.118825. Appendix C Supplementary data The following is the Supplementary material related to this article. MMC S1 Heating model. MMC S2 Residential energy management: commented illustrative day. References [1] Matignon L. Laurent G. Le Fort-Piat N. Independent reinforcement learners in cooperative Markov games: A survey regarding coordination problems Knowl Eng Rev 27 1 2012 1 31 10.1017/S0269888912000057 L. Matignon, G. Laurent, N. Le Fort-Piat, Independent reinforcement learners in cooperative Markov games: A survey regarding coordination problems, Knowledge Engineering Review 27 (1) (2012) 1\u201331. doi:10.1017/S0269888912000057. [2] Masson-Delmotte V. Global warming of 1.5C. An IPCC special report on the impacts of global warming of 1.5C above pre-industrial levels and related global greenhouse gas emission pathways, in the context of strengthening the global response to the threat of climate change 2018 Masson-Delmotte, V., Global Warming of 1.5C. An IPCC Special Report on the impacts of global warming of 1.5C above pre-industrial levels and related global greenhouse gas emission pathways, in the context of strengthening the global response to the threat of climate change (2018). [3] Bose S. Low S. Some emerging challenges in electricity markets Smart grid control, power elec edition 2019 29 45 10.1007/978-3-319-98310-3_2 S. Bose, S. Low, Some Emerging Challenges in Electricity Markets, in: Smart Grid Control, power elec Edition, 2019, pp. 29\u201345. doi:10.1007/978-3-319-98310-3˙2. [4] Léautier T.-O. Imperfect markets and imperfect regulation: An introduction to the microeconomics and political economy of power markets 2019 MIT Press T.-O. Léautier, Imperfect Markets and Imperfect Regulation: An Introduction to the Microeconomics and Political Economy of Power Markets, MIT Press, 2019. [5] Vázquez-Canteli J. Nagy Z. Reinforcement learning for demand response: A review of algorithms and modeling techniques Appl Energy 235 2018 2019 1072 1089 10.1016/j.apenergy.2018.11.002 J. Vázquez-Canteli, Z. Nagy, Reinforcement learning for demand response: A review of algorithms and modeling techniques, Applied Energy 235 (Oct 2018) (2019) 1072\u20131089. doi:10.1016/j.apenergy.2018.11.002. [6] Pumphrey K. Walker S. Andoni M. Robu V. Green hope or red herring? Examining consumer perceptions of peer-to-peer energy trading in the United Kingdom Energy Res Soc Sci 68 2019 2020 101603 10.1016/j.erss.2020.101603 K. Pumphrey, S. Walker, M. Andoni, V. Robu, Green hope or red herring? Examining consumer perceptions of peer-to-peer energy trading in the United Kingdom, Energy Research and Social Science 68 (September 2019) (2020) 101603. doi:10.1016/j.erss.2020.101603. [7] Department for Business Energy and Industrial Strategy Energy consumption in the UK 2021 Department for Business Energy and Industrial Strategy, Energy consumption in the UK (2021). [8] Agency I.R.E. Renewable power generation costs in 2018 2018 10.1007/SpringerReference_7300 arXiv:arXiv:1011.1669v3 I. R. E. Agency, Renewable Power Generation Costs in 2018, 2018. arXiv:arXiv:1011.1669v3, doi:10.1007/SpringerReference˙7300. [9] BloomberNEF 2019 Battery price survey 2019 BloomberNEF, 2019 Battery Price Survey (2019). [10] Charles River Associates An assessment of the economic value of demand-side participation in the balancing mechanism and an evaluation of options to improve access 2017 Charles River Associates, An assessment of the economic value of demand-side participation in the Balancing Mechanism and an evaluation of options to improve access (2017). [11] Chen T. Su W. Indirect customer-to-customer energy trading with reinforcement learning IEEE Trans Smart Grid 10 4 2019 4338 4348 10.1109/TSG.2018.2857449 T. Chen, W. Su, Indirect Customer-to-Customer Energy Trading with Reinforcement Learning, IEEE Transactions on Smart Grid 10 (4) (2019) 4338\u20134348. doi:10.1109/TSG.2018.2857449. [12] Bugden D. Stedman R. A synthetic view of acceptance and engagement with smart meters in the United States Energy Res Soc Sci 47 2018 2019 137 145 10.1016/j.erss.2018.08.025 D. Bugden, R. Stedman, A synthetic view of acceptance and engagement with smart meters in the United States, Energy Research and Social Science 47 (January 2018) (2019) 137\u2013145. doi:10.1016/j.erss.2018.08.025. [13] Moret F. Pinson P. Energy collectives: A community and fairness based approach to future electricity markets IEEE Trans Power Syst 34 5 2019 3994 4004 10.1109/TPWRS.2018.2808961 F. Moret, P. Pinson, Energy Collectives: A Community and Fairness Based Approach to Future Electricity Markets, IEEE Transactions on Power Systems 34 (5) (2019) 3994\u20134004. doi:10.1109/TPWRS.2018.2808961. [14] Boyd S. Convex optimization theory 2009 25 S. Boyd, Convex optimization theory, Vol. 25, 2009. http://citeseerx.ist.psu.edu/viewdoc/download?rep=rep1&type=pdf&doi=10.1.1.214.7707 [15] Dasgupta S. Computer science: A very short introduction 2016 Oxford University Press 10.1093/actrade/9780198733461.001.0001 S. Dasgupta, Computer Science: A Very Short Introduction, Oxford University Press, 2016. doi:10.1093/actrade/9780198733461.001.0001. [16] Recht B. A tour of reinforcement learning: The view from continuous control ArXiv 2018 10.1146/annurev-control-053018-023825 arXiv:1806.09460 B. Recht, A tour of reinforcement learning: The view from continuous control, arXiv (2018). arXiv:1806.09460, doi:10.1146/annurev-control-053018-023825. [17] Fran CÇcois Lavet V. Contributions to deep reinforcement learning and its applications in smartgrids 2017 V. François Lavet, Contributions to deep reinforcement learning and its applications in smartgrids (2017). [18] Sen S. Sekaran M. Hale J. Learning to coordinate without sharing information Proc Natl Conf Artif Intell 1 1994 426 431 S. Sen, M. Sekaran, J. Hale, Learning to coordinate without sharing information, Proceedings of the National Conference on Artificial Intelligence 1 (1994) 426\u2013431. [19] Ruelens F. Residential demand response of thermostatically controlled loads using batch reinforcement learning IEEE Trans Smart Grid 8 5 2017 2149 2159 10.1109/TSG.2016.2517211 F. Ruelens, Residential Demand Response of Thermostatically Controlled Loads Using Batch Reinforcement Learning, IEEE Transactions on Smart Grid 8 (5) (2017) 2149\u20132159. doi:10.1109/TSG.2016.2517211. [20] Wooldridge M. Intelligent agents: The key concepts 2002 Springer Berlin, Heidelberg, Berlin, Heidelberg M. Wooldridge, Intelligent Agents: The Key Concepts, Springer Berlin Heidelberg, Berlin, Heidelberg, 2002. [21] Morstyn T. Farrell N. Darby S. McCulloch M. Using peer-to-peer energy-trading platforms to incentivize prosumers to form federated power plants Nat Energy 3 2 2018 94 101 T. Morstyn, N. Farrell, S. Darby, M. McCulloch, Using peer-to-peer energy-trading platforms to incentivize prosumers to form federated power plants, Nat Energy 3 (2) (2018) 94\u2013101. [22] Sutton R.S. Barto A.G. Reinforcement learning : An introduction [electronic resource], adaptive computation and machine learning 1998 MIT Press Cambridge, Mass. R. S. Sutton, A. G. Barto, Reinforcement learning : an introduction [electronic resource], Adaptive computation and machine learning, MIT Press, Cambridge, Mass., 1998. [23] Antonopoulos I. Artificial intelligence and machine learning approaches to energy demand-side response: A systematic review Renew Sustain Energy Rev 130 April 2020 109899 10.1016/j.rser.2020.109899 I. Antonopoulos, Artificial intelligence and machine learning approaches to energy demand-side response: A systematic review, Renewable and Sustainable Energy Reviews 130 (April) (2020) 109899. doi:10.1016/j.rser.2020.109899. [24] Schellenberg C. Lohan J. Dimache L. Comparison of metaheuristic optimisation methods for grid-edge technology that leverages heat pumps and thermal energy storage Renew Sustain Energy Rev 131 June 2020 109966 10.1016/j.rser.2020.109966 C. Schellenberg, J. Lohan, L. Dimache, Comparison of metaheuristic optimisation methods for grid-edge technology that leverages heat pumps and thermal energy storage, Renewable and Sustainable Energy Reviews 131 (June) (2020) 109966. doi:10.1016/j.rser.2020.109966. https://doi.org/10.1016/j.rser.2020.109966 [25] Charbonnier F. Morstyn T. McCulloch M. Coordination of resources at the edge of the electricity grid: systematic review and taxonomy 2022 arXiv:2202.03786 F. Charbonnier, T. Morstyn, M. McCulloch, Coordination of resources at the edge of the electricity grid: systematic review and taxonomy (2022). arXiv:2202.03786. [26] O\u2019Neill D. Levorato M. Goldsmith A. Mitra U. Residential demand response using reinforcement learning 2010 First IEEE international conference on smart grid communications 2010 409 414 10.1109/smartgrid.2010.5622078 D. O\u2019Neill, M. Levorato, A. Goldsmith, U. Mitra, Residential Demand Response Using Reinforcement Learning, 2010 First IEEE International Conference on Smart Grid Communications (2010) 409\u2013414 doi:10.1109/smartgrid.2010.5622078. [27] Darby S.J. Demand response and smart technology in theory and practice: Customer experiences and system actors Energy Policy 143 April 2020 111573 10.1016/j.enpol.2020.111573 S. J. Darby, Demand response and smart technology in theory and practice: Customer experiences and system actors, Energy Policy 143 (April) (2020) 111573. doi:10.1016/j.enpol.2020.111573. https://doi.org/10.1016/j.enpol.2020.111573 [28] Powell W. Approximate dynamic programming: Solving the curses of dimensionality second ed. Wiley series in probability and statistics 2011 J. Wiley & Sons Hoboken, N.J W. Powell, Approximate dynamic programming: solving the curses of dimensionality, 2nd Edition, Wiley series in probability and statistics, J. Wiley & Sons, Hoboken, N.J., 2011. [29] Lu R. Hong S.H. Incentive-based demand response for smart grid with reinforcement learning and deep neural network Appl Energy 236 2018 2019 937 949 10.1016/j.apenergy.2018.12.061 R. Lu, S. H. Hong, Incentive-based demand response for smart grid with reinforcement learning and deep neural network, Applied Energy 236 (December 2018) (2019) 937\u2013949. doi:10.1016/j.apenergy.2018.12.061. [30] Kim B. Zhang Y. Van Der Schaar M. Lee J. Dynamic pricing and energy consumption scheduling with reinforcement learning IEEE Trans Smart Grid 7 5 2016 2187 2198 B. Kim, Y. Zhang, M. Van Der Schaar, J. Lee, Dynamic Pricing and Energy Consumption Scheduling With Reinforcement Learning, IEEE Transactions on Smart Grid 7 (5) (2016) 2187\u20132198. [31] Babar M. Nguyen P.H. Cuk V. Kamphuis I.G. Bongaerts M. Hanzelka Z. The evaluation of agile demand response: An applied methodology IEEE Trans Smart Grid 9 6 2018 6118 6127 10.1109/TSG.2017.2703643 M. Babar, P. H. Nguyen, V. Cuk, I. G. Kamphuis, M. Bongaerts, Z. Hanzelka, The evaluation of agile demand response: An applied methodology, IEEE Transactions on Smart Grid 9 (6) (2018) 6118\u20136127. doi:10.1109/TSG.2017.2703643. [32] Vayá MG, Roselló LB, Andersson G. Optimal bidding of plug-in electric vehicles in a market-based control setup. In: Proceedings - 2014 power systems computation conference. 2014, http://dx.doi.org/10.1109/PSCC.2014.7038108. M. G. Vayá, L. B. Roselló, G. Andersson, Optimal bidding of plug-in electric vehicles in a market-based control setup, Proceedings - 2014 Power Systems Computation Conference, PSCC 2014 (2014). doi:10.1109/PSCC.2014.7038108. [33] Ye Y. Qiu D. Sun M. Papadaskalopoulos D. Strbac G. Deep reinforcement learning for strategic bidding in electricity markets IEEE Trans Smart Grid 11 2 2020 1343 1355 10.1109/TSG.2019.2936142 Y. Ye, D. Qiu, M. Sun, D. Papadaskalopoulos, G. Strbac, Deep Reinforcement Learning for Strategic Bidding in Electricity Markets, IEEE Transactions on Smart Grid 11 (2) (2020) 1343\u20131355. doi:10.1109/TSG.2019.2936142. [34] Dauer D. Flath C.M. Ströhle P. Weinhardt C. Market-based EV charging coordination Proceedings - 2013 IEEE/WIC/ACM international conference on intelligent agent technology, IAT 2013, Vol. 2 2013 102 107 10.1109/WI-IAT.2013.97 D. Dauer, C. M. Flath, P. Ströhle, C. Weinhardt, Market-based EV charging coordination, Proceedings - 2013 IEEE/WIC/ACM International Conference on Intelligent Agent Technology, IAT 2013 2 (2013) 102\u2013107. doi:10.1109/WI-IAT.2013.97. [35] Sun Y. Somani A. Carroll T. Learning based bidding strategy for HVAC systems in double auction retail energy markets Proceedings of the American control conference 2015 2015 2912 2917 10.1109/ACC.2015.7171177 Y. Sun, A. Somani, T. Carroll, Learning based bidding strategy for HVAC systems in double auction retail energy markets, Proceedings of the American Control Conference 2015-July (2015) 2912\u20132917. doi:10.1109/ACC.2015.7171177. [36] Kim J.G. Lee B. Automatic P2P energy trading model based on reinforcement learning using long short-term delayed reward Energies 13 20 2020 10.3390/en13205359 J. G. Kim, B. Lee, Automatic P2P energy trading model based on reinforcement learning using long short-term delayed reward, Energies 13 (20) (2020). doi:10.3390/en13205359. [37] Claessens B.J. Vandael S. Ruelens F. De Craemer K. Beusen B. Peak shaving of a heterogeneous cluster of residential flexibility carriers using reinforcement learning 2013 4th IEEE/PES innovative smart grid technologies Europe, ISGT Europe 2013 2013 1 5 10.1109/ISGTEurope.2013.6695254 B. J. Claessens, S. Vandael, F. Ruelens, K. De Craemer, B. Beusen, Peak shaving of a heterogeneous cluster of residential flexibility carriers using reinforcement learning, 2013 4th IEEE/PES Innovative Smart Grid Technologies Europe, ISGT Europe 2013 (2013) 1\u20135 doi:10.1109/ISGTEurope.2013.6695254. [38] Zhang X. Bao T. Yu T. Yang B. Han C. Deep transfer Q-learning with virtual leader-follower for supply\u2013demand stackelberg game of smart grid Energy 133 2017 348 365 10.1016/j.energy.2017.05.114 X. Zhang, T. Bao, T. Yu, B. Yang, C. Han, Deep transfer Q-learning with virtual leader-follower for supply-demand Stackelberg game of smart grid, Energy 133 (2017) 348\u2013365. doi:10.1016/j.energy.2017.05.114. [39] Dusparic I. Maximizing renewable energy use with decentralized residential demand response 2015 IEEE 1st International smart cities conference 2015 10.1109/ISC2.2015.7366212 I. Dusparic, Maximizing renewable energy use with decentralized residential demand response, 2015 IEEE 1st International Smart Cities Conference, ISC2 2015 (2015). doi:10.1109/ISC2.2015.7366212. [40] Dusparic I. Multi-agent residential demand response based on load forecasting 2013 1st IEEE conference on technologies for sustainability 2013 90 96 10.1109/SusTech.2013.6617303 I. Dusparic, Multi-agent residential demand response based on load forecasting, 2013 1st IEEE Conference on Technologies for Sustainability, SusTech 2013 (2013) 90\u201396 doi:10.1109/SusTech.2013.6617303. [41] Hurtado L.A. Mocanu E. Nguyen P.H. Gibescu M. Kamphuis R.I. Enabling cooperative behavior for building demand response based on extended joint action learning IEEE Trans Ind Inf 14 1 2018 127 136 10.1109/TII.2017.2753408 L. A. Hurtado, E. Mocanu, P. H. Nguyen, M. Gibescu, R. I. Kamphuis, Enabling Cooperative Behavior for Building Demand Response Based on Extended Joint Action Learning, IEEE Transactions on Industrial Informatics 14 (1) (2018) 127\u2013136. doi:10.1109/TII.2017.2753408. [42] Morstyn T. Mcculloch M. Peer-to-peer energy trading Analytics for the sharing economy: Mathematics, engineering and business perspectives (March) 2020 10.1007/978-3-030-35032-1 T. Morstyn, M. Mcculloch, Peer-to-Peer Energy Trading, Analytics for the Sharing Economy: Mathematics, Engineering and Business Perspectives (March) (2020). doi:10.1007/978-3-030-35032-1. [43] Taylor A. Accelerating learning in multi-objective systems through transfer learning Proc Int Joint Conf Neural Netw 2014 2298 2305 10.1109/IJCNN.2014.6889438 A. Taylor, Accelerating Learning in multi-objective systems through Transfer Learning, Proceedings of the International Joint Conference on Neural Networks (2014) 2298\u20132305 doi:10.1109/IJCNN.2014.6889438. [44] Herbert S. Models of bounded rationality 1982 MIT Press Cambridge, Mass. ; London S. Herbert, Models of bounded rationality, MIT Press, Cambridge, Mass. London, 1982. [45] Guerrero J. Gebbran D. Mhanna S. Chapman A.C. Verbi CČc G. Towards a transactive energy system for integration of distributed energy resources: Home energy management, distributed optimal power flow, and peer-to-peer energy trading Renew Sustain Energy Rev 132 2020 J. Guerrero, D. Gebbran, S. Mhanna, A. C. Chapman, G. Verbič, Towards a transactive energy system for integration of distributed energy resources: Home energy management, distributed optimal power flow, and peer-to-peer energy trading, Renewable & sustainable energy reviews 132 (2020). [46] Cao J. Deep reinforcement learning based energy storage arbitrage with accurate lithium-ion battery degradation model IEEE Trans Smart Grid 14 8 2019 1 9 J. Cao, Deep Reinforcement Learning Based Energy Storage Arbitrage With Accurate Lithium-ion Battery Degradation Model, IEEE Transactions on Smart Grid 14 (8) (2019) 1\u20139. [47] Yang Y. Hao J. Zheng Y. Yu C. Large-scale home energy management using entropy-based collective multiagent deep reinforcement learning framework 2019 630 636 Y. Yang, J. Hao, Y. Zheng, C. Yu, Large-Scale Home Energy Management Using Entropy-Based Collective Multiagent Deep Reinforcement Learning Framework (2019) 630\u2013636. [48] Crozier C. Apostolopoulou D. McCulloch M. Mitigating the impact of personal vehicle electrification: A power generation perspective Energy Policy 118 2013 2018 474 481 10.1016/j.enpol.2018.03.056 C. Crozier, D. Apostolopoulou, M. McCulloch, Mitigating the impact of personal vehicle electrification: A power generation perspective, Energy Policy 118 (2013) (2018) 474\u2013481. doi:10.1016/j.enpol.2018.03.056. [49] Rozada S. Apostolopoulou D. Alonso E. Load frequency control: A deep multi-agent reinforcement learning approach IEEE Power Energy Soc General Meeting 2020 2020 0 4 10.1109/PESGM41954.2020.9281614 S. Rozada, D. Apostolopoulou, E. Alonso, Load frequency control: A deep multi-agent reinforcement learning approach, IEEE Power and Energy Society General Meeting 2020-August (2020) 0\u20134. doi:10.1109/PESGM41954.2020.9281614. [50] Kraemer L. Banerjee B. Multi-agent reinforcement learning as a rehearsal for decentralized planning Neurocomputing 190 2016 82 94 10.1016/j.neucom.2016.01.031 L. Kraemer, B. Banerjee, Multi-agent reinforcement learning as a rehearsal for decentralized planning, Neurocomputing 190 (2016) 82\u201394. doi:10.1016/j.neucom.2016.01.031. [51] Buşoniu L. Babuška R. De Schutter B. A comprehensive survey of multiagent reinforcement learning IEEE Trans Syst, Man Cybern Part C: Appl Rev 38 2 2008 156 172 10.1109/TSMCC.2007.913919 L. Busoniu, R. Babuška, B. De Schutter, A comprehensive survey of multiagent reinforcement learning, IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews 38 (2) (2008) 156\u2013172. doi:10.1109/TSMCC.2007.913919. [52] Parry M. Climate change 2007: impacts, adaptation and vulnerability Published for the intergovernmental panel on climate change [by] 2007 Cambridge University Press Cambridge M. Parry, Climate change 2007: impacts, adaptation and vulnerability, Published for the Intergovernmental Panel on Climate Change [by] Cambridge University Press, Cambridge, 2007. [53] Morstyn T. McCulloch M. Multiclass energy management for peer-to-peer energy trading driven by prosumer preferences IEEE Trans Power Syst 34 5 2019 4005 4014 10.1109/TPWRS.2018.2834472 T. Morstyn, M. McCulloch, Multiclass Energy Management for Peer-to-Peer Energy Trading Driven by Prosumer Preferences, IEEE Transactions on Power Systems 34 (5) (2019) 4005\u20134014. doi:10.1109/TPWRS.2018.2834472. [54] Coffrin C. Van Hentenryck P. Bent R. Approximating line losses and apparent power in AC power flow linearizations IEEE Power Energy Soc General Meeting 2012 1 8 10.1109/PESGM.2012.6345342 C. Coffrin, P. Van Hentenryck, R. Bent, Approximating line losses and apparent power in AC power flow linearizations, IEEE Power and Energy Society General Meeting (2012) 1\u20138 doi:10.1109/PESGM.2012.6345342. [55] Morstyn T. Teytelboym A. Hepburn C. McCulloch M. Integrating P2P energy trading with probabilistic distribution locational marginal pricing IEEE Trans Smart Grid 11 4 2020 3095 3106 10.1109/TSG.2019.2963238 T. Morstyn, A. Teytelboym, C. Hepburn, M. McCulloch, Integrating P2P Energy Trading with Probabilistic Distribution Locational Marginal Pricing, IEEE Transactions on Smart Grid 11 (4) (2020) 3095\u20133106. doi:10.1109/TSG.2019.2963238. [56] Dufo-López R. Lujano-Rojas J.M. Bernal-Agustín J.L. Comparison of different lead\u2013acid battery lifetime prediction models for use in simulation of stand-alone photovoltaic systems Appl Energy 115 2014 242 253 R. Dufo-López, J. M. Lujano-Rojas, J. L. Bernal-Agustín, Comparison of different lead\u2013acid battery lifetime prediction models for use in simulation of stand-alone photovoltaic systems, Applied energy 115 (2014) 242\u2013253. [57] ISO Calculation of energy use for space heating and cooling ISO/FDIS 13790:2007(e) 2007 ISO, Calculation of Energy Use for Space Heating and Cooling ISO/FDIS 13790:2007(E) (2007). [58] Sachs O. Field evaluation of programmable thermostats 2012 O. Sachs, Field Evaluation of Programmable Thermostats (2012). [59] Matignon L. Laurent G.J. Le Fort-piat N. Hysteretic Q-learning : An algorithm for decentralized reinforcement learning in cooperative multi-agent teams Proceedings of the 2007 IEEE/RSJ international conference on intelligent robots and systems 2007 IEEE 64 69 L. Matignon, G. J. Laurent, N. Le Fort-piat, Hysteretic Q-Learning : an algorithm for Decentralized Reinforcement Learning in Cooperative Multi-Agent Teams ., in: Proceedings of the 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, IEEE, 2007, pp. 64\u201369. [60] Vinyals O. Grandmaster level in StarCraft II using multi-agent reinforcement learning Nature 575 November 2019 10.1038/s41586-019-1724-z O. Vinyals, Grandmaster level in StarCraft II using multi-agent reinforcement learning, Nature 575 (November) (2019). doi:10.1038/s41586-019-1724-z. [61] Wolpert David Tumer Kagan Optimal payoff functions for members of collectives Advances in Complex Systems 04 2002 10.1142/S0219525901000188 Wolpert, David Tumer, Kagan, 2002, 03, , Optimal Payoff Functions for Members of Collectives, 04, Advances in Complex Systems, [62] Foerster Jakob N. Farquhar Gregory Afouras Triantafyllos Nardelli Nantas Whiteson Shimon Counterfactual multi-agent policy gradients 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 2018 9781577358008 2974 2982 arXiv:1705.08926 arXiv, 1705.08926, Foerster, Jakob N. Farquhar, Gregory Afouras, Triantafyllos Nardelli, Nantas Whiteson, Shimon, 1705.08926, 9781577358008, 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, COMA,RL,credit assignment,multi-agent,reinforcement learning, mendeley-Reinforcement Learning, mendeley-COMA,RL,credit assignment,multi-agent,reinforcement learning, 2974\u20132982, Counterfactual multi-agent policy gradients [63] Wardle R. Dataset (TC1a): Basic profiling of domestic smart meter customers 2014 R. Wardle, Dataset (TC1a): Basic Profiling of Domestic Smart Meter Customers (2014). [64] Wardle R. Dataset (TC5): Enhanced profiling of domestic customers with solar photovoltaics (PV) 2014 R. Wardle, Dataset (TC5): Enhanced Profiling of Domestic Customers with Solar Photovoltaics (PV) (2014). [65] Department for Transport National travel survey 2002\u20132017 2019 10.5255/UKDA-SN-5340-10 Department for Transport, National Travel Survey 2002-2017 (2019). doi:http://doi.org/10.5255/UKDA-SN-5340-10. [66] Crozier C. Apostolopoulou D. McCulloch M. Numerical analysis of national travel data to assess the impact of UK fleet electrification 20th Power systems computation conference 2018 1 7 10.23919/PSCC.2018.8450584 arXiv:1711.01440 C. Crozier, D. Apostolopoulou, M. McCulloch, Numerical analysis of national travel data to assess the impact of UK fleet electrification, 20th Power Systems Computation Conference, PSCC 2018 (2018) 1\u20137 arXiv:1711.01440, doi:10.23919/PSCC.2018.8450584. [67] Lloyd S. Least squares quantization in PCM IEEE Trans Inform Theory 28 2 1982 129 137 10.1109/TIT.1982.1056489 S. Lloyd, Least squares quantization in PCM, IEEE Transactions on Information Theory 28 (2) (1982) 129\u2013137. doi:10.1109/TIT.1982.1056489. [68] Hirst D. Commons briefing paper SNO5927: Carbon price floor (CPF) and the price support mechanism 2018 D. Hirst, Commons Briefing Paper SNO5927: Carbon Price Floor (CPF) and the price support mechanism (2018). [69] Weather Wunderground London city airport weather history 2020 Weather Wunderground, London City Airport weather history (2020). [70] Octopus Energy Octopus energy API 2019 Octopus Energy, Octopus Energy API (2019). [71] National Grid ESO Environmental defense fund Europe 2020 University of Oxford Department of Computer Science WWF, Carbon Intensity API National Grid ESO, Environmental Defense Fund Europe, University of Oxford Department of Computer Science, WWF, Carbon Intensity API (2020). [72] Brown J, Chambers J, Rogers A. SMITE : Using Smart Meters to Infer the Thermal Efficiency of Residential Homes. In: The 7th ACM international conference on systems for energy- efficient buildings, cities, and transportation. 2020. J. Brown, J. Chambers, A. Rogers, SMITE : Using Smart Meters to Infer the Thermal Efficiency of Residential Homes, in: The 7th ACM International Conference on Systems for Energy- Efficient Buildings, Cities, and Transportation (BuildSys \u201920), 2020. [73] Tan M. Multi-agent reinforcement learning : Independent vs. Cooperative agents 1993 M. Tan, Multi-Agent Reinforcement Learning : Independent vs . Cooperative Agents (1993). [74] Rashid T. Farquhar G. Peng B. Whiteson S. Weighted QMIX: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning Adv Neural Inf Process Syst 2020 2020 arXiv:2006.10800 T. Rashid, G. Farquhar, B. Peng, S. Whiteson, Weighted QMIX: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning, Advances in Neural Information Processing Systems 2020-December (2020). arXiv:2006.10800. [75] HOMER Energy HOMER pro 3.14 user manual 2020 HOMER Energy, HOMER Pro 3.14 User Manual (2020). [76] Schram W. Empirical evaluation of V2G round-trip efficiency SEST 2020-3rd international conference on smart energy systems and technologies (October) 2020 10.1109/SEST48500.2020.9203459 W. Schram, Empirical evaluation of V2G round-trip efficiency, SEST 2020 - 3rd International Conference on Smart Energy Systems and Technologies (October) (2020). doi:10.1109/SEST48500.2020.9203459. [77] Becker V. Kleiminger W. Coroamă V. Mattern F. Estimating the savings potential of occupancy-based heating strategies Energy Inform 1 S1 2018 10.1186/s42162-018-0022-6 V. Becker, W. Kleiminger, V. Coroamă, F. Mattern, Estimating the savings potential of occupancy-based heating strategies, Energy Informatics 1 (S1) (2018). doi:10.1186/s42162-018-0022-6. [78] BRE Sap 2012 9.92 the government\u2019s standard assessment procedure for energy rating of dwellings 2014 arXiv:9809069v1 BRE, SAP 2012 9.92 The Government\u2019s Standard Assessment Procedure for Energy Rating of Dwellings (2014). arXiv:9809069v1. [79] British Standards Heating systems in buildings. Method for calculation of the design heat load 2009 1 89 Ics 91.140.10 (January) British Standards, Heating systems in buildings. Method for calculation of the design heat load, Ics 91.140.10 (January) (2009) 1\u201389.",
    "scopus-id": "85126650177",
    "coredata": {
        "eid": "1-s2.0-S0306261922002689",
        "dc:description": "This paper proposes a novel scalable type of multi-agent reinforcement learning-based coordination for distributed residential energy. Cooperating agents learn to control the flexibility offered by electric vehicles, space heating and flexible loads in a partially observable stochastic environment. In the standard independent Q-learning approach, the coordination performance of agents under partial observability drops at scale in stochastic environments. Here, the novel combination of learning from off-line convex optimisations on historical data and isolating marginal contributions to total rewards in reward signals increases stability and performance at scale. Using fixed-size Q-tables, prosumers are able to assess their marginal impact on total system objectives without sharing personal data either with each other or with a central coordinator. Case studies are used to assess the fitness of different combinations of exploration sources, reward definitions, and multi-agent learning frameworks. It is demonstrated that the proposed strategies create value at individual and system levels thanks to reductions in the costs of energy imports, losses, distribution network congestion, battery depreciation and greenhouse gas emissions.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2022-05-15",
        "openaccessUserLicense": "http://creativecommons.org/licenses/by/4.0/",
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S0306261922002689",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Charbonnier, Flora"
            },
            {
                "@_fa": "true",
                "$": "Morstyn, Thomas"
            },
            {
                "@_fa": "true",
                "$": "McCulloch, Malcolm D."
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S0306261922002689"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S0306261922002689"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": "Full",
        "pii": "S0306-2619(22)00268-9",
        "prism:volume": "314",
        "articleNumber": "118825",
        "prism:publisher": "The Authors. Published by Elsevier Ltd.",
        "dc:title": "Scalable multi-agent reinforcement learning for distributed control of residential energy flexibility",
        "prism:copyright": "© 2022 The Authors. Published by Elsevier Ltd.",
        "openaccess": "1",
        "prism:issn": "03062619",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "Energy management system"
            },
            {
                "@_fa": "true",
                "$": "Multi-agent reinforcement learning"
            },
            {
                "@_fa": "true",
                "$": "Demand-side response"
            },
            {
                "@_fa": "true",
                "$": "Peer-to-peer"
            },
            {
                "@_fa": "true",
                "$": "Prosumer"
            },
            {
                "@_fa": "true",
                "$": "Smart grid"
            }
        ],
        "openaccessArticle": "true",
        "prism:publicationName": "Applied Energy",
        "openaccessSponsorType": "FundingBody",
        "prism:pageRange": "118825",
        "pubType": "fla",
        "prism:coverDisplayDate": "15 May 2022",
        "prism:doi": "10.1016/j.apenergy.2022.118825",
        "prism:startingPage": "118825",
        "dc:identifier": "doi:10.1016/j.apenergy.2022.118825",
        "openaccessSponsorName": "JISC UK 2022: Hybrid journals"
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "200",
            "@width": "489",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922002689-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "108438",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "211",
            "@width": "523",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922002689-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "105692",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "222",
            "@width": "516",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922002689-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "113073",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "330",
            "@width": "697",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922002689-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "169088",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "90",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922002689-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "74916",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "88",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922002689-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "75631",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "94",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922002689-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "73347",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "104",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922002689-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "78536",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "887",
            "@width": "2167",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922002689-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "341351",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "935",
            "@width": "2316",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922002689-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "368946",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "983",
            "@width": "2285",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922002689-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "337601",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1460",
            "@width": "3085",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922002689-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "807015",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922002689-mmc2.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "APPLICATION",
            "@size": "326845",
            "@ref": "mmc2",
            "@mimetype": "application/pdf"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922002689-mmc1.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "APPLICATION",
            "@size": "347752",
            "@ref": "mmc1",
            "@mimetype": "application/pdf"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261922002689-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "1223508",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85126650177"
    }
}}