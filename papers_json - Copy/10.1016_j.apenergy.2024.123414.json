{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85192857528",
    "originalText": "serial JL 271429 291210 291702 291731 291787 291877 291878 291881 31 Applied Energy APPLIEDENERGY 2024-05-14 2024-05-14 2024-05-14 2024-05-14 2024-09-19T14:59:20 1-s2.0-S0306261924007979 S0306-2619(24)00797-9 S0306261924007979 10.1016/j.apenergy.2024.123414 S300 S300.1 FULL-TEXT 1-s2.0-S0306261924X00100 2024-09-19T14:33:31.253176Z 0 0 20240801 2024 2024-05-14T16:30:02.174346Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor grantsponsorid highlightsabst nomenclature primabst ref 0306-2619 03062619 true 367 367 C Volume 367 61 123414 123414 123414 20240801 1 August 2024 2024-08-01 2024 Research Papers article fla © 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies. SCALABLEENERGYMANAGEMENTAPPROACHRESIDENTIALHYBRIDENERGYSYSTEMUSINGMULTIAGENTDEEPREINFORCEMENTLEARNING WANG Z Nomenclature 1 Introduction 2 Related work 2.1 Literature review on RL approaches for building energy management 2.1.1 Single-agent control 2.1.2 Multi-agent control 2.2 Contributions 3 Methodology 3.1 Reinforcement learning 3.1.1 Markov decision process 3.1.2 Value-based learning and policy-based learning 3.1.3 On-policy and off-policy learning 3.2 Model-based simulation 3.3 Single and multi-agent environment and systems theory 3.4 Reinforcement learning algorithms in multi-agent systems 3.4.1 Multi-agent reward functions 3.4.2 Imitation learning 3.5 Approaches to training and execution in MADRL 4 Case study 4.1 Introduction of ZEH and dataset 4.2 Multi-stage MAPPO framework 4.3 Building thermal dynamic modeling 4.4 MADRL structure in the case study 4.5 Integration of MARL with energy systems 4.6 Experiment setting 4.7 Training process 5 Result and discussion 5.1 Result of the training process 5.2 Comparison of testing result 5.2.1 Indoor temperature fluctuations 5.2.2 Dynamic energy management 5.2.3 Cost analysis 6 Conclusion and future work CRediT authorship contribution statement Acknowledgments Appendix A Multi-agent algorithm A.1 Q-learning A.2 DQN A.3 Actor-critic (AC) method A.4 PPO References WEI 2022 52 63 Y CLARKE 2021 1087 1089 J WU 2020 115656 W SVETOZAREVIC 2019 671 682 B GIELEN 2019 38 50 D TAIMINISTRYOFECONOMYJAPAN 2021 2021 UNDERSTANDINGCURRENTENERGYSITUATIONINJAPANPART1 ZHANG 2021 X LI 2020 109862 Y TASUKUKUWABARA 2021 D HOWJAPANREACHCARBONNEUTRALITYBY2050 ZHANG 2024 109047 X FU 2021 Y JORISSEN 2019 180 192 F ARROYO 2020 472 486 J ATAM 2016 86 111 E ARROYO 2022 J WEI 2017 1 6 T PROCEEDINGS54THANNUALDESIGNAUTOMATIONCONFERENCE2017 DEEPREINFORCEMENTLEARNINGFORBUILDINGHVACCONTROL MORIYAMA 2018 45 59 T METHODSAPPLICATIONSFORMODELINGSIMULATIONCOMPLEXSYSTEMS REINFORCEMENTLEARNINGTESTBEDFORPOWERCONSUMPTIONOPTIMIZATION ZHANG 2018 148 157 Z PROCEEDINGS5THCONFERENCESYSTEMSFORBUILTENVIRONMENTS PRACTICALIMPLEMENTATIONEVALUATIONDEEPREINFORCEMENTLEARNINGCONTROLFORARADIANTHEATINGSYSTEM ARULKUMARAN 2017 26 38 K YU 2021 407 419 L JIANG 2018 158 168 C ANDERSON 1997 421 429 C MOZER 1998 M PROCAAAISPRINGSYMPINTELLIGENTENVIRONMENTS NEURALNETWORKHOUSEENVIRONMENTHATADAPTSINHABITANTS FADDEL 2020 1 6 S 2020SOUTHEASTCON DATADRIVENQLEARNINGFORCOMMERCIALHVACCONTROL SOARES 2020 A YAN 2010 Q RESEARCHESCURSEDIMENSIONALITYINREINFORCEMENTLEARNING MNIH 2015 529 533 V BRANDI 2020 110225 S WANG 2023 X AZUATALAM 2020 D SCHREIBER 2020 T YANG 2021 T DU 2021 106959 Y BIEMANN 2021 M GAO 2022 Y CORACI 2023 D LI 2023 127627 Y QIN 2023 126209 H WEINBERG 2023 D NGUYEN 2020 3826 3839 T KRNJAIC 2022 A SCALABLEMULTIAGENTREINFORCEMENTLEARNINGFORWAREHOUSELOGISTICSROBOTICHUMANCOWORKERS SHALEVSHWARTZ 2016 S SAFEMULTIAGENTREINFORCEMENTLEARNINGFORAUTONOMOUSDRIVING PEAKE 2020 15 22 A 2020IEEE32NDINTERNATIONALCONFERENCETOOLSARTIFICIALINTELLIGENCEICTAI MULTIAGENTREINFORCEMENTLEARNINGFORCOOPERATIVEADAPTIVECRUISECONTROL ZHOU 2020 M SMARTSSCALABLEMULTIAGENTREINFORCEMENTLEARNINGTRAININGSCHOOLFORAUTONOMOUSDRIVING ROESCH 2020 6900 M QIU 2021 2913 2920 D SHAVANDI 2022 118124 A GAO 2022 120021 Y NWEYE 2022 100202 K SHEN 2022 R YU 2022 109458 L SUTTON 2018 R REINFORCEMENTLEARNINGINTRODUCTION SONG 1999 39 H KEPCOMPANY BROCKMAN 2016 G OPENAIGYM NAGY 2023 Z MNIH 2013 V PLAYINGATARIDEEPREINFORCEMENTLEARNING LILLICRAP 2015 T CONTINUOUSCONTROLDEEPREINFORCEMENTLEARNING FUJIMOTO 2018 1587 1596 S INTERNATIONALCONFERENCEMACHINELEARNING ADDRESSINGFUNCTIONAPPROXIMATIONERRORINACTORCRITICMETHODS SCHULMAN 2017 J PROXIMALPOLICYOPTIMIZATIONALGORITHMS SCHULMAN 2015 1889 1897 J INTERNATIONALCONFERENCEMACHINELEARNING TRUSTREGIONPOLICYOPTIMIZATION WANGX2024X123414 WANGX2024X123414XZ 2026-05-14T00:00:00.000Z 2026-05-14T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies. 2024-05-16T04:26:26.490Z http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/aggregated-refined Xiangjiang Plan \u201cDevelopment of Smart Building Management Technologies Towards Carbon Neutrality XJ20220028 National Natural Science Foundation of China 52308098 NSFC National Natural Science Foundation of China http://data.elsevier.com/vocabulary/SciValFunders/501100001809 http://sws.geonames.org/1814991 Shandong Natural Science Foundation ZR2021QE084 Natural Science Foundation of Shandong Province http://data.elsevier.com/vocabulary/SciValFunders/501100007129 http://sws.geonames.org/1814991/ This study was supported by the National Natural Science Foundation of China \\u201CResearch on operation optimization strategy of energy flexible buildings based on synergizing data-driven and physics mechanism approach\\u201D (No. 52308098 ), the Shandong Natural Science Foundation \\u201CResearch on Flexible District Integrated Energy System under High Penetration Level of Renewable Energy\\u201D (No. ZR2021QE084 ) and the Xiangjiang Plan \\u201CDevelopment of Smart Building Management Technologies Towards Carbon Neutrality\\u201D (No. XJ20220028 ). https://doi.org/10.15223/policy-017 https://doi.org/10.15223/policy-037 https://doi.org/10.15223/policy-012 https://doi.org/10.15223/policy-029 https://doi.org/10.15223/policy-004 item S0306-2619(24)00797-9 S0306261924007979 1-s2.0-S0306261924007979 10.1016/j.apenergy.2024.123414 271429 2024-09-19T14:33:31.253176Z 2024-08-01 1-s2.0-S0306261924007979-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/MAIN/application/pdf/ed7560275c6d1466fa8d5f72ec140440/main.pdf main.pdf pdf true 5431663 MAIN 20 1-s2.0-S0306261924007979-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/PREVIEW/image/png/941c8c894d5208fe48dba438655e788e/main_1.png main_1.png png 49516 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0306261924007979-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr3/DOWNSAMPLED/image/jpeg/788689bed25c5b04bcb9f6aa7fb63910/gr3.jpg gr3 gr3.jpg jpg 108468 488 726 IMAGE-DOWNSAMPLED 1-s2.0-S0306261924007979-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr4/DOWNSAMPLED/image/jpeg/6d7e61cfe42d4d7eb93473e2720a54bc/gr4.jpg gr4 gr4.jpg jpg 126229 808 774 IMAGE-DOWNSAMPLED 1-s2.0-S0306261924007979-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr5/DOWNSAMPLED/image/jpeg/277f3388540a687934cef37a422a2bc1/gr5.jpg gr5 gr5.jpg jpg 43831 422 687 IMAGE-DOWNSAMPLED 1-s2.0-S0306261924007979-gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr10/DOWNSAMPLED/image/jpeg/60fe62ae37cbe16fa15ec9d8b6015bb2/gr10.jpg gr10 gr10.jpg jpg 149746 716 758 IMAGE-DOWNSAMPLED 1-s2.0-S0306261924007979-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr6/DOWNSAMPLED/image/jpeg/dd4eef30fe9b502ba755e813492b9ab0/gr6.jpg gr6 gr6.jpg jpg 42279 354 535 IMAGE-DOWNSAMPLED 1-s2.0-S0306261924007979-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr1/DOWNSAMPLED/image/jpeg/c6df76aa16b8f860baaffbb84d9f9c99/gr1.jpg gr1 gr1.jpg jpg 29153 337 717 IMAGE-DOWNSAMPLED 1-s2.0-S0306261924007979-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr2/DOWNSAMPLED/image/jpeg/a6d0d1aba86b687fae4c2fa96a92e654/gr2.jpg gr2 gr2.jpg jpg 61894 351 802 IMAGE-DOWNSAMPLED 1-s2.0-S0306261924007979-fx2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/fx2/DOWNSAMPLED/image/jpeg/4222d970e8c7af36b83e25b18e3ba3ae/fx2.jpg fx2 fx2.jpg jpg 42026 505 389 IMAGE-DOWNSAMPLED 1-s2.0-S0306261924007979-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr7/DOWNSAMPLED/image/jpeg/47f766cd3ad9401c5ba74607c812a490/gr7.jpg gr7 gr7.jpg jpg 34883 367 546 IMAGE-DOWNSAMPLED 1-s2.0-S0306261924007979-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr8/DOWNSAMPLED/image/jpeg/f765311344f2f1991445596751f4ebc7/gr8.jpg gr8 gr8.jpg jpg 83436 745 801 IMAGE-DOWNSAMPLED 1-s2.0-S0306261924007979-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr9/DOWNSAMPLED/image/jpeg/1a28522066e94db7160f4bdd2b639b35/gr9.jpg gr9 gr9.jpg jpg 201377 754 800 IMAGE-DOWNSAMPLED 1-s2.0-S0306261924007979-fx1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/fx1/DOWNSAMPLED/image/jpeg/35a19fbfde73c570723a7f4b64899d01/fx1.jpg fx1 fx1.jpg jpg 54676 437 491 IMAGE-DOWNSAMPLED 1-s2.0-S0306261924007979-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr3/THUMBNAIL/image/gif/31c472da1524141fec91e271cba11f34/gr3.sml gr3 gr3.sml sml 15692 147 219 IMAGE-THUMBNAIL 1-s2.0-S0306261924007979-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr4/THUMBNAIL/image/gif/9849e565075ebab07637998b1076a03c/gr4.sml gr4 gr4.sml sml 10294 164 157 IMAGE-THUMBNAIL 1-s2.0-S0306261924007979-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr5/THUMBNAIL/image/gif/596b49e9a374ba9f8b450ee9e6420278/gr5.sml gr5 gr5.sml sml 6004 134 219 IMAGE-THUMBNAIL 1-s2.0-S0306261924007979-gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr10/THUMBNAIL/image/gif/f0d4986ab20ff5798386566dbdd21668/gr10.sml gr10 gr10.sml sml 13975 163 173 IMAGE-THUMBNAIL 1-s2.0-S0306261924007979-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr6/THUMBNAIL/image/gif/68c18f9966a1625790848e216d241faa/gr6.sml gr6 gr6.sml sml 9020 145 219 IMAGE-THUMBNAIL 1-s2.0-S0306261924007979-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr1/THUMBNAIL/image/gif/88234fbde057919b1bc1f0f3032eafbb/gr1.sml gr1 gr1.sml sml 5573 103 219 IMAGE-THUMBNAIL 1-s2.0-S0306261924007979-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr2/THUMBNAIL/image/gif/9fb0b137f5b020575376240368e3294f/gr2.sml gr2 gr2.sml sml 7481 96 219 IMAGE-THUMBNAIL 1-s2.0-S0306261924007979-fx2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/fx2/THUMBNAIL/image/gif/7acf1b0d4d6cc0557fb15901d4d9952c/fx2.sml fx2 fx2.sml sml 5075 164 126 IMAGE-THUMBNAIL 1-s2.0-S0306261924007979-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr7/THUMBNAIL/image/gif/2a6f554beb598b559ea5fb9cfab5740c/gr7.sml gr7 gr7.sml sml 6373 147 219 IMAGE-THUMBNAIL 1-s2.0-S0306261924007979-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr8/THUMBNAIL/image/gif/23c27fc7664b06fdb9479028d3795fb0/gr8.sml gr8 gr8.sml sml 7448 164 176 IMAGE-THUMBNAIL 1-s2.0-S0306261924007979-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/gr9/THUMBNAIL/image/gif/c97b2213faf77e4077c257865c1ffc59/gr9.sml gr9 gr9.sml sml 13331 164 174 IMAGE-THUMBNAIL 1-s2.0-S0306261924007979-fx1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/fx1/THUMBNAIL/image/gif/29666a05ab5c8578166825fd88aff0d9/fx1.sml fx1 fx1.sml sml 7778 164 184 IMAGE-THUMBNAIL 1-s2.0-S0306261924007979-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/HIGHRES/image/jpeg/dc65c4c5c634439c35562b8449465e91/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 1079937 2160 3215 IMAGE-HIGH-RES 1-s2.0-S0306261924007979-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/HIGHRES/image/jpeg/475eeb6088a8982888f65c690f7159fd/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 935658 3576 3427 IMAGE-HIGH-RES 1-s2.0-S0306261924007979-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/HIGHRES/image/jpeg/11a7852ad412922e1ce6b86f45202681/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 288160 1868 3042 IMAGE-HIGH-RES 1-s2.0-S0306261924007979-gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/HIGHRES/image/jpeg/b3de435eb4052c2d98e58004b35d77eb/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 1318923 3167 3354 IMAGE-HIGH-RES 1-s2.0-S0306261924007979-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/HIGHRES/image/jpeg/f1a12dfa23d7ec6d50fe557a2c56f51c/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 330710 1569 2368 IMAGE-HIGH-RES 1-s2.0-S0306261924007979-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/HIGHRES/image/jpeg/c18e0dfb8629fe24a306f6664cfd70b9/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 229240 1492 3174 IMAGE-HIGH-RES 1-s2.0-S0306261924007979-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/HIGHRES/image/jpeg/1948d5f3d0f2610a20d2db958a7c3df2/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 476077 1555 3549 IMAGE-HIGH-RES 1-s2.0-S0306261924007979-fx2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/HIGHRES/image/jpeg/b3b874c7803bae3c5e75c1447b363cf0/fx2_lrg.jpg fx2 fx2_lrg.jpg jpg 301465 2237 1723 IMAGE-HIGH-RES 1-s2.0-S0306261924007979-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/HIGHRES/image/jpeg/c8bd0b7ce3b3450ca5bc43d5f98eadc5/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 278842 1625 2417 IMAGE-HIGH-RES 1-s2.0-S0306261924007979-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/HIGHRES/image/jpeg/d0d42253752e8f6b37dec6783d21fcb5/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 550871 3298 3548 IMAGE-HIGH-RES 1-s2.0-S0306261924007979-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/HIGHRES/image/jpeg/7f50592dcf0c957773014de719c59691/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 799529 2004 2126 IMAGE-HIGH-RES 1-s2.0-S0306261924007979-fx1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/HIGHRES/image/jpeg/9998baf739d2a66e8a91b13cf12d787d/fx1_lrg.jpg fx1 fx1_lrg.jpg jpg 410586 1936 2175 IMAGE-HIGH-RES 1-s2.0-S0306261924007979-si1.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/a33dd25f997a1184aa5527dd8def1142/si1.svg si1 si1.svg svg 1689 ALTIMG 1-s2.0-S0306261924007979-si10.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/c9951f9e0afc0ac14de95dee11e59231/si10.svg si10 si10.svg svg 2492 ALTIMG 1-s2.0-S0306261924007979-si100.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/c6fcf06553bea4598c9a8c3491e78540/si100.svg si100 si100.svg svg 3902 ALTIMG 1-s2.0-S0306261924007979-si101.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/15c485b347f517fb761c5d4f0d9fb5ad/si101.svg si101 si101.svg svg 6182 ALTIMG 1-s2.0-S0306261924007979-si103.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/b5bcdb84b30e9796c5761d3b1c70c51e/si103.svg si103 si103.svg svg 1972 ALTIMG 1-s2.0-S0306261924007979-si106.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/f870b0e57a58c56a734b43b161591004/si106.svg si106 si106.svg svg 3683 ALTIMG 1-s2.0-S0306261924007979-si107.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/1073ed428afdb5858bf78a1c5ee1996e/si107.svg si107 si107.svg svg 1210 ALTIMG 1-s2.0-S0306261924007979-si109.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/f5c2068efbe4ac998adee61f1df5eb51/si109.svg si109 si109.svg svg 12065 ALTIMG 1-s2.0-S0306261924007979-si11.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/9a339f583523b71fc91b32c55810b379/si11.svg si11 si11.svg svg 970 ALTIMG 1-s2.0-S0306261924007979-si114.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/ee24f3183337ccb05493e81219e2e5bf/si114.svg si114 si114.svg svg 5077 ALTIMG 1-s2.0-S0306261924007979-si117.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/b06fef93a2c37817aba6f41a7ec95f96/si117.svg si117 si117.svg svg 1182 ALTIMG 1-s2.0-S0306261924007979-si12.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/f2481b4110987285609f56ab2fcf0199/si12.svg si12 si12.svg svg 804 ALTIMG 1-s2.0-S0306261924007979-si120.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/a433f75dffd1a7725d175b1da254cb89/si120.svg si120 si120.svg svg 1032 ALTIMG 1-s2.0-S0306261924007979-si122.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/11c595e7dd337418b4a30d7826a4bf0e/si122.svg si122 si122.svg svg 1659 ALTIMG 1-s2.0-S0306261924007979-si125.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/37b91ac5d77bc818cb6ea61e4a965354/si125.svg si125 si125.svg svg 1582 ALTIMG 1-s2.0-S0306261924007979-si126.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/5e1a15c8eaf2f415127e786f002df290/si126.svg si126 si126.svg svg 2378 ALTIMG 1-s2.0-S0306261924007979-si127.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/0574ca1c72175189fc09bdec11521957/si127.svg si127 si127.svg svg 858 ALTIMG 1-s2.0-S0306261924007979-si128.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/8aaa4171d8d555f42d1fc535560460bb/si128.svg si128 si128.svg svg 4183 ALTIMG 1-s2.0-S0306261924007979-si13.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/3d1673cb9cae42d4ea64bc992d85311b/si13.svg si13 si13.svg svg 1092 ALTIMG 1-s2.0-S0306261924007979-si130.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/6f31d7b84eeade28821114e77ce12307/si130.svg si130 si130.svg svg 9974 ALTIMG 1-s2.0-S0306261924007979-si131.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/45712c1dcdfb9cec16411378e8cd97ad/si131.svg si131 si131.svg svg 21640 ALTIMG 1-s2.0-S0306261924007979-si132.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/6c77a3143def0d889912a4f05216f5cb/si132.svg si132 si132.svg svg 6061 ALTIMG 1-s2.0-S0306261924007979-si134.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/9a4bb31d7dddd1132bfd17caba02f35a/si134.svg si134 si134.svg svg 2346 ALTIMG 1-s2.0-S0306261924007979-si135.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/616884948bf2a2a8e5fd9d112f9f2a13/si135.svg si135 si135.svg svg 2424 ALTIMG 1-s2.0-S0306261924007979-si136.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/0dab4a3edcd96281a77549c6f23a13c6/si136.svg si136 si136.svg svg 20207 ALTIMG 1-s2.0-S0306261924007979-si137.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/74db69538a2c4dc688ed800da2c0d745/si137.svg si137 si137.svg svg 8577 ALTIMG 1-s2.0-S0306261924007979-si138.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/1f7f68cbb477badcc19c710fa861c621/si138.svg si138 si138.svg svg 483 ALTIMG 1-s2.0-S0306261924007979-si139.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/1d7c65cd269091e42cd13512bbdf8e83/si139.svg si139 si139.svg svg 2723 ALTIMG 1-s2.0-S0306261924007979-si14.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/b826d902ccb3afa9431f4415beb5beae/si14.svg si14 si14.svg svg 911 ALTIMG 1-s2.0-S0306261924007979-si140.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/3ba5c0ef3d6d693fceca9e5b20c3b549/si140.svg si140 si140.svg svg 1419 ALTIMG 1-s2.0-S0306261924007979-si15.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/da17d0c70763cfc0c91a3b79d8a90387/si15.svg si15 si15.svg svg 4337 ALTIMG 1-s2.0-S0306261924007979-si16.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/16461f26df0960c8c81d366d8ef4bbc9/si16.svg si16 si16.svg svg 989 ALTIMG 1-s2.0-S0306261924007979-si17.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/78c3bd3ac69bd4fda7cb7d923121370e/si17.svg si17 si17.svg svg 1301 ALTIMG 1-s2.0-S0306261924007979-si18.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/7e5eb2918da449f217e0d6e13e4e1c61/si18.svg si18 si18.svg svg 994 ALTIMG 1-s2.0-S0306261924007979-si2.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/291c954ed42310fdc90853ef3e16b238/si2.svg si2 si2.svg svg 3018 ALTIMG 1-s2.0-S0306261924007979-si22.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/693e14d820910297a628f24e52f87cc3/si22.svg si22 si22.svg svg 892 ALTIMG 1-s2.0-S0306261924007979-si24.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/8f9d2d296dd515474c27138d9261dcc6/si24.svg si24 si24.svg svg 13882 ALTIMG 1-s2.0-S0306261924007979-si25.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/f27c9cc3d77f65bdb7cbb4845d2d040e/si25.svg si25 si25.svg svg 2078 ALTIMG 1-s2.0-S0306261924007979-si26.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/390aa0654c5e28b2dcbc8a436f1c404a/si26.svg si26 si26.svg svg 917 ALTIMG 1-s2.0-S0306261924007979-si27.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/a875ce1f735335682cdaefe104218b00/si27.svg si27 si27.svg svg 1863 ALTIMG 1-s2.0-S0306261924007979-si28.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/7ec6608deab36d6d37a4cfd8211c08b6/si28.svg si28 si28.svg svg 1238 ALTIMG 1-s2.0-S0306261924007979-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/9aa6aa158e0bfeb024c8e61fde1323d9/si3.svg si3 si3.svg svg 2412 ALTIMG 1-s2.0-S0306261924007979-si30.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/c1d6e3cf7b167390112764c7c294243c/si30.svg si30 si30.svg svg 4376 ALTIMG 1-s2.0-S0306261924007979-si31.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/de432f9e8b3c0303d06ffeed2e5183e6/si31.svg si31 si31.svg svg 847 ALTIMG 1-s2.0-S0306261924007979-si32.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/1b4459408c83742fec7c8dbc2e25095c/si32.svg si32 si32.svg svg 2214 ALTIMG 1-s2.0-S0306261924007979-si33.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/08ae05a97e7f0c4ea9256fb3ef8c82cd/si33.svg si33 si33.svg svg 11375 ALTIMG 1-s2.0-S0306261924007979-si34.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/924aca5d178811858869c2c0a2bb664b/si34.svg si34 si34.svg svg 4640 ALTIMG 1-s2.0-S0306261924007979-si35.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/2d2852442d17a9f06d4a18c164ccf4e5/si35.svg si35 si35.svg svg 1665 ALTIMG 1-s2.0-S0306261924007979-si36.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/bb3b605570426a1cae6d3f5f98344888/si36.svg si36 si36.svg svg 941 ALTIMG 1-s2.0-S0306261924007979-si38.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/aab7788aef398e4a9e66a8604e09c21a/si38.svg si38 si38.svg svg 1497 ALTIMG 1-s2.0-S0306261924007979-si39.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/b0bf4d897492b8dfa6e36214304d5419/si39.svg si39 si39.svg svg 9032 ALTIMG 1-s2.0-S0306261924007979-si4.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/d1c63d868f6223943403ae05876ef51c/si4.svg si4 si4.svg svg 1538 ALTIMG 1-s2.0-S0306261924007979-si40.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/82a5a553d351ac8a899f01f731527933/si40.svg si40 si40.svg svg 978 ALTIMG 1-s2.0-S0306261924007979-si41.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/2561e746aab24c285a1abab0b5896352/si41.svg si41 si41.svg svg 1701 ALTIMG 1-s2.0-S0306261924007979-si43.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/666a649fd29557a259bddcce67597e59/si43.svg si43 si43.svg svg 8985 ALTIMG 1-s2.0-S0306261924007979-si44.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/58bcb165e878a378dc9e0901b55406f5/si44.svg si44 si44.svg svg 10356 ALTIMG 1-s2.0-S0306261924007979-si48.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/7dda5497e92e95e25fdfaa157d132207/si48.svg si48 si48.svg svg 1038 ALTIMG 1-s2.0-S0306261924007979-si49.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/822a14e688128c00b57b667eccc98eaf/si49.svg si49 si49.svg svg 9524 ALTIMG 1-s2.0-S0306261924007979-si5.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/a72255515538e5f62ff193e7163f68ff/si5.svg si5 si5.svg svg 5277 ALTIMG 1-s2.0-S0306261924007979-si50.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/1ebf045fb3a7115c45756890276e8877/si50.svg si50 si50.svg svg 3669 ALTIMG 1-s2.0-S0306261924007979-si51.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/1758d73a62018319f10f9a8afc2ed5bb/si51.svg si51 si51.svg svg 1935 ALTIMG 1-s2.0-S0306261924007979-si53.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/16d03cd44ae46775dba63746604b098f/si53.svg si53 si53.svg svg 8744 ALTIMG 1-s2.0-S0306261924007979-si54.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/fb556cd9deb6a6ef5392eb2478fb32bb/si54.svg si54 si54.svg svg 13368 ALTIMG 1-s2.0-S0306261924007979-si55.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/1373ea04ec0c927f6105a3f3ee3d1d35/si55.svg si55 si55.svg svg 26491 ALTIMG 1-s2.0-S0306261924007979-si56.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/f9b5e5bb069820d08957844a67fbfe84/si56.svg si56 si56.svg svg 1641 ALTIMG 1-s2.0-S0306261924007979-si57.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/18f837b121b715c0116c3b43cfcc295d/si57.svg si57 si57.svg svg 3932 ALTIMG 1-s2.0-S0306261924007979-si6.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/089d92095a07abdcb56d7621feb2ca7e/si6.svg si6 si6.svg svg 4523 ALTIMG 1-s2.0-S0306261924007979-si61.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/4c79ed64b6ade43b1bda4063490af0a4/si61.svg si61 si61.svg svg 5134 ALTIMG 1-s2.0-S0306261924007979-si62.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/baf6170f88ceec03d4a10f9fe3699d21/si62.svg si62 si62.svg svg 9406 ALTIMG 1-s2.0-S0306261924007979-si64.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/98529d48d51c70d23f93c83dd2b2d069/si64.svg si64 si64.svg svg 2070 ALTIMG 1-s2.0-S0306261924007979-si66.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/d9eaebcd963767a92579de3a00c7ad1b/si66.svg si66 si66.svg svg 7064 ALTIMG 1-s2.0-S0306261924007979-si67.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/c72dcfd03ce1978fdd7b42d9bbf6b581/si67.svg si67 si67.svg svg 6570 ALTIMG 1-s2.0-S0306261924007979-si68.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/e06037c947f991f582c6827f68c2edb6/si68.svg si68 si68.svg svg 3026 ALTIMG 1-s2.0-S0306261924007979-si69.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/e1c08d9b9156e827c9ea6b6a12aa6b82/si69.svg si69 si69.svg svg 8387 ALTIMG 1-s2.0-S0306261924007979-si7.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/6d1ce9a1c3bcfa6224d800954815ae9b/si7.svg si7 si7.svg svg 3557 ALTIMG 1-s2.0-S0306261924007979-si70.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/ccc44f3e2690fcc68432d3ac05cba74d/si70.svg si70 si70.svg svg 7816 ALTIMG 1-s2.0-S0306261924007979-si72.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/f29fec8fd9d5b3bb03d4e9753447649c/si72.svg si72 si72.svg svg 14636 ALTIMG 1-s2.0-S0306261924007979-si73.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/5ed3576992e901369ed83de7c67d15ae/si73.svg si73 si73.svg svg 1292 ALTIMG 1-s2.0-S0306261924007979-si74.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/99370542f37473288c03ab79b69d8c04/si74.svg si74 si74.svg svg 7771 ALTIMG 1-s2.0-S0306261924007979-si75.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/0298fef603dbcf42db523b5a3c9c3b97/si75.svg si75 si75.svg svg 1301 ALTIMG 1-s2.0-S0306261924007979-si76.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/5e11e1aa3681ef2bddf82010f0f1e06d/si76.svg si76 si76.svg svg 1695 ALTIMG 1-s2.0-S0306261924007979-si79.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/902642a5bc3d7ce40dec3b76e71b87d7/si79.svg si79 si79.svg svg 11682 ALTIMG 1-s2.0-S0306261924007979-si81.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/4b5708173bdef8286f9161b911bc536f/si81.svg si81 si81.svg svg 3012 ALTIMG 1-s2.0-S0306261924007979-si83.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/b93d917359f06eeb5317c3fc13cea381/si83.svg si83 si83.svg svg 4333 ALTIMG 1-s2.0-S0306261924007979-si88.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/3c2a9a46f8052fc3aa7e904d325d18cf/si88.svg si88 si88.svg svg 911 ALTIMG 1-s2.0-S0306261924007979-si90.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/f231542cf07d73db35d0959bb53a363f/si90.svg si90 si90.svg svg 7098 ALTIMG 1-s2.0-S0306261924007979-si92.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/74026ed1ffa7378b823c351c986526b3/si92.svg si92 si92.svg svg 1306 ALTIMG 1-s2.0-S0306261924007979-si93.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/b2044dc075c1197e669e960fc55e704b/si93.svg si93 si93.svg svg 5076 ALTIMG 1-s2.0-S0306261924007979-si94.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/4206dbb823e8b483c4ef491d1948b953/si94.svg si94 si94.svg svg 12464 ALTIMG 1-s2.0-S0306261924007979-si95.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/051a4b51b79be64785dcbdc4329a4e1b/si95.svg si95 si95.svg svg 3988 ALTIMG 1-s2.0-S0306261924007979-si99.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261924007979/image/svg+xml/612da244df94d714cb6776e329187af0/si99.svg si99 si99.svg svg 9509 ALTIMG 1-s2.0-S0306261924007979-am.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:10068GQFWGW/MAIN/application/pdf/c140147e33774888ca21d6be358c3f90/am.pdf am am.pdf pdf false 3992905 AAM-PDF APEN 123414 123414 S0306-2619(24)00797-9 10.1016/j.apenergy.2024.123414 Elsevier Ltd Fig. 1 Comparison of single-agent RL and MARL frameworks. Fig. 1 Fig. 2 First-floor layout and 1\u20131 profile of examined house. Fig. 2 Fig. 3 Monitored temperature and electricity profile of examined house. Fig. 3 Fig. 4 Schematic of MAPPO system. Fig. 4 Fig. 5 Reward function of indoor thermal comfort. Fig. 5 Fig. 6 Structure of centralized training with decentralized execution. Fig. 6 Fig. 7 Monitored training process of different stages. Fig. 7 Fig. 8 Daily cost results and indoor temperature variation in cold week and warm weeks. Fig. 8 Fig. 9 Testing results of a cold week. Fig. 9 Fig. 10 Testing results of a warm week. Fig. 10 Table 1 Relevant review works of RL and DRL in BES. Table 1 Ref & Year Algorithm Building type Controlled System Control parameters Purposes [23], 1998 Q-learning Residential Residential comfort system ON/OFF status of the air heating, lighting, ventilation, and water heating Improved energy efficiency [30], 2020 PPO Commercial HVAC Temperature set point Improved energy efficiency, thermal comfort, and demand response [28], 2020 DRL Office Heating systems Supply water temperature Energy savings and thermal comfort [24], 2020 Q-learning Commercial HVAC ON/OFF status of the chilled water flow Cost savings and thermal comfort [31], 2020 DRL, DDPG Business and research facility Cooling system Temperature set-point and mass flow of the Chiller Cost savings [25], 2020 Q-learning Residential Heating Pump (HP) and Battery Operation of heat pump and battery charge/discharge cycles Maximizing photovoltaic production self-consumption [34], 2021 SAC, TD3, PPO, TRPO Data-center HVAC Temperature set point and mass flow rate Cost savings and thermal comfort [33], 2021 DDPG Residential HVAC Temperature set point Cost savings and thermal comfort [32], 2022 DDQN Residential HP Power Cost savings [35], 2022 DDPG, TD3 Office Generators, Solar photovoltaics, and Batteries Power of Generators, solar photovoltaics, battery charge/discharge cycles Efficient off-grid operation, battery management [36], 2023 SAC Office HP Heating power Cost savings and thermal comfort [37], 2023 D3QN Residential HVAC Power Cost savings and thermal comfort [29], 2023 DQN Commercial TES Cooling with chillers, duplex chillers, and ice tanks Improved energy efficiency Table 2 The states space for space heating agent. Table 2 state unit Indoor temperature T r °C Envelope temperature °C Hour \u2013 Onsite PV generation P gen Wh Time-of-Use Pricing p ele Yen Outdoor temperature T o °C Table 3 The united states space for battery agent. Table 3 state unit Hour sin \u2013 Hour cos \u2013 Base load P base Wh Onsite PV generation P gen Wh TOU Pricing p ele Yen SoC \u2013 The optimal power policy of space heating agent P ppo Wh Table 4 Algorithm parameters in Stage 1 and Stage 2. Table 4 Algorithm Parameter Single-agent PPO Multi-agent PPO Actor learning rate 0.001 0.0005 Critic learning rate 0.004 0.0005 Clip range 0.2 0.2 Batch size 48*7 4 Replay buffer capacity 50,000 3628 Discount factor 0.9 0.99 Number of epochs 200 200 GAE lambda \u2013 0.95 Max gradient norm 0.5 10 Table 5 Environment parameters in Stage 1 and Stage 2. Table 5 Environment Parameter Single-agent PPO Multi-agent PPO Thermal comfort α 0.005 0.005 Thermal comfort λ 0.2 0.2 Electricity cost β 0.000008 0.000035 State of Charge δ \u2013 0.000005 Table 6 Result comparison of different control methods in a cold week. Table 6 Parameter Single-agent Multi-agent PI Mean-Tr (°C) 22.11 21.74 23.59 Electricity cost (Yen) 2882 1722 4041 Self-sufficiency ratio 17.76% 48.42% 13.56% PV self-consumption ratio 33.05% 44.23% 28.45% Table 7 Result comparison of different control methods in a warm week. Table 7 Parameter Single-agent Multi-agent PI Mean-Tr (°C) 23.17 23.42 23.75 Electricity cost (Yen) 2062 918 2670 Self-sufficiency ratio 25.25% 63.13% 17.03% PV self-consumption ratio 29.16% 36.70% 18.23% Scalable energy management approach of residential hybrid energy system using multi-agent deep reinforcement learning Zixuan Wang Writing \u2013 original draft Visualization Software Resources Methodology Investigation Formal analysis Data curation Conceptualization a Fu Xiao Writing \u2013 review & editing Resources b Yi Ran Visualization Data curation a Yanxue Li Writing \u2013 review & editing Writing \u2013 original draft Software Resources Methodology Investigation Funding acquisition Formal analysis Conceptualization a b \u204e Yang Xu Writing \u2013 review & editing c a Innovation Institute for Sustainable Maritime Architecture Research and Technology, Qingdao University of Technology, Qingdao 266033, China Innovation Institute for Sustainable Maritime Architecture Research and Technology Qingdao University of Technology Qingdao 266033 China Innovation Institute for Sustainable Maritime Architecture Research and Technology, Qingdao University of Technology, Qingdao 266033, China b Department of Building Environment and Energy Engineering, The Hong Kong Polytechnic University, Hong Kong, China Department of Building Environment and Energy Engineering The Hong Kong Polytechnic University Hong Kong, China Department of Building Environment and Energy Engineering, The Hong Kong Polytechnic University, Hong Kong, China c School of Information and Control Engineering, Qingdao University of Technology, 777 Jialingjiang Road, Qingdao 266520, Shandong, China School of Information and Control Engineering Qingdao University of Technology 777 Jialingjiang Road Qingdao Shandong 266520 China School of Information and Control Engineering, Qingdao University of Technology, 777 Jialingjiang Road, Qingdao, 266520, Shandong, China \u204e Corresponding author at: Innovation Institute for Sustainable Maritime Architecture Research and Technology, Qingdao University of Technology, Qingdao 266033, China. Innovation Institute for Sustainable Maritime Architecture Research and Technology Qingdao University of Technology Qingdao 266033 China Deploying renewable energy and implementing smart energy management strategies are crucial for decarbonizing Building Energy Systems (BES). Despite recent advancements in data-driven Deep Reinforcement Learning (DRL) for BES optimization, significant challenges still exist, such as the time-consuming and data-intensive nature of training DRL controllers and the complexity of environment dynamics in Multi-Agent Reinforcement Learning (MARL). Consequently, these obstacles impede the synchronization and coordination of multiple agent control, leading to slow DRL convergence performance. To address these issues. This paper proposes a novel approach to optimize hybrid building energy systems. We introduce an integrated system combining a multi-stage Proximal Policy Optimization (PPO) on-policy framework with Imitation Learning (IL), interacting with the model environment. To improve scalability and robustness of Multi-agent Systems (MAS), this approach is designed to enhance training efficiency with centralized training and decentralized execution. Simulation results of case studies demonstrate the effectiveness of the Multi-agent Deep Reinforcement Learning (MADRL) model in optimizing the operations of hybrid building energy systems in terms of indoor thermal comfort and energy efficiency. Results show the proposed framework significantly improve performance in achieving convergence in just 50 episodes for dynamic decision-making. The scalability and robustness of the proposed model have been validated across various scenarios. Compared with the baseline during cold and warm weeks, the proposed control approach achieved improvements of 34.86% and 46.10% in energy self-sufficiency ratio, respectively. Additionally, the developed MADRL effectively improved solar photovoltaic (PV) self-consumption and reduced household energy costs. Notably, it increased the average indoor temperature closer to the desired set-point by 1.33 °C, and improved the self-consumption ratio by 15.78% in the colder week and 18.47% in the warmer week, compared to baseline measurements. These findings highlight the advantages of the multi-stage PPO on-policy framework, enabling faster learning and reduced training time, resulting in cost-effective solutions and enhanced solar PV self-consumption. Keywords Multi-agent reinforcement learning Schedule optimization Multi-stage Thermal comfort Energy cost Data availability Data will be made available on request. Nomenclature AI Artificial intelligence RL Reinforcement learning DRL Deep reinforcement learning MAS Multi-agent systems MARL Multi-agent reinforcement learning MADRL Multi-agent deep reinforcement learning BES Building energy systems GFG Greenhouse gas ZEH Zero Energy House ZEB Zero Energy Building HP Heating pump PPO Proximal policy optimization MAPPO Multi-agent proximal policy optimization RC Resistance-capacitance network TOU Time of use price PV Photovoltaic HVAC Heating, ventilation, and air conditioning PI Proportional-integral (control system) THS Thermal energy storage PCS Personal comfort systems RMSE Root Mean Squared Error SoC State of Charge (battery charge state) LSTM Long Short-Term Memory DQN Deep Q-Network DDQN Double Deep Q networks D3QN Dueling Double Deep Q-Networks SAC Soft Actor-Critic TD3 Twin Delayed Deep Deterministic policy gradient TRPO Trust Region Policy Optimization DDPG Deep Deterministic Policy Gradient PPO Proximal Policy Optimization SASAR State-Action-Reward-State-Action MDP Markov Decision Process PINN Physics-Informed Neural Network 1 Introduction Given the international imperative to achieve carbon neutrality, as delineated in the Paris Agreement [1], the building sector is identified as a major contributor to global energy consumption and CO2 emissions, responsible for roughly 40% and 28%, respectively [2]. Considering that nearly 90% of our time indoors [3], it is vital to enhance energy efficiency and promote renewable energy use as key strategies to mitigate greenhouse gas (GHG) emissions in this sector [4,5]. In alignment with these goals, the Japanese government, the Japanese government has pledged to reach net-zero emissions by 2050 [6]. This pledge is manifested in its strategic focus on Zero Energy House (ZEH) and Zero Energy Building (ZEB) standards for new buildings post-2030 [7\u201310]. Solar PV systems are central to achieving an annual net-zero energy balance in buildings. The variability of renewable energy sources and demand poses challenges in optimizing BES operations [11]. While Model Predictive Control (MPC) is extensively utilized in BES simulation and optimization [12\u201314], its effectiveness is often hampered by the extensive effort needed for model implementation and the inherent uncertainties in predictive models [15]. Alternatively, Reinforcement Learning (RL) offers a flexible, model-free approach for BES operation optimization, effectively adapting to unpredictable future changes [16\u201318]. RL's capacity to handle random future variations without complex predictive models positions it as an attractive choice for intelligent and adaptive control in BES operations [19]. However, the application of single-agent RL frameworks in building energy management often encounters issues with slow convergence [18,20] and scalability, particularly in hybrid energy systems. Effective control of BES requires cooperation among multiple agents. A RL system that incorporates multiple agents is referred to as a Multi-agent Reinforcement Learning (MARL) system [21]. The coordinated interaction among these agents in a controlled environment for building energy systems has emerged as a promising alternative to improve energy savings and economic performance. 2 Related work 2.1 Literature review on RL approaches for building energy management 2.1.1 Single-agent control RL control has been extensively studied in the field of building energy management, as detailed in Table 1 , with initial research dating back to 1997. Anderson et al. [22] implemented a Q-learning algorithm to control the opening degree of a heating coil's valve, demonstrating that the RL agent reduced the temperature control error by approximately 8.5% compared to a Proportional-Integral (PI) controller. Mozer et al. [23] investigated the use of Q-learning for residential comfort systems, including air heating, lighting, ventilation, and water heating, to develop adaptive self-programming home systems. Recent studies, such as [24], have utilized data-driven RL approaches with Q-learning to optimize HVAC systems in commercial settings, focusing on safety and cost-effectiveness. Another study [25] optimized the utilization of stored energy in residential settings, particularly to maximize the self-consumption of solar PV production by adjusting energy usage, thus improving the self-consumption rate of onsite PV generation without compromising end-user comfort. However, Q-learning faces significant challenges. First, its scalability is limited as the size of the Q-table increases linearly with the number of potential inputs, making it impractical for BES with extensive state spaces, leading to the \u201ccurse of dimensionality\u201d [26]. Second, this method only updates value function estimates for explored states, ignoring unexplored ones, which may result in inefficient learning trajectories, particularly in environments with infinite state or action dimensions. The introduction of the Deep Q-Network (DQN) by DeepMind in 2015 [27] addressed these scalability and high dimensionality challenges. By utilizing a deep neural network to approximate the Q-function, DQN has outperformed human capabilities in games like Atari 2600. Leveraging this advancement, researchers have explored integrating DQN with BES to promote system intellectualization. Further research has demonstrated the potential of DRL. For instance, [28] employed DRL to optimize the supply water temperature in office building heating systems, achieving energy savings between 5% and 12% and enhanced temperature control. Similarly, [29] utilized a Deep Q-network RL agent with a detailed heat transfer simulation model to optimize the control of ice-based thermal energy storage (TES) systems in commercial buildings, resulting in a 7.6% cost reduction. To address the overestimation of value functions in DQN, other research has explored different algorithms for BES optimization: [30] applied the PPO algorithm, achieving up to 22% energy savings; [31] compared DQN with other algorithms like DDPG (Deep Deterministic Policy Gradient), demonstrating effective load shifting and dual benefits of cost and comfort. [32]combining Double Deep Q networks (DDQN) to autonomously manage indoor environments in smart homes, achieving superior energy efficiency to reduce the cost. [33] employs DDPG to enhance HVAC efficiency, reducing energy costs and maintaining comfort more effectively than traditional and single-task methods in various scenarios. [34] evaluated four Actor-Critic (AC) algorithms, highlighting their capability to maintain thermal stability and enhance energy efficiency across diverse weather conditions. Moreover, innovative approaches like Reinforced Predictive Control (RL-MPC) [15] have combined Model Predictive Control (MPC) and RL for enhanced state estimation and dynamic optimization, aimed at achieving performance comparable to MPC with greater adaptability in uncertain environments. The research into optimizing off-grid operations and battery safety in renewable BES [35], incorporating LSTM (Long Short-Term Memory) neural networks in DRL [36], and addressing model-free RL challenges [37] demonstrates the growing diversity in RL applications. Additionally, the effectiveness of TES systems in public buildings for load shifting under Time-Of-Use (TOU) tariffs [38] surpasses traditional control methods in managing cooling load complexities. Despite these advancements, the single-agent RL approach encounters inherent limitations in large-scale, complex BES environments, as evidenced by the scenario presented in [20]. Energy systems comprising multiple agents with an extensive range of actions lead to an impractically large number of possible combinations, revealing significant challenges in scalability and data management. This issue is further compounded by the intensive data requirements for training and the low measurement sampling frequency typical in BES, as highlighted in [39]. Such challenges necessitate more efficient usage and a shift towards approaches capable of handling the complexity and scale of modern BES. 2.1.2 Multi-agent control BES management, particularly in buildings equipped with various devices, requires advanced control strategies that go beyond the capabilities of traditional single-agent RL algorithms. These conventional methods often encounter challenges related to convergence speed and system stability, particularly when managing integrated systems that combine cooling, heating, and power management. Traditional single-agent RL algorithms, while foundational, frequently struggle in complex BES environments due to their inherent limitations in handling dynamic responses, sensor delays, and actuator activities. These methods often fail to scale effectively when faced with the intricate coordination of multiple components. In contrast, MADRL offers a robust alternative, employing a collaborative control approach that excels in managing such complexities through cooperative interactions among multiple decision-making agents [40]. This is evidenced by the versatility of MADRL applications, ranging from multi-robot warehouse management [41] to autonomous driving [42\u201344] and automated trading in electronic markets [45\u201347], showing its adaptability and effectiveness across various domains. A significant advantage of MADRL over traditional approaches is its enhanced convergence speed and stability in operations. MADRL's ability to distribute decision-making and learning across multiple agents allows for more robust handling of the non-stationary environments typical of integrated BES. For instance, Liang et al. [20] demonstrated how a MADRL algorithm, integrated with an attention mechanism, optimized HVAC systems in commercial buildings, achieving notable improvements in energy efficiency. The integration of renewable energy into BES has been greatly advanced by MADRL algorithms, enhancing energy flexibility and grid responsiveness [48]. Additionally, the application of MADRL in grid-interactive buildings [49] and environments like CityLearn illustrates its capability to alter electricity demand curves by controlling building energy storage systems. Advanced techniques such as Dueling Double Deep Q-Networks (D3QN) and Value-Decomposition Networks [50] have facilitated single-agent and multi-agent optimization in BES. Furthermore, an attention-based MADRL algorithm [51] has been proposed for coordinating Personal Comfort Systems (PCS) and HVAC systems in office spaces, striking a balance between energy consumption and occupant comfort, a task that single-agent systems often find challenging. However, challenges persist in implementing MADRL within BES. Training of RL controllers is often time-consuming and data-intensive, as highlighted by [22], who also emphasized the need for improved control security, robustness and enhanced generalization capabilities. The complexity of environment dynamics in MADRL, where transition probabilities depend on joint agent actions, creates a non-stationary environment. This poses challenges for individual agents' adaptation and often results in slow convergence rates [32]. Moreover, the volume of data and the transmission frequency can significantly burden communication channels, particularly in RL methods that demand intensive communication. This issue poses a major obstacle in achieving synchronized and coordinated actions among multiple agents. 2.2 Contributions Through the review above, we have identified that integrating multiple agents introduces substantial complexity, exacerbating the challenge of achieving efficient algorithmic convergence. In response to these challenges, this study proposes a novel approach for BES optimization. By integrating a multi-stage PPO on-policy algorithm with IL within a MARL framework, our research endeavors to ensure efficient and reliable operations of BES and improve the training efficiency of MAS in dynamic and complex environments. Meanwhile, avoiding blindly applying complex multi-agent models. The main contributions of this study are outlined as follows: \u2022 Integration of multi-stage PPO algorithm with IL: We present an innovative strategy that merges a multi-stage PPO algorithm with IL. This approach adeptly surmounts the convergence challenge in MAS training, a critical improvement given the typical time and computational demands of RL algorithms. As BES evolve, for instance through renovations or the adoption of new technologies, a MAS can more readily scale, facilitating the inclusion of new agents without the need for a comprehensive system redesign. \u2022 Innovative shared reward function: The paper presents a novel shared reward function for MARL that assigns different parts of rewards based on the individual states of each agent to encourage collaborative energy optimization. This technique departs from conventional shared reward systems, enabling precise battery energy management that corresponds with photovoltaic output and the energy demands of ZEH. This advancement represents a substantial leap in MARL applications, melding cost-effectiveness with the maintenance of comfort, thereby propelling sustainable energy management practices forward. \u2022 Enhanced optimization performance in ZEH: The application of the multi-agent PPO algorithm within ZEH markedly elevates the optimization performance of various devices without compromising thermal comfort. This improvement is particularly relevant in energy-efficient building controls, which require balancing energy flexibility on the demand side with thermal comfort. \u2022 Validation of ZEH with diverse data sets: another contribution of this study is the validation of the proposed algorithms using real building datasets from different periods. This validation effectively demonstrates the scalability of our proposed method to diverse environmental conditions, reinforcing the practical applicability of our approach. The structure of this paper is organized as follows: Section 2 offers a comprehensive literature review, placing our work within the broader context of RL applied to BES. Section 3 details the MADRL framework and our algorithmic contributions. Section 4 provides a detailed case study to illustrate the practical application of our approach. Section 5 discusses the outcomes of our case study. Finally, Section 6 concludes the paper with a summary of our findings and suggestions for future research directions. 3 Methodology 3.1 Reinforcement learning Prior to delving into the algorithm, it is imperative first to define the fundamental concept of reinforcement learning and its associated parameters. RL constitutes a subset of machine learning, characterized by a learning process that occurs through interaction with the environment. This method employs a reward-guided learning approach, where the agent is not explicitly instructed on which actions to take at each step; rather, it learns from the outcomes of its interactions with the environment [52]. RL has rapidly evolved, boasting a wide range of algorithms and applications. It has also emerged as one of the most dynamic and actively researched areas in the field of artificial intelligence (AI). 3.1.1 Markov decision process The Markov Decision Process (MDP) [53] represents an advanced iteration of the Markov chain concept, furnishing a comprehensive mathematical framework designed to articulate decision-making scenarios. Almost all Reinforcement learning problems can be modeled as an MDP process. MDP can be represented by five important elements. \u2022 States S: A comprehensive ensemble representing all conceivable states within the environment, each state encapsulates essential information required for informed decision-making in reinforcement learning at every step. \u2022 Actions A: This set encompasses all potential decisions an agent can make to transition from one state to another, serving as the primary means for the agent's interaction with the environment. \u2022 A transition probability P s \u2032 s a : it is represented as a function or a model that determines the probability of moving from one state s to another s \u2032 , given a particular action a . \u2022 Reward R: The reward function defines the immediate feedback acquired by the agent after performing an action a for moving from one state s to s \u2032 . It's a critical component as it guides the learning process by reinforcing desirable actions and discouraging undesirable ones. \u2022 A discount factor γ : In the MDP process, the discount factor γ , ranging between 0 and 1, arbitrates between the significance of immediate versus future rewards. A value of 0 prioritizes immediate rewards, overlooking future gains, while a value of 1 place greater emphasis on future rewards, reducing the relative importance of immediate outcomes. This factor plays a crucial role in shaping the temporal emphasis of reward valuation in decision-making processes. 3.1.2 Value-based learning and policy-based learning In the RL domain, two primary methodologies are predominant: Value-Based Learning and Policy-Based Learning. While both approaches are integral to the RL framework, they significantly diverge in their decision-making and learning strategies. Value-based learning: the primary focus is on estimating a value function, which signifies the expected cumulative reward an agent can accumulate, either from a specific state or following a particular action in each state. This function is often exemplified as a Q-function or a V-function in value iteration, guiding the agent's decisions. The agent aims to learn the optimal value function, offering a measure of the highest achievable rewards. This is defined as: (1) Q π s a = E π ∑ k = 0 ∞ γ k R t + k + 1 s t = s a t = a The expectation E π indicates that the expected return is calculated under the policy π . γ k represents the discount factor raised to the power of k , discounting rewards received k time steps in the future. A prevalent approach involves utilizing a parametrized representation of the Q-function, such as a neural network. The parameters of this network are adjusted by minimizing the difference between the true action-value function and its approximation. Techniques incorporating this methodology include Q-learning and Deep Q-Learning, as detailed in Appendix A. Policy-based Learning: In this approach, the policy π a s θ is typically depicted as a probability distribution over actions given states, parameterized by θ . This represents the probability of selecting each possible action, with the policy updating the parameter through gradient ascent on expected return J θ . This expression can be described as: (2) J θ = E π θ ∑ t = 0 ∞ γ t R s t a t Here, R s t a t signifies the immediate reward received upon taking action a t at time t , and γ is the discount factor, computing the present value of future rewards. This formula encapsulates the expected cumulative discounted reward from the initial state under the policy π θ . Notable algorithms in this category includes Soft Actor-Critic (SAC) and PPO algorithm, which are elaborated upon in Appendix A. 3.1.3 On-policy and off-policy learning A crucial classification of RL algorithms hinges on the data source used for policy improvement. On-policy learning employs a single policy simultaneously for decision-making and deriving insights from the resulting rewards. This approach means that the exploration policy is identical to the evaluated policy. A notable example of on-policy learning is the State-Action-Reward-State-Action (SARSA) algorithm. In SARSA, policy updates are closely tied to the actions taken under the current policy, directly affecting future decisions. In contrast, off-policy learning methods create a distinction between the \u201cbehavior policy\u201d (used for exploring the environment and gathering data) and the \u201ctarget policy\u201d (which is evaluated and optimized). This separation allows for greater flexibility, freeing the learning process from the limitations of trajectories generated solely under the current policy. Off-policy methods can thus incorporate learnings from experiences gathered under previous policies or even hypothetical situations. A prime example of off-policy learning is the Q-learning algorithm, detailed in Appendix. A, where updates to the value function depend on choosing the best action based on current estimates, regardless of the actions implemented under the behavior policy. In the realm of BMS, real-time policy adjustment is vital for effective control. On-policy learning algorithms continuously fine-tune the policy based on recent environmental interactions. This constant adjustment is particularly beneficial in BES contexts, where system dynamics or external factors, like weather conditions or occupancy patterns, often change rapidly. In these situations, the policy is continuously evaluated and revised to align with the system's present state, enabling more dynamic responses to real-time variations. This strategy reduces the risks associated with abrupt or dangerous changes that might result from the more exploratory tactics typical of off-policy methods. 3.2 Model-based simulation Balancing model complexity and computational efficiency is essential when developing a thermodynamic model for BES. A gray-box model, although detailed, presents considerable computational challenges for the iterative learning process in RL. As a result, employing a simplified method utilizing a reduced-order resistance-capacitance (RC) network becomes more feasible for managing space heating systems. This model integrates building physical characteristics and empirical data for parameter calibration, and its accuracy is assessed using the Root Mean Squared Error (RMSE) metric, consistent with methodologies in our previous studies [7]. Regarding RL applicability, the model treats the ZEH as a singular thermal zone focusing exclusively on heating. This simplification ensures that the model sufficiently captures vital thermal dynamics for DRL interaction while maintaining computational efficiency. Such a model provides a useful interactive environment for DRL algorithms, enabling the exploration of energy optimization strategies while ensuring indoor thermal comfort. 3.3 Single and multi-agent environment and systems theory As their designations imply, a single-agent environment encompasses only one agent, while a multi-agent environment contains several agents. Single-agent reinforcement learning involves a lone agent developing an optimal policy through trial and error to maximize cumulative rewards from the environment. In contrast, multi-agent environments feature multiple agents interacting in a shared, dynamic setting. These interactions span a range from cooperative to competitive or a blend of both. The agents aim not only to maximize their own rewards but also to influence the outcomes of their peers. This complex and often unpredictable interplay makes multi-agent reinforcement learning especially relevant for fields that require advanced coordination and adaptability, such as robotics [41] and economics [45\u201347]. Differing from single-agent learning, which focuses on the independent decision-making of one agent, multi-agent learning highlights the complex dynamics and interdependencies in shared environments, underscoring the unique challenges and opportunities for collaboration in these intricate systems. The frameworks of single-agent RL and MARL are described in Fig. 1 . 3.4 Reinforcement learning algorithms in multi-agent systems MAS exhibit complex and dynamic characteristics, where agents interact within a shared environment to achieve individual or collective goals. The emergence of DRL has introduced sophisticated capabilities in learning and decision-making, particularly advantageous in MAS contexts. The transition from single-agent to complex multi-agent environments highlights DRL's ability to manage the dynamic interactions among agents in a shared space. Critical algorithms have played a significant role in this development. Q-learning, while foundational, is limited in complex state spaces, which led to the advent of DQN that incorporate deep neural networks to better navigate these complexities. However, in multi-agent settings, DQN encounters stability challenges, prompting the adoption of AC methods. These methods combine value and policy-based learning, enhancing decision-making in continuous action spaces, but they face issues with high variance in policy updates in dynamic MAS. To address these limitations, PPO has emerged as an effective algorithm for MAS, ensuring stable learning progressions and consistent policy improvements, vital for handling the complexities of residential hybrid energy systems. The choice of multi-agent PPO for this study is based on its ability to maintain stable learning paths and consistent policy enhancements in intricate, evolving MAS environments. For comprehensive theoretical formulations and algorithm details, see Appendix A. Section 3.4.1 focuses on the settings of the reward function in MADRL, 3.4.2 introduces IL as a supervised learning paradigm distinct from traditional DRL. Accurately modeling complex interactions directly can be challenging. However, by leveraging expert demonstrations, IL provides a practical method to enhance the efficiency of agent training and policy development in MAS. 3.4.1 Multi-agent reward functions MARL can be classified along various dimensions. As outlined in Section 3.5, this classification involves distinguishing methods based on the accessibility of information during their training and execution phases. The choice of agent rewards is crucial, as it directly influences the learning dynamics and emergent behaviors of agents within the system. These reward frameworks are categorized as follows: Individualistic reward functions: This approach assigns each agent a unique reward function, as illustrated in Eq. (3): (3) R individual , i = R i a i s Here, agent i receives a reward based on its action a i and the current state s . This setup emphasizes the direct influence of an agent's actions on its individual goals, encouraging the development of independent policies. Under certain conditions, this can lead to competitive interactions among agents, particularly if R individual , i = − R individual , i + 1 occurred. Although this approach facilitates the achievement of individual objectives, aligning these goals with the broader system objectives, especially in cooperative scenarios, remains challenging. Shared reward functions: This method employs a collective reward strategy, where all agents receive the same reward irrespective of their individual contributions, as defined in Eq. (4): (4) R shared s a = ∑ i = 1 N R i s a i It provides a uniform reward to all agents regardless of individual performance. This fosters cooperation by considering the overall state s and the actions a i of each agent, denoted by i . By aggregating the rewards across all N agents, it steers behaviors towards a shared objective. Nevertheless, it risks enabling a \u201cfree-rider\u201d effect, where in certain agents might reap the benefits of collective action without substantial individual contribution. Difference reward functions: The difference reward framework evaluates an agent's contribution relative to the collective outcome, as depicted in Eq. (5): (5) D i a i s = R total s a − R total s a − i Here, R total denotes the total system reward and a − i represents the joint actions of all agents expect for agent i . This framework gauges an agent's impact by contrasting the collective outcome with a counterfactual scenario where the agent's participation is excluded. It incentivizes behaviors that benefit the system's overall performance, promoting a harmonious alignment between individual and group objectives. The paradigms of reward functions In MARL are crucial in shaping the learning processes and the resulting behaviors of the agents. Individualistic rewards encourage autonomy, shared rewards motivate cooperative behavior, and difference rewards provide a balance between individual contributions and collective goals. Consequently, the selection of a reward function is a critical strategic decision that substantially influences the learning efficiency and the alignment of agent behaviors with the broader objectives of MAS. 3.4.2 Imitation learning IL is a supervised approach that seeks to establish mappings between states and actions. This method effectively generalizes expert strategies to unexplored states, akin to a multi-class classification problem when the action sets are finite. Its principal advantage is the ability to extrapolate the expert's strategies to new states, which can diminish the time required for learning and augment the efficacy of MAS. In the context of multi-agent reinforcement learning systems, fostering effective communication among agents during the training phase presents a significant challenge. The complexity inherent in agent interactions becomes markedly intensified in scenarios where communication must occur over noisy and unreliable channels. Therefore, the development of robust methods to address these challenges is of paramount importance. While the battery's observation state in this study does not directly mirror the collective actions of the agents, it embodies a form of behavior learned within a single-agent framework. This approach can be viewed as an extension of the principles of IL, wherein pre-learned strategies inform the cooperative learning process within a multi-agent context. Thereby, it aids in navigating the intricacies of real-time communication and decision-making among agents. 3.5 Approaches to training and execution in MADRL MADRL frameworks are primarily differentiated by their methodologies in processing information during training and execution. These frameworks can be classified into three main approaches based on the availability of information and the coordination among agents: \u2022 Centralized training and execution In this paradigm, both training and execution phases are centralized, allowing for a unified strategy development. Agents learn and decide collectively, which ensures consistent policy application but may limit scalability and responsiveness to environmental variations. The centralized approach may also introduce bottlenecks due to information aggregation from all agents. \u2022 Decentralized training and execution Contrasting with centralized methodologies, this approach decentralizes both training and execution. Agents learn and act independently, enhancing scalability and the ability to respond to local changes. Despite these advantages, the absence of centralized coordination may result in training instability and difficulties in achieving unified group objectives. \u2022 Centralized training with decentralized execution This hybrid method employs centralized training alongside decentralized execution. During the training phase, agents benefit from a collective information pool to develop their policies, which are subsequently executed autonomously. This approach offers the coordinated learning advantages of centralization and the operational flexibility of decentralization, rendering it well-suited for complex multi-agent scenarios. 4 Case study 4.1 Introduction of ZEH and dataset This study selects a ZEH located in Kyushu, Japan. Constructed as a two-story edifice utilizing a combination of lightweight steel and wood, the house features insulation with glass wool, which substantially reduces thermal conductivity and augments energy efficiency. The integration of high-performance insulation, proficient systems, and renewable energy sources underscores the dwelling's dedication to sustainability. Architectural specifics are provided in [37]. The ground floor's design and the tactical positioning of indoor temperature sensors are depicted in Fig. 2 , where the sensors are indicated by red circles. These fixed recording points are pivotal for assessing the building's thermal efficacy, with data systematically collected at 30-min intervals from January 1st to March 30th, 2020. For temperature recordings, the TR-72wf device was utilized, offering an accuracy of ±0.5 °C across a 0\u201355 °C range. Fig. 3 extensively visualizes the monitored data, illustrating variations in indoor and external temperatures, as well as the dynamics between the 9.6 kWp rooftop PV system output and the home's electrical consumption. It further incorporates time-of-use electricity tariffs and clarifies the relationship among energy production, usage patterns, and pricing structures. Comprehensive details on electricity tariffs can be found on the Kyushu Electric Power Company's website at [54]. 4.2 Multi-stage MAPPO framework Fig. 4 illustrates the detailed framework and algorithmic development that form the basis of this study, organized into two primary phases: Stage 1, depicted in Fig. 4 (a), and Stage 2, depicted in Fig. 4 (b), described as follows: Stage 1: The initial stage involves a single space heating agent, operating within a deep reinforcement learning context to regulate indoor temperatures to a desired comfort level, specifically targeting approximately 22 °C, while also aiming to minimize energy consumption. A notable advancement in this phase is the strategic use of PV generation, which serves to lower energy costs through the enhanced use of locally generated PV power. The focus of this stage is on optimizing the energy efficiency of the space heating mechanism, taking into account the solar irradiance and the financial aspects of the energy consumed. Stage 2: Advancing to a more complex framework, Stage 2 integrates a battery storage agent that collaborates with the space heating agent within a MAS. This configuration does not rely on direct observational learning from the space heating control as part of the MARL paradigm. Rather, the decision-making process for the battery agent is informed by the optimal policy established in the single-agent setting of Stage 1. By incorporating principles of IL, this phase utilizes the pre-established space heating strategy to guide the battery's operational decisions. This method accelerates the multi-agent training convergence by enabling the battery agent to adopt an already validated policy, thus promoting efficient energy management and a synergistic relationship between the agents. Reward mechanisms are fine-tuned to ensure that the agents achieve their individual operational goals while collectively advancing the overall energy optimization of the ZEH. 4.3 Building thermal dynamic modeling In this research, we employ the RC network-based 2R2C model for thermodynamic modeling of building environments. Selected for its capacity to effectively balance computational efficiency with modeling precision, this reduced-order model enables the simulation of thermal dynamics within buildings. Importantly, it permits the incorporation of RL algorithms to enhance energy efficiency, adapting established methods to our current framework. For a comprehensive derivation and analysis of the 2R2C model, especially its state-space representation and application, please see our previous work [7]. 4.4 MADRL structure in the case study For the MADRL framework presented in this paper, we designate the space heating system as the sole agent in Stage 1 and incorporate both space heating and battery systems as agents in Stage 2. Stage 1: Single agent structure (1). Definition of state space for a single agent space heating system At this stage of the research, the state space is delineated by a set of physical parameters that characterize the indoor environment accurately. The state space, as interpreted by the Space Heating agent, includes five specific dimensions. These parameters are detailed in Table 2 . (2). Definition of action space for a single agent space heating System The action space of a single-agent space heating system is predominantly determined by its heating mode power settings. Characterized by a continuum of power control values, this space is bounded between 0 and 1, where 0 denotes the minimum and 1 represents the maximum power output. The system's rated power is set at 2500 W. In actual application scenarios, the real power output is derived by scaling the normalized action value to this rated power, thus accurately reflecting the operational parameters of the space heating system within its defined action space. (3). Definition of reward function for a single agent space heating System The reward function for this stage is designed to align with the energy optimization goals of the ZEH system. The primary objectives include minimizing electricity costs and maintaining indoor thermal comfort. This entails maximizing onsite PV energy utilization while ensuring an optimal indoor temperature throughout the control period. To achieve these aims, the reward function comprises two main elements: the first applies a penalty for electricity usage by the space heating agent, and the second penalizes discomfort caused by indoor temperatures straying from the comfort zone. This approach encourages maximizing onsite PV energy use while discouraging conditions that result in discomfort or excessive energy use. The reward function is mathematically defined as follows: (6) R t = − α ⋅ C t − β ⋅ T r _ pen (7) T r _ pen = 1 − exp − 0.5 T r − T m 2 + range T r (8) range T r = − λ T r − T lower _ bound 2 if T r < T lower _ bound − λ T upper _ bound − T r 2 if T r > T upper _ bound 0 if T lower _ bound ≤ T r ≤ T upper _ bound The reward function in Eq. (6) constitutes a weighted sum of two components: C t , which quantifies the cost of electricity, and T r _ pen , representing the penalty for thermal discomfort. The hyperparameter α and β dictate the proportion of their respective penalties in each step. As formulated in Eq. (7), the thermal comfort penalty T r _ pen integrates an exponential term and a range penalty function range T r . The exponential term 1 − exp − 0.5 T r − T m 2 penalizes deviations of the indoor temperature T r from a desired temperature T m , while the range function detailed in Eq. (8) imposes a quadratic penalty on the indoor temperature T r when it deviates from an predefined comfort zone, demarcated by T lower _ bound and T upper _ bound , we establish T m = 22 °C, T lower _ bound = 20 °C and T upper _ bound = 24 °C within this study, and the reward T r _ pen varies with the indoor temperature as illustrated in the Fig. 5 . The comprehensive reward function thus integrates the cost of electricity and the level of thermal comfort, steering the space heating system towards an operation that is both energy-efficient and conducive to maintaining thermal comfort. This synthesis of economic and comfort considerations manifests a balanced approach to optimizing the operation of the heating system within the ZEH energy system's constraints. Stage 2: Multi-agent structure. In this phase, we have enhanced the previously developed single-agent energy system framework by integrating a battery agent into the setup from Stage 1. This enhancement requires the concurrent retraining of both the space heating and battery agents to ensure integrated efficiency and peak performance. Our methodology employs a centralized training but decentralized execution paradigm within the realm of MARL, as depicted in Fig. 6 . This innovative approach merges the strengths of imitation and reinforcement learning to elevate the control system efficiency of the ZEH energy system. A pivotal aspect of this framework is the informed decision-making process of the battery agent, informed by an optimal control strategy previously derived for the space heating agent. This integration promotes more effective energy resource management in ZEH by harnessing the collective capabilities of both agents synergistically. The training process acts as a heuristic reference, equipping the battery agent with a model of the space heating agent's efficient energy consumption for interaction. The battery agent's reward function is tailored to encourage suitable charging and discharging actions within operational safety margins, approving behaviors that enhance the system's overall energy optimization. Meanwhile, the reward function for the space heating agent is retained from the initial phase, concentrating on thermal comfort and energy cost reduction. Our proposed framework refines the single-agent space heating control model by granting a battery agent access to an established optimal policy, echoing the expert guidance seen in IL frameworks. Diverging from traditional IL that necessitates direct mimicry, our strategy promotes a collaborative dynamic among agents, utilizing the established efficacy of a pre-existing policy to expedite the MAS's learning process. It aligns the agents' individual aims with the ZEH's aggregate energy management objectives, fostering a cohesive system that synergistically integrates component functions for superior overall performance. 4.5 Integration of MARL with energy systems (1). Definition of state space for the MAS In the current stage of the MARL system, the state space for an space heating agent was established in Stage 1. However, a transformation has been applied to enhance the representation of time. To introduce periodicity, the hour of day variables are expressed using sine and cosine functions, enabling a seamless transition between the end and start of a daily cycle. Mathematically, this is represented by two variables: (9) Hour m sin = sin m 24 2 π , Hour m cos = cos m 24 2 π Where the hour m is sequences from 1 to 24 in a linear pattern. The same methodology applies to any time step during the day, modifying the denominator in Eq. (9) to 48 to account for the number of 30-min intervals. The battery agent's state space comprises seven unique dimensions to characterize the environment, as detailed in Table 3 . (2). Definition of action space for the MARL system The action space for the space heating system within the multi-agent system aligns with the one established in Stage 1. The action space for the battery agent is delineated as a continuum of power control values, ranging from −1 to 1. Here, positive values correspond to charging, while negative values denote discharging actions. The system's rated capacity is set at 5000 W. In practical scenarios, the actual power output is determined by scaling the normalized action value to the 5000 W rated capacity. Additionally, a nearly 15% energy loss incurred during battery charging or discharging is considered to ensure a precise depiction of the battery's operational parameters within its action space. (3). Definition of reward function for the MARL Within our MARL framework, agents independently make decisions based on their observational states, yet they collectively undergo training and accumulate rewards in unison. This paradigm introduces a refined version of Shared Reward Functions, which, unlike traditional designs that allocate rewards based on the collective actions of all agents, distributes rewards based on the actions and states of individual agents. This approach to reward allocation is mathematically represented as follows: (10) R shared s i a i = R Here, R symbolizes the uniform reward distributed across all agents, while s i and a i denote the observational state and the action of agent i , respectively. This innovative reward structure encourages a learning environment where agents are motivated to engage in cooperative behaviors driven by individual observations that are harmonized with the collective aim of reaching a shared goal. In this phase, the multi-agent system's central aim is to optimize the state to enable the battery to charge or discharge in tandem with onsite PV generation, thereby maximizing energy savings while sustaining thermal comfort within the ZEH. The comprehensive reward function is outlined in Eq. 11. The shared reward is primarily focused on promoting the smart balancing of battery charging and discharging, taking into account the limitations of energy availability and demand: (11) R t = − α ⋅ C t − β ⋅ T r _ pen − δ ⋅ P SoC Where δ is the hyperparameter for the state of SoC, and P SoC is the penalty for each step. The reward framework hence incentivizes the battery agent to operate efficiently, while the total reward reflects the multi-faceted goals of energy optimization within the ZEH. \u2022 Efficient utilization of PV generation: The energy management framework gives precedence to meeting household energy requirements through PV generated power, addressing both space heating energy needs and the baseline load. Excess PV energy, produced beyond these primary needs, is channeled into battery storage. This approach underscores the efficient use of renewable energy, prioritizing maximal onsite usage before drawing from the grid supply. \u2022 SoC management: The reward system in our energy management framework is carefully crafted to keep the SoC within a nearly ideal range of 20% to 95%. Exceeding these thresholds incurs a 10-point penalty, underlining the significance of proficient SoC regulation. The reward function imposes penalties when the battery's activities do not align with optimal energy practices, such as unwarranted discharging during sufficient PV generation or incorrect charging and discharging patterns. \u2022 Cost minimization: The reward mechanism detailed in Eq. (11) incorporates economic factors by aiming to reduce grid electricity expenses. It rewards actions that lessen grid reliance, especially those that achieve thermal comfort through the space heating system, and assigns rewards accordingly at each time step. Additionally, the system encourages the battery agent to discharge during times when household energy demand surpasses PV production, thereby reducing reliance on more expensive grid electricity. Conversely, in periods of high PV availability, the system promotes energy storage to optimize the balance between energy demand and renewable generation. 4.6 Experiment setting Algorithm 1 Multi-agent proximal policy optimization algorithm. Unlabelled Image The primary objective of this study is to investigate the potential of building thermal mass to enable energy flexibility and augment the use of onsite PV generation. To achieve this, we have developed a novel MAPPO framework, comprising two sequential stages. This framework enables space heating and battery controllers to enhance their efficiency through interactions with an optimized RC network model, thereby reducing the reliance on extensive datasets and training episodes. The training dataset covers the period from January 1st to March 30th, 2020. For model validation, we selected two distinct test periods: a colder phase from February 17th to February 23rd, 2020, and a warmer phase from March 16th to March 23rd, 2020. The training procedure is bifurcated into two stages: Stage 1 focuses on training a single agent to formulate an optimal policy for the space heating system, thus establishing a foundation for the more complex Stage 2. In Stage 2, the multi-agent system builds upon the knowledge gained in the initial stage. The multi-agent PPO control algorithm for the building energy system integrates space heating and battery systems in Stage 2, as outlined in Algorithm 1. This approach employs centralized training with decentralized execution, utilizing global information during the learning phase while enabling individual agents to make independent decisions during operational deployment. The training process is further detailed in Algorithm 2. Algorithm 2 Stage 2 training process. Unlabelled Image 4.7 Training process The training component iterates until convergence of the algorithm, enabling an assessment of the efficacy of the learned policies in practice. With the exception of the baseline control, all computational models leverage acceleration through the NVIDIA GeForce GTX 1660 SUPER graphics card. The code for all experimental environments is developed utilizing the OpenAI Gym library [55], The primary hyperparameters for the algorithm designs across different stages are presented in Tables 4 and 5 , respectively. 5 Result and discussion 5.1 Result of the training process During the agent training phase, the focus is on enhancing agent performance. Fig. 7 displays the accumulated rewards per episode across 50 iterations in Stage 2 and 200 iterations in Stage 1 within our reinforcement learning framework, demonstrating the agents' performance evolution. Stage 1 utilizes a single-agent PPO algorithm for the space heating system, represented by a blue line and labeled as \u201cSingle agent\u201d. The optimal policies formulated in this stage form the basis for the battery storage agent's strategy in Stage 2. The green line, labeled \u201cMulti agent\u201d, represents the performance of a MAS, calculated by averaging rewards across different random seeds. This graph effectively showcases the progression and refinement of policies from a single-agent to a multi-agent configuration, underlining the MAS's effective convergence in just 50 episodes. 5.2 Comparison of testing result In this section, we concentrate on analyzing the performance of the MAPPO algorithm within the ZEH energy system, with a particular focus on training efficiency. Our examination encompasses a comparison among three control methods: a single-agent PPO for space heating, a multi-agent PPO that incorporates both space heating and battery storage systems, and the baseline results obtained under a Proportional-Integral (PI) control strategy. This analysis is contextualized against two distinct ambient temperature conditions\u2014one being lower and the other higher. Our evaluation criteria include the stabilization of indoor temperature, the reduction in energy costs, and the ratios of energy self-sufficiency and PV self-consumption. The findings offer comprehensive insights into the comparative effectiveness of these control strategies under diverse environmental conditions. 5.2.1 Indoor temperature fluctuations This section offers a detailed analysis of indoor temperature variations using various control strategies under fluctuating ambient temperature conditions. The results are graphically depicted in Fig. 9 (a) and 10 (a), with quantitative summaries in Tables 6 and 7 . Specifically, Fig. 9 (a) examines temperature fluctuations during a colder week, while Fig. 10 (a) focuses on a warmer period. The X-axis represents time, with shaded areas indicating night-time periods of lower electricity tariffs. The Y-axis shows indoor temperature values. The figures utilize color coding to differentiate operational temperature ranges: purple for the single-agent approach, green for the multi-agent strategy, yellow for the baseline under PI control, and blue for ambient temperatures. Red dotted and dashed lines represent the desired average temperature and acceptable temperature range, respectively. Comparative analysis of different weeks is presented in Tables 6 and 7. During the colder week, the mean temperature values T r for the single-agent PPO, multi-agent PPO, and PI control strategies were 22.11 °C, 21.74 °C, and 23.59 °C, respectively. In the warm week, the average temperature were 23.17 °C, 23.42 °C, and 23.75 °C, respectively. These comparisons highlight the effectiveness of advanced control strategies in maintaining comfortable indoor temperatures, with the single-agent PPO algorithm most closely aligning with the target thermal set-point in both weeks. Fig. 8(b) and Fig. 8(d) show daily temperature variances during different test weeks using box plots. The single-agent PPO approach demonstrates superior temperature regulation, achieving more precise control of indoor climate. In contrast, the multi-agent PPO method consistently maintains temperatures within the comfort range, particularly during the colder week, although it exhibits more variation in the warmer week. These findings emphasize the benefits of reinforcement learning algorithms in enhancing energy efficiency and thermal comfort compared to the conventional PI method. 5.2.2 Dynamic energy management Fig. 9 (c) and 10 (c) detail the energy management performance under the multi-agent PPO control system during tested periods. The Y-axis distinguishes positive values for battery charging and negative values for discharging. A blue line with yellow circles shows PV generation; red bars indicate direct PV utilization for energy demand onsite, while orange bars show excess PV energy diverted to battery storage. Green bars highlight the battery discharge, typically during high electricity prices or low PV generation periods. The pattern of direct PV use followed by battery storage and discharge aligns with the strategic goal of minimizing costs. The battery is primarily used when economically beneficial, demonstrating an advanced approach to residential energy management. The multi-agent PPO system balances immediate energy needs with storage, thus enhancing operational efficiency and cost-effectiveness. Integrating battery storage and space heating systems within this framework allows for a flexible response to fluctuating energy demands, illustrating the system's capacity for adaptively managing energy resources to reduce costs and maintain efficiency. Tables 6 and 7 complement this analysis with a quantitative evaluation of different control methods, focusing on self-sufficiency and self-consumption ratios. The single-agent PPO system shows respectable performance, with Self-sufficiency ratios of 17.76% and 25.25% for colder and warmer weeks, respectively, and Self-consumption ratios of 33.05% and 29.16%. In comparison, the multi-agent PPO system excels, achieving Self-sufficiency ratios of 48.42% and 63.13% and Self-consumption ratios of 44.23% and 36.70% for the respective periods, underlining its superior efficiency in utilizing generated PV energy and reducing dependence on external electricity sources. The baseline PI control method demonstrates lower efficiency, with Self-sufficiency ratios of 13.56% and 17.03% and Self-consumption ratios of 28.45% and 18.23% for the colder and warmer weeks, respectively. Collectively, these analyses underscore the intricate balance achieved by the multi-agent PPO system in managing immediate energy consumption, storage, and strategic energy source utilization, ensuring operational efficiency and cost-effectiveness. This balance highlights the system's adaptability in optimizing energy resources to decrease costs while maintaining high energy efficiency. Fig. 9 (d) and 10 (d) illustrate the dynamic performance of energy management using the multi-agent system framework. The graphs quantify electricity consumption with distinct markers: the total residential load is shown by an orange line with blue circles, onsite PV consumption by yellow bars, battery discharge by blue bars, and grid imports by green bars. These data reveal a regular daily pattern of residential load efficiently met by PV generation during peak daytime demands. After dusk, strategic battery storage use becomes apparent, timed to align with reduced PV generation or increased electricity tariffs. Grid imports supplement when PV generation is insufficient, illustrating a carefully balanced operational approach that optimizes the use of solar energy, battery storage, and reasonable grid use for improved cost-efficiency and energy self-sufficiency in residential settings. 5.2.3 Cost analysis This section evaluates the cost efficiency of various control strategies, as illustrated in Fig. 9 (b) and Fig. 10 (b), corresponding to cold and warm weeks, respectively. These figures graphically represent electricity pricing trends over time, with the X-axis showing the timestep and the Y-axis denoting cost in Yen. The measurement results under PI control strategy, depicted by an orange line with a blue circle, exhibits considerable variability in energy costs, particularly with notable peaks during daylight hours. In contrast, the single-agent control for space heating, represented by a blue line with a yellow triangle, shows moderate cost fluctuations, occasionally reaching the higher peaks observed with the PI control strategy. However, the most noteworthy observation pertains to the multi-agent PPO strategy, indicated by a green bar. This strategy consistently maintains a lower cost profile throughout the day, with infrequent and smaller peaks, even during night-time when electricity prices are lower. Tables 6 and 7 offer a comparative analysis of optimized total costs. During the cold period, the traditional PI control incurred the highest total costs, amounting to 4041 Yen, while in the warm week, its costs were 2670 Yen. The single-agent system had expenses of 2882 Yen in the colder period and 2062 Yen in the warm week. Remarkably, the multi-agent PPO system achieved considerable cost savings, with total expenditures of only 1722 Yen in the cold period and 918 Yen in the warm week. Fig. 8 (a) and Fig. 8 (c) provide further insight into the daily cost efficiency of these control methods throughout the colder and warmer weeks, respectively. Presented as bar charts with consecutive dates on the X-axis and total daily energy consumption cost on the Y-axis, these figures highlight the cost efficiency of the multi-agent PPO control method. It consistently demonstrates lower cumulative weekly energy costs compared to other strategies, emphasizing the effectiveness and scalability of integrated energy management systems in optimizing real-time operational costs under varying environmental conditions. 6 Conclusion and future work This study presents an optimization algorithm for ZEH with an integrated battery system utilizing a multi-agent PPO control framework. By designing optimal reward functions and setting environmental constraints, particularly through bifurcating the control system into two stages, we achieve off-grid operation conditions, maintain indoor temperature stability, optimize energy consumption costs, and enhance self-sufficiency and self-consumption ratios. Compared to the baseline PI control method, the proposed multi-agent PPO algorithm offers superior energy distribution and storage management, thereby increasing operational efficiency and thermal comfort. The integration of battery storage in this framework heightens the system's adaptability to the fluctuating nature of energy demand and supply, exemplifying the multi-agent system's ability to customize energy strategies to reduce costs and maximize onsite PV use. The main contributions are as follows: 1. Traditional RL algorithms frequently encounter convergence issues in multi-agent method training, necessitating significant time and computational resources. Our work addresses this challenge by integrating a multi-stage PPO algorithm with IL techniques. This integration marks a significant innovation in building control systems, resulting in a power system that is more flexible, robust, and cost-effective. 2. We introduce an innovative shared reward function calibrated to allocate rewards based on each agent's state. This strategic design encourages cooperative behavior among agents, targeting optimized energy use. Our method, distinct from conventional shared reward systems, exhibits enhanced precision in battery energy management, finely tuned to the varying dynamics of PV output and energy demands in ZEH. The new reward function strikes a relative balance between cost efficiency and thermal comfort in building energy management. 3. Compared to the baseline model, the multi-agent PPO algorithm substantially improves optimization performance for various devices in ZEH, while maintaining thermal comfort. Notably, it increases the average indoor temperature closer to the desired set-point by 1.33 °C and increases the self-sufficiency ratio by 34.86% in the colder week and 46.10% in the warmer week. It also improves the self-consumption ratio by 15.78% in the colder week and 18.47% in the warmer week, compared to the measured result. 4. The robustness and scalability of the proposed methods have been validated against real building data in diverse scenarios. While the multi-stage approach successfully tackles complex challenges in MARL for energy systems, there is scope for further research to integrate this algorithm with a broader spectrum of renewable energy sources and smart grid technologies. Such expansion is essential to address the increasing demand for energy systems that are both efficient and sustainable. Additionally, the potential of this approach for wider application across different building types and climatic conditions is substantial. The adaptability of these algorithms to varied environmental scenarios highlights their potential versatility. Future investigations should focus on assessing and enhancing the universality and resilience of these algorithms to ensure their effectiveness across a diverse spectrum of practical environments. Sample efficiency is paramount in offline learning RL algorithms for BES control, defined by selecting suitable algorithms, MDP modifications, and dynamic environmental models, as noted in [39]. The primary challenge lies in the need for an accurate environmental model. This study employed a reduced-order RC network for thermal dynamic modeling, which balances complexity and accuracy. However, a significant limitation of applying gray-box modeling for dynamic control is its inability to scale to detailed physical models due to the non-convex nature of its parameter estimation, resulting in unreliable simulation outcomes [56]. Recent advances in physics-informed neural networks (PINNs) offer a novel approach where models are learned directly from data while adhering to physical constraints, thus enhancing both data efficiency and model accuracy. Therefore, the application of physics-informed neural networks could provide valuable insights into model-based simulation and control. Additionally, extending this research to encompass a cluster of buildings within a community could address the challenge posed by varying state-action spaces between buildings and different devices, a major obstacle in scaling these solutions. There is a pressing need for algorithms capable of producing agents that can execute effective and valuable actions either with minimal training or without any training at all. Rapid fine-tuning of the policy, coupled with the integration of expert knowledge, is crucial for streamlining the deployment of MARL systems in real-world scenarios. This integrated approach will advance our understanding of MARL applications in energy systems, expanding the limits of what is achievable in sustainable and efficient energy management. CRediT authorship contribution statement Zixuan Wang: Writing \u2013 original draft, Visualization, Software, Resources, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Fu Xiao: Writing \u2013 review & editing, Resources. Yi Ran: Visualization, Data curation. Yanxue Li: Writing \u2013 review & editing, Writing \u2013 original draft, Software, Resources, Methodology, Investigation, Funding acquisition, Formal analysis, Conceptualization. Yang Xu: Writing \u2013 review & editing. Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments This study was supported by the National Natural Science Foundation of China \u201cResearch on operation optimization strategy of energy flexible buildings based on synergizing data-driven and physics mechanism approach\u201d (No. 52308098), the Shandong Natural Science Foundation \u201cResearch on Flexible District Integrated Energy System under High Penetration Level of Renewable Energy\u201d (No. ZR2021QE084) and the Xiangjiang Plan \u201cDevelopment of Smart Building Management Technologies Towards Carbon Neutrality\u201d (No. XJ20220028). Appendix A Multi-agent algorithm A.1 Q-learning In the field of RL algorithm, Q-learning is a widely-used method. It achieves efficiency by iteratively refining a Q-table that represents the expected utility of actions in various states. The essence of Q-learning lies in its Q-function show in Eq. (12), This function, typically denoted by Q π s a , represents the expected cumulative reward of taking an action a in state s and subsequently adhering to a specific policy π which adheres to the Bellman equation to guide the update process. The symbol γ represents the discount factor, while r symbolizes the reward received after taking action a . The term max a \u2032 Q s \u2032 a \u2032 denotes the maximum expected future reward transition to next state s \u2032 , considering all possible actions a \u2032 . Balancing exploration of the environment with exploitation of acquired knowledge, typically managed by strategies like the ε-greedy policy, this tabular method enables Q-learning to converge towards the optimal action-value function Q π ∗ s a , making it highly effective for decision-making tasks. (12) Q s a ← Q s a + α r + γ max a \u2032 Q s \u2032 a \u2032 − Q s a However, in the complex environment of MARL, the tabular approach will encounter two primary limitations. The first pertains to the scalability of the tabular approach, as the table's size scales linearly with the number of potential inputs to the value function. Such an expansion pattern renders the approach impractical for complex problems where the state space is excessively large. The second limitation is that the tabular method updates value function estimates exclusively for states that have been explored, leaving the value function for unexplored states unchanged. This isolated update methodology can result in an inefficient learning trajectory in environments with infinite state or action dimensions. An exemplar case is the state space concerning outdoor temperature and the power of a space heating system in the ambit of controlling BES. In contrast, deep learning emerges as a robust and flexible alternative for function approximation, capable of autonomously discerning and delineating state features. This proficiency allows deep learning to adeptly model non-linear and complex functions, thus fostering generalization to novel, unseen states. Deep learning's superior capability for intricate function representation and state adaptability is a compelling justification for its preference over conventional tabular techniques, particularly in scenarios requiring such complex function portrayal. The introduction of Deep Q-Networks (DQN) heralds a significant advancement in reinforcement learning, amalgamating the tabular approach with the strengths of deep neural networks. Developed by DeepMind researchers in 2013 [57], DQN addresses the challenges of scalability and high dimensionality inherent in classical Q-learning by utilizing a deep neural network to approximate the Q-function. In 2015 [27], the same team further refined this method by incorporating experience replay, which augments learning efficiency by leveraging a repository of past experiences. Moreover, the implementation of fixed Q-targets, through a separate network for target generation, has stabilized the training process. These enhancements underscore deep learning's potential to amplify reinforcement learning in intricate, evolving environments. A.2 DQN The DQN algorithm exhibits remarkable efficiency in processing high-dimensional state vectors and has demonstrated capabilities surpassing human performance in numerous tasks. Nonetheless, however, it only efficient in scenarios with a limited set of actions, relies on a greedy strategy to select the action with the highest Q-value. Therefore, this method is inherently unsuitable for continuous action spaces, where the spectrum of potential actions is extensive and cannot be discretely separate in BES. A.3 Actor-critic (AC) method Addressing the challenges highlighted in Section A.2, researchers have proposed several algorithms, including DDPG [58], Twin Delayed DDPG (TD3) [59], and PPO [60], to effectively navigate the intricacies of continuous action spaces. The actor-critic algorithms, a subsect of policy gradient algorithms in RL, are characterized by the ability to manage continuous action spaces. This approach typically integrates a policy-based (actor) network with a value-based (critic) network. The actor is tasked with selecting actions according to the current policy, π θ a s , which maps states s to actions a , and the policy network parameters is θ , which is subsequently optimized to maximize the future reward as show in Eq. (13). (13) Δ θ = α ⋅ ∇ θ log π θ a s ⋅ R t Here, π θ a s denotes the policy function, ∇ θ log π θ a s is the gradient of the logarithm concerning parameters θ , R t represents the reward at timestep t , and α is the learning rate. The critic assesses actions by estimating the value function through the state-action value function Q s a , with parameters w , It aims to minimize the discrepancy between the predicted and actual rewards following action a , as detailed in Eq. (14). (14) Δ w = β ⋅ R t + γ ⋅ Q w s \u2032 a \u2032 − Q w s a ⋅ ∇ w Q w s a In this equation, R t represents the reward received after taking the action a in the state s , γ is the discount factor, the Q w s \u2032 a \u2032 is Q-value estimates the value for the state-action pair with w , and β represents the learning rate. This temporal difference adjustment serves to refine the Q-value estimates towards greater accuracy. A.4 PPO In the realm of RL algorithms, policy parameters are updated iteratively using gradients per the policy gradient theorem to target enhanced expected returns. However, such updates, even when implemented with minimal learning rates, can significantly alter the policy, potentially undermining performance. To counteract this volatility, trust regions [61], which confine the policy parameter space to minimal changes, are employed, ensuring performance within these regions does not substantially deteriorate. Hence, PPO [60] has made certain improvements compared with prior actor-critic algorithm, as shown below: \u2022 Simplified Trust Region Approach: PPO simplifies Trust Region Policy Optimization (TRPO) by substituting its intricate constrained optimization with a clipped objective function. This function constrains the importance of sampling weights within a predefined range (e.g., 1 + ϵ ), thereby limiting policy updates and reducing computational demands. \u2022 Efficient Data Reuse: PPO leverages the same batch of data for multiple updates via importance sampling, thereby improving data efficiency. Ease of Implementation and Robustness: PPO reduces the quantity of hyperparameters relative to other AC methods, enhancing its robustness and reliability across diverse RL scenarios and tasks. PPO is initialized with two neural networks: the actor network, denoted by π , with randomly initialized parameters θ , and the critic, denoted by V , with parameter w , The training loop persists until convergence is achieved. Within each environment interaction, the agent observes the current state s t , selects an action a t from the current actor network's policy π and then applies this action, accruing an immediate reward r t and transitioning to the next state s t + 1 . The inner loop, indexed by epoch e , utilizes importance sampling weights ρ s t a t as a correction factor to adjust the off-policy data to better reflect the updated policy π . The importance sampling weight is defined as in Eq. (15). (15) ρ s t a t = π a t s t θ π old a t s t (16) A adv s t a t = r t − V s t if s t + 1 is terminal r t + γV s t + 1 − V s t otherwise Utilizing the weights, PPO can update the policy multiple times using the same dataset. The calculation of the advantage A adv s t a t varies depending on whether the subsequent state s t + 1 is terminal as depicted in Eq. (16). If terminal, the advantage is simply the difference between the reward and the value estimated by the critic network for the current state. Otherwise, it includes the discounted value of the next state, thus representing future rewards in the calculation. The update process for the actor and critic networks involves computing respective loss functions; the actor loss function L θ and critic loss function L ω are shown in Eq. (17) and Eq. (18) respectively. (17) L CLIP θ = \u2010 min ρ s t a t A adv ( s t a t ) clip ( ρ ( s t a t ) 1 − ϵ 1 + ϵ ) A adv ( s t a t ) (18) L ω = E V ω s t − r t 2 Here, ϵ is a hyperparameter that determines the degree to which the policy is allowed to deviate from the previous policy π old . E denotes the expected value of the squared discrepancies between the estimated state values and the observed returns. Within the PPO algorithm, the update mechanisms for both the actor and critic have been further refined. The actor loss L θ in PPO is calculated through a clipped surrogate objective function, incorporating the importance sampling weight ρ s t a t , which balances the policy update distribution and the advantage function A adv s t a t . This clipping constrains updates within a predefined range, mitigating the risk of excessive policy updates. Conversely, the critic employs a loss function L ω , congruent with traditional actor-critic methods but adapted for integration with the actor's clipped objective. The critic's goal is to converge to an accurate estimation of expected returns, providing a reliable baseline for the actor to calculate the advantage function. These methodological enhancements have established PPO as a widely adopted algorithm in the field of deep reinforcement learning, especially in complex environments with continuous action spaces. References [1] Y.-M. Wei K. Chen J.-N. Kang W. Chen X.-Y. Wang X. Zhang Policy and Management of Carbon Peaking and Carbon Neutrality: a literature review Engineering 14 2022 52 63 Y.-M. Wei, K. Chen, J.-N. Kang, W. Chen, X.-Y. Wang, X. Zhang, Policy and Management of Carbon Peaking and Carbon Neutrality: A Literature Review, Engineering 14 (2022) 52-63. [2] J. Clarke J. Searle Active building demonstrators for a low-carbon future Nat Energy 6 12 2021 1087 1089 J. Clarke, J. Searle, Active Building demonstrators for a low-carbon future, Nature Energy 6(12) (2021) 1087-1089. [3] W. Wu B. Dong Q. Wang M. Kong D. Yan J. An A novel mobility-based approach to derive urban-scale building occupant profiles and analyze impacts on building energy consumption Appl Energy 278 2020 115656 W. Wu, B. Dong, Q. Wang, M. Kong, D. Yan, J. An, Y. Liu, A novel mobility-based approach to derive urban-scale building occupant profiles and analyze impacts on building energy consumption, Applied Energy 278 (2020) 115656. [4] B. Svetozarevic M. Begle P. Jayathissa S. Caranovic R.F. Shepherd Z. Nagy Dynamic photovoltaic building envelopes for adaptive energy and comfort management Nat Energy 4 8 2019 671 682 B. Svetozarevic, M. Begle, P. Jayathissa, S. Caranovic, R.F. Shepherd, Z. Nagy, I. Hischier, J. Hofer, A. Schlueter, Dynamic photovoltaic building envelopes for adaptive energy and comfort management, Nature Energy 4(8) (2019) 671-682. [5] D. Gielen F. Boshell D. Saygin M.D. Bazilian N. Wagner R. Gorini The role of renewable energy in the global energy transformation Energ Strat Rev 24 2019 38 50 D. Gielen, F. Boshell, D. Saygin, M.D. Bazilian, N. Wagner, R. Gorini, The role of renewable energy in the global energy transformation, Energy Strategy Reviews 24 (2019) 38-50. [6] T.a.I. Ministry of Economy, Japan Understanding the current energy situation in Japan (part 1) 2021 Ministry of Economy, Trade and Industry Japan 2021 T.a.I. Ministry of Economy, Japan, 2021 \u2013 Understanding the current energy situation in Japan (Part 1), Ministry of Economy, Trade and Industry, Japan, 2021. [7] X. Zhang W. Gao Y. Li Z. Wang Y. Ushifusa Y. Ruan Operational performance and load flexibility analysis of Japanese zero energy house Int J Environ Res Public Health 18 13 2021 X. Zhang, W. Gao, Y. Li, Z. Wang, Y. Ushifusa, Y. Ruan, Operational Performance and Load Flexibility Analysis of Japanese Zero Energy House, Int J Environ Res Public Health 18(13) (2021). [8] Y. Li W. Gao X. Zhang Y. Ruan Y. Ushifusa F. Hiroatsu Techno-economic performance analysis of zero energy house applications with home energy management system in Japan Energ Buildings 214 2020 109862 Y. Li, W. Gao, X. Zhang, Y. Ruan, Y. Ushifusa, F. Hiroatsu, Techno-economic performance analysis of zero energy house applications with home energy management system in Japan, Energy and Buildings 214 (2020) 109862. [9] D.M. Tasuku Kuwabara Benjamin Sauer Yuito Yamada How Japan could reach carbon neutrality by 2050 2021 McKinsey & Company D.M. Tasuku Kuwabara, Benjamin Sauer, Yuito Yamada, How Japan could reach carbon neutrality by 2050, McKinsey & Company, 2021. [10] X. Zhang F. Xiao Y. Li Y. Ran W. Gao Flexible coupling and grid-responsive scheduling assessments of distributed energy resources within existing zero energy houses J Building Eng 87 2024 109047 X. Zhang, F. Xiao, Y. Li, Y. Ran, W. Gao, Flexible coupling and grid-responsive scheduling assessments of distributed energy resources within existing zero energy houses, Journal of Building Engineering 87 (2024) 109047. [11] Y. Fu Z. O\u2019Neill V. Adetola A flexible and generic functional mock-up unit based threat injection framework for grid-interactive efficient buildings: a case study in Modelica Energ Buildings 250 2021 Y. Fu, Z. O'\u2019Neill, V. Adetola, A flexible and generic functional mock-up unit based threat injection framework for grid-interactive efficient buildings: A case study in Modelica, Energy and Buildings 250 (2021). [12] F. Jorissen W. Boydens L. Helsen TACO, an automated toolchain for model predictive control of building systems: implementation and verification J Build Perform Simul 12 2 2019 180 192 F. Jorissen, W. Boydens, L. Helsen, TACO, an automated toolchain for model predictive control of building systems: implementation and verification, Journal of building performance simulation 12(2) (2019) 180-192. [13] J. Arroyo F. Spiessens L. Helsen Identification of multi-zone grey-box building models for use in model predictive control J Build Perform Simul 13 4 2020 472 486 J. Arroyo, F. Spiessens, L. Helsen, Identification of multi-zone grey-box building models for use in model predictive control, Journal of Building Performance Simulation 13(4) (2020) 472-486. [14] E. Atam L. Helsen Control-oriented thermal modeling of multizone buildings: methods and issues: intelligent control of a building system IEEE Control Syst Mag 36 3 2016 86 111 E. Atam, L. Helsen, Control-Oriented Thermal Modeling of Multizone Buildings: Methods and Issues: Intelligent Control of a Building System, IEEE Control Systems Magazine 36(3) (2016) 86-111. [15] J. Arroyo C. Manna F. Spiessens L. Helsen Reinforced model predictive control (RL-MPC) for building energy management Appl Energy 309 2022 J. Arroyo, C. Manna, F. Spiessens, L. Helsen, Reinforced model predictive control (RL-MPC) for building energy management, Applied Energy 309 (2022). [16] T. Wei Y. Wang Q. Zhu Deep reinforcement learning for building HVAC control Proceedings of the 54th annual design automation conference 2017 2017 1 6 T. Wei, Y. Wang, Q. Zhu, Deep Reinforcement Learning for Building HVAC Control, Proceedings of the 54th Annual Design Automation Conference 2017, 2017, pp. 1-6. [17] T. Moriyama G. De Magistris M. Tatsubori T.-H. Pham A. Munawar R. Tachibana Reinforcement learning testbed for power-consumption optimization L. Li K. Hasegawa S. Tanaka Methods and applications for modeling and simulation of complex systems 2018 Springer Singapore Singapore 45 59 T. Moriyama, G. De Magistris, M. Tatsubori, T.-H. Pham, A. Munawar, R. Tachibana, Reinforcement Learning Testbed for Power-Consumption Optimization, in: L. Li, K. Hasegawa, S. Tanaka (Eds.) Methods and Applications for Modeling and Simulation of Complex Systems, Springer Singapore, Singapore, 2018, pp. 45-59. [18] Z. Zhang K.P. Lam Practical implementation and evaluation of deep reinforcement learning control for a radiant heating system Proceedings of the 5th conference on systems for built environments 2018 148 157 Z. Zhang, K.P. Lam, Practical implementation and evaluation of deep reinforcement learning control for a radiant heating system, Proceedings of the 5th Conference on Systems for Built Environments, 2018, pp. 148-157. [19] K. Arulkumaran M.P. Deisenroth M. Brundage A.A. Bharath Deep reinforcement learning a brief survey IEEE Signal Process Mag 34 6 2017 26 38 K. Arulkumaran, M.P. Deisenroth, M. Brundage, A.A. Bharath, Deep Reinforcement Learning A brief survey, IEEE SIGNAL PROCESSING MAGAZINE 34(6) (2017) 26-38. [20] L. Yu Y. Sun Z. Xu C. Shen D. Yue T. Jiang Multi-agent deep reinforcement learning for HVAC control in commercial buildings IEEE Trans Smart Grid 12 1 2021 407 419 L. Yu, Y. Sun, Z. Xu, C. Shen, D. Yue, T. Jiang, X. Guan, Multi-Agent Deep Reinforcement Learning for HVAC Control in Commercial Buildings, IEEE Transactions on Smart Grid 12(1) (2021) 407-419. [21] C.X. Jiang Z.X. Jing X.R. Cui T.Y. Ji Q.H. Wu Multiple agents and reinforcement learning for modelling charging loads of electric taxis Appl Energy 222 2018 158 168 C.X. Jiang, Z.X. Jing, X.R. Cui, T.Y. Ji, Q.H. Wu, Multiple agents and reinforcement learning for modelling charging loads of electric taxis, Applied Energy 222 (2018) 158-168. [22] C.W. Anderson D.C. Hittle A.D. Katz R.M. Kretchmar Synthesis of reinforcement learning, neural networks and PI control applied to a simulated heating coil Artif Intell Eng 11 4 1997 421 429 C.W. Anderson, D.C. Hittle, A.D. Katz, R.M. Kretchmar, Synthesis of reinforcement learning, neural networks and PI control applied to a simulated heating coil, Artificial Intelligence in Engineering 11(4) (1997) 421-429. [23] M.C. Mozer The neural network house: An environment hat adapts to its inhabitants Proc. AAAI spring symp. intelligent environments 1998 M.C. Mozer, The neural network house: An environment hat adapts to its inhabitants, Proc. AAAI Spring Symp. Intelligent Environments, 1998. [24] S. Faddel G. Tian Q. Zhou H. Aburub Data driven Q-learning for commercial HVAC control 2020 SoutheastCon 2020 1 6 S. Faddel, G. Tian, Q. Zhou, H. Aburub, Data Driven Q-Learning for Commercial HVAC Control, 2020 SoutheastCon, 2020, pp. 1-6. [25] A. Soares D. Geysen F. Spiessens D. Ectors O. De Somer K. Vanthournout Using reinforcement learning for maximizing residential self-consumption \u2013 results from a field test Energ Buildings 207 2020 A. Soares, D. Geysen, F. Spiessens, D. Ectors, O. De Somer, K. Vanthournout, Using reinforcement learning for maximizing residential self-consumption \u2013 Results from a field test, Energy and Buildings 207 (2020). [26] Q. Yan Researches on the curse of dimensionality in reinforcement learning 2010 Q. Yan, Researches on the curse of dimensionality in reinforcement learning, (2010). [27] V. Mnih K. Kavukcuoglu D. Silver A.A. Rusu J. Veness M.G. Bellemare Human-level control through deep reinforcement learning Nature 518 7540 2015 529 533 V. Mnih, K. Kavukcuoglu, D. Silver, A.A. Rusu, J. Veness, M.G. Bellemare, A. Graves, M. Riedmiller, A.K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, D. Hassabis, Human-level control through deep reinforcement learning, NATURE 518(7540) (2015) 529-533. [28] S. Brandi M.S. Piscitelli M. Martellacci A. Capozzoli Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings Energ Buildings 224 2020 110225 S. Brandi, M.S. Piscitelli, M. Martellacci, A. Capozzoli, Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings, Energy and Buildings 224 (2020) 110225. [29] X. Wang X. Kang J. An H. Chen D. Yan Reinforcement learning approach for optimal control of ice-based thermal energy storage (TES) systems in commercial buildings Energ Buildings 301 2023 X. Wang, X. Kang, J. An, H. Chen, D. Yan, Reinforcement learning approach for optimal control of ice-based thermal energy storage (TES) systems in commercial buildings, Energy and Buildings 301 (2023). [30] D. Azuatalam W.-L. Lee F. de Nijs A. Liebman Reinforcement learning for whole-building HVAC control and demand response Energy AI 2 2020 D. Azuatalam, W.-L. Lee, F. de Nijs, A. Liebman, Reinforcement learning for whole-building HVAC control and demand response, Energy and AI 2 (2020). [31] T. Schreiber S. Eschweiler M. Baranski D. Müller Application of two promising reinforcement learning algorithms for load shifting in a cooling supply system Energ Buildings 229 2020 T. Schreiber, S. Eschweiler, M. Baranski, D. Müller, Application of two promising Reinforcement Learning algorithms for load shifting in a cooling supply system, Energy and Buildings 229 (2020). [32] T. Yang L. Zhao W. Li J. Wu A.Y. Zomaya Towards healthy and cost-effective indoor environment management in smart homes: a deep reinforcement learning approach Appl Energy 300 2021 T. Yang, L. Zhao, W. Li, J. Wu, A.Y. Zomaya, Towards healthy and cost-effective indoor environment management in smart homes: A deep reinforcement learning approach, Applied Energy 300 (2021). [33] Y. Du F. Li J. Munk K. Kurte O. Kotevska K. Amasyali Multi-task deep reinforcement learning for intelligent multi-zone residential HVAC control Electr Power Syst Res 192 2021 106959 Y. Du, F. Li, J. Munk, K. Kurte, O. Kotevska, K. Amasyali, H. Zandi, Multi-task deep reinforcement learning for intelligent multi-zone residential HVAC control, Electric Power Systems Research 192 (2021) 106959. [34] M. Biemann F. Scheller X. Liu L. Huang Experimental evaluation of model-free reinforcement learning algorithms for continuous HVAC control Appl Energy 298 2021 M. Biemann, F. Scheller, X. Liu, L. Huang, Experimental evaluation of model-free reinforcement learning algorithms for continuous HVAC control, Applied Energy 298 (2021). [35] Y. Gao Y. Matsunami S. Miyata Y. Akashi Operational optimization for off-grid renewable building energy system using deep reinforcement learning Appl Energy 325 2022 Y. Gao, Y. Matsunami, S. Miyata, Y. Akashi, Operational optimization for off-grid renewable building energy system using deep reinforcement learning, Applied Energy 325 (2022). [36] D. Coraci S. Brandi A. Capozzoli Effective pre-training of a deep reinforcement learning agent by means of long short-term memory models for thermal energy management in buildings Energy Convers Manag 291 2023 D. Coraci, S. Brandi, A. Capozzoli, Effective pre-training of a deep reinforcement learning agent by means of long short-term memory models for thermal energy management in buildings, Energy Conversion and Management 291 (2023). [37] Y. Li Z. Wang W. Xu W. Gao Y. Xu F. Xiao Modeling and energy dynamic control for a ZEH via hybrid model-based deep reinforcement learning Energy 277 2023 127627 Y. Li, Z. Wang, W. Xu, W. Gao, Y. Xu, F. Xiao, Modeling and energy dynamic control for a ZEH via hybrid model-based deep reinforcement learning, Energy 277 (2023) 127627. [38] H. Qin Z. Yu T. Li X. Liu L. Li Energy-efficient heating control for nearly zero energy residential buildings with deep reinforcement learning Energy 264 2023 126209 H. Qin, Z. Yu, T. Li, X. Liu, L. Li, Energy-efficient heating control for nearly zero energy residential buildings with deep reinforcement learning, Energy 264 (2023) 126209. [39] D. Weinberg Q. Wang T.O. Timoudas C. Fischione A review of reinforcement learning for controlling building energy systems from a computer science perspective Sustain Cities Soc 89 2023 D. Weinberg, Q. Wang, T.O. Timoudas, C. Fischione, A Review of Reinforcement Learning for Controlling Building Energy Systems From a Computer Science Perspective, Sustainable Cities and Society 89 (2023). [40] T.T. Nguyen N.D. Nguyen S. Nahavandi Deep reinforcement learning for multiagent systems: a review of challenges, solutions, and applications IEEE Trans Cybernet 50 9 2020 3826 3839 T.T. Nguyen, N.D. Nguyen, S. Nahavandi, Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications, IEEE TRANSACTIONS ON CYBERNETICS 50(9) (2020) 3826-3839. [41] A. Krnjaic J.D. Thomas G. Papoudakis L. Schäfer P. Börsting S.V. Albrecht Scalable multi-agent reinforcement learning for warehouse logistics with robotic and human co-workers arXiv preprint arXiv:2212.11498 2022 A. Krnjaic, J.D. Thomas, G. Papoudakis, L. Schäfer, P. Börsting, S.V. Albrecht, Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers, arXiv preprint arXiv:2212.11498 (2022). [42] S. Shalev-Shwartz S. Shammah A. Shashua Safe, multi-agent, reinforcement learning for autonomous driving arXiv preprint arXiv:1610.03295 2016 S. Shalev-Shwartz, S. Shammah, A. Shashua, Safe, multi-agent, reinforcement learning for autonomous driving, arXiv preprint arXiv:1610.03295 (2016). [43] A. Peake J. McCalmon B. Raiford T. Liu S. Alqahtani Multi-agent reinforcement learning for cooperative adaptive cruise control 2020 IEEE 32nd international conference on tools with artificial intelligence (ICTAI) 2020 IEEE 15 22 A. Peake, J. McCalmon, B. Raiford, T. Liu, S. Alqahtani, Multi-agent reinforcement learning for cooperative adaptive cruise control, 2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI), IEEE, 2020, pp. 15-22. [44] M. Zhou J. Luo J. Villella Y. Yang D. Rusu J. Miao Smarts: Scalable multi-agent reinforcement learning training school for autonomous driving arXiv preprint arXiv:2010.09776 2020 M. Zhou, J. Luo, J. Villella, Y. Yang, D. Rusu, J. Miao, W. Zhang, M. Alban, I. Fadakar, Z. Chen, Smarts: Scalable multi-agent reinforcement learning training school for autonomous driving, arXiv preprint arXiv:2010.09776 (2020). [45] M. Roesch C. Linder R. Zimmermann A. Rudolf A. Hohmann G. Reinhart Smart grid for industry using multi-agent reinforcement learning Appl Sci 10 19 2020 6900 M. Roesch, C. Linder, R. Zimmermann, A. Rudolf, A. Hohmann, G. Reinhart, Smart grid for industry using multi-agent reinforcement learning, Applied Sciences 10(19) (2020) 6900. [46] D. Qiu J. Wang J. Wang G. Strbac Multi-agent reinforcement learning for automated peer-to-peer energy trading in double-side auction market IJCAI 2021 2913 2920 D. Qiu, J. Wang, J. Wang, G. Strbac, Multi-Agent Reinforcement Learning for Automated Peer-to-Peer Energy Trading in Double-Side Auction Market, IJCAI, 2021, pp. 2913-2920. [47] A. Shavandi M. Khedmati A multi-agent deep reinforcement learning framework for algorithmic trading in financial markets Expert Syst Appl 208 2022 118124 A. Shavandi, M. Khedmati, A multi-agent deep reinforcement learning framework for algorithmic trading in financial markets, Expert Systems with Applications 208 (2022) 118124. [48] Y. Gao Y. Matsunami S. Miyata Y. Akashi Multi-agent reinforcement learning dealing with hybrid action spaces: a case study for off-grid oriented renewable building energy system Appl Energy 326 2022 120021 Y. Gao, Y. Matsunami, S. Miyata, Y. Akashi, Multi-agent reinforcement learning dealing with hybrid action spaces: A case study for off-grid oriented renewable building energy system, Applied Energy 326 (2022) 120021. [49] K. Nweye B. Liu P. Stone Z. Nagy Real-world challenges for multi-agent reinforcement learning in grid-interactive buildings Energy AI 10 2022 100202 K. Nweye, B. Liu, P. Stone, Z. Nagy, Real-world challenges for multi-agent reinforcement learning in grid-interactive buildings, Energy and AI 10 (2022) 100202. [50] R. Shen S. Zhong X. Wen Q. An R. Zheng Y. Li Multi-agent deep reinforcement learning optimization framework for building energy system with renewable energy Appl Energy 312 2022 R. Shen, S. Zhong, X. Wen, Q. An, R. Zheng, Y. Li, J. Zhao, Multi-agent deep reinforcement learning optimization framework for building energy system with renewable energy, Applied Energy 312 (2022). [51] L. Yu Z. Xu T. Zhang X. Guan D. Yue Energy-efficient personalized thermal comfort control in office buildings based on multi-agent deep reinforcement learning Build Environ 223 2022 109458 L. Yu, Z. Xu, T. Zhang, X. Guan, D. Yue, Energy-efficient personalized thermal comfort control in office buildings based on multi-agent deep reinforcement learning, Building and Environment 223 (2022) 109458. [52] R.S. Sutton A.G. Barto Reinforcement learning: An introduction 2018 MIT press R.S. Sutton, A.G. Barto, Reinforcement learning: An introduction, MIT press2018. [53] H. Song C.-C. Liu J. Lawarree R. Dahlgren Optimal electricity supply bidding by Markov decision process IEEE Power Eng Rev 19 7 1999 39 H. Song, C.-C. Liu, J. Lawarree, R. Dahlgren, Optimal electricity supply bidding by Markov decision process, IEEE Power Engineering Review 19(7) (1999) 39-39. [54] K.E.P. Company https://www.kyuden.co.jp/user_menu_plan_denka-de-night.html 2023 K.E.P. Company, 2023. https://www.kyuden.co.jp/user_menu_plan_denka-de-night.html. [55] G. Brockman V. Cheung L. Pettersson J. Schneider J. Schulman J. Tang Openai gym arXiv preprint arXiv:1606.01540 2016 G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, W. Zaremba, Openai gym, arXiv preprint arXiv:1606.01540 (2016). [56] Z. Nagy G. Henze S. Dey J. Arroyo L. Helsen X. Zhang Ten questions concerning reinforcement learning for building energy management Build Environ 241 2023 Z. Nagy, G. Henze, S. Dey, J. Arroyo, L. Helsen, X. Zhang, B. Chen, K. Amasyali, K. Kurte, A. Zamzam, H. Zandi, J. Drgoňa, M. Quintana, S. McCullogh, J.Y. Park, H. Li, T. Hong, S. Brandi, G. pPinto, A. Capozzoli, D. Vrabie, M. Bergés, K. Nweye, T. Marzullo, A. Bernstein, Ten questions concerning reinforcement learning for building energy management, Building and Environment 241 (2023). [57] V. Mnih K. Kavukcuoglu D. Silver A. Graves I. Antonoglou D. Wierstra Playing atari with deep reinforcement learning 2013 V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, M.J.a.p.a. Riedmiller, Playing atari with deep reinforcement learning, (2013). [58] T.P. Lillicrap J.J. Hunt A. Pritzel N. Heess T. Erez Y. Tassa Continuous control with deep reinforcement learning 2015 T.P. Lillicrap, J.J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, D.J.a.p.a. Wierstra, Continuous control with deep reinforcement learning, (2015). [59] S. Fujimoto H. Hoof D. Meger Addressing function approximation error in actor-critic methods International conference on machine learning 2018 PMLR 1587 1596 S. Fujimoto, H. Hoof, D. Meger, Addressing function approximation error in actor-critic methods, International conference on machine learning, PMLR, 2018, pp. 1587-1596. [60] J. Schulman F. Wolski P. Dhariwal A. Radford O.J.A.P.A. Klimov Proximal policy optimization algorithms 2017 J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O.J.a.p.a. Klimov, Proximal policy optimization algorithms, (2017). [61] J. Schulman S. Levine P. Abbeel M. Jordan P. Moritz Trust region policy optimization International conference on machine learning 2015 PMLR 1889 1897 J. Schulman, S. Levine, P. Abbeel, M. Jordan, P. Moritz, Trust region policy optimization, International conference on machine learning, PMLR, 2015, pp. 1889-1897.",
    "scopus-id": "85192857528",
    "coredata": {
        "eid": "1-s2.0-S0306261924007979",
        "dc:description": "Deploying renewable energy and implementing smart energy management strategies are crucial for decarbonizing Building Energy Systems (BES). Despite recent advancements in data-driven Deep Reinforcement Learning (DRL) for BES optimization, significant challenges still exist, such as the time-consuming and data-intensive nature of training DRL controllers and the complexity of environment dynamics in Multi-Agent Reinforcement Learning (MARL). Consequently, these obstacles impede the synchronization and coordination of multiple agent control, leading to slow DRL convergence performance. To address these issues. This paper proposes a novel approach to optimize hybrid building energy systems. We introduce an integrated system combining a multi-stage Proximal Policy Optimization (PPO) on-policy framework with Imitation Learning (IL), interacting with the model environment. To improve scalability and robustness of Multi-agent Systems (MAS), this approach is designed to enhance training efficiency with centralized training and decentralized execution. Simulation results of case studies demonstrate the effectiveness of the Multi-agent Deep Reinforcement Learning (MADRL) model in optimizing the operations of hybrid building energy systems in terms of indoor thermal comfort and energy efficiency. Results show the proposed framework significantly improve performance in achieving convergence in just 50 episodes for dynamic decision-making. The scalability and robustness of the proposed model have been validated across various scenarios. Compared with the baseline during cold and warm weeks, the proposed control approach achieved improvements of 34.86% and 46.10% in energy self-sufficiency ratio, respectively. Additionally, the developed MADRL effectively improved solar photovoltaic (PV) self-consumption and reduced household energy costs. Notably, it increased the average indoor temperature closer to the desired set-point by 1.33 °C, and improved the self-consumption ratio by 15.78% in the colder week and 18.47% in the warmer week, compared to baseline measurements. These findings highlight the advantages of the multi-stage PPO on-policy framework, enabling faster learning and reduced training time, resulting in cost-effective solutions and enhanced solar PV self-consumption.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2024-08-01",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S0306261924007979",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Wang, Zixuan"
            },
            {
                "@_fa": "true",
                "$": "Xiao, Fu"
            },
            {
                "@_fa": "true",
                "$": "Ran, Yi"
            },
            {
                "@_fa": "true",
                "$": "Li, Yanxue"
            },
            {
                "@_fa": "true",
                "$": "Xu, Yang"
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S0306261924007979"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S0306261924007979"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S0306-2619(24)00797-9",
        "prism:volume": "367",
        "articleNumber": "123414",
        "prism:publisher": "Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
        "dc:title": "Scalable energy management approach of residential hybrid energy system using multi-agent deep reinforcement learning",
        "prism:copyright": "© 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
        "openaccess": "0",
        "prism:issn": "03062619",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "Multi-agent reinforcement learning"
            },
            {
                "@_fa": "true",
                "$": "Schedule optimization"
            },
            {
                "@_fa": "true",
                "$": "Multi-stage"
            },
            {
                "@_fa": "true",
                "$": "Thermal comfort"
            },
            {
                "@_fa": "true",
                "$": "Energy cost"
            }
        ],
        "openaccessArticle": "false",
        "prism:publicationName": "Applied Energy",
        "openaccessSponsorType": null,
        "prism:pageRange": "123414",
        "pubType": "fla",
        "prism:coverDisplayDate": "1 August 2024",
        "prism:doi": "10.1016/j.apenergy.2024.123414",
        "prism:startingPage": "123414",
        "dc:identifier": "doi:10.1016/j.apenergy.2024.123414",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "488",
            "@width": "726",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "108468",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "808",
            "@width": "774",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "126229",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "422",
            "@width": "687",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "43831",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "716",
            "@width": "758",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr10.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "149746",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "354",
            "@width": "535",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "42279",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "337",
            "@width": "717",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "29153",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "351",
            "@width": "802",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "61894",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "505",
            "@width": "389",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-fx2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "42026",
            "@ref": "fx2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "367",
            "@width": "546",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "34883",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "745",
            "@width": "801",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr8.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "83436",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "754",
            "@width": "800",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr9.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "201377",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "437",
            "@width": "491",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-fx1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "54676",
            "@ref": "fx1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "147",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "15692",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "157",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "10294",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "134",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "6004",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "173",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr10.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "13975",
            "@ref": "gr10",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "145",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "9020",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "103",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "5573",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "96",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7481",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "126",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-fx2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "5075",
            "@ref": "fx2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "147",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "6373",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "176",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr8.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7448",
            "@ref": "gr8",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "174",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr9.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "13331",
            "@ref": "gr9",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "184",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-fx1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7778",
            "@ref": "fx1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "2160",
            "@width": "3215",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1079937",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "3576",
            "@width": "3427",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "935658",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1868",
            "@width": "3042",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "288160",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "3167",
            "@width": "3354",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr10_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1318923",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1569",
            "@width": "2368",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "330710",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1492",
            "@width": "3174",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "229240",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1555",
            "@width": "3549",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "476077",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2237",
            "@width": "1723",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-fx2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "301465",
            "@ref": "fx2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1625",
            "@width": "2417",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "278842",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "3298",
            "@width": "3548",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr8_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "550871",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2004",
            "@width": "2126",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-gr9_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "799529",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1936",
            "@width": "2175",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-fx1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "410586",
            "@ref": "fx1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si1.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1689",
            "@ref": "si1",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si10.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2492",
            "@ref": "si10",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si100.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3902",
            "@ref": "si100",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si101.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6182",
            "@ref": "si101",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si103.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1972",
            "@ref": "si103",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si106.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3683",
            "@ref": "si106",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si107.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1210",
            "@ref": "si107",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si109.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "12065",
            "@ref": "si109",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si11.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "970",
            "@ref": "si11",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si114.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5077",
            "@ref": "si114",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si117.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1182",
            "@ref": "si117",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si12.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "804",
            "@ref": "si12",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si120.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1032",
            "@ref": "si120",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si122.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1659",
            "@ref": "si122",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si125.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1582",
            "@ref": "si125",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si126.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2378",
            "@ref": "si126",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si127.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "858",
            "@ref": "si127",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si128.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4183",
            "@ref": "si128",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si13.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1092",
            "@ref": "si13",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si130.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9974",
            "@ref": "si130",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si131.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "21640",
            "@ref": "si131",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si132.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6061",
            "@ref": "si132",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si134.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2346",
            "@ref": "si134",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si135.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2424",
            "@ref": "si135",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si136.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "20207",
            "@ref": "si136",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si137.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8577",
            "@ref": "si137",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si138.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "483",
            "@ref": "si138",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si139.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2723",
            "@ref": "si139",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si14.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "911",
            "@ref": "si14",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si140.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1419",
            "@ref": "si140",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si15.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4337",
            "@ref": "si15",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si16.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "989",
            "@ref": "si16",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si17.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1301",
            "@ref": "si17",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si18.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "994",
            "@ref": "si18",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si2.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3018",
            "@ref": "si2",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si22.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "892",
            "@ref": "si22",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si24.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "13882",
            "@ref": "si24",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si25.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2078",
            "@ref": "si25",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si26.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "917",
            "@ref": "si26",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si27.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1863",
            "@ref": "si27",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si28.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1238",
            "@ref": "si28",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si3.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2412",
            "@ref": "si3",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si30.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4376",
            "@ref": "si30",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si31.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "847",
            "@ref": "si31",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si32.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2214",
            "@ref": "si32",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si33.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11375",
            "@ref": "si33",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si34.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4640",
            "@ref": "si34",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si35.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1665",
            "@ref": "si35",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si36.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "941",
            "@ref": "si36",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si38.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1497",
            "@ref": "si38",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si39.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9032",
            "@ref": "si39",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si4.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1538",
            "@ref": "si4",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si40.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "978",
            "@ref": "si40",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si41.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1701",
            "@ref": "si41",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si43.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8985",
            "@ref": "si43",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si44.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "10356",
            "@ref": "si44",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si48.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1038",
            "@ref": "si48",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si49.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9524",
            "@ref": "si49",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si5.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5277",
            "@ref": "si5",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si50.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3669",
            "@ref": "si50",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si51.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1935",
            "@ref": "si51",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si53.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8744",
            "@ref": "si53",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si54.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "13368",
            "@ref": "si54",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si55.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "26491",
            "@ref": "si55",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si56.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1641",
            "@ref": "si56",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si57.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3932",
            "@ref": "si57",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si6.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4523",
            "@ref": "si6",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si61.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5134",
            "@ref": "si61",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si62.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9406",
            "@ref": "si62",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si64.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2070",
            "@ref": "si64",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si66.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7064",
            "@ref": "si66",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si67.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6570",
            "@ref": "si67",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si68.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3026",
            "@ref": "si68",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si69.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8387",
            "@ref": "si69",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si7.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3557",
            "@ref": "si7",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si70.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7816",
            "@ref": "si70",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si72.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "14636",
            "@ref": "si72",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si73.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1292",
            "@ref": "si73",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si74.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7771",
            "@ref": "si74",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si75.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1301",
            "@ref": "si75",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si76.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1695",
            "@ref": "si76",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si79.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11682",
            "@ref": "si79",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si81.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3012",
            "@ref": "si81",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si83.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4333",
            "@ref": "si83",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si88.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "911",
            "@ref": "si88",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si90.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7098",
            "@ref": "si90",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si92.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1306",
            "@ref": "si92",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si93.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5076",
            "@ref": "si93",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si94.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "12464",
            "@ref": "si94",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si95.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3988",
            "@ref": "si95",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-si99.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9509",
            "@ref": "si99",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261924007979-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "3992905",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85192857528"
    }
}}