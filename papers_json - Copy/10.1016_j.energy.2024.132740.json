{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85201238101",
    "originalText": "serial JL 271090 291210 291702 291711 291731 291877 291878 31 Energy ENERGY 2024-08-08 2024-08-08 2024-08-15 2024-08-15 2024-08-22T22:57:55 1-s2.0-S0360544224025143 S0360-5442(24)02514-3 S0360544224025143 10.1016/j.energy.2024.132740 S300 S300.1 FULL-TEXT 1-s2.0-S0360544224X00146 2024-10-08T01:07:01.157563Z 0 0 20241030 2024 2024-08-08T15:55:44.825076Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath affil appendices articletitle auth authfirstini authfull authlast grantsponsor grantsponsorid highlightsabst misctext orcid primabst ref 0360-5442 03605442 true 307 307 C Volume 307 180 132740 132740 132740 20241030 30 October 2024 2024-10-30 2024 Full Length Articles article fla © 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies. ACOMPARATIVESTUDYDQND3QNFORHVACSYSTEMOPTIMIZATIONCONTROL QIN H 1 Introduction 1.1 Optimal control of HVAC systems 1.2 Reinforcement learning applied to HVAC 1.3 Motivation and research structure 2 Methodology 2.1 Deep reinforcement learning algorithms 2.2 Data-driven cooling side system model 2.3 Model-free controller diagram 2.4 Configuration and initialization of two Q-networks 3 Case study 3.1 Case system 3.2 Model validation 3.3 Experimental design 4 Results and discussion 4.1 The impact of the Q-network structure 4.2 Training randomness and deployment stability 4.3 Control performance after full convergence 4.4 Future Outlook 5 Conclusion CRediT authorship contribution statement Appendix. A Hyper-parameters adjustment range References WANG 2008 3 32 S FERREIRA 2012 238 251 P HUANG 2015 203 216 H KUSIAK 2010 3092 3102 A KILLIAN 2016 403 412 M PRIVARA 2013 8 22 S CANNON 2004 229 237 M CHATTERJEE 2023 110766 A HAN 2021 137 148 M DU 2020 106959 Y FANG 2022 118552 X SOLINAS 2024 121749 F BLAD 2022 125290 C FANG 2022 118552 X KADAMALA 2024 100131 K LIU 2024 111197 X LI 2017 Y VOLODYMYR 2019 529 533 M ZHANG 2023 102173 Y BIEMANN 2021 117164 M LIU 2022 124857 X FANG 2022 118552 X DENG 2022 108680 X FU 2022 112284 Q REN 2022 103207 M GUO 2022 125095 X LI 2023 127627 Y FU 2023 110546 Q QIU 2020 110055 S JIANG 2021 110833 Z CHEN 2018 195 205 Y LIU 2018 1616 1625 T LIU 2018 544 555 T ZOU 2020 106535 Z DENG 2021 110860 Z YUAN 2018 31 36 W VANHASSELT 2016 H PROCEEDINGSAAAICONFERENCEARTIFICIALINTELLIGENCE DEEPREINFORCEMENTLEARNINGDOUBLEQLEARNINGC WANG 2016 1995 2003 Z INTERNATIONALCONFERENCEMACHINELEARNING DUELINGNETWORKARCHITECTURESFORDEEPREINFORCEMENTLEARNINGC TSAI 2016 301 312 C AFRAM 2017 96 113 A LIU 2020 Y CHEN 2016 785 794 T AFRAM 2017 96 113 A FAYED 2019 202 210 H YU 2020 407 419 L DU 2021 116117 Y BRANDI 2020 110225 S BIEMANN 2021 117164 M JIANG 2020 265 279 J MAGALHAES 2017 332 343 S QINX2024X132740 QINX2024X132740XH 2026-08-15T00:00:00.000Z 2026-08-15T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies. 2024-09-04T00:54:45.939Z http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/aggregated-refined SVR SVR Stichting Volksbond Rotterdam http://data.elsevier.com/vocabulary/SciValFunders/100019572 http://sws.geonames.org/2750405/ http://sws.geonames.org/1814991 In this paper, various techniques such as multivariate polynomial regression (MPR) [39], artificial neural networks (ANN) [40], support vector machines (SVR) [41], and XGBoost [42] are investigated as potential modeling methods. The model&apos;s accuracy and training efficiency are profoundly influenced by hyperparameter configuration. To minimize human intervention, hyperparameter selection is automated through algorithms. For the MPR model, regression models of degrees 1\\u20135 are assessed, and the most accurate one is selected by the algorithm. The Best Network after Multiple Iterations (BNMI) algorithm determines the optimum parameters for the ANN [43]. Grid search algorithm optimizes the hyperparameters of SVR and XGBoost [44], with the specific hyperparameter range for each model detailed in Appendix A. https://doi.org/10.15223/policy-017 https://doi.org/10.15223/policy-037 https://doi.org/10.15223/policy-012 https://doi.org/10.15223/policy-029 https://doi.org/10.15223/policy-004 item S0360-5442(24)02514-3 S0360544224025143 1-s2.0-S0360544224025143 10.1016/j.energy.2024.132740 271090 2024-10-08T01:07:01.157563Z 2024-10-30 1-s2.0-S0360544224025143-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/MAIN/application/pdf/513a1cb51f83c9236fb5a6b17f997c9b/main.pdf main.pdf pdf true 14944593 MAIN 16 1-s2.0-S0360544224025143-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/PREVIEW/image/png/671246b04503cd9df6bc18ddf35aad56/main_1.png main_1.png png 53498 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0360544224025143-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr3/DOWNSAMPLED/image/jpeg/31840ee3cf914e27d04af10b0457b3d7/gr3.jpg gr3 gr3.jpg jpg 98322 241 691 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224025143-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr4/DOWNSAMPLED/image/jpeg/c59a1ca08b2f8d738a8063c03bc4925b/gr4.jpg gr4 gr4.jpg jpg 176547 507 691 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224025143-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr1/DOWNSAMPLED/image/jpeg/0802f39c1afe6e9ee0d58a25165fdce1/gr1.jpg gr1 gr1.jpg jpg 57623 437 535 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224025143-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr2/DOWNSAMPLED/image/jpeg/f92c688708874bd2ba6228b5dc58bffd/gr2.jpg gr2 gr2.jpg jpg 33800 271 388 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224025143-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr7/DOWNSAMPLED/image/jpeg/19319e3e9908f4b2537b9586c67d8f3e/gr7.jpg gr7 gr7.jpg jpg 265035 517 691 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224025143-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr8/DOWNSAMPLED/image/jpeg/2cfd14635fcddb3281ff583dc98bbe42/gr8.jpg gr8 gr8.jpg jpg 188372 532 691 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224025143-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr5/DOWNSAMPLED/image/jpeg/b5cd68b9ad575bc26ad4a7ab5d4235f6/gr5.jpg gr5 gr5.jpg jpg 266944 530 691 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224025143-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr6/DOWNSAMPLED/image/jpeg/d6e413326f4cef507ae584efdfd68865/gr6.jpg gr6 gr6.jpg jpg 255971 514 691 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224025143-gr11.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr11/DOWNSAMPLED/image/jpeg/28f7273e47c052da378ea4051bb94f50/gr11.jpg gr11 gr11.jpg jpg 178522 378 535 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224025143-gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr10/DOWNSAMPLED/image/jpeg/5fc3d89791656f16f6d3b301c022f785/gr10.jpg gr10 gr10.jpg jpg 189534 363 535 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224025143-gr13.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr13/DOWNSAMPLED/image/jpeg/838a6e4ef23261729509cfe048dcea6a/gr13.jpg gr13 gr13.jpg jpg 166691 379 535 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224025143-gr12.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr12/DOWNSAMPLED/image/jpeg/c31b6e70ddce2e1c2c3c7890e7be2223/gr12.jpg gr12 gr12.jpg jpg 162389 359 535 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224025143-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr9/DOWNSAMPLED/image/jpeg/4ca4c9d8ee2c16361d2166814b6641a0/gr9.jpg gr9 gr9.jpg jpg 186574 374 535 IMAGE-DOWNSAMPLED 1-s2.0-S0360544224025143-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr3/THUMBNAIL/image/gif/ab6e828d9008e6a53ebd26d59c84d4ee/gr3.sml gr3 gr3.sml sml 70358 76 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224025143-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr4/THUMBNAIL/image/gif/68ce22cc619dd74d335a2d61d44e9417/gr4.sml gr4 gr4.sml sml 77855 161 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224025143-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr1/THUMBNAIL/image/gif/8c65ae7b38a2726a9d96431e225ed781/gr1.sml gr1 gr1.sml sml 14337 163 200 IMAGE-THUMBNAIL 1-s2.0-S0360544224025143-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr2/THUMBNAIL/image/gif/5193fa12333f944b016fd6526ffbe0bd/gr2.sml gr2 gr2.sml sml 13646 153 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224025143-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr7/THUMBNAIL/image/gif/1809148dac63ebec9760b980a164d341/gr7.sml gr7 gr7.sml sml 89492 164 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224025143-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr8/THUMBNAIL/image/gif/ccd3036c4b200aae86846fd15e96c3af/gr8.sml gr8 gr8.sml sml 79683 164 213 IMAGE-THUMBNAIL 1-s2.0-S0360544224025143-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr5/THUMBNAIL/image/gif/6163ce01746f5c52be16439652b0cc9f/gr5.sml gr5 gr5.sml sml 89314 164 214 IMAGE-THUMBNAIL 1-s2.0-S0360544224025143-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr6/THUMBNAIL/image/gif/ad7bc5a4c5cbe9291056a195e7678be2/gr6.sml gr6 gr6.sml sml 88976 163 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224025143-gr11.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr11/THUMBNAIL/image/gif/7a67d3979e1b32ed71bbda52bb0c08c1/gr11.sml gr11 gr11.sml sml 88373 155 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224025143-gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr10/THUMBNAIL/image/gif/9d85b297befbb43ce1638466021a23e9/gr10.sml gr10 gr10.sml sml 90012 148 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224025143-gr13.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/gr13/THUMBNAIL/image/gif/48cf5061e91a9ba23e3ccccc3d674a65/gr13.sml gr13 gr13.sml sml 84121 155 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224025143-gr12.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1362496221015970/gr12/THUMBNAIL/image/gif/9f5fc12bfdb796ab3c556a90d518ef34/gr12.sml gr12 gr12.sml sml 83633 147 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224025143-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1362496216012060/gr9/THUMBNAIL/image/gif/6c8586e3e7e175d6195bd72a70be15db/gr9.sml gr9 gr9.sml sml 89656 153 219 IMAGE-THUMBNAIL 1-s2.0-S0360544224025143-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/HIGHRES/image/jpeg/963c0979a43ffb7572693e3dcbe42b18/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 359994 1068 3058 IMAGE-HIGH-RES 1-s2.0-S0360544224025143-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/HIGHRES/image/jpeg/6ed1146352b69057e89e3ce4c71e5170/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 948266 2243 3059 IMAGE-HIGH-RES 1-s2.0-S0360544224025143-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/HIGHRES/image/jpeg/c5e6f885d46e8e37221bfffc6a263b6f/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 343431 1938 2371 IMAGE-HIGH-RES 1-s2.0-S0360544224025143-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/HIGHRES/image/jpeg/c167a261fc9a8bcfdf07abbcfc7bb6aa/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 207191 1200 1721 IMAGE-HIGH-RES 1-s2.0-S0360544224025143-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/HIGHRES/image/jpeg/67851cb65f5d86a66781f45c5f6f095f/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 1647616 2288 3059 IMAGE-HIGH-RES 1-s2.0-S0360544224025143-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/HIGHRES/image/jpeg/f913ca99c7a713664c406bf8cf4157e5/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 1021216 2357 3059 IMAGE-HIGH-RES 1-s2.0-S0360544224025143-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/HIGHRES/image/jpeg/4ddeee61f00c88947a7177d0547447a9/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 1725742 2345 3059 IMAGE-HIGH-RES 1-s2.0-S0360544224025143-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/HIGHRES/image/jpeg/fa4c5395c487f033b77f6d129fc81f61/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 1609194 2276 3059 IMAGE-HIGH-RES 1-s2.0-S0360544224025143-gr11_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/HIGHRES/image/jpeg/b23d124b72d61e4e1e75a831e26f238f/gr11_lrg.jpg gr11 gr11_lrg.jpg jpg 932359 1673 2370 IMAGE-HIGH-RES 1-s2.0-S0360544224025143-gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/HIGHRES/image/jpeg/80efb22461805c44482204520c920402/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 987536 1606 2370 IMAGE-HIGH-RES 1-s2.0-S0360544224025143-gr13_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1362496221015970/HIGHRES/image/jpeg/d41a9f98f0d80da06ff50f0012c50ae1/gr13_lrg.jpg gr13 gr13_lrg.jpg jpg 831256 1681 2370 IMAGE-HIGH-RES 1-s2.0-S0360544224025143-gr12_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1362496216017233/HIGHRES/image/jpeg/fa67350a38d18e5201527d9e5b1163bd/gr12_lrg.jpg gr12 gr12_lrg.jpg jpg 766590 1590 2370 IMAGE-HIGH-RES 1-s2.0-S0360544224025143-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1362496216017233/HIGHRES/image/jpeg/9dca687c4565968af0f30a018ff3afc2/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 1000687 1658 2369 IMAGE-HIGH-RES 1-s2.0-S0360544224025143-si7.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/image/svg+xml/626fad8923a0ba504b9d098a7155e023/si7.svg si7 si7.svg svg 133317 ALTIMG 1-s2.0-S0360544224025143-si8.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/image/svg+xml/679f6f8df34d51ed4463bc9b82da83db/si8.svg si8 si8.svg svg 13224 ALTIMG 1-s2.0-S0360544224025143-si6.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/image/svg+xml/f0894843a8c0d78a333d0e1e18fa21dd/si6.svg si6 si6.svg svg 102316 ALTIMG 1-s2.0-S0360544224025143-si1.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/image/svg+xml/85571268ee12c67ddb4ee4d37dfca3cd/si1.svg si1 si1.svg svg 79502 ALTIMG 1-s2.0-S0360544224025143-si5.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/image/svg+xml/7f860a037d5e5e1096f0887bf456c386/si5.svg si5 si5.svg svg 46252 ALTIMG 1-s2.0-S0360544224025143-si4.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/image/svg+xml/f08e424a9c9228808207e18c9797a7ef/si4.svg si4 si4.svg svg 84464 ALTIMG 1-s2.0-S0360544224025143-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/image/svg+xml/37ff41b455e0e5af2fecaa5f3ecb841b/si3.svg si3 si3.svg svg 81213 ALTIMG 1-s2.0-S0360544224025143-si2.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544224025143/image/svg+xml/10ed1a7731437b6eb61fbda9e69bc513/si2.svg si2 si2.svg svg 48392 ALTIMG 1-s2.0-S0360544224025143-am.pdf am am.pdf pdf 22755849 AAM-PDF https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:10NWCQ9L37Q/MAIN/application/pdf/d5b38464678e1cf17e66ed3a7bf0cf43/am.pdf EGY 132740 132740 S0360-5442(24)02514-3 10.1016/j.energy.2024.132740 Elsevier Ltd Fig. 1 Cooling water temperature balance calculation. Fig. 1 Fig. 2 The execution process of the value-based deep reinforcement learning algorithm. Fig. 2 Fig. 3 Q network structure and update process of DQN and D3QN algorithms. Fig. 3 Fig. 4 Prediction accuracy of the cooling-side system for different projects. Fig. 4 Fig. 5 Influence of Q network structure on the optimization results of Project 1. Fig. 5 Fig. 6 Influence of Q network structure on the optimization results of Project 2. Fig. 6 Fig. 7 Influence of Q network structure on the optimization results of Project 3. Fig. 7 Fig. 8 Evolution of five rounds. Fig. 8 Fig. 9 The optimization effects of the D3QN and DQN control algorithms on Project 1. Fig. 9 Fig. 10 The optimization effects of the D3QN and DQN control algorithms on Project 2. Fig. 10 Fig. 11 The optimization effects of the D3QN and DQN control algorithms on Project 3. Fig. 11 Fig. 12 The equipment frequency and cooling side COP of the D3QN and DQN control algorithms on Project 1. Fig. 12 Fig. 13 The Cooling water temperature and flow of the D3QN and DQN control algorithms on Project 1. Fig. 13 Table 1 Input parameters and output parameters of the device model. Table 1 device model Input output the chiller COP model Cooling load the chiller COP Chilled water supply temperature Cooling water return temperature Cooling water flow Cooling side performance model Number of cooling pumps Cooling water flow Cooling pump frequency Cooling pump energy consumption model Cooling pump frequency Cooling pump power Cooling water flow Cooling tower performance model Cooling tower inlet temperature Cooling tower outlet temperature Cooling tower fan frequency Outdoor wet bulb temperature Cooling tower energy consumption model Cooling tower fan frequency Cooling tower fan power Table 2 Common variables in the state space of existing research. Table 2 variable symbol Value range unit moment t hour [0,24] \u2013 Outdoor air temperature T out [-20,30] °C Solar radiation G hor [0,735] W/m2 Room temperature T in [10,30] °C Continuous on/off time of chiller t chiller [ 0 , + ∞ ) \u2013 Relative humidity of outdoor air RH [0,100] % Outdoor wind speed WS [ 0 , + ∞ ) m/s Outdoor wind direction WD [0,7] \u2013 Table 3 The state space used in this paper. Table 3 variable symbol Value range unit Outdoor wet bulb temperature T w [-20,40] °C System cooling load CL [ 0 , + ∞ ) kW Table 4 Different project action space Settings. Table 4 variable Value range Space size Project 1: a lithium battery production workshop Cooling tower frequency setting [30,50] 441 Cooling pump frequency setting [30,50] Item 2: A pharmaceutical factory Cooling tower frequency setting [30,50] 441 Cooling pump frequency setting [30,50] Item 3: A subway station Cooling water supply and return temperature difference [2,6] (0.5 °C) 45 Cooling tower approaching temperature [2,6] (1 °C) Table 5 List of hyper-parameters. Table 5 Number Hyper-parameter Value 1 basic batch size 256 2 Replay memory size 10000 3 Discount factor 0.91 4 Learning rate 0.0001 5 Initial exploration 0.9 6 Final exploration 0.1 7 Replay target frequency 1 8 Soft update rate 0.1 9 Soft update iteration 24 Table 6 The relevant information of the cooling-side equipment of HVAC systems in these three projects. Table 6 Equipment Number parameters Project 1: A lithium battery production workshop Chiller 8 Cooling capacity:1407 kW, Power: 1106 kW 2 Cooling capacity:2990 kW, Power:1775 kW Cooling water pump 10 Flow rate:1950 m3/h, Power:200 kW Cooling tower 22 Flow rate:1250 m3/h, Power:45 kW Project 2: A pharmaceutical factory Chiller 2 Cooling capacity:2304 kW, Power:402 kW 2 Cooling capacity:2280 kW, Power:374 kW Cooling water pump 4 Flow rate:360 m3/h, Power:45 kW Cooling tower 4 Flow rate:310 m3/h, Power:22 kW Project 3: A subway station Chiller 2 Cooling capacity:1059 kW, Power:159.7 kW Cooling water pump 2 Flow rate:240 m3/h, Power:14.7Kw Cooling tower 2 Flow rate:260 m3/h, Power:7.5Kw Table 7 Historical data for different projects. Table 7 projects Start and end dates of historical data average COP(−) energy consumption (kWh) Project 1 2022.8.1\u20132022.10.2 4.907 15,396,609 Project 2 2023.5.29\u20132023.7.11 5.485 602,452 Project 3 2019.6.27\u20132019.9.19 5.585 369,216 Table 8 The best optimization results for each project under different Q-network structures. Table 8 Projects Methods Number of layers of Q-network Optimal number of neurons Final reward Final consumption (kWh) Average COP project 1 DQN 1 12 5,346,146 13,027,376 5.193 2 64,12 5,373,212 13,015,913 5.197 3 12,32,160 5,383,259 13,003,085 5.201 D3QN 1 12 5,360,637 13,019,422 5.199 2 128,12 5,388,024 13,002,434 5.206 3 12,160,32 5,387,143 13,012,070 5.200 project 2 DQN 1 24 1,503,136 553010 5.898 2 32,24 1,562,624 551341 5.918 3 12,160,384 1,575,154 551559 5.922 D3QN 1 12 1,546,009 552318 5.906 2 24,160 1,574,531 551053 5.921 3 64,256,256 1,577,973 550132 5.926 project 3 DQN 1 12 1,716,385 389809 5.312 2 12,64 1,750,591 389753 5.498 3 24,32,24 1,789,944 369188 5.591 D3QN 1 128 1,721,433 382445 5.573 2 24,128 1,782,004 382369 5.575 3 128,128,12 1,745,160 382582 5.574 Table 9 Combination of the top five neuron counts with the highest gain (when the number of hidden layers is 2). Table 9 project 1 project 2 project 3 number Q Network number Q Network number Q Network DQN 1 64_12 1 32_256 1 128_384 2 32_12 2 160_384 2 128_160 3 160_12 3 64_12 3 160_12 4 128_12 4 64_160 4 64_12 5 24_12 5 12_64 5 512_32 D3QN 1 128_12 1 512_160 1 24_64 2 64_12 2 24_160 2 256_32 3 12_12 3 64_64 3 384_12 4 12_24 4 64_12 4 128_12 5 128_64 5 12_12 5 64_12 Table 10 Comparative table of rewards for DQN and D3QN algorithms over five rounds of training. Table 10 Project 1 DQN Project 1 D3QN Project 2 DQN Project 2 D3QN Project 3 DQN Project 3 D3QN Round 1 5,385,256 5,781,891 1,960,475 2,071,858 3,092,967 3,175,635 Round 2 5,421,813 5,781,187 2,074,635 2,094,332 3,098,670 3,162,726 Round 3 5,432,927 5,805,317 2,017,960 2,068,226 3,124,781 3,114,879 Round 4 5,412,020 5,791,357 2,054,407 2,083,819 3,071,262 3,128,822 Round 5 5,463,477 5,799,357 2,030,098 2,056,413 3,134,558 3,131,162 Average 5,423,099 5,791,822 2,027,515 2,074,930 3,104,448 3,142,645 Standard deviation 25626 9496 38822 13059 22744 22740 A comparative study of DQN and D3QN for HVAC system optimization control Haosen Qin Writing \u2013 original draft Software Methodology Formal analysis Conceptualization a Tao Meng Writing \u2013 review & editing Validation Conceptualization b Kan Chen Writing \u2013 review & editing Methodology Conceptualization b Zhengwei Li Writing \u2013 review & editing Supervision Methodology Conceptualization a \u204e a School of Mechanical Engineering, Tongji University, Shanghai, China School of Mechanical Engineering Tongji University Shanghai China School of Mechanical Engineering, Tongji University, Shanghai, China b Midea Building Technologies, Shanghai, China Midea Building Technologies Shanghai China Midea Building Technologies, Shanghai, China \u204e Corresponding author. Handling editor: X Zhao Ensuring the optimal performance of Heating, Ventilation, and Air Conditioning (HVAC) systems is paramount for achieving energy efficiency. This paper investigates the application of deep reinforcement learning algorithms in HVAC system control, aiming to identify the most suitable Q-network structure for optimizing HVAC systems and comparing the performance of Deep Q-learning (DQN) and Double Dueling Deep Q-learning (D3QN) algorithms. Initially, this paper evaluates and analyses existing literature to perform a normalization treatment on the state space. Through systematic simulation and rigorous data analysis, the impact of the Q-network structure on the efficacy of the DQN and D3QN algorithms is evaluated, resulting in the proposal of specific values for the Q-network structures within these two algorithms. Subsequently, comparisons are drawn on the optimization effectiveness, stability and reliability of these algorithms across diverse engineering projects. Results highlight the superiority of the D3QN algorithm over the DQN algorithm regarding both optimization effectiveness and stability across all evaluated projects. The proposed efficient Q-network structure comprises two hidden layers, with 64 and 12 neurons respectively in each layer. The findings of this paper are crucial in providing insights for HVAC systems control optimization using reinforcement learning and pave the way for advanced research and applications in the future. Data availability The authors do not have permission to share data. 1 Introduction Heating, Ventilation, and Air Conditioning (HVAC) systems account for a significant amount of global energy consumption and carbon dioxide emissions. For this reason, enhancing the energy efficiency of HVAC systems is a critical strategy for reducing energy usage and mitigating environmental impacts. One effective approach is the application of Deep Reinforcement Learning (DRL) algorithms to optimize control parameters of cooling units within these systems. Nevertheless, existing research has insufficient focus on the structure of the Q-network and its potential to impact the effectiveness of DRL. Addressing this gap, this paper presents a methodical examination of the effect of the Q-network structure on the performance of two prominent DRL algorithms: Deep Q-learning (DQN) and Double Dueling Deep Q-learning (D3QN). Moreover, this paper offers a comprehensive evaluation of the optimization capability, stability, and reliability of these DRL procedures within a simulation environment, constructed using real-world data streams. This analytical approach provides valuable insights into effective HVAC control optimizations. 1.1 Optimal control of HVAC systems The purpose of HVAC system's optimal control is to identify the most effective control actions that balance minimized energy usage, comfortable indoor thermal conditions, and occupant comfort. This optimization process contemplates a multitude of factors such as external climatic conditions, thermal load requirements, and equipment characteristics, among others. Achieving this balance between energy efficiency and comfort, mainly through the execution of advanced control algorithms and strategies, leads to significant energy savings and improved system operation. Existing optimization control techniques can be broadly categorized into four distinct methods: performance map-based methods, rule-based methods, model-based methods, and model-free methods [1]. Performance map-based and rule-based control strategies fall under the category of offline optimization methods. However, these two techniques possess inherent limitations. They are heavily reliant on predefined rules and logic, which may decrease their adaptability to ever-changing, dynamic conditions. As a result, they can struggle to respond accurately to these alterations, which may lead to reduced reliability and potential inaccuracies. Additionally, with the increase in the number of optimization components, the complexity of operating modes and states heightens. This complexity can create difficulties in considering all rules of each component under various conditions, thus complicating the pursuit of global system optimization. A different approach to control methodology is model-based control, which uses mathematical models to predict system behavior and optimize control actions accordingly [2\u20134]. Model-based optimization has attracted significant interest in the field of HVAC systems. Its real-time optimization process enables efficient energy management and improved comfort control. Despite the promising benefits, model-based control is not immune to limitations within the HVAC context [5\u20137]. One such hindrance is the mathematical models' accuracy, which could fail to fully emulate the complexities and uncertainties of actual systems\u2014resulting in divergence between the model's predictions and the reality of system behavior. Furthermore, the successful implementation of real-time model-based control requires extensive computational resources, including advanced hardware and sophisticated computational algorithms. Another significant dependence in model-based control is on the availability of accurate system data for model calibration and validation. Accurate data capture can present challenges due to sensor limitations, measurement errors, and system variations. On the other hand, recent advancements have brought forward model-free control methods for optimizing decision-making processes in HVAC systems [8\u201310]. These methods do not necessitate explicit mathematical models of system dynamics, rather they learn control policies directly from available data. Among these is the increasingly popular DRL. Capitalizing on the capabilities of neural networks, DRL can tackle complex data sets and learn highly-effective control policies that can address various scenarios. These policies enhance system energy efficiency while ensuring occupant comfort [11]. Solinas et al. demonstrated that direct online reinforcement learning methods can avoid costly simulations, providing a reliable and cost-effective solution for HVAC optimization [12]. But notably, the application of DRL in the HVAC industry is still in the experimental and research stage. Most of the recent studies over the past three years (2022\u20132024) have focused on simulations, lacking sufficient real-world case studies for support [13\u201316]. However, with continued research and development, DRL holds great potential to significantly enhance the performance and efficiency of HVAC systems, while maintaining a comfortable interior atmosphere. 1.2 Reinforcement learning applied to HVAC Model-free control methods, such as reinforcement learning (RL), have recently attracted substantial interest in addressing multifaceted sequential problems [17]. Their appeal stems from their ability to iteratively evolve and polish control strategies via algorithm-environment interactions and trial-and-error learning, thereby eliminating the need for model accuracy. Q-learning, an RL algorithm variant, uses a Q-table to record reward evaluations for every decision. However, within HVAC systems with a multitude of sensors and apparatus, the vast state space makes establishing a detailed Q-table via the Q-learning algorithm impractical. To maneuver this issue, Mnih et al. [18] introduced DQN, which conveniently replaces the Q-table with a neural network that takes in state information as input and outputs estimated returns for each decision. To heighten the stability and convergence speed of DQN, new iterations like Double Deep Q-learning (DDQN) and Dueling Deep Q-learning (Dueling DQN) have been put forth. A method incorporating DDQN and Dueling DQN known as D3QN, has demonstrated promising optimization results in fields like vehicle control and robotics [19]. The aforementioned DQN and D3QN both output discrete action spaces. More sophisticated RL algorithms, such as Actor-Critic (AC) and Deep Deterministic Policy Gradient (DDPG), are capable of outputting continuous action spaces [20,21]. However, given that most HVAC control instructions are inherently discrete (such as pump frequency and chiller outlet water temperature), and considering the early stage of RL application in the HVAC field requiring more data and real-case studies for support, the most pressing task is to develop an RL control method of moderate complexity. This method should have low hardware requirements, and not rely heavily on hyperparameter tuning, enabling rapid deployment across various building environments. This approach helps facilitate the transition of RL algorithms from theoretical research towards real-world application and provides an effective and controllable benchmark for evaluating the performance of RL in HVAC system control. Thus, this research focuses on the application of DQN and D3QN algorithms in the HVAC field. While several researchers have already applied DQN and D3QN to HVAC systems through simulation and other means, they have dedicated little attention to exploring how different Q-network architectures can impact algorithm performance in this context. For example, Fang et al. [22], utilized the DQN algorithm to govern various HVAC elements like chillers, and fans among others, with their system operating on a six-dimensional state space and a Q-network structure with two hidden layers each containing 512 nodes. Conversely, Deng et al. [23] applied a state space of fifteen dimensions and a Q-network structure with four hidden layers, each containing 512 nodes. Fu et al. [24] employed a Q-network with two hidden layers, each layer containing 64 neurons. Ren et al. [25] exploited a six-dimensional state space and a Q-network with two hidden layers with the first hidden layer comprising 32 nodes and 16 nodes for the second hidden layer. The inconsistency in state space and Q-network structures among these studies poses challenges for applying reinforcement learning in the HVAC field. Therefore, there is a need for a systematic investigation of the state space and Q-network structure to identify optimal parameter values that can enhance performance in different types of buildings. DQN's efficacy in HVAC control has been widely studied, yet it suffers from overestimations during Q-value function learning. D3QN serves as an improvement in this respect, dividing Q-value function learning into two stages: one neural network (the 'current network') is employed for action selection, and another distinct neural network (the 'target network') is utilized for action evaluation. By applying this dual Q-network technique, D3QN effectively alleviates the instability brought about by overestimations. Moreover, D3QN's Q-network structure has been designed in two separate segments: one segment learns the state-value function while the other learns the advantage function. This design enables the Q-network to better apprehend the advantage of each action, thereby estimating the action-value function more accurately. These improvements demonstrate that D3QN exhibits superior performance in handling complex tasks with large state spaces. However, in the context of HVAC system optimization, the size of the state space is relatively smaller compared to tasks such as natural language processing or image recognition [26,27]. Consequently, whether these improvements made by D3QN to amend DQN's inherent flaws can be effectively expressed, calls for further validation and research. 1.3 Motivation and research structure As previously highlighted, the inconsistent nature of state spaces and Q-network structures in current studies presents a substantial obstacle when incorporating reinforcement learning into the HVAC sector. Notably, many HVAC systems lack appropriate data dimensions, such as cooling water flow or chilled water flow, making modeling and fine-tuning Q-network structures for specific projects difficult. In response to this, an imperative need exists to identify a sub-optimal combination of Q-network structures that consistently provide reasonable optimization across distinct HVAC projects, thereby enhancing the practical applicability for engineering practices. Underpinning the unique traits of reinforcement learning, where the neural network is leveraged instantly during training, the breadth and depth of the said network ought not to be overly extensive. This notion contrasts with the conventional understanding in the realm of deep learning. Consequently, instead of complicating the Q-network excessively, streamlining the network capacity while preserving some degree of redundancy often aligns more appropriately with the real-world demands of the HVAC systems. Moreover, while extensive exploration of the D3QN algorithm has been carried out in large-scale models such as automatic control and image classification, its deployment in the HVAC sector, which usually engages with smaller parameter scales, stands to be unique. The D3QN utilizes dual Q-networks and separates the final hidden layer of the Q-network into value function and advantage function components, thereby bolstering the algorithm's stability and convergence speed. Nevertheless, as compared to DQN, these enhancements complicate the algorithm further \u2014 the question of whether D3QN can maintain its efficiency in the HVAC field, a field characterized by fewer parameters, warrants investigation. Moreover, in the context of a more complex Q-network, determining whether a satisfactory optimization result can be achieved across different projects with a certain set of Q-network parameters is worth exploring. Given the issues described above, this research aims to accomplish the following specific objectives. (1) Investigate and unify the state space settings in the existing literature specifically for water-cooled chiller system cooling side optimization. (2) Investigate the impact of Q-network structure on the performance of DQN and D3QN algorithms, and provide recommended values for the Q-network structures of these two algorithms. (3) Analyze the randomness and stability of the DQN and D3QN algorithms in the training and deployment of HVAC systems under the recommended Q-network setting values. (4) Examine the control performance of the DQN and D3QN algorithms after full convergence, specifically their ability to enhance the system's Coefficient of Performance (COP). By achieving these objectives, this paper will contribute to a better understanding of the optimal control of HVAC systems using reinforcement learning algorithms, and provide valuable insights for improving their performance in this domain. 2 Methodology 2.1 Deep reinforcement learning algorithms Reinforcement Learning (RL) is a dynamic process that fosters the steady development and refinement of control strategies through trial-and-error interactions between an agent and its environment, making it particularly suitable for addressing continuous multi-step decision-making problems under conditions of uncertainty. The RL agent and environment interaction can be formalized as a Markov Decision Process (MDP). This process is characterized by the four-element tuple: <S, A, P, R>, where S represents the state, A is the action, P signifies the state-transition probability, and R represents the reward. Unlike Model Predictive Control (MPC), which relies on a known state transition probability matrix, reinforcement learning necessitates the agent's ability to update an unknown state transition probability matrix through continuous trial and error. Deep Reinforcement Learning (DRL), a fusion of Reinforcement Learning (RL) and Deep Learning (DL), is capable of handling problems with high-dimensional and continuous state spaces. DRL comprises two primary types of algorithms: value-based methods [28\u201333] and policy-based methods [8,10,34\u201336]. Value-based methods, such as Q-learning, primarily operate by learning a value function to select the optimal action, making them typically applicable to problems with discrete action spaces. Each possible action has a predicted Q-value (the expected return over a future period of time) in these methods, and the action with the highest predicted Q-value is selected. Despite their high stability, value-based methods may encounter difficulties when dealing with problems with continuous action spaces. Conversely, policy-based methods, such as Actor-Critic, optimize directly in the policy space. These methods can handle problems with continuous action spaces but require maintaining both the action network and the value network simultaneously, resulting in higher computational costs. In HVAC system control, where parameters like cooling tower frequency are essentially discrete, value-based methods are appropriate. Q-learning, a commonly used value-based method, can effectively solve these types of problems by iteratively updating the Q-values and selecting the optimal action based on these Q-values, thereby achieving control of the environment. Q-learning is a benchmark algorithm in reinforcement learning, with widespread applications in various fields. It stores the computed Q-values in a table (Q-table), where both the state and action are discrete values. To handle continuous state spaces, DQN replaces the Q-table with a deep neural network, known as the Q-network. However, in the DQN algorithm, a strong correlation exists between the Q-network's output values and its own updates, creating a hindrance for the algorithm's convergence. The DDQN algorithm addresses this issue by employing two identical Q-networks [37]. The Dueling-DQN algorithm follows a similar process to the DQN algorithm, differing only in the introduction of a value function and an advantage function between the hidden and output layers to accelerate the convergence speed [38]. The D3QN algorithm, which combines the principles of DDQN and Dueling-DQN, further augment the algorithm's performance. 2.2 Data-driven cooling side system model The creation of the air conditioning cooling side simulation environment necessitates equipment models, including models for chiller COP, energy consumption and performance of the cooling pump, as well as models for the energy consumption and performance of the cooling tower. The input and output parameters for these equipment models are detailed in Table 1 .Given the variations in data characteristics and dataset sizes for different devices, the process of development equipment models requires initial comparison of different modeling methods based on operational data from actual scenarios, with the most suitable method being selected for equipment model construction. In this paper, various techniques such as multivariate polynomial regression (MPR) [39], artificial neural networks (ANN) [40], support vector machines (SVR) [41], and XGBoost [42] are investigated as potential modeling methods. The model's accuracy and training efficiency are profoundly influenced by hyperparameter configuration. To minimize human intervention, hyperparameter selection is automated through algorithms. For the MPR model, regression models of degrees 1\u20135 are assessed, and the most accurate one is selected by the algorithm. The Best Network after Multiple Iterations (BNMI) algorithm determines the optimum parameters for the ANN [43]. Grid search algorithm optimizes the hyperparameters of SVR and XGBoost [44], with the specific hyperparameter range for each model detailed in Appendix A. Once the most appropriate equipment models are identified, corresponding boundary conditions need to be set in accordance with actual circumstances, such as equipment operating status, environmental temperature, and the flow rate of the cooling water. The simulation of the cooling side is then conducted by balancing the cooling water flow rate and heat exchange, and integrating the constructed equipment models. In this process, the supply and return water temperatures of the cooling water are solved iteratively through the COP model of the chiller unit and the performance model of the cooling tower. The precise iterative process is illustrated in Fig. 1 . 2.3 Model-free controller diagram Fig. 2 describes the execution process of the deep reinforcement learning algorithm. This algorithm employs an ε-greedy policy to select actions, interacts with the environment, and receives feedback. The interaction data is stored in an experience replay buffer, and mini-batch sampling is used to train the Q-network. The Q-values are updated using the Bellman equation. The interaction environment is the cooling side simulation environment established before. The experience replay library aims to help the Q-network better remember historical data. The Q-network part follows the general process of the DQN algorithm. The application of the ε-greedy strategy helps to find the optimal balance point between exploration and exploitation, thereby accelerating the convergence of the algorithm. The core of the reinforcement learning algorithm lies in updating the Q-values during this process. Fig. 3 displays the Q-network structures and their update processes for DQN and D3QN. DQN does not have a Q-table to save the current results of all Q-values. Hence, DQN uses the experience replay technique to store the rewards and state updates obtained from each interaction with the environment. DQN generally learns Q-values by minimizing the loss function in Eq. (1). D3QN introduces a dueling architecture that decomposes the Q-value into a value function and an advantage function, as shown in Eq. (2). This architecture enables the model to learn the value of each state and the advantage of executing different actions under that state separately. D3QN also incorporates double Q-learning, which uses two networks (one for selecting actions, the other for evaluating those actions) to reduce the problem of overestimating Q-values [39]. D3QN typically learns Q-values by minimizing the loss function in Eq. (3). (1) L o s s = ( r t + γ max a t + 1 ∈ A Q t + 1 − Q t ) 2 Where, Loss represents the loss function, r denotes the reward value, γ is the discount rate, a symbolizes the action, Q stands for Q-value, and the subscript t represents the time step. (2) Q ( S , A ; w , α , β ) = V ( S ; w , α ) + A ( S , A ; w , β ) Where, ω represents the network parameters of the common part, α denotes network parameters of the value function part, and β symbolizes the network parameters of the advantage function part. (3) L o s s = ( r t + γ max a t + 1 ∈ A Q t + 1 \u2032 − Q t ) 2 Where, Q represents the calculation result of the current Q-network; Q\u2032 denotes the calculation result of the target Q-network. 2.4 Configuration and initialization of two Q-networks (1) State Space The definition of the state space significantly influences the configuration of the Q-network structure. As such, an analysis of the state space in current research along with normalization processing needs to be conducted before exploring the Q-network structure. Table 2 lists common variables in the state space from existing research, mainly concerning building load [45\u201348]. In light of this, the state in this study is defined as the following: 1) Outdoor wet bulb temperature (T w), which has a significant impact on the system's heat dissipation performance; 2) System cooling load (CL), which has a strong correlation with most variables in existing research. The state space used in this paper is shown in Table 3 . (2) Action Space The action space should be set according to the specific controllable parameters of the project. Therefore, the setting of the action space varies for each project, as shown in Table 4 . (3) Reward Function The application of reinforcement learning algorithms to HVAC systems has a significant characteristic: it directly uses the system's COP as the optimization target, aiming to maximize the system's COP. To further accelerate the convergence speed of the algorithm, referencing principles proposed in relevant literature [49], this study defines the cost function in the form of a piecewise function: (4) r ( s , a ) = { 1000 + C O P * 100 C O P ≥ 6 700 + C O P * 10 5.5 ≤ C O P ≤ 6 300 + C O P * 10 5 ≤ C O P ≤ 6 − 500 + C O P C O P ≤ 5 (4) Decision-making This paper adopts an improved ε-greedy strategy. This strategy tends to perform more exploration during the initial deployment phase of the algorithm, aiming to update the Q-values of different actions. As time progresses, the strategy will gradually transition towards a stable operating state. The ε updating process shown in eq. (5) reflects the operational mechanism of this strategy: (5) ε t + 1 = ε t * α i f ε t > ε min Where, ε t represents the current ε value, ε t+1 represents the updated ε value, ε min represents the minimum value of ε, and α represents the decay factor of ε. In this paper, α is set to 0.9995, and ε min is set to 0.2. (5) hyper-parameters Although further fine-tuning of hyperparameters could potentially yield greater benefits for the algorithm, it's an undoubtedly laborious and resource-intensive task. More importantly, in real-world building environments, the feasibility of performing hyperparameter search is markedly reduced. Therefore, even in the presence of suboptimal hyperparameters, the algorithm should maintain effective learning for task. The hyper-parameters used in this paper are shown in Table 5 . 3 Case study 3.1 Case system Reinforcement learning's application in HVAC is still in its early phase, with few projects and limited data available. Therefore, the existing data is insufficient to comprehensively cover all usage scenarios of HVAC systems. Rather, the study should focus on extracting features from various scenarios instead of confining the research to specific instances. The extracted features should represent the HVAC system's significant attributes and operational conditions which contribute to algorithm performance enhancement. As such, features can be extracted from the following facets: Environmental attributes: Climate factors such as location, season, and weather could influence the HVAC system's energy efficiency. Thus, these environmental parameters could serve as valuable extracted features. Purpose characteristics: The building types where HVAC systems are employed, e.g., residential buildings, commercial high-rises, factories, and the building's specific design, e.g., size, structure, materials, might affect its energy efficiency. User demand attributes: Different users might have varying preferences for HVAC systems' temperature, humidity, etc. User frequency, timing, and preference could be another feature category. Equipment properties: Variance in types, specifications, and operational states of HVAC equipment such as fans, condensers, evaporators could also be included as features. In light of these aspects, this study establishes that the 'building load' adequately represents the critical characteristics and operational conditions of HVAC systems, capturing all the four aspects mentioned above. To allow a diverse and complex representation of HVAC system control scenarios, this study uses three distinct types and scales of projects: a lithium battery production workshop, a pharmaceutical factory, and a subway station. Each project has its unique purposes and load characteristics, providing a comprehensive overview of different usage instances. Table 6 presents the relevant information of the cooling-side equipment of HVAC systems in these three projects. 3.2 Model validation Table 7 displays the start and end dates of the historical operation data for each projects used to build the cooling side system model, as well as the average COP and cumulative energy consumption data of the HVAC system during this period. The cooling-side system model was developed based on the historical operation data and implemented on the Python 3.8. The inputs of the model are listed in Table 1, and the outputs are the system COP and energy consumption. The predictive coefficient of determination (R2, Eq. (6)) and mean squared error (MSE, Eq. (7)) are used as error metrics to evaluate the accuracy of the equipment model [50]. The scatter plots in Fig. 4 (a)\u20134(f) illustrate the actual versus predicted values, with diagonal lines representing perfect predictions and additional lines indicating ±15 % and ±30 % error margins. Data points closer to the diagonal line signify higher prediction accuracy. Specifically: Fig. 4(a) and (b) present the prediction accuracy for the system COP and energy consumption for Project 1. Fig. 4(c) and (d) combine these metrics for Project 2. Fig. 4(e) and (f) do the same for Project 3. The figures and flow charts clearly demonstrate the model's prediction accuracy by showing how closely the sample points align with the diagonal line. (6) R 2 = ∑ i = 1 n [ y predicted − y \u203e observed ] 2 ∑ i = 1 n [ y observed − y \u203e observed ] 2 i = 1 , 2 , \u2026 , n (7) M S E = ∑ i = 1 n [ y observed − y predicted ] 2 n , ∈ [ 0 , + ∞ ) , i = 1 , 2 , \u2026 , n 3.3 Experimental design The purpose of this paper is to compare the efficiency, stability, and reliability of DQN and D3QN algorithms in optimizing HVAC system control across various projects. Additionally, it seeks to investigate the impact of the Q-network structure on algorithm performance. This paper selects three distinctive projects as the subjects of experimentation, namely Project 1 (a lithium battery production workshop), Project 2 a pharmaceutical factory), and Project 3 (a subway station). For precision, it's pivotal to declare that the control methodology introduced in this study only controls the frequency of the cooling water pump and the cooling tower to optimize the system's COP. The start-stop of the cooling water pump directly corresponds with that of the chiller, and the number of cooling towers follows the system's original control strategy. This focus provides a streamlined and manageable process while still offering significant results and insights. Although reinforcement learning algorithms typically require a long training period to achieve optimal results, real-world applications often only have limited training time available. Therefore, after initializing the Q-network randomly and deploying the algorithm in the simulation environment, this paper immediately begins quantifying the optimization results. Training and testing are conducted in parallel without pre-training the algorithm, to ensure consistency with the actual algorithm deployment environment. This paper conducts three sets of experiments to assess the influence of the Q-network structure, the impact of training randomness, and the optimization performance of the two algorithms. Initially, the Q-network structure, encompassing the number of hidden layers and the quantity of neurons per layer, was employed as an experimental variable. The performance of the algorithms under different combinations of Q-network structures was then analyzed. Subsequently, each algorithm underwent five rounds of independent, iterative training to explore the impact of training randomness on optimization results. Finally, to better appraise the HVAC system control performance of the various algorithms following comprehensive convergence, Project 1 was used as the test case. When using project 1 to test algorithm performance, the DQN and D3QN algorithms were subjected to a sufficient length of training to ensure their strategies reached comprehensive convergence. 4 Results and discussion 4.1 The impact of the Q-network structure The Q-network structure significantly influences the performance and efficacy of the reinforcement learning algorithm. The structure includes factors such as the number of hidden layers, the number of neurons per hidden layer, and the activation functions implemented. The number of hidden layers in a Q-network can influence the model's capacity to understand complex relationships between states and actions. Increasing layers can enhance the model's representation power but can also make training more challenging, especially with limited data. Experimentation with different numbers of hidden layers and analyzing their impact on the optimization performance is essential. Similarly, the number of neurons in each layer affects the model's ability to capture and represent useful information. Over fitting may result from having too many neurons, while under fitting may occur when there aren't enough neurons. The optimal number of neurons, therefore, depends on the balance between model complexity and generalization ability. A network larger in size may offer more capacity for learning complex representations but may also be prone to overfitting and unstable training. Therefore, finding the optimal network architecture is of utmost importance. Figs. 5\u20137 illustrate how changes in a Q-network's structure can impact the optimization results of the DQN and D3QN algorithms across three separate projects. The figures also demonstrate that as the Q-network increases in complexity, significant effort and careful configuration are required to achieve optimal results. The horizontal axis represents the number of hidden layers in the Q-network and the number of neurons in each hidden layer (for example, \"12_12_12\u2033 indicates a Q-network with three hidden layers and each hidden layer containing 12 neurons). The analyses show that when the number of hidden layers is constant, both DQN and D3QN algorithms exhibit similar optimization trends that are not linearly dependent on the number of neurons. When the Q-network has only one hidden layer, an increase in the number of neurons tends to cause a decrease and fluctuation in the optimization performance. When the Q-network consists of two hidden layers, an increase in neurons within the second hidden layer results in a decreased optimization performance, while the neuron count in the first hidden layer has a less profound impact on the optimization outcome. This trend becomes more pronounced when the Q-network incorporates three hidden layers, where the optimization performance peaks when the last two hidden layers have a relatively lower number of neurons. The optimal performance of the DQN algorithm improves as the Q-network increases its hidden layers. However, such increases also result in greater fluctuations in the optimization results and demand more effort to determine the appropriate combination of neuron numbers. Improper combinations can significantly diminish the optimization performance. On the other hand, the optimization performance of the D3QN algorithm can attain an optimal state when the Q-network holds only two hidden layers. Further increase in the number of hidden layers does not appreciably enhance its performance, and, in fact, it could potentially decrease. Table 8 shows the optimal combination of neuron numbers and the final optimization results for the Q-network under different numbers of hidden layers. On one hand, the optimization performance of the D3QN algorithm can reach an optimal state even when the Q-network has two hidden layers. On the other hand, even with the optimal combination of neuron numbers, the DQN algorithm using a Q-network with three hidden layers can only achieve limited improvements. The aforementioned analysis suggests that excessively large or deep neural networks are unfit for optimizing and controlling HVAC systems using reinforcement learning. This finding primarily results from two reasons. Firstly, deep learning models undergo a full-fledged training process before being put to use, whereas reinforcement learning models require immediate application for decision-making, even after just a few seconds of training. This means that shallow network structures are necessary for reinforcement learning to achieve fast convergence. Secondly, reinforcement learning training data is relatively unstable compared to supervised learning training data. Unlike supervised learning, reinforcement learning cannot segregate the data into distinct training and testing sets to mitigate overfitting. This makes it crucial for deep reinforcement learning to refrain from using overly complex or wide network structures to avoid overfitting. Consequently, this paper recommends the use of two-hidden-layer structures while employing DQN or D3QN algorithms for HVAC system optimization. Table 9 shows the top five sets of neuron number combinations with the highest benefits for each project when there are two hidden layers. The combination of (64,12) consistently yielded high benefits across multiple projects, thus it is recommended to use the (64,12) Q-network structure. 4.2 Training randomness and deployment stability Training randomness refers to the inherent unpredictability within the reinforcement learning algorithm during the training process, as depicted in Eq. (5). The exploration process of reinforcement learning, which involves learning from interactions with the environment, often contains a random element to encourage the algorithm to explore various actions and states. Deployment stability, conversely, pertains to the reliability and stability of the trained model when applied in a real-world environment. It evaluates whether the learned policies can consistently perform well and make dependable decisions across different scenarios. Both training randomness and deployment stability are crucial factors to consider in reinforcement learning. In this paper, each algorithm undergoes five rounds of independent, iterative training to examine the randomness inherent in different algorithms and the effects of this randomness on optimization results. Each round consists of 20 episodes, aimed at exploring the long-term deployment capability and stability of the algorithm. Fig. 8 provides the results of this process. Fig. 8a, b, and 8c display the computational results of the D3QN algorithm in projects 1, 2, and 3, respectively, while Fig. 8d, e, and 8f illustrate the computational results of the DQN algorithm in the same respective projects. Table 10 lists the standard deviation and average of the rewards obtained after these five training rounds. In all instances, D3QN's average reward surpasses that of DQN, indicating D3QN's superior optimization performance in these HVAC projects. Additionally, the standard deviation calculations suggest a higher D3QN stability, with smaller standard deviations indicating lower volatility during training. Comparing the coefficients of variation (standard deviation divided by the average), it is found that D3QN's coefficient of variation is lower than DQN's in all scenarios, which also highlights D3QN's advantages in terms of optimization performance and stability. In conclusion, these data suggest the superiority of the D3QN algorithm over the DQN algorithm in terms of average rewards, stability, and reliability, for all considered cases. Project 3 has the least complex system, with its HVAC system comprising only two chillers, and its action space dimension being 45 (a combination of the chiller's cooling water outlet temperature and the cooling tower's approach temperature). In contrast, both Projects 2 and 3 have action space dimensions that increase to 441 (a combination of the cooling tower frequency and cooling water pump frequency) but consist of 4 and 10 chillers, respectively. Thus, Project 3's system complexity is considered the highest among the cases. A more detailed analysis of the specific projects in Fig. 8 and Table 9 reveals that the relative advantage of D3QN over DQN is more significant in Project 2 than in Project 1, and likewise in Project 3 than in Project 2. This suggests that as the number of controlled system devices and the parameters being controlled increase, the advantages of D3QN over DQN in terms of optimization results, stability, and reliability become increasingly pronounced. 4.3 Control performance after full convergence The control performance after full convergence refers to the ability of a reinforcement learning algorithm to effectively control an HVAC system once the algorithm has fully learned and optimized its policy. Upon full convergence, the algorithm should consistently make optimal or near-optimal decisions for HVAC system control. Performance can be evaluated using various metrics such as total energy consumption, average indoor temperature and humidity, comfort complaints, and other performance indicators. Understanding the control performance after full convergence is important for assessing reinforcement learning algorithms' efficacy and reliability in controlling HVAC systems. Figs. 9\u201311 now illustrate the optimization effects of the D3QN and DQN algorithms on project 1, project 2, and project 3, respectively. As noted earlier, the complexity of the air conditioning system decreases from project 1 to project 3, impacting the optimization results and the comparative performance of D3QN and DQN. The analysis reveals a gradual reduction in the advantage of D3QN over DQN in terms of optimization results, stability, and reliability as the system complexity decreases across the three projects. Specifically, the average COP under the original strategy for projects 1, 2, and 3 is 4.90, 5.48, and 5.58, respectively. For project 1, the average COP under D3QN and DQN control is 5.30 and 5.25, respectively, corresponding to energy savings rates of 17.28 % and 16.29 % compared to the original control strategy. For project 2, the average COP under D3QN and DQN control is 6.28 and 6.22, respectively, corresponding to energy savings rates of 13.83 % and 12.91 %. For project 3, the average COP under D3QN and DQN control is 5.70 and 5.69, with energy savings rates of 3.45 % and 3.1 %, respectively, compared to the original control strategy. Due to word limitations, this paper focuses on a continuous seven-day dataset from Project 1, selected for its ability to best highlight the differences between the D3QN and DQN control strategies, and to illustrate the optimization effects of these algorithms upon full convergence. Project 1 involves a large-scale pharmaceutical factory with a high base level energy consumption and relatively low sensitivity to indoor temperature fluctuations. Therefore, the COP is used to evaluate control performance. The original control strategy for Project 1 sets the cooling tower outlet temperature based on the approach temperature, with the cooling water pump maintaining a 6 °C difference between supply and return water temperatures. Fig. 12 provides a seven-day continuous view of the control effect for Project 1. Specifically, Fig. 12a reflects hourly outdoor wet-bulb temperature and system cooling load. Figs. 12b and c shows the hourly set values of the cooling tower and cooling water pump frequency for both D3QN and DQN algorithms. Fig. 12d displays the system's COP for three control methods: manual control, D3QN, and DQN. Fig. 10 provides detailed insights into the operation metrics of the cooling side using the D3QN and DQN control strategies. Fig. 13 a shows the chiller cooling water inlet temperature. Fig. 13b presents the temperature difference of the chiller cooling water (supply and return). Fig. 13c illustrates the cooling water flow rate, and finally, Fig. 13d highlights the achieved cooling side heat transfer. Figs. 12 and 13 illustrate continuous data over seven days, encompassing both stable and unstable load conditions. The first five days represent a stable load condition, with a minimum load of 50,764 kW and a maximum load of 53,722 kW, and a standard deviation of 822 kW. The last two days shift to an unstable load condition, where the minimum load drops to 38,846 kW and the maximum load increases to 60,456 kW, with a significant rise in standard deviation to 3249 kW, indicating greatly increased load variability. The D3QN control strategy exhibits significant advantages in optimizing equipment performance and reducing energy consumption. Under the D3QN strategy, the chiller cooling water inlet temperature is maintained between 29.0 °C and 32.1 °C, with an average of 31.0 °C and a standard deviation of 0.67 °C, while the average temperature difference between supply and return cooling water is 7.9 °C. In contrast, under the DQN strategy, the cooling water inlet temperature ranges from 28.7 °C to 31.9 °C, with an average of 30.1 °C and a standard deviation of 0.48 °C, and the average temperature difference is 6 °C. The D3QN and DQN strategies more accurately identify the characteristics of the system's cooling water pumps and cooling towers\u2014being numerous and high-power\u2014thus optimizing by increasing the temperature difference and reducing the cooling water flow (lowering tower and pump frequencies). Specifically, the D3QN strategy is more aggressive, opting for higher cooling water inlet temperatures and larger temperature differences. This approach increases the chiller energy consumption by 828 kWh (0.35 %) compared to the DQN strategy but significantly reduces energy consumption of the cooling water pumps by 5157 kWh (30.67 %) and the cooling towers by 2892 kWh (23.25 %). Despite the increased chiller energy consumption, the overall system energy consumption is reduced by 7221 kWh (1.70 %), highlighting the superiority of the D3QN strategy in finding the global optimum solution and its value in practical applications. The D3QN strategy consistently maintains higher cooling water inlet temperatures and greater temperature differences between the supply and return water (operating at lower tower and pump frequencies). During changes in system load, it proactively adjusts settings to aggressively meet end-load demands. This is evident from the standard deviations of the cooling water inlet temperature and temperature difference: under stable load conditions for the first five days, the standard deviations for the D3QN-controlled water supply temperature and temperature difference are 0.61 and 0.89, respectively, compared to 0.35 and 0.42 for the DQN strategy. Under the unstable load conditions of the last two days, these standard deviations increase to 0.78 and 1.25 for D3QN, and only 0.66 and 0.86 for DQN. In these two conditions, the energy savings rates of D3QN compared to DQN are 1.6 % and 1.8 %, respectively. This further proves the exceptional performance and superiority of the D3QN strategy during significant load variations. 4.4 Future Outlook Looking to the future, there are several potential directions for further development and exploration in the field of optimizing HVAC systems using reinforcement learning. (1) Advanced algorithms: Future research could explore and develop more advanced reinforcement learning algorithms capable of better tackling the complexities and challenges of HVAC system optimization. This includes algorithms that can efficiently navigate high-dimensional state and action spaces, explore the search space effectively, and adapt to environmental changes. (2) Incorporation of domain knowledge: The integration of domain knowledge into the reinforcement learning process could enhance algorithm performance and efficiency. Using existing knowledge about HVAC systems, including physics-based models or expert guidelines, can provide reinforcement learning algorithms with prior knowledge, enhancing the optimization process. (3) Transfer learning and generalization: Techniques of transfer learning can be deployed to leverage knowledge gleaned from one HVAC system to enhance the optimization of another. Methods of generalization could assist the reinforcement learning algorithm in generalizing its learned policies to unseen states and actions, equipping it to adapt to various HVAC systems and environments. (4) Validation and deployment in real systems: Validating and deploying reinforcement learning algorithms in real-world HVAC systems is essential to evaluate their performance and feasibility. Conducting thorough real-world experiments and case studies can provide valuable insights into the practical benefits and challenges of using reinforcement learning for HVAC system optimization. Overall, future research efforts can focus on advancing the algorithms, leveraging domain knowledge, exploring transfer learning and generalization techniques, enabling online learning and adaptive control, and validating the effectiveness and practicality of reinforcement learning in real-world HVAC systems. 5 Conclusion This paper investigates the application of deep reinforcement learning algorithms in Heating, Ventilation, and Air Conditioning (HVAC) system control. Through a review and analysis of existing literature, this paper standardizes the state space configuration. Simulations and data analyses are performed to analyze the impact of the Q-network structure on the performance of the Deep Q-learning (DQN) and Double Dueling Deep Q-learning (D3QN) algorithms and compare their optimization effectiveness, stability, and reliability across different scenarios. The main contributions and findings of this research are as follows. (1) The Q-network structure, encompassing both the number of hidden layers and neurons per layer, significantly impacts the performance of DQN and D3QN algorithms. Using overly complex or broad network structures may lead to overfitting and unstable training. A Q-network structure with two hidden layers, containing 64 and 12 neurons respectively, is recommended for both DQN and D3QN. (2) The Q-network structure, encompassing both the number of hidden layers and neurons per layer, significantly impacts the performance of DQN and D3QN algorithms. Using overly complex or broad network structures may lead to overfitting and unstable training. A Q-network structure with two hidden layers, containing 64 and 12 neurons respectively, is recommended for both DQN and D3QN. (3) The D3QN algorithm consistently outperforms the DQN algorithm across all projects considered, whether in terms of optimization performance, stability, or reliability. This indicates that the D3QN algorithm possesses superior control capability and adaptability, equipping it to better handle the diversity and complexity associated with HVAC system control. (4) Both DQN and D3QN algorithms significantly enhance the HVAC system's coefficient of performance (COP). In the initial stages of deployment, there is no significant difference in optimization effectiveness between these two algorithms. However, with progressive training, D3QN showcases a more robust efficacy in improving the system's COP compared to DQN. (5) Although the project type and scale do impact algorithm performance, they are not the decisive factors. Generally, the larger and more complex the project, the more pronounced D3QN's advantage over DQN. Even in smaller, simpler projects, D3QN continues to demonstrate superior control results. (6) This research proposes several potential future research directions, including the exploration and development of advanced reinforcement learning algorithms, the integration of domain knowledge, the application of transfer learning and generalization methods, and the validation and implementation of reinforcement learning algorithms in real-world systems. CRediT authorship contribution statement Haosen Qin: Writing \u2013 original draft, Software, Methodology, Formal analysis, Conceptualization. Tao Meng: Writing \u2013 review & editing, Validation, Conceptualization. Kan Chen: Writing \u2013 review & editing, Methodology, Conceptualization. Zhengwei Li: Writing \u2013 review & editing, Supervision, Methodology, Conceptualization. Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Appendix. A Hyper-parameters adjustment range Table A1 Grid search hyper-parameters of SVR Table A1 Hyper-parameters Description Grid range SVR Kernel Mapping methods for higher dimensional spaces RBF C The larger the value of the punishment coefficient C, the less smooth the decision boundary. [0.1, 0.5, 1, 2, 3, 4, 5, 10, 50,100, 500] gamma The coefficient of the kernel, The larger the value, the more complex the model. [0.1,10] XGBoost n_estimators Number of decision trees in the model [5, 10, 50, 75, 100, \u2026, 500] Max_depth Maximum depth of each decision tree [1, 2, 3, 4, 5, 10, 50, 100] learning_rate Weight reduction factor for each decision tree [0.001,0.01,0.1,0.2,0.3,0.5] min_child_weight Minimum child node weight threshold 1 gamma Loss reduction threshold due to decision tree splitting [0,0.1,0.5,1] ANN Number of hidden layers Each hidden layer is a fully connected layer [2,10] Number of neurons Number of neurons per hidden layer [5, 10, 15, 20, \u2026, 300] MPR Highest number of terms The highest degree of terms in polynomials [1,2,3,4,5] References [1] S. Wang Z. Ma Supervisory and optimal control of building HVAC systems: a review HVAC R Res 14 1 2008 3 32 Wang S, Ma Z. Supervisory and optimal control of building HVAC systems: a review[J]. HVAC R Res, 2008, 14(1): 3-32. [2] P.M. Ferreira A.E. Ruano S. Silva Neural networks based predictive control for thermal comfort and energy savings in public buildings Energy Build 55 2012 238 251 Ferreira P M, Ruano A E, Silva S, et al. Neural networks based predictive control for thermal comfort and energy savings in public buildings[J]. Energy Build, 2012, 55:238-251. [3] H. Huang L. Chen E. Hu A new model predictive control scheme for energy and cost savings in commercial buildings: an airport terminal building case study Build Environ 89 jul 2015 203 216 Huang H, Chen L, Hu E. A new model predictive control scheme for energy and cost savings in commercial buildings: an airport terminal building case study[J]. Build Environ, 2015, 89(jul.):203-216. [4] A. Kusiak M. Li F. Tang Modeling and optimization of HVAC energy consumption Appl Energy 87 10 2010 3092 3102 Kusiak A, Li M, Tang F . Modeling and optimization of HVAC energy consumption[J]. Appl Energy, 2010, 87(10):3092-3102. [5] M. Killian M. Kozek Ten questions concerning model predictive control for energy efficient buildings Build Environ 105 aug 2016 403 412 Killian M, Kozek M. Ten questions concerning model predictive control for energy efficient buildings[J]. Build Environ, 2016, 105(aug.):403-412. [6] S. Privara J. Cigler Z. Váňa Building modeling as a crucial part for building predictive control Energy Build 56 2013 8 22 Privara S, Cigler J, Vana Z, et al. Building modeling as a crucial part for building predictive control[J]. Energy Build, 2013, 56: 8-22. [7] M. Cannon Efficient nonlinear model predictive control algorithms Annu Rev Control 28 Pt2 2004 229 237 Cannon M . Efficient nonlinear model predictive control algorithms[J]. Annu Rev Control, 2004, 28(Pt2):p.229-237. [8] A. Chatterjee D. Khovalyg Dynamic indoor thermal environment using reinforcement learning-based controls: opportunities and challenges Build Environ 2023 110766 Chatterjee A, Khovalyg D. Dynamic indoor thermal environment using reinforcement learning-based controls: opportunities and challenges[J]. Build Environ, 2023: 110766. [9] M. Han J. Zhao X. Zhang The reinforcement learning method for occupant behavior in building control: a review Energy and Built Environment 2 2 2021 137 148 Han M, Zhao J, Zhang X, et al. The reinforcement learning method for occupant behavior in building control: a review[J]. Energy and Built Environment, 2021, 2(2): 137-148. [10] Y. Du F. Li J. Munk Multi-task deep reinforcement learning for intelligent multi-zone residential HVAC control Elec Power Syst Res 192 2 2020 106959 Du Y, Li F, Munk J, et al. Multi-task deep reinforcement learning for intelligent multi-zone residential HVAC control[J]. Elec Power Syst Res, 2020, 192(2):106959. [11] X. Fang G. Gong G. Li Deep reinforcement learning optimal control strategy for temperature setpoint real-time reset in multi-zone building HVAC system Appl Therm Eng 212 2022 118552 Fang X, Gong G, Li G, et al. Deep reinforcement learning optimal control strategy for temperature setpoint real-time reset in multi-zone building HVAC system[J]. Appl Therm Eng, 2022, 212: 118552. [12] F.M. Solinas A. Macii E. Patti An online reinforcement learning approach for HVAC control Expert Syst Appl 238 2024 121749 Solinas F M, Macii A, Patti E, et al. An online reinforcement learning approach for HVAC control[J]. Expert Syst Appl, 2024, 238: 121749. [13] C. Blad S. Bøgh C.S. Kallesøe Data-driven offline reinforcement learning for HVAC-systems Energy 261 2022 125290 Blad C, Boegh S, Kallesoee C S. Data-driven offline reinforcement learning for HVAC-systems[J]. Energy, 2022, 261: 125290. [14] X. Fang G. Gong G. Li Deep reinforcement learning optimal control strategy for temperature setpoint real-time reset in multi-zone building HVAC system Appl Therm Eng 212 2022 118552 Fang X, Gong G, Li G, et al. Deep reinforcement learning optimal control strategy for temperature setpoint real-time reset in multi-zone building HVAC system[J]. Appl Therm Eng, 2022, 212: 118552. [15] K. Kadamala D. Chambers E. Barrett Enhancing HVAC control systems through transfer learning with deep reinforcement learning agents Smart Energy 13 2024 100131 Kadamala K, Chambers D, Barrett E. Enhancing HVAC control systems through transfer learning with deep reinforcement learning agents[J]. Smart Energy, 2024, 13: 100131. [16] X. Liu Z. Gou Occupant-centric HVAC and window control: a reinforcement learning model for enhancing indoor thermal comfort and energy efficiency Build Environ 250 2024 111197 Liu X, Gou Z. Occupant-centric HVAC and window control: a reinforcement learning model for enhancing indoor thermal comfort and energy efficiency[J]. Build Environ, 2024, 250: 111197. [17] Y. Li Deep reinforcement learning: an overview[J] arXiv preprint arXiv:1701.07274 2017 Li Y. Deep reinforcement learning: an overview[J]. arXiv preprint arXiv:1701.07274, 2017. [18] M. Volodymyr K. Koray S. David Human-level control through deep reinforcement learning Nature 518 7540 2019 529 533 Volodymyr M, Koray K, David S, et al. Human-level control through deep reinforcement learning[J]. Nature, 2019, 518(7540):529-533. [19] Y. Zhang J. Li G. Mu Deep reinforcement learning enabled UAV-IRS-assisted secure mobile edge computing network Physical Communication 61 2023 102173 Zhang Y, Li J, Mu G, et al. Deep reinforcement learning enabled UAV-IRS-assisted secure mobile edge computing network[J]. Physical Communication, 2023, 61: 102173. [20] M. Biemann F. Scheller X. Liu Experimental evaluation of model-free reinforcement learning algorithms for continuous HVAC control Appl Energy 298 2021 117164 Biemann M, Scheller F, Liu X, et al. Experimental evaluation of model-free reinforcement learning algorithms for continuous HVAC control[J]. Appl Energy, 2021, 298: 117164. [21] X. Liu M. Ren Z. Yang A multi-step predictive deep reinforcement learning algorithm for HVAC control systems in smart buildings Energy 259 2022 124857 Liu X, Ren M, Yang Z, et al. A multi-step predictive deep reinforcement learning algorithm for HVAC control systems in smart buildings[J]. Energy, 2022, 259: 124857. [22] X. Fang G. Gong G. Li Deep reinforcement learning optimal control strategy for temperature setpoint real-time reset in multi-zone building HVAC system Appl Therm Eng 212 2022 118552 Fang X, Gong G, Li G, et al. Deep reinforcement learning optimal control strategy for temperature setpoint real-time reset in multi-zone building HVAC system[J]. Appl Therm Eng, 2022, 212: 118552 [23] X. Deng Y. Zhang H. Qi Towards optimal HVAC control in non-stationary building environments combining active change detection and deep reinforcement learning J]. Building and environment 211 2022 108680 Deng X, Zhang Y, Qi H. Towards optimal HVAC control in non-stationary building environments combining active change detection and deep reinforcement learning[J]. Building and environment, 2022, 211: 108680 [24] Q. Fu X. Chen S. Ma Optimal control method of HVAC based on multi-agent deep reinforcement learning Energy Build 270 2022 112284 Fu Q, Chen X, Ma S, et al. Optimal control method of HVAC based on multi-agent deep reinforcement learning[J]. Energy Build, 2022, 270: 112284 [25] M. Ren X. Liu Z. Yang A novel forecasting based scheduling method for household energy management system based on deep reinforcement learning Sustain Cities Soc 76 2022 103207 Ren M, Liu X, Yang Z, et al. A novel forecasting based scheduling method for household energy management system based on deep reinforcement learning[J]. Sustain Cities Soc, 2022, 76: 103207. [26] X. Guo X. Yan Z. Chen Research on energy management strategy of heavy-duty fuel cell hybrid vehicles based on dueling-double-deep Q-network Energy 260 2022 125095 Guo X, Yan X, Chen Z, et al. Research on energy management strategy of heavy-duty fuel cell hybrid vehicles based on dueling-double-deep Q-network[J]. Energy, 2022, 260: 125095. [27] Y. Li Z. Wang W. Xu Modeling and energy dynamic control for a ZEH via hybrid model-based deep reinforcement learning Energy 277 2023 127627 Li Y, Wang Z, Xu W, et al. Modeling and energy dynamic control for a ZEH via hybrid model-based deep reinforcement learning[J]. Energy, 2023, 277: 127627. [28] Q. Fu Z. Li Z. Ding ED-DQN: an event-driven deep reinforcement learning control method for multi-zone residential buildings Build Environ 2023 110546 Fu Q, Li Z, Ding Z, et al. ED-DQN: an event-driven deep reinforcement learning control method for multi-zone residential buildings[J]. Build Environ, 2023: 110546. [29] S. Qiu Z. Li Z. Li Model-free control method based on reinforcement learning for building cooling water systems: validation by measured data-based simulation Energy Build 218 2020 110055 Qiu S, Li Z, Li Z, et al. Model-free control method based on reinforcement learning for building cooling water systems: validation by measured data-based simulation[J]. Energy Build, 2020, 218: 110055. [30] Z. Jiang M.J. Risbeck V. Ramamurti Building HVAC control with reinforcement learning for reduction of energy cost and demand charge Energy Build 239 2021 110833 Jiang Z, Risbeck M J, Ramamurti V, et al. Building HVAC control with reinforcement learning for reduction of energy cost and demand charge[J]. Energy Build, 2021, 239: 110833. [31] Y. Chen L.K. Norford H.W. Samuelson Optimal control of HVAC and window systems for natural ventilation through reinforcement learning Energy Build 169 JUN 2018 195 205 Chen Y, Norford L K, Samuelson H W, et al. Optimal control of HVAC and window systems for natural ventilation through reinforcement learning[J]. Energy Build, 2018, 169(JUN.):195-205. [32] T. Liu X. Hu A Bi-level control for energy efficiency improvement of a hybrid tracked vehicle IEEE Trans Ind Inform 14 2018 1616 1625 Liu T, Hu X. A Bi-level control for energy efficiency improvement of a hybrid tracked vehicle. IEEE Trans Ind Inform 2018;14:1616-1625. [33] T. Liu B. Wang C. Yang Online Markov chain-based energy management for a hybrid tracked vehicle with speedy Q-learning Energy 160 2018 544 555 Liu T, Wang B, Yang C. Online Markov chain-based energy management for a hybrid tracked vehicle with speedy Q-learning. Energy 2018;160:544-555 [34] Z. Zou X. Yu S. Ergan Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network Build Environ 168 2020 106535 Zou Z, Yu X, Ergan S. Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network[J]. Build Environ, 2020, 168: 106535. [35] Z. Deng Q. Chen Reinforcement learning of occupant behavior model for cross-building transfer learning to various HVAC control systems Energy Build 238 5\u20136 2021 110860 Deng Z, Chen Q. Reinforcement learning of occupant behavior model for cross-building transfer learning to various HVAC control systems[J]. Energy Build, 2021, 238(5-6):110860. [36] Wang Yuan Kirubakaran A novel approach to feedback control with deep reinforcement learning - ScienceDirect IFAC-PapersOnLine 51 18 2018 31 36 Yuan, Wang, Kirubakaran, et al. A novel approach to feedback control with deep reinforcement learning - ScienceDirect[J]. IFAC-PapersOnLine, 2018, 51(18):31-36. [37] H Van Hasselt A Guez D Silver Deep reinforcement learning with double q-learning[C] Proceedings of the AAAI conference on artificial intelligence 30 2016 (1) Hasselt H V, Guez A, Silver D. Deep reinforcement learning with double Q-learning[J]. Computer ence, 2015. [38] Z. Wang T. Schaul M. Hessel Dueling network architectures for deep reinforcement learning[C] International conference on machine learning 2016 PMLR 1995 2003 Wang Z, Schaul T, Hessel M, et al. Dueling network architectures for deep reinforcement learning[C]//International conference on machine learning. PMLR, 2016: 1995-2003. [39] C.L. Tsai W.T. Chen C.S. Chang Polynomial-Fourier series model for analyzing and predicting electricity consumption in buildings Energy Build 127 2016 301 312 Tsai C L, Chen W T, Chang C S. Polynomial-Fourier series model for analyzing and predicting electricity consumption in buildings[J]. Energy Build, 2016, 127: 301-312. [40] A. Afram F. Janabi-Sharifi A.S. Fung Artificial neural network (ANN) based model predictive control (MPC) and optimization of HVAC systems: a state of the art review and case study of a residential HVAC system Energy Build 141 APR 2017 96 113 Afram A, Janabi-Sharifi F, Fung A S, et al. Artificial neural network (ANN) based model predictive control (MPC) and optimization of HVAC systems: a state of the art review and case study of a residential HVAC system[J]. Energy Build, 2017, 141(APR.):96-113. [41] Yang Chen Liu Hongyu Zhang Limao Wu Xianguo Wang Xian Jia Energy consumption prediction and diagnosis of public buildings based on support vector machine learning: a case study in China J Clean Prod 272 2020 10.1016/j.jclepro.2020.122542 Liu, Yang Chen, Hongyu Zhang, Limao Wu, Xianguo Wang, Xian Jia. (2020). Energy consumption prediction and diagnosis of public buildings based on support vector machine learning: a case study in China. J Clean Prod. 272. 10.1016/j.jclepro.2020.122542. [42] T. Chen C. Guestrin Xgboost: a scalable tree boosting system[C Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining 2016 785 794 Chen T, Guestrin C. Xgboost: a scalable tree boosting system[C]//Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. 2016: 785-794. [43] A. Afram F. Janabi-Sharifi A.S. Fung Artificial neural network (ANN) based model predictive control (MPC) and optimization of HVAC systems: a state of the art review and case study of a residential HVAC system Energy Build 141 APR 2017 96 113 Afram A, Janabi-Sharifi F, Fung A S, et al. Artificial neural network (ANN) based model predictive control (MPC) and optimization of HVAC systems: a state of the art review and case study of a residential HVAC system[J]. Energy Build, 2017, 141(APR.):96-113. [44] H.A. Fayed A.F. Atiya Speed up grid-search for parameter selection of support vector machines Appl Soft Comput 80 2019 202 210 Fayed H A, Atiya A F. Speed up grid-search for parameter selection of support vector machines[J]. Appl Soft Comput, 2019, 80:202-210. [45] L. Yu Y. Sun Z. Xu Multi-agent deep reinforcement learning for HVAC control in commercial buildings IEEE Trans Smart Grid 12 1 2020 407 419 Yu L, Sun Y, Xu Z, et al. Multi-agent deep reinforcement learning for HVAC control in commercial buildings[J]. IEEE Trans Smart Grid, 2020, 12(1): 407-419. [46] Y. Du H. Zandi O. Kotevska Intelligent multi-zone residential HVAC control strategy based on deep reinforcement learning Appl Energy 281 2021 116117 Du Y, Zandi H, Kotevska O, et al. Intelligent multi-zone residential HVAC control strategy based on deep reinforcement learning[J]. Appl Energy, 2021, 281: 116117. [47] S. Brandi M.S. Piscitelli M. Martellacci Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings Energy Build 224 2020 110225 Brandi S, Piscitelli M S, Martellacci M, et al. Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings[J]. Energy Build, 2020, 224: 110225. [48] M. Biemann F. Scheller X. Liu Experimental evaluation of model-free reinforcement learning algorithms for continuous HVAC control Appl Energy 298 2021 117164 Biemann M, Scheller F, Liu X, et al. Experimental evaluation of model-free reinforcement learning algorithms for continuous HVAC control[J]. Appl Energy, 2021, 298: 117164. [49] J. Jiang X. Zeng D. Guzzetti Path planning for asteroid hopping rovers with pre-trained deep reinforcement learning architectures Acta Astronaut 171 2020 265 279 J Jiang, X Zeng, Guzzetti D, et al. Path planning for asteroid hopping rovers with pre-trained deep reinforcement learning architectures[J]. Acta Astronaut, 2020, 171:265-279. [50] S.M.C. Magalhães V.M.S. Leal I.M. Horta Modelling the relationship between heating energy use and indoor temperatures in residential buildings through Artificial Neural Networks considering occupant behavior Energy Build 151 2017 332 343 Magalhaes S M C, Leal V M S, Horta I M. Modelling the relationship between heating energy use and indoor temperatures in residential buildings through Artificial Neural Networks considering occupant behavior[J]. Energy Build, 2017, 151: 332-343.",
    "scopus-id": "85201238101",
    "coredata": {
        "eid": "1-s2.0-S0360544224025143",
        "dc:description": "Ensuring the optimal performance of Heating, Ventilation, and Air Conditioning (HVAC) systems is paramount for achieving energy efficiency. This paper investigates the application of deep reinforcement learning algorithms in HVAC system control, aiming to identify the most suitable Q-network structure for optimizing HVAC systems and comparing the performance of Deep Q-learning (DQN) and Double Dueling Deep Q-learning (D3QN) algorithms. Initially, this paper evaluates and analyses existing literature to perform a normalization treatment on the state space. Through systematic simulation and rigorous data analysis, the impact of the Q-network structure on the efficacy of the DQN and D3QN algorithms is evaluated, resulting in the proposal of specific values for the Q-network structures within these two algorithms. Subsequently, comparisons are drawn on the optimization effectiveness, stability and reliability of these algorithms across diverse engineering projects. Results highlight the superiority of the D3QN algorithm over the DQN algorithm regarding both optimization effectiveness and stability across all evaluated projects. The proposed efficient Q-network structure comprises two hidden layers, with 64 and 12 neurons respectively in each layer. The findings of this paper are crucial in providing insights for HVAC systems control optimization using reinforcement learning and pave the way for advanced research and applications in the future.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2024-10-30",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S0360544224025143",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Qin, Haosen"
            },
            {
                "@_fa": "true",
                "$": "Meng, Tao"
            },
            {
                "@_fa": "true",
                "$": "Chen, Kan"
            },
            {
                "@_fa": "true",
                "$": "Li, Zhengwei"
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S0360544224025143"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S0360544224025143"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S0360-5442(24)02514-3",
        "prism:volume": "307",
        "articleNumber": "132740",
        "prism:publisher": "Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
        "dc:title": "A comparative study of DQN and D3QN for HVAC system optimization control",
        "prism:copyright": "© 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
        "openaccess": "0",
        "prism:issn": "03605442",
        "openaccessArticle": "false",
        "prism:publicationName": "Energy",
        "openaccessSponsorType": null,
        "prism:pageRange": "132740",
        "pubType": "fla",
        "prism:coverDisplayDate": "30 October 2024",
        "prism:doi": "10.1016/j.energy.2024.132740",
        "prism:startingPage": "132740",
        "dc:identifier": "doi:10.1016/j.energy.2024.132740",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "241",
            "@width": "691",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "98322",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "507",
            "@width": "691",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "176547",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "437",
            "@width": "535",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "57623",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "271",
            "@width": "388",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "33800",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "517",
            "@width": "691",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "265035",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "532",
            "@width": "691",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr8.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "188372",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "530",
            "@width": "691",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "266944",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "514",
            "@width": "691",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "255971",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "378",
            "@width": "535",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr11.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "178522",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "363",
            "@width": "535",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr10.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "189534",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "379",
            "@width": "535",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr13.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "166691",
            "@ref": "gr13",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "359",
            "@width": "535",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr12.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "162389",
            "@ref": "gr12",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "374",
            "@width": "535",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr9.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "186574",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "76",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "70358",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "161",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "77855",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "200",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "14337",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "153",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "13646",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "89492",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "213",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr8.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "79683",
            "@ref": "gr8",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "214",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "89314",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "88976",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "155",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr11.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "88373",
            "@ref": "gr11",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "148",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr10.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "90012",
            "@ref": "gr10",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "155",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr13.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "84121",
            "@ref": "gr13",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "147",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr12.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "83633",
            "@ref": "gr12",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "153",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr9.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "89656",
            "@ref": "gr9",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "1068",
            "@width": "3058",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "359994",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2243",
            "@width": "3059",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "948266",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1938",
            "@width": "2371",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "343431",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1200",
            "@width": "1721",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "207191",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2288",
            "@width": "3059",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1647616",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2357",
            "@width": "3059",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr8_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1021216",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2345",
            "@width": "3059",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1725742",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2276",
            "@width": "3059",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1609194",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1673",
            "@width": "2370",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr11_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "932359",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1606",
            "@width": "2370",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr10_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "987536",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1681",
            "@width": "2370",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr13_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "831256",
            "@ref": "gr13",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1590",
            "@width": "2370",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr12_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "766590",
            "@ref": "gr12",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1658",
            "@width": "2369",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-gr9_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1000687",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-si7.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "133317",
            "@ref": "si7",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-si8.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "13224",
            "@ref": "si8",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-si6.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "102316",
            "@ref": "si6",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-si1.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "79502",
            "@ref": "si1",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-si5.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "46252",
            "@ref": "si5",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-si4.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "84464",
            "@ref": "si4",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-si3.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "81213",
            "@ref": "si3",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-si2.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "48392",
            "@ref": "si2",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544224025143-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "22755849",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85201238101"
    }
}}