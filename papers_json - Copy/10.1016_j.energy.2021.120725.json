{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85105332601",
    "originalText": "serial JL 271090 291210 291702 291711 291731 291877 291878 31 Energy ENERGY 2021-04-27 2021-04-27 2021-05-04 2021-05-04 2022-07-20T18:57:27 1-s2.0-S0360544221009737 S0360-5442(21)00973-7 S0360544221009737 10.1016/j.energy.2021.120725 S300 S300.4 FULL-TEXT 1-s2.0-S0360544221X00102 2024-01-25T15:29:21.839824Z 0 0 20210815 2021 2021-04-27T05:28:14.080613Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath affil appendices articletitle auth authfirstini authfull authkeywords authlast highlightsabst orcid primabst ref 0360-5442 03605442 true 229 229 C Volume 229 105 120725 120725 120725 20210815 15 August 2021 2021-08-15 2021 Articles from the Special issue on SDEWES 2020 ; Edited by Neven Duić, Natasa Markovska, Antonio Piacentino, Brian Vad Mathiesen article fla © 2021 Elsevier Ltd. All rights reserved. COORDINATEDENERGYMANAGEMENTFORACLUSTERBUILDINGSTHROUGHDEEPREINFORCEMENTLEARNING PINTO G 1 Introduction 1.1 Related works and contributions 2 Methods 2.1 Soft actor critic 2.2 The CityLearn simulation environment 3 Methodological framework 4 Case study 4.1 Description of the cluster of buildings 4.2 Energy systems and control objectives 4.3 Baseline rule-based control 4.4 Design of the deep reinforcement learning controller 4.4.1 Action-space design 4.4.2 State-space design 4.4.3 Reward design 4.5 Training and deployment 4.5.1 Training phase 4.5.2 Deployment phase 5 Results 5.1 Training results 5.1.1 Comparison of controllers at single building level 5.1.2 Comparison of controllers at cluster level 5.1.3 Comparison of controllers at grid level 5.2 Deployment of deep reinforcement learning controller in different climatic conditions 6 Discussion 7 Conclusion and future perspectives Nomenclature Credit author statement References LEIBOWICZ 2018 1311 1325 B 2014 IEASTATISTICSWORLDENERGYSTATISTICSBALANCES JENSEN 2017 25 34 S SHEN 2016 143 154 L ODWYER 2019 581 597 E HU 2021 110248 M FIORINI 2019 1048 1076 L GUERRERO 2020 J VERSCHAE 2016 922 935 R ZHOU 2020 118882 K CHANG 2013 1490 1504 T MARTINOPOULOS 2018 687 699 G AFRAM 2014 343 355 A SERALE 2018 G GONZATO 2019 109421 S WANG 2020 115036 Z MASON 2019 300 312 K BRANDI 2020 110225 S VAZQUEZCANTELI 2019 1072 1089 J KAZMI 2018 159 168 H LU 2019 937 949 R RUELENS 2017 2149 2159 F VAZQUEZCANTELI 2017 415 420 J KAZMI 2016 1 15 H RUELENS 2018 3792 3800 F LAM 2020 603 619 E PRINCPRACTCONSTRAINTPROGRAM LARGENEIGHBORHOODSEARCHFORTEMPERATURECONTROLDEMANDRESPONSE VAZQUEZCANTELI 2018 J DESOMER 2017 O USINGREINFORCEMENTLEARNINGFORDEMANDRESPONSEDOMESTICHOTWATERBUFFERSAREALLIFEDEMONSTRATION KOFINAS 2018 53 67 P VAZQUEZCANTELI 2020 J BUILDSYS20 MULTIAGENTREINFORCEMENTLEARNINGITERATIVESEQUENTIALACTIONSELECTIONFORLOADSHAPINGGRIDINTERACTIVECONNECTEDBUILDINGS MARINESCU 2015 1897 1898 A MARINESCU 2017 A PREDICTIONBASEDMULTIAGENTREINFORCEMENTLEARNINGININHERENTLYNONSTATIONARYENVIRONMENTS HUANG 2019 684 694 P VAZQUEZCANTELI 2019 J VAZQUEZCANTELI 2019 356 357 J PROC6THACMINTCONFSYSTENERGYEFFICIENTBUILDCITIESTRANSP CITYLEARNV10OPENAIGYMENVIRONMENTFORDEMANDRESPONSEDEEPREINFORCEMENTLEARNING SUTTON 1998 R REINFORCEMENTLEARNINGINTRODUCTION WATKINS 1992 279 292 C QLEARNINGMACHLEARN MNIH 2013 V PLAYINGATARIDEEPREINFORCEMENTLEARNING HAARNOJA 2018 2976 2989 T 35THINTCONFMACHLEARNICML2018 SOFTACTORCRITICOFFPOLICYMAXIMUMENTROPYDEEPREINFORCEMENTLEARNINGASTOCHASTICACTOR HAARNOJA 2018 T SOFTACTORCRITICALGORITHMSAPPLICATIONS VAZQUEZCANTELI 2019 J CITYLEARNGITHUBREPOSITORY HENZE 2003 259 275 G 2019 CONDIZIONATORIDARIAREFRIGERATORIDILIQUIDOEPOMPEDICALORECONCOMPRESSOREELETTRICOPERILRISCALDAMENTOEILRAFFRESCAMENTODEGLIAMBIENTIMETODIDIPROVAEVALUTAZIONEACARICOPARZIALEECALCOLODELRENDIMENTOSTAGIONALE PINTOX2021X120725 PINTOX2021X120725XG 2023-05-04T00:00:00.000Z 2023-05-04T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2021 Elsevier Ltd. All rights reserved. 2021-06-05T04:10:50.691Z http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/aggregated-refined 0 https://doi.org/10.15223/policy-017 https://doi.org/10.15223/policy-037 https://doi.org/10.15223/policy-012 https://doi.org/10.15223/policy-029 https://doi.org/10.15223/policy-004 item S0360-5442(21)00973-7 S0360544221009737 1-s2.0-S0360544221009737 10.1016/j.energy.2021.120725 271090 2022-07-20T21:27:36.065624Z 2021-08-15 1-s2.0-S0360544221009737-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/MAIN/application/pdf/fdc6cd350a7b88753f50e79e087aa44e/main.pdf main.pdf pdf true 2376030 MAIN 13 1-s2.0-S0360544221009737-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/PREVIEW/image/png/ae0c0d7f3cd3ddb5933c506fc25266a6/main_1.png main_1.png png 56082 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0360544221009737-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr3/DOWNSAMPLED/image/jpeg/89a5356d9c0e3a74d13567b7024c89f9/gr3.jpg gr3 gr3.jpg jpg 47636 207 669 IMAGE-DOWNSAMPLED 1-s2.0-S0360544221009737-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr1/DOWNSAMPLED/image/jpeg/f32400ec2684f0ffe1db34fa8c97f2e3/gr1.jpg gr1 gr1.jpg jpg 15047 201 389 IMAGE-DOWNSAMPLED 1-s2.0-S0360544221009737-gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr10/HIGHRES/image/jpeg/514e62d0aa56e021905cd670f42405f8/gr10.jpg gr10 gr10.jpg jpg 31636 304 388 IMAGE-DOWNSAMPLED 1-s2.0-S0360544221009737-gr11.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr11/DOWNSAMPLED/image/jpeg/6bdfe02aacb1a59ed42adad0880af8ff/gr11.jpg gr11 gr11.jpg jpg 50106 386 535 IMAGE-DOWNSAMPLED 1-s2.0-S0360544221009737-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr6/DOWNSAMPLED/image/jpeg/1489628d17a056d3b517191b5f149865/gr6.jpg gr6 gr6.jpg jpg 20557 288 389 IMAGE-DOWNSAMPLED 1-s2.0-S0360544221009737-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr7/THUMBNAIL/image/jpeg/f31ac88f4b113839b23090165927855e/gr7.jpg gr7 gr7.jpg jpg 53058 344 624 IMAGE-DOWNSAMPLED 1-s2.0-S0360544221009737-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr4/HIGHRES/image/jpeg/82061c0fb99abc9545d24aec13f5d35a/gr4.jpg gr4 gr4.jpg jpg 52895 220 669 IMAGE-DOWNSAMPLED 1-s2.0-S0360544221009737-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr2/DOWNSAMPLED/image/jpeg/e39fb51cd50c34c067718018ac47726d/gr2.jpg gr2 gr2.jpg jpg 38966 160 624 IMAGE-DOWNSAMPLED 1-s2.0-S0360544221009737-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr5/DOWNSAMPLED/image/jpeg/1c51093e58e11134b9cae183ae830c86/gr5.jpg gr5 gr5.jpg jpg 53300 308 624 IMAGE-DOWNSAMPLED 1-s2.0-S0360544221009737-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr8/DOWNSAMPLED/image/jpeg/350f893300121211c2a74c39d8b46633/gr8.jpg gr8 gr8.jpg jpg 89495 459 691 IMAGE-DOWNSAMPLED 1-s2.0-S0360544221009737-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr9/DOWNSAMPLED/image/jpeg/ad16336a39b9005949569803785dccd8/gr9.jpg gr9 gr9.jpg jpg 33640 340 387 IMAGE-DOWNSAMPLED 1-s2.0-S0360544221009737-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr3/THUMBNAIL/image/gif/8064dd18c77978c4917c939514b32b5f/gr3.sml gr3 gr3.sml sml 7745 68 219 IMAGE-THUMBNAIL 1-s2.0-S0360544221009737-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr1/THUMBNAIL/image/gif/ed38ddc068facd41e6f667ebf19812df/gr1.sml gr1 gr1.sml sml 4419 113 219 IMAGE-THUMBNAIL 1-s2.0-S0360544221009737-gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr10/THUMBNAIL/image/gif/1156ab21ff649fc3a1a3b762cd263dbf/gr10.sml gr10 gr10.sml sml 7590 164 209 IMAGE-THUMBNAIL 1-s2.0-S0360544221009737-gr11.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr11/THUMBNAIL/image/gif/21813eb9127eba92f8ae19c917325400/gr11.sml gr11 gr11.sml sml 8823 158 219 IMAGE-THUMBNAIL 1-s2.0-S0360544221009737-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr6/THUMBNAIL/image/gif/878811a48a15118ca37ec7b4a9486741/gr6.sml gr6 gr6.sml sml 5733 162 219 IMAGE-THUMBNAIL 1-s2.0-S0360544221009737-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr7/THUMBNAIL/image/gif/3900c65c9fcff3369332768dfa949c72/gr7.sml gr7 gr7.sml sml 7332 121 219 IMAGE-THUMBNAIL 1-s2.0-S0360544221009737-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr4/THUMBNAIL/image/gif/cffed31db8c58750bcd2affffa48126b/gr4.sml gr4 gr4.sml sml 7049 72 219 IMAGE-THUMBNAIL 1-s2.0-S0360544221009737-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr2/THUMBNAIL/image/gif/daa1f7aaaae586ec4275605b08c8dd40/gr2.sml gr2 gr2.sml sml 4750 56 219 IMAGE-THUMBNAIL 1-s2.0-S0360544221009737-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr5/HIGHRES/image/gif/bea3d8d5bc0aa5fd76c3edd40aa61854/gr5.sml gr5 gr5.sml sml 7766 108 219 IMAGE-THUMBNAIL 1-s2.0-S0360544221009737-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr8/THUMBNAIL/image/gif/0963976368d0e81e11c5e0b3536a3e5a/gr8.sml gr8 gr8.sml sml 10449 145 219 IMAGE-THUMBNAIL 1-s2.0-S0360544221009737-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/gr9/THUMBNAIL/image/gif/9447cea672a1991c946fd90d1cc2e10f/gr9.sml gr9 gr9.sml sml 8678 164 186 IMAGE-THUMBNAIL 1-s2.0-S0360544221009737-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/HIGHRES/image/jpeg/f6e955bb3ee620fb2afc87a101265e21/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 338638 915 2962 IMAGE-HIGH-RES 1-s2.0-S0360544221009737-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/DOWNSAMPLED/image/jpeg/eac1bad83968bc3a49cca91b4a4f9219/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 111208 889 1722 IMAGE-HIGH-RES 1-s2.0-S0360544221009737-gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/HIGHRES/image/jpeg/1a4d6f01be1161d6f9f0334d24f79746/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 242405 1348 1720 IMAGE-HIGH-RES 1-s2.0-S0360544221009737-gr11_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/HIGHRES/image/jpeg/7b280515356093f55b6b45dda8f98c36/gr11_lrg.jpg gr11 gr11_lrg.jpg jpg 433862 1712 2370 IMAGE-HIGH-RES 1-s2.0-S0360544221009737-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/HIGHRES/image/jpeg/b7010a0e901acc5d0da915ccc15d6354/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 138082 1274 1722 IMAGE-HIGH-RES 1-s2.0-S0360544221009737-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/HIGHRES/image/jpeg/36d625f68e50dfdf0ed458ba386f5202/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 378177 1522 2764 IMAGE-HIGH-RES 1-s2.0-S0360544221009737-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/THUMBNAIL/image/jpeg/f148582a211df9241422e28b0a860260/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 359227 972 2961 IMAGE-HIGH-RES 1-s2.0-S0360544221009737-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/DOWNSAMPLED/image/jpeg/6f43fe76fd619c212de0ead7d0de17f3/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 322847 708 2763 IMAGE-HIGH-RES 1-s2.0-S0360544221009737-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/MAIN/image/jpeg/b2a51987ae27976a708e4fe15d10db86/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 439198 1366 2764 IMAGE-HIGH-RES 1-s2.0-S0360544221009737-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/HIGHRES/image/jpeg/dfa0a48ba2e6aab0d732b29457328236/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 634482 2031 3059 IMAGE-HIGH-RES 1-s2.0-S0360544221009737-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/HIGHRES/image/jpeg/8761bb30b9a22511eb5da219849c0022/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 232987 1507 1713 IMAGE-HIGH-RES 1-s2.0-S0360544221009737-si9.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/935a9e7f47154da9881def6ec61dbfcd/si9.svg si9 si9.svg svg 5809 ALTIMG 1-s2.0-S0360544221009737-si27.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/7b9b9a8cf5e5dddc9a481f3871e0d30c/si27.svg si27 si27.svg svg 8669 ALTIMG 1-s2.0-S0360544221009737-si46.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/f48c62bd82d1c825c4e20e59b08eac26/si46.svg si46 si46.svg svg 4456 ALTIMG 1-s2.0-S0360544221009737-si6.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/3886354189dff8dff3c0caa996184a13/si6.svg si6 si6.svg svg 2867 ALTIMG 1-s2.0-S0360544221009737-si42.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/07ebc9768c552623ebb6e77442be43ed/si42.svg si42 si42.svg svg 5137 ALTIMG 1-s2.0-S0360544221009737-si15.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/b286c7bda748aeccd1b5f55183a4018b/si15.svg si15 si15.svg svg 1522 ALTIMG 1-s2.0-S0360544221009737-si16.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/093167b56e6acd18d8cdd402c50f0ba6/si16.svg si16 si16.svg svg 2877 ALTIMG 1-s2.0-S0360544221009737-si5.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/5d638712cdd640248db18b7f626509b8/si5.svg si5 si5.svg svg 5524 ALTIMG 1-s2.0-S0360544221009737-si2.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/94ddddd298c3e83edeec2fdec3739d52/si2.svg si2 si2.svg svg 18920 ALTIMG 1-s2.0-S0360544221009737-si20.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/111a00a9ee96a9b0095bf2f8dfd02f18/si20.svg si20 si20.svg svg 2793 ALTIMG 1-s2.0-S0360544221009737-si34.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/a520dbf1765a7ccfda991b5b7cb02a36/si34.svg si34 si34.svg svg 4564 ALTIMG 1-s2.0-S0360544221009737-si48.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/69d8b0c4ae71a9a708239f1cf00e578d/si48.svg si48 si48.svg svg 1311 ALTIMG 1-s2.0-S0360544221009737-si1.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/e9d06a20f53aa31bca4ac5e3abbcba10/si1.svg si1 si1.svg svg 9073 ALTIMG 1-s2.0-S0360544221009737-si4.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/3bee23b2ebaecf71c5ff6f4206ad475a/si4.svg si4 si4.svg svg 1664 ALTIMG 1-s2.0-S0360544221009737-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/a34753748a9202cf4beba3857a73477f/si3.svg si3 si3.svg svg 15217 ALTIMG 1-s2.0-S0360544221009737-si26.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/04834a593711afaf8c331464f866b308/si26.svg si26 si26.svg svg 10311 ALTIMG 1-s2.0-S0360544221009737-si37.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/452c9a4585f38e9712b5bf51ab5e1ae2/si37.svg si37 si37.svg svg 17440 ALTIMG 1-s2.0-S0360544221009737-si45.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/f6102d83c27ad8bd849028aed3b1c45d/si45.svg si45 si45.svg svg 4644 ALTIMG 1-s2.0-S0360544221009737-si49.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/02930b1b2b4e2402f4ac7598cecd3eb8/si49.svg si49 si49.svg svg 2502 ALTIMG 1-s2.0-S0360544221009737-si31.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/9116c0fa535c0e8e58f2e3d31d7d05ed/si31.svg si31 si31.svg svg 1334 ALTIMG 1-s2.0-S0360544221009737-si33.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/14395e75adc2f98ae5fe55ee3bd9c05e/si33.svg si33 si33.svg svg 5589 ALTIMG 1-s2.0-S0360544221009737-si38.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/251a2982fb4c6711407bdfaa326290b6/si38.svg si38 si38.svg svg 24849 ALTIMG 1-s2.0-S0360544221009737-si29.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/baafbcd2ab046c351ee9a9f6d6a32674/si29.svg si29 si29.svg svg 4025 ALTIMG 1-s2.0-S0360544221009737-si13.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/ead6eee9b9dd616b90f14b5c84490df8/si13.svg si13 si13.svg svg 19907 ALTIMG 1-s2.0-S0360544221009737-si43.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/50b9986ba3fd392906897823f2981509/si43.svg si43 si43.svg svg 54671 ALTIMG 1-s2.0-S0360544221009737-si28.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/866b3ed0810fb6cccf5b4d81eaf99661/si28.svg si28 si28.svg svg 2290 ALTIMG 1-s2.0-S0360544221009737-si22.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/ca5b2db9a9f743f8a41fdf7e1f4f33bc/si22.svg si22 si22.svg svg 1146 ALTIMG 1-s2.0-S0360544221009737-si14.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/fa0613c72366e91cb65c302034c7dc5b/si14.svg si14 si14.svg svg 23239 ALTIMG 1-s2.0-S0360544221009737-si23.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/7e71765ce6cda566c338d456236208ec/si23.svg si23 si23.svg svg 1429 ALTIMG 1-s2.0-S0360544221009737-si47.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/9d6edb15b2d8faa82c1f1844dacd4fbd/si47.svg si47 si47.svg svg 1344 ALTIMG 1-s2.0-S0360544221009737-si10.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/461ffdb2002849ea3636bdc5d9322443/si10.svg si10 si10.svg svg 1239 ALTIMG 1-s2.0-S0360544221009737-si40.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/9e9145f15ef055fe4b2a251676c6eac8/si40.svg si40 si40.svg svg 8491 ALTIMG 1-s2.0-S0360544221009737-si36.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/4a4d6997ddad2711c1d87f0252ed7a8d/si36.svg si36 si36.svg svg 13081 ALTIMG 1-s2.0-S0360544221009737-si17.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/0e0c9d806f0dff6219ca1fc99ccaec4a/si17.svg si17 si17.svg svg 3217 ALTIMG 1-s2.0-S0360544221009737-si21.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/ff92545510848f64d73edce831493a03/si21.svg si21 si21.svg svg 1207 ALTIMG 1-s2.0-S0360544221009737-si12.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/dd054757b72db3017a4a06091d0f22d6/si12.svg si12 si12.svg svg 8984 ALTIMG 1-s2.0-S0360544221009737-si32.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/564f7c031dc59e3f4fca6e457ff187ee/si32.svg si32 si32.svg svg 5137 ALTIMG 1-s2.0-S0360544221009737-si7.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/2247cc995937292997eac2bfcd5ad048/si7.svg si7 si7.svg svg 6258 ALTIMG 1-s2.0-S0360544221009737-si44.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/4f47c93bd86b10759c9fc76145839593/si44.svg si44 si44.svg svg 20487 ALTIMG 1-s2.0-S0360544221009737-si19.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/a2cea94faa61f4a88200a4afbea16e8c/si19.svg si19 si19.svg svg 5897 ALTIMG 1-s2.0-S0360544221009737-si41.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/f34647e6de56c822f289d49c41a42d40/si41.svg si41 si41.svg svg 19714 ALTIMG 1-s2.0-S0360544221009737-si8.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/aea1594db32e9c8ff30d8e66c9377d1e/si8.svg si8 si8.svg svg 14579 ALTIMG 1-s2.0-S0360544221009737-si30.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/0d971f5d1cba8723141190535f2d6f81/si30.svg si30 si30.svg svg 47925 ALTIMG 1-s2.0-S0360544221009737-si39.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/8315836a272895c6ed1af1c69c0ec8bd/si39.svg si39 si39.svg svg 11326 ALTIMG 1-s2.0-S0360544221009737-si24.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/b19451d1a4304ab77048a8b2edb3bbe7/si24.svg si24 si24.svg svg 43465 ALTIMG 1-s2.0-S0360544221009737-si35.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/94305dc96da6a61dac0ad6629ad0eb9b/si35.svg si35 si35.svg svg 10456 ALTIMG 1-s2.0-S0360544221009737-si18.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/0c1afcebe3701a552a236aa63ce6b626/si18.svg si18 si18.svg svg 24972 ALTIMG 1-s2.0-S0360544221009737-si50.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/59a21560d3993665cef15a696386674c/si50.svg si50 si50.svg svg 1814 ALTIMG 1-s2.0-S0360544221009737-si25.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/9189bcdee4c54264d7fbdf3f5e62a5fc/si25.svg si25 si25.svg svg 49267 ALTIMG 1-s2.0-S0360544221009737-si11.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360544221009737/STRIPIN/image/svg+xml/1aa193a9ee533f25f161bc9feb870c8e/si11.svg si11 si11.svg svg 6198 ALTIMG 1-s2.0-S0360544221009737-am.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:10Q5B502WM7/MAIN/application/pdf/a19c42f03a3b8f1906f5ce36f820c9f2/am.pdf am am.pdf pdf false 1483935 AAM-PDF EGY 120725 120725 S0360-5442(21)00973-7 10.1016/j.energy.2021.120725 Elsevier Ltd Fig. 1 Reinforcement learning control framework. Fig. 1 Fig. 2 Actor-Critic Environment interaction and neural networks in DRL. Fig. 2 Fig. 3 Framework of the application of DRL control. Fig. 3 Fig. 4 Load Profile for each building (left) and cluster profile electricity and PV production (right). Fig. 4 Fig. 5 State-action space representation of the DRL controller. Fig. 5 Fig. 6 Temperature distribution of the different deployment climates. Fig. 6 Fig. 7 State of charge of storages and forcing variables scaled between 0 and 1. Fig. 7 Fig. 8 Comparison between uncoordinated and coordinated energy management. Fig. 8 Fig. 9 State of charge of storages averaged over a day. Fig. 9 Fig. 10 Load duration curve for the base case without energy storages in buildings and the two control strategies. Fig. 10 Fig. 11 KPI comparison for the four deployment cases. Fig. 11 Table 1 Soft actor-critic algorithm. Table 1 Input: θ 1 , θ 2 , φ Initial parameters θ ¯ 1 ← θ 1 , θ ¯ 2 ← θ 2 Initialize target network weights D ← 0 Initialize an empty replay buffer for each iteration do for each environment step do a t ∼ π φ ( a t | s t ) Sample action from the policy s t + 1 ∼ p ( s t + 1 | s t , a t ) Sample transition from the environment D ← D ∪ { ( s t , a t , r ( s t , a t ) , s t + 1 ) } Store the transition in the replay buffer end for for each gradient step do θ i ← θ i − λ Q ∇ θ i J Q ( θ i ) for i ∈ { 1,2 } Update of the Q-function parameters φ i ← φ i − λ π ∇ φ J π ( φ ) Update policy parameters α ← α − λ ∇ α J ( α ) Adjust temperature θ ¯ i ← τ θ ¯ i + ( 1 − τ ) θ ¯ i for i ∈ { 1,2 } Soft update of the target network weight end for end for Output: θ 1 , θ 2 , φ Optimised parameters Table 2 Building and energy systems properties. Table 2 Type Surface [m2] Volume [m3] Cold Storage Capacity [kWh] Electric Heater Capacity [kW] Hot Storage Capacity [kWh] PV Capacity [kW] Building 1 Office 5000 13,700 235 17 50 0 Building 2 Restaurant 230 710 150 25 75 25 Building 3 Retail 2300 14,000 200 23 70 20 Building 4 Retail 2100 10,800 185 35 105 0 Table 3 State-space. Table 3 Variable group Variable Unit Weather Temperature °C Temperature Forecast (6 h) °C Direct Solar Radiation W/m2 Direct Solar radiation Forecast (6 h) W/m2 District Total Load kW Electricity Price \u20ac/kWh Electricity Price Forecast (1,2,3 h) \u20ac/kWh Hour of day h Building Non-shiftable load kW Heat Pump Efficiency [−] Solar generation W/m2 Cooling Storage SOC [−] DHW SOC [−] Table 4 Hyperparameter settings. Table 4 Variable Value 1 DNN architecture 2 Layers 2 Neurons per hidden layer 256 3 DNN Optimizer Adam 4 Batch size 512 5 Learning rate λ 0.003 6 Decay rate τ 0.005 7 Temperature∗ α Starting = 1, Final = 0.05 8 Entropy coefficient∗ H Starting = 8, Final = 5 9 Target model update 1 10 Episode Length 2208 Control Steps (92 days) 11 Training Episodes 5 Table 5 Reward and KPI evolution over training period. Table 5 Episode 1 Episode 2 Episode 3 Episode 4 Episode 5 Deployment Reward −343 −337 −297 −271 −271 −265 Cost 1.1 1.1 1.06 0.98 0.97 0.96 Peak 1.07 1.29 0.96 0.96 0.96 0.88 Table 6 Comparison between performances of the two control strategies. Table 6 Energy Consumption Electricity Cost Peak Peak-to-average ratio (PAR) Average daily peak Average daily PAR Manually Optimised RBC 1 1 1 1 1 1 DRL 1.03 0.96 0.88 0.96 0.90 0.94 Coordinated energy management for a cluster of buildings through deep reinforcement learning Giuseppe Pinto a Marco Savino Piscitelli a José Ramón Vázquez-Canteli b Zoltán Nagy b Alfonso Capozzoli a ∗ a Politecnico di Torino, Department of Energy, TEBE research group, BAEDA Lab, Corso Duca degli Abruzzi 24, 10129, Torino, Italy Politecnico di Torino Department of Energy TEBE research group BAEDA Lab Corso Duca degli Abruzzi 24 Torino 10129 Italy Politecnico di Torino, Department of Energy, BAEDA Lab, Corso Duca degli Abruzzi 24, 10129, Torino, Italy b Intelligent Environment Laboratory, Department of Civil, Architectural and Environmental Engineering, The University of Texas, Austin, TX, 78712, USA Intelligent Environment Laboratory Department of Civil, Architectural and Environmental Engineering The University of Texas Austin TX 78712 USA Intelligent Environment Laboratory, Department of Civil, Architectural and Environmental Engineering, The University of Texas, Austin, TX, 78712, USA ∗ Corresponding author. Advanced control strategies can enable energy flexibility in buildings by enhancing on-site renewable energy exploitation and storage operation, significantly reducing both energy costs and emissions. However, when the energy management is faced shifting from a single building to a cluster of buildings, uncoordinated strategies may have negative effects on the grid reliability, causing undesirable new peaks. To overcome these limitations, the paper explores the opportunity to enhance energy flexibility of a cluster of buildings, taking advantage from the mutual collaboration between single buildings by pursuing a coordinated approach in energy management. This is achieved using Deep Reinforcement Learning (DRL), an adaptive model-free control algorithm, employed to manage the thermal storages of a cluster of four buildings equipped with different energy systems. The controller was designed to flatten the cluster load profile while optimizing energy consumption of each building. The coordinated energy management controller is tested and compared against a manually optimised rule-based one. Results shows a reduction of operational costs of about 4%, together with a decrease of peak demand up to 12%. Furthermore, the control strategy allows to reduce the average daily peak and average peak-to-average ratio by 10 and 6% respectively, highlighting the benefits of a coordinated approach. Keywords Coordinated energy management Deep reinforcement learning Building energy flexibility Peak demand reduction Grid interaction 1 Introduction The current energy transition is deeply changing the way energy is used and generated. The need of a further decarbonisation of the building sector [1], together with the rapid growth of urban areas, has fostered the use of distributed renewable energy resources. Nonetheless, the rapid penetration of renewable energy sources, characterised by their stochastic behaviour, represents the main cause of an intermittent injection of electricity into the power grid, which can jeopardize grid stability [2]. A recent solution lies in a new paradigm of energy management, which shifts from the supply-side to building demand-side control. The latter exploits the novel concept of building energy flexibility, that represents the ability of adapting energy consumption and storage operation without compromising technical and comfort constraints, to increase on-site renewable energy consumption, reduce costs and provide services to the grid (i.e. load shifting, peak shaving) [3]. Among the different strategies aimed at increasing grid stability arises demand response (DR). DR programs are designed to control power demand through different mechanisms that can be classified as i) price-based mechanisms, which aim to encourage consumption in specific periods of the day by reducing tariffs, and ii) event-based mechanisms, such as load curtailment, which are used to preserve network reliability. However, the adoption of price-based programs in some circumstances could be a double-edged sword, causing new undesirable peaks of demand during periods with low electricity prices [4]. In this framework, building energy management should leverage automated algorithms capable to adapt to a changing environment and to learn from user's behaviour and historical building-related data to optimise, coordinate and control the different actors of the smart grids (e.g., producers, service providers, consumers) [5]. A novel approach that aims to exploit the benefits of DR programs while avoiding peak rebounds is represented by the coordinated building energy management [6]. This concept arises from the necessity to manage the aggregated power demand of cluster of buildings with the aim of optimizing its energy demand shape while considering at single building level user's needs, renewable energy production and the diversity of consumption patterns and energy systems. 1.1 Related works and contributions Coordinated energy management of buildings can be addressed with a centralised or decentralised approach to the control task, where a centralised controller is assumed to have all the information about the current state of the entire cluster of buildings considered, while decentralised controllers act at single building level [7]. In Ref. [8] the authors discussed the advantages of competition, coordination and peer-to-peer transaction to achieve global objectives (e.g., cost minimization, peak shaving), demonstrating the greater impact of such approach with respect to individual management. In the literature, most of the papers assessing the effectiveness of a coordinated approach in the energy environment, were mainly devoted to electric vehicle charging strategies for providing DR services [9] or shaving peaks [10], and to schedulable appliances for load shifting [11]. Few efforts have been devoted to the coordination at building cluster level of heating ventilation and air conditioning systems (HVACs), which typically represent one of the largest energy end-uses in buildings [12]. This mainly because, with respect to other applications, the management of HVAC systems (including storage systems) is highly influenced by weather conditions, occupant behaviour, comfort requirements, and building features, that highly increase the complexity of the control problem. Advanced optimal control of HVAC systems at single building level has been widely analysed in the literature, with predictive based control [13,14]. Among these techniques, model predictive control (MPC) stands out for its ability to optimise complex systems, exploiting a dynamic model to predict building behaviour. However, it requires a detailed model, which enable its application especially at single building scale and rarely at cluster level [15], neglecting the potential benefits of a coordinated approach. Moreover, model-based approaches are not always effective for real-life implementation at large scale, due to the evolution of the environment (e.g., retrofits, PV integration), and the computational cost associated to the modelling of a cluster of buildings, that can constrain the control scalability. In this context many researchers are investigating the use of reinforcement learning (RL) as a valuable control approach in buildings. RL, in spite of its adaptive and potentially model-free nature, fits the needs for an effective implementation of energy management in cluster of buildings [16]. RL agent directly learns an optimal control policy through a trial-and-error interaction with the environment, with adaptability potential in case of changes in the environment [17] such as retrofits [18] or demand response programs [19]. In [20] the potential of RL demand response on the market was assessed, while in Ref. [21] an incentive-based demand response based on RL was proposed. RL control approach has been used in residential demand response [22] or applied to control the operation of different systems, including heat pumps [23], domestic hot water (DHW) [24] and electric water heater [25]. To provide non-intrusive demand response programs, a lot of attention was devoted to the exploitation of control strategy, for indoor temperature setting [26] and thermal storage operation for heating [27] and DHW energy management [28]. Recently, few studies have started to put emphasis on cooperative or competitive coordination mechanisms [29] to account for demand peak shifting when multiple agents take the same control decisions [30]. The presented literature review shows that most studies in the past years have implemented RL for single-agent systems which act greedily and independently of each other, neglecting the opportunity provided by a coordinated control to flatten the peaks on the grid rather than shifting them. It is not surprising that in the past years the need for multi-agent coordination in DR applications was not fully adopted, as the lack of it does not always lead to shifts in the peak demand or \u201crebound\u201d effects in the daily load profiles. Indeed, in urban settings where the amount of energy storage capacity is not very high, building agents can enable DR without coordination and still be successful in reducing the peaks of electric demand. However, due to the trend towards a wide adoption of electric vehicles and other storage devices such as batteries, this is subject to change in the near future [31,32]. As energy storage devices become more abundant and electrical demand more volatile due to the presence of more renewable energy resources and EV charging stations, properly coordinating all these energy systems in an adaptive manner can be critical without a centralised control or multi-agent cooperation. Nevertheless some pioneering studies have already demonstrated the advantages of a coordinated approach in HVAC systems using advanced control strategies in simulated environment, including heuristic control [33] and reinforcement learning control [34]. This paper explores the opportunity of enhancing demand flexibility for a cluster of buildings by implementing a coordinated energy management, using Deep Reinforcement Learning (DRL) to manage the thermal storages of four buildings equipped with different energy systems. The controller was designed with the objective to flatten the total load profile of the cluster while optimizing energy consumption of each building. For benchmarking purposes, the coordinated energy management is then tested and compared against a manually optimised rule-based controller. On the basis of the literature review the main novelty of the paper can be summarised as follows: \u2022 The paper exploits a single-agent RL centralised controller with a strategy explicitly designed to consider the benefits at multiple levels (i.e., single building, cluster and grid level), against a most common rule-based control strategy that optimise single buildings. \u2022 The paper makes use of a novel simulation environment, CityLearn [35], an OpenAI Gym environment specifically designed to allow RL implementations for the built environment, enhanced to consider a variable electricity price. \u2022 The DRL controller used in this work exploits a state-of-the-art continuous control algorithm i.e., soft actor critic (SAC). The control performances of the agent were deeply analysed to highlights the benefits provided by coordinated energy management. The paper is organised as follows: Section 2 introduces the methods adopted for developing and testing the controllers, including algorithms and simulation environment. Then, Section 3 describes the methodological framework at the basis of the analysis. Section 4 introduces the case study, explaining in detail the energy modelling of the system and the controllers design and training. Section 5 presents the results of the training and deployment phase, while discussion of results is given in Section 6. Eventually, conclusions and future works are presented in Section 6. 2 Methods Reinforcement learning is a branch of machine learning mainly aimed at solving control problems. It combines the advantages of dynamic programming, with a trial-and-error approach. RL uses an agent-based control, where the agent learns through the interaction with the controlled environment. Reinforcement learning can be formalized using a Markov decision process (MDP), a discrete-time stochastic control process [36]. MDP provides a mathematical framework for modelling decision making in situations where outcomes are partly random and partly under the control of an agent. Markov Decision Process are represented using a 4-tuple ( S , A , P , R ) made up of: 1. State space (S) The state describes the environment completely. Here, must be noticed that state is a term used to represent the environment, while the information seen by the agent, that are a mathematical description of the environment, relevant and informative to the decision to be made are called observations. Often, the agent can see only a part of the state, dealing with the so-called Partially Observable Markov Decision Process (POMDP). In this paper, observation space and state space are considered equal. 2. Action space (A) The action is the decision made by the agent on how to control the environment. 3. Transition probability (P) The transition probability P ( s t + 1 = s \u2032 | s t = s , a t = a ) = P : S × A × S \u2032 is the probability that, starting in s and performing action a at the time t, the next state will be s\u2019. MDP satisfies the Markov Property, which states the memoryless of the stochastic process, represented as. P ( s t + 1 = s \u2032 | s t ) = P ( s t + 1 = s \u2032 | s 1 , s 2 , \u2026 , s t ) 4. Reward function (R) The reward function is used to map the immediate reward r with the tuple. S × A × S \u2032 The main goal of the agent is to find the optimal control Policy ( π ) . A control policy is a mapping between states and actions π : S → A , and it has the aim to maximize the cumulative reward over a time horizon. This concept is summarised introducing the expected return G, that represent the cumulative sum of the reward G = ∑ k = 0 ∞ γ k r t + k + 1 . Where γ ∈ [ 0,1 ] is the discount factor for future rewards. An agent employing γ equal to 1 considers future rewards as important as current ones, while an agent with γ equal to 0 assign higher values to states that lead to high immediate rewards. For sake of clarity, an example with an energy system is provided in Fig. 1 where the controller (agent) is connected to a heat pump and a thermal storage to satisfy the building cooling demand over the summer season. The controller has the role of minimizing electricity cost (reward function) charging and discharging (actions) the storage to satisfy the building demand. The reward function can be minimized charging the storage during night hours, when efficiency is higher and electricity price is low (states). The exploitation of these information allows the controller to find the optimal policy. The control problem can be defined by two closely related value function, namely the state-value function v π ( s ) and action-value function q π ( s , a ) , shown below: (1) v π ( s ) = E [ r t + 1 + γ v π ( s \u2032 ) | S t = s , S t + 1 = s \u2032 ] (2) q π ( s , a ) = E [ r t + 1 + γ q π ( s \u2032 , a \u2032 ) | S t = s , A t = a ] These functions are used to show the expected return of a control policy π at a state or a {state, action} tuple. One of the advantages of the RL is that in its model-free version, the values of v π and q π are directly learned from experience, without explicitly calculating the transition probabilities. RL algorithms can improve their policy in two different ways: i) on-policy methods, which attempt to evaluate the policy that is used to make decisions and ii) off-policy methods, which evaluate a policy different from that used to generate the data. Among RL algorithms, the most used one, due to its simplicity, is Q-learning [37]. In Q-learning transitions can be represented with a tabular approach that stores the state-action values (Q-values) that are updated as follows: (3) Q s , a ← Q s , a + μ Q ( s , a ) + γ m a x a ( Q s ' , a ' − Q ( s , a ) ) where s is the next state and μ ∈ [ 0,1 ] is the learning rate, which determines to what degree new knowledge overrides old knowledge. When μ is equal to 1 new knowledge completely substitutes old knowledge, while for μ set equal 0 no learning happens. One of the advantages of the off-policy learning lies in their ability to learn from other agent, opening the door to the experience replay, in which previous information is re-used to enhance the current policy. Despite these advantages, a tabular representation of real-world problem may be unfeasible, due to large state and action spaces that needs to be stored. 2.1 Soft actor critic The combination of RL and high-capacity function approximators such as Deep Neural Networks (DNN) demonstrated to overcome computational problem renewing the interest for the RL topic and promoting its extension to complex problems [38]. Among Deep Reinforcement Learning (DRL) algorithms, an actor-critic method was selected in this paper for its ability to combine advantages of both value-based and policy-based methods. The main idea behind actor-critic is to split the problem using two deep artificial neural networks (see Fig. 2). The actor maps the current state to the action that it estimates to be optimal (policy-based), while the critic evaluates the actions by computing the value function (value-based). The key components of soft actor-critic [39] are: \u2022 An actor-critic architecture, used to map policy and value function with different networks; \u2022 The off-policy formulation, that allows reusing previously collected data, stored in a replay buffer ( D ) to increase data efficiency; \u2022 The entropy maximization formulation, that helps stabilize the algorithm and the exploration SAC learns three different functions: (i) the actor (mapped through the policy function with parameters φ ), (ii) the critic (mapped with the soft Q-function with parameters θ ) and (iii) the value function V , defined as: (4) V ( s t ) = E a t ∼ π [ Q ( s t , a t ) − α log π ( a t | s t ) ] = E a t ∼ π [ Q ( s t , a t ) ] + α E a t ∼ π [ log π ( a t | s t ) ] = E a t ∼ π [ Q ( s t , a t ) ] + α H Differently from standard RL algorithm, maximum entropy reinforcement learning optimizes policies to maximize both the expected return and the expected entropy of the policy as follows: (5) π ∗ = arg max π φ ∑ t = 0 T E ( s t , a t ) ∼ ρ π [ r ( s t , a t ) + α H ( π φ ( · | s t ) ) ] where ( s t , a t ) ∼ ρ π is a state-action pair sampled from the agent's policy, and r ( s t , a t ) is the reward for a given state-action pair. Due to the entropy term, H , the agent attempts to maximize the returns while behaving as randomly as possible. The final policy used in the evaluation of the algorithm can be made deterministic by selecting the expected value of the policy as the final action. The parameters of the critic networks are updated by minimizing the expected error J Q , which is given by: (6) J Q ( θ ) = E ( S t , a t ) ∼ D [ 1 2 ( Q θ ( s t , a t ) − ( r ( s t , a t ) + γ E s t + 1 ∼ p [ V θ ¯ ( s t + 1 ) ] ) ) 2 ] where the value function is implicitly parameterized through the soft Q-function parameters in Equation (6). On the other hand, the temperature parameter α determines the relative importance of the entropy term against the reward, and thus controls the stochasticity of the optimal policy. A high value of the temperature parameters may lead to a uniform behaviour, while a low value of the temperature parameter will only maximize the reward. SAC is highly influenced by the temperature parameter, that needs to be properly tuned to achieve good performances. Unfortunately, tuning this hyper-parameter is very hard, since entropy can vary both across actions and over time, as the policy becomes better. To overcome these problems, in this study a recent version of the SAC that employs alpha automatic optimization [40] was used. To ease the comprehension, the main algorithm logics are summarised in Table 1 . 2.2 The CityLearn simulation environment In order to train, implement and test the developed RL controller, a new simulation environment, CityLearn [35,41], was used. The environment is specifically built to enable training and evaluation of reinforcement learning models for demand response in smart cities through energy simulations with an hourly control timestep. Moreover, the simulation environment makes it possible to control heterogeneous cluster of buildings and offers the possibility to easily implement RL-algorithm to manage cooling and domestic hot water storages with centralised or distributed controllers. The aim is to facilitate and standardize the evaluation of RL agents to enable the comparison of different algorithms. CityLearn is well suited for the easy implementation of both centralised and decentralised multi-agent RL control systems, as well as for the implementation of single-agent independent RL controllers. The environment allows to control multiple thermal energy storage devices within the building, including water tanks of domestic hot water, cooling and heating systems. The energy demand for space cooling is satisfied by air-to-water heat pumps, and the heating energy is supplied by electric heaters. Furthermore, it also accounts for photovoltaic generation and allows the user to select different performance metrics related to load-shaping. Additionally, users can select up to 28 different state variables which include current weather conditions and forecasts, or the state of charge of the different energy storage devices. 3 Methodological framework The section reports the methodological framework adopted in the present paper, with the aim of describing each stage of the process, including the development, training and deployment of DRL control agent. The framework unfolds over four different stages, as shown in Fig. 3 . RL Problem formulation: the first stage of the framework was aimed at defining the main components of the reinforcement learning control problem. The action-space includes all the possible control actions that can be taken by the control agent. Considering that the aim of the paper is to coordinate multiple buildings, the action space includes multiple actions, 2 for each building. The state-space is a set of variables related to the controlled environment which are fed to the agent to learn the optimal control policy which maximizes the reward function. Eventually, a reward function was formulated to describe the performance of the control agent with respect to control objectives. Training phase: in the second stage of the process the DRL agent was trained. As previously introduced in Section 2, DRL agent is characterised by many hyperparameters which require appropriate tuning. In order to enhance the reproducibility of the work, a description about the setting of hyperparameters was provided. The training process was implemented in an off-line fashion using the same training episode (i.e. a time period representative of the specific control problem) multiple times in order to refine agent's control policy. Training Results: the agent was firstly tested with the same climate used for training, with the aim to specifically analyse the effect of the learned policy on multiple levels, including single buildings, cluster and then on the grid. The performances of the DRL controller were analysed against an RBC controller, by evaluating various key performances indicators (KPI), specifically tailored for each scale of analysis (i.e., single building level or cluster of buildings level). Deployment: to evaluate the robustness of the trained agent, the algorithm was deployed in four different climates, which also lead to different building thermal-related loads. The agent was tested through a static deployment in one episode and compared with the RBC also during this stage. 4 Case study The DRL algorithm described in Section 2.1 was used to control a complex environment that consists of a cluster of 4 commercial buildings, whose load profiles have been assessed through dynamic simulations in EnergyPlus. Each building is equipped with a heat pump, to satisfy cooling demand, an electric heater to meet DHW demand and both cooling and DHW storages. For each building, the heat pump is sized to always match cooling demand, considering a safety factor to account for reduced capacity in case of low external temperature. On the other hand, storages capacity is three times the maximum hourly demand for both cooling and DHW loads [42]. Moreover, two out of four buildings are equipped with photovoltaic systems. Table 2 reports for each building their geometrical features and the main design details of the energy systems. The energy systems are managed by a single-agent centralised DRL controller, which aims to reduce costs and to flatten the aggregated load profile of the cluster reducing peaks. 4.1 Description of the cluster of buildings The aggregated load pattern of the cluster can result from heterogeneous single building profiles, characterised by both very different intensities and shape. Fig. 4 shows the electrical consumption patterns for the first three days analysed. On the left part, it is displayed the load profile for each of the 4 buildings included in the cluster analysed, while on the right part it is showed the total profile. In particular, Building 1 and 4 are characterised by homogeneous daily load profiles, with a peak in the morning, while Building 2 has sudden peak during the evening and Building 3 may have more than a peak per day. As a result, considering that the load profile of the cluster is highly influenced by the single building energy behaviour, to achieve an optimal control at cluster level a coordination at both low and high level is needed. Moreover, the right part of Fig. 4 shows the breakdown of the cluster electrical demand for cooling, DHW and appliances and the PV production in Buildings 2 and 3. This representation is useful to underline the electrical demand for cooling and DHW, on which the RL controller can act to enhance cluster flexibility. In fact, since electrical cooling demand represents a large part of the cluster load, the analysis focused only to the summer period (1st June to 31st August). 4.2 Energy systems and control objectives The control problem consists in the optimal management (i.e., charging and discharging process) of the 8 storages to satisfy cooling and DHW demand of the four buildings. The goal of the control policy is to minimize costs and to avoid peaks at cluster level. The most influencing factor to take into account are the energy cost and the heat pump efficiency. In particular, the energy cost considered in the paper is based on the Austin (Texas) electricity tariffs [43]. In detail, were assumed an off-peak electricity tariff during night-time period 20:00\u20137:00 (0.03025 $/kWh) and an on-peak electricity tariff during daytime period 7:00\u201320:00 (0.06605 $/kWh). On the other hand, the efficiency of the heat pump was modified from CityLearn original implementation to consider partial load ratio (PLR) and the effect of external temperature not only on the coefficient of performance (COP), but also on the design capacity. A description of the relation among COP, design capacity (DC) and external temperature was defined according to real data sheet of heat pumps. Moreover, the heat pump operation at part load condition was modelled according to UNI EN 14825 [44]. Eventually, COP was evaluated according to Equation (7). The external temperature rise has a twofold effect, firstly it reduces COP (increasing electricity consumption) and secondly it increases the maximum cooling power deliverable by the heat pump. Moreover, heat pump efficiency is influenced not only by external variables, but also from controller actions affecting the cooling load. Finally, the fraction in Equation (7) accounts for part load ratio and intermitting operation of the heat pump. (7) C O P T , P L R = C O P T T i ∗ Q c o o l i n g a c t i o n D C T i 0.9 ∗ Q c o o l i n g a c t i o n D C T i + 0.1 4.3 Baseline rule-based control The effectiveness of the DRL controller was assessed through a comparison with a manually optimised rule-based controller. In the baseline strategy, both cooling and DHW storages are charged during the night period, when electricity price is lower and heat pumps can take advantage, in terms of efficiency, from lower temperature (i.e., higher values of COP). To limit peak demand, the charging process was spread over the whole night period, while the discharging process is homogeneous throughout the day. 4.4 Design of the deep reinforcement learning controller The DRL control algorithm described in section 2.1 was trained and tested in the CityLearn environment, including constraints related to the maximum charging and discharging rate of the storages and ensuring that cooling and DHW demands are always met. In the next sub-sections, action space design is presented, together with the description of the reward function design and of the state-space, to properly characterize the DRL control problem. 4.4.1 Action-space design The analysed case study deals with multiple buildings, each one with two storages that could be controlled. Therefore, the two actions have different targets: the first one is related to the operation of the cold storage, that can be charged by the heat pump to store energy or discharged to meet building cooling load; the second action is related to the operation of the hot storage, that can be charged by an electric heater or discharged to meet DHW demand. Since each building has different storages and heat pump capacities, the action space makes use of normalized values. In particular, the controller uses actions between −1 and 1, where −1 represents the full storage discharge in the control timestep and 1 represents the full storage charge. However, considering that a full charge/discharge in a single timestep is not feasible, in this work, the action space was constrained into the interval [-0.33,0.33], imposing therefore a complete charge or discharge time of 3 hours according to Ref. [42]. In conclusion, at each control time step the agent selects 8 values (one for each storage) to charge or discharge the energy storage devices. This information is used to select the best actions that maximize the reward function. 4.4.2 State-space design The states represent the environment as it is observed by the control agent. At each control timestep, the agent choses among the available actions according to the values assumed by the states. In particular, states should be easy to measure in real-world implementation, and they should be selected according to the meaningfulness of the information they provide for predicting the reward function. The variables used for representing the state-space are reported in Table 3 and in the following further described. The variables used for representing the state-space can be categorised as weather, district and building states. Weather states, such as outdoor temperature and direct solar radiation, were selected because of their strong influence on the magnitude of building loads for space cooling. Additionally, their 6 hours-ahead values were used to provide useful information about temperature and solar radiation changes and enhancing the predictive capabilities of the controller. District states includes variables that assume the same value for all the buildings over time, such as hour of day, electricity price, forecast of the electricity price and weather conditions. Building states include variables related to the electricity production (photovoltaic system) and consumption of the buildings (non-shiftable load). These states are specific of the single building, that can have different energy systems (PV) or trend of consumption. Additionally, heat pump efficiency, cooling and domestic hot water state of charge were included. Fig. 5 summarizes the states and action space interaction selected in this work. The central controller receives as states high-level information such as weather conditions and electrical demand of the whole cluster of buildings. Moreover, it also receives low-level information from each building such as appliances loads and energy systems information. 4.4.3 Reward design The reward function plays a key role for defining how the agent assess the quality of the control policy during the learning phase. It was conceived to allow the agent learning a control policy during training period which minimize the energy cost at cluster level and reduce the demand peaks. In particular, the reward was formulated as follows: (8) R = ∑ i = 1 n e i 2 ∗ c e l i where e i 2 is the squared energy consumption of the i-th building, while c e l is the electricity tariff in that time step. To obtain a more uniform load profile at cluster level, the controller tries to minimize the sum of the squared consumption of each building for each time step. This formulation was chosen since the squared minimization approach tries to flatten the profile rather than shifting the consumption to low electricity tariff, avoiding simultaneous charge (and discharge) of storages. In order to consider the economic aspect of the problem, the electricity tariff in the specific timestep was included. Moreover, due to the relation between consumed energy and costs, the controller tries to minimize energy consumption, increasing system efficiency. The design of the reward function highly influences DRL performances, searching compromises between energy savings and grid stability. 4.5 Training and deployment The subsection describes the setting of hyperparameters during the training phase. Then, a description of the different climatic conditions analysed for the deployment phase is presented. 4.5.1 Training phase The DRL framework is characterised by several hyperparameters that strongly affect the behaviour of the control agent. The aim of this subsection is to illustrate the hyperparameters set during the formulation of the control problem. For the sake of reproducibility, Table 4 reports the value of the main hyperparameters. In particular, the two hyperparameters that mostly influence the results are the number of training episodes and the temperature α . Differently from many other control fields, the number of training episodes is relatively low. This is justified by the problem nature, in which actions are constrained by energy balance, finding the optimal policy quickly. Furthermore, as explained in 2.1 α highly influences the outcome of the policy. While in certain application α could be set a-priori as a constant, in this study a version of SAC algorithm that optimizes the temperature parameter was adopted. As a reference, both starting and final value of temperature and entropy coefficient are provided below. As previously stated in Section 4.1, a training episode includes 3 months, from 1st of June to 31st of August (2208 control steps). The weather file used in this work for the training phase is referred to the climatic zone of the USA named 2A, Hot-Humid. 4.5.2 Deployment phase In the last phase of the process the agent was deployed for the same cluster of buildings but considering four different climates to assess the adaptability capabilities of the learned control policy to different configurations related to the controlled environment. Each agent was deployed for one episode including the period between 1st June and 31st August. The first climate is 2A Hot-humid: this climate is the same on which the agent was trained on. This scenario is compared to the baseline RBC to assess the effectiveness of the trained agent. Then, the adaptability is tested with the deployment of the agent in warm-humid climate (3A), mixed-humid climate (4A) and cold-humid climate (5A). The thermal related load patterns changed according to climatic conditions. Fig. 6 shows the patterns of outdoor air temperature in the four climates selected, highlighting how the external temperature is strongly different in amplitude and distribution. In particular, the Climate 2A, the one on which the agent is trained on, has a distribution with a narrow amplitude, with a mean temperature of about 27.5 °C. On the other hand, climates 3A and 4A have a different temperature distribution, but the same mean value of about 25.5 °C. Lastly, Climate 5A is the coldest climate considered, with a mean temperature of 22.5 °C and a more uniform distribution with respect to Climate 2A. 5 Results The section reports the results of the implemented framework. Firstly, a brief evolution of the control policy is presented. Then, a comparison between the two control strategies by analysing the results with a focus at single building scale and cluster level is provided. Eventually, the analysis focuses on the load curve and on the role of storage devices for the grid stability. To this purpose, a further comparison is performed computing the load duration curve for the cluster of buildings also considering the case without storages. Furthermore, to summarise the performance of the two control strategies a numeric comparison is provided. 5.1 Training results The subsection presents the evolution of the DRL control strategy over the training period and compare it with the RBC. In particular, Table 5 reports the evolution of the reward function over the training period, together with the normalized values of cost and peak compared to the RBC (where a value smaller than 1 suggest a better performance of the DRL). The first episode is used to store states and actions and after that it can be observed a quick convergence of both cost and peak term, that stabilize after episode 4. To prove this point, a sensitivity analysis was performed on the number of training episodes, spanning from 5 to 20, which showed little to no improvements with more than 5 episodes. 5.1.1 Comparison of controllers at single building level Fig. 7 shows the charging and discharging patterns of both storages determined by the RBC and RL controller. In particular, the figure shows the days related to the maximum peak demand of the RBC, to highlight the difference with the DRL control strategy. Moreover, the figure shows the relation between the control process and the forcing variables (i.e., external temperature and the electricity price). To allow an easier comparison, all quantities were normalized on maximum values between 0 and 1, where the maximum temperature is 35 °C. It can be observed that RBC charges both storages mutually at a lower rate, exploiting off-peak tariff and the highest COP of the heat pump. However, to exploit off-peak tariff and avoid sudden peaks, RBC control strategy leads the heat pump to work at part load. Moreover, it has no information about outdoor temperature evolution and so on the efficiency of the heat pump. On the other hand, the DRL controller learns to charge the two storages as soon as the electricity price and the temperature tend to decrease. However, the main difference is related to the discharge pattern, since DHW is used as soon as needed to reduce electricity demand, while cooling storage is discharged when external temperature is high, avoiding using heat pump when the COP is low. 5.1.2 Comparison of controllers at cluster level Fig. 8 shows a comparison between the aggregate electrical load obtained through the implementation of the RBC controller in the simulation environment, in which each building is optimised to minimize its own costs, and the DRL controller, that optimises cluster behaviour. In particular, Fig. 8 shows three days during which the RBC determined the occurrence of load peak at cluster level that could cause stress on the grid. As shown in Fig. 8, the DRL controller is capable to better flat the aggregate load profile and to diversify the charge time of the storages among the buildings in the cluster. As a result, cluster profile is more homogeneous and, in this particular situation, a great reduction can be observed looking at the two peaks (hour 605 and 655). This result does not represent the average performance of the DRL controller but highlights the potential of buildings coordination in increasing grid stability in specific situations. To understand how these results have been achieved, Fig. 9 shows the average evolution of the state of charge related to the storage device. As can be seen, the cooling and DHW storages are charged during the night homogeneously, as the RBC. The main differences is related to the storage discharges. In particular, as soon as the electricity price increases, the DHW storages start the discharge phase almost simultaneously, since they are only influenced by the electricity price. On the other hand, the agent learned the dependency between external temperature and heat pump COP (the higher the temperature lower the COP). As a result, the optimal policy discharges the cooling storage during the hottest hour, maximising heat pump efficiency. 5.1.3 Comparison of controllers at grid level To highlight the flexibility provided by the introduced framework in terms of load profile flattening, the load duration curves resulting from the application of RBC and DRL control and from the case without storages were compared in Fig. 10 . As can be seen, the baseload increases with both RBC and DRL, underlying the importance of the storages in increasing buildings energy flexibility. However, RBC leads to the creation of new undesirable peaks (as shown inside the \u201czoom\u201d area) while DRL algorithm, thanks to the coordinated approach, is able to reduce them. Eventually, to provide a comprehensive analysis of the results different KPIs are introduced to compare the performances of the control strategies. In particular, the KPIs chosen are the total energy consumption, the total energy cost, maximum peak, average daily peak, peak-to-average ratio (PAR) and daily peak-to-average ratio. These KPIs have been chosen to summarise the advantages of DRL control strategies at cluster level (energy consumption, costs, maximum peak and peak-to-average ratio) and the effect on the grid (average daily peak and daily peak-to-average ratio). Table 6 shows the performance of the two control strategies with respect to the main criteria selected. In order to allow an easier comparison, the values of KPIs are normalized on the RBC values. DRL outperforms the manually optimised RBC. In particular, DRL controller exploits the storage charge and discharge to increase heat pump efficiency, while slightly reduces electricity cost. Nevertheless, it must be noticed that the manually optimised RBC already took full advantage from off-peak electricity tariff, therefore the economic improvement of DRL over RBC are closely related to the more efficient use of energy. On the other hand, the coordinated approach showed good results at cluster level, reducing maximum peak of 12% and average daily peak of 10%. Moreover, the PAR and average daily PAR reduction of 4 and 6% respectively highlight the benefits of building coordination that can be translated into a more homogenous energy consumption. Furthermore, the advantage provided by the increased grid stability could be translated into reduced electricity tariff, with additional advantages for users. The DRL approach is able to reduce peaks of 12% with respect to the RBC and 8% with respect to the no storage case, but more importantly the peak demand rapidly decreases, resulting in a more homogeneous profile. 5.2 Deployment of deep reinforcement learning controller in different climatic conditions The last section analyses the deployment of the agent in the other 3 climates described in 4.5.2. After the training and deployment of the agent in Climate 2A, a simulation of 3 months was run using the trained agent with climate 3A, 4A and 5A. To evaluate the performances of the agent in the new climates, as done before, the DRL controller was compared with the RBC controller, analysing the previously introduced KPIs and normalizing them on the RBC values. Fig. 11 summarizes the results of the deployment phase, where 100 represents the RBC performance. It can be seen how the controller is able, also in Climate 3A, to flatten the load pattern. This is highlighted by the peak and PAR reduction, looking both at maximum and daily values. These results are achieved consuming slightly more electricity with respect to the RBC, but with the same energy cost. Looking at Climate 4A, it can be noticed a peak reduction of around 5%, but with negligible effect on the PAR. On the other hand, looking at the average daily values, it can be noticed that the daily PAR is 10% lower with respect to the RBC, highlighting the more homogeneous consumption. Eventually, analysing climate 5A, the coldest one, it can be seen how the cost slightly increases, around 3%, however there are great improvement at district level, with a peak reduction of 11% and an average daily PAR 22% lower with respect to the RBC case. 6 Discussion The presented paper aims to exploit model-free DRL controller to coordinate the energy management of a cluster of buildings. The analysis is performed with the CityLearn environment, an openAI gym environment where a detailed representation of the heat pump and a variable electricity price have been implemented. The DRL controller was designed to act on the DHW and cold storages of 4 buildings to optimise both energy costs and peak demand at cluster level. The control problem analysed involved renewable energy sources, variable electricity price and building coordination. To compare the DRL performances and underline the effect of a coordinated energy management versus a single building optimization, a manually optimised RBC controller baseline was introduced. Despite the complex environment, the DRL controller found the optimal policy to exploit environment behaviour, consuming energy more efficiently and charging and discharging storages to optimise the cluster load profile. Additionally, due to the problem nature, the solution was found with a very short training period of 5 episodes. The analysis highlights how the real-world implementation could be done with a relatively small amount of data for the training, proving the versatility of the proposed approach. However, to study the interaction among states, actions and rewards it is still necessary a simulation environment when dealing with a district scale. Looking at the problem formulation, forecast information on electricity price and weather helped to rapidly find an optimal policy, highlighting how important is the proper design of the state-space. Moreover, the design of the reward function plays a key role for the DRL controller behaviour. It is therefore necessary to find an optimal trade-off between the advantages of single users and cluster that are bounded to the case study. During the work, the adoption of the square minimization was found to be effective at both single building and cluster level, proving to be scalable independently by the number of buildings. To test its adaptability, the controller was deployed considering four different climatic conditions. The results highlight that the controller flattened the cluster load profile, almost independently from the external conditions, while the economic performances varied with the different cases. Even with the same (or slightly higher) electricity costs, the services provided to the grid, such as peak reduction and load shaping, justify the adoption of the DRL controller with respect to the RBC. Eventually, the strength of the proposed approach is not only the mere improvement of energy performances, but the opportunity provided by its adaptive nature to account the cluster environment evolution. In fact, a large environment may involve rapid changes, such as consumption pattern modification and demand response programs. 7 Conclusion and future perspectives The present paper discussed the design and application of a DRL controller with the aim to coordinate multiple buildings in a novel simulation environment. The problem was formulated to provide benefits for users and grid, while a specific analysis to assess how this performance can be achieved was performed. A fundamental aspect is related to the development of proper state space and reward function. The effectiveness of the reward function was proven by the deployment phase, in which the DRL agent exploited the policy learned during training in different scenarios. In particular, the developed controller has shown a cost reduction of around 4% with peak reduction of 12%. Moreover, daily peaks were reduced on average by 8%, decreasing daily PAR with values that varies from 6 to 22%. The research has shown how the single-agent centralised DRL controller was able to coordinate different buildings, increasing grid stability and reducing energy costs. Future works will be focused on: \u2022 The implementation and comparison between the proposed centralised controller and a decentralised DRL approach, in which the controllers can cooperate or compete. The opportunity provided by a multi-agent configuration are several. Firstly, specific reward function can be designed and tailored according to renewable electricity production; secondly for each building the relative importance among the objectives as reduce their cost or flatten the profile can be decided, with the aim of facilitating the participation to demand response programs. Moreover, in multi-agent configuration the control policy could be potentially transferred in similar buildings. \u2022 Enhancing the simulation environment for considering variations of the indoor thermo-hygrometric conditions in buildings. In fact, in this study, building loads are evaluated with EnergyPlus software considering a fixed internal temperature. To provide a more realistic implementation, CityLearn will be modified using black-box models to analyse the relation among cooling loads provided to the buildings and the internal temperature. This has the twofold advantages to exploit building thermal mass and consider thermal comfort for users. \u2022 Implementing dynamic electricity price tariff and demand response programs to study the interaction of buildings with the grid. The use of a dynamic electricity price tariff can further increase the benefit provided by a more refined controller with respect to reactive controller. Furthermore, online implementation of adaptive controller could provide an efficient management strategy not only to the DR event, but also to avoid the rebound effect, thanks to the coordinated approach. A major effort to build upon this research work will be then focused on fully addressing all the mentioned challenges that are behind the next generation of \u201csmart districts\u201d in smart cities. Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Nomenclature Symbols A Action space a Action c e l Electricity price D Replay Buffer e Energy consumption G Return P Transition Probabilities q Action-value r Reward S State space s State v State-value α Temperature parameter γ Discount factor θ Soft-Q network parameters λ Learning rate φ Policy network parameters τ Decay rate H Shannon Entropy of the policy π Policy π∗ Optimal Policy Abbreviations COP Coefficient of Performance DC Design Capacity DHW Domestic Hot Water DNN Deep Neural Network DR Demand Response DRL Deep Reinforcement Learning EMS Energy Management System HVAC Heating, Ventilation and Air Conditioning KPI Key Performance Indicator MDP Markov Decision Process MPC Model Predictive Control PAR Peak-to-average ratio PLR Partial-load ratio POMDP Partially Observable Markov Decision Process PV Photovoltaic RBC Rule Base Control RL Reinforcement Learning SAC Soft Actor-Critic SOC State-of-Charge Credit author statement Giuseppe Pinto: Conceptualization, Methodology, Investigation, Visualization, Formal analysis, Software, Writing \u2013 original draft; Marco Savino Piscitelli: Methodology, Formal analysis, Writing \u2013 review & editing; José Ramón Vázquez-Canteli: Conceptualization, Data curation, Methodology, Writing \u2013 review & editing; Zoltán Nagy: Conceptualization, Methodology, Writing \u2013 review & editing; Alfonso Capozzoli: Conceptualization, Methodology, Formal analysis, Writing \u2013 review & editing, Supervision, Project administration References [1] B.D. Leibowicz C.M. Lanham M.T. Brozynski J.R. Vázquez-Canteli N. Castillo Z. Nagy Optimal decarbonization pathways for urban residential building energy services Appl Energy 230 2018 1311 1325 10.1016/j.apenergy.2018.09.046 Leibowicz BD, Lanham CM, Brozynski MT, Vazquez-Canteli JR, Castillo N, Nagy Z. Optimal decarbonization pathways for urban residential building energy services. Appl Energy 2018;230:1311-1325. https://doi.org/10.1016/j.apenergy.2018.09.046. [2] International Energy Agency IEA IEA statistics: world energy statistics and balances 2014 International Energy Agency IEA. IEA statistics: world energy statistics and balances. 2014. [3] S.Ø. Jensen A. Marszal-Pomianowska R. Lollini W. Pasut A. Knotzer P. Engelmann IEA EBC annex 67 energy flexible buildings Energy Build 155 2017 25 34 10.1016/j.enbuild.2017.08.044 Jensen SOE, Marszal-Pomianowska A, Lollini R, Pasut W, Knotzer A, Engelmann P, et al. IEA EBC annex 67 energy flexible buildings. Energy Build 2017;155:25-34. https://doi.org/https://doi.org/10.1016/j.enbuild.2017.08.044. [4] L. Shen Z. Li Y. Sun Performance evaluation of conventional demand response at building-group-level under different electricity pricings Energy Build 128 2016 143 154 10.1016/j.enbuild.2016.06.082 Shen L, Li Z, Sun Y. Performance evaluation of conventional demand response at building-group-level under different electricity pricings. Energy Build 2016;128:143-154. https://doi.org/https://doi.org/10.1016/j.enbuild.2016.06.082. [5] E. O'Dwyer I. Pan S. Acha N. Shah Smart energy systems for sustainable smart cities: current developments, trends and future directions Appl Energy 237 2019 581 597 10.1016/j.apenergy.2019.01.024 O'Dwyer E, Pan I, Acha S, Shah N. Smart energy systems for sustainable smart cities: current developments, trends and future directions. Appl Energy 2019;237:581-597. https://doi.org/10.1016/j.apenergy.2019.01.024. [6] M. Hu F. Xiao S. Wang Neighborhood-level coordination and negotiation techniques for managing demand-side flexibility in residential microgrids Renew Sustain Energy Rev 135 2021 110248 10.1016/j.rser.2020.110248 Hu M, Xiao F, Wang S. Neighborhood-level coordination and negotiation techniques for managing demand-side flexibility in residential microgrids. Renew Sustain Energy Rev 2021;135:110248. https://doi.org/10.1016/j.rser.2020.110248. [7] L. Fiorini M. Aiello Energy management for user's thermal and power needs: a survey Energy Rep 5 2019 1048 1076 10.1016/j.egyr.2019.08.003 Fiorini L, Aiello M. Energy management for user's thermal and power needs: a survey. Energy Rep 2019;5:1048-1076. https://doi.org/10.1016/j.egyr.2019.08.003. [8] J. Guerrero D. Gebbran S. Mhanna A.C. Chapman G. Verbič Towards a transactive energy system for integration of distributed energy resources: home energy management, distributed optimal power flow, and peer-to-peer energy trading Renew Sustain Energy Rev 132 2020 10.1016/j.rser.2020.110000 Guerrero J, Gebbran D, Mhanna S, Chapman AC, Verbic G. Towards a transactive energy system for integration of distributed energy resources: home energy management, distributed optimal power flow, and peer-to-peer energy trading. Renew Sustain Energy Rev 2020;132. https://doi.org/10.1016/j.rser.2020.110000. [9] R. Verschae H. Kawashima T. Kato T. Matsuyama Coordinated energy management for inter-community imbalance minimization Renew Energy 87 2016 922 935 10.1016/j.renene.2015.07.039 Verschae R, Kawashima H, Kato T, Matsuyama T. Coordinated energy management for inter-community imbalance minimization. Renew Energy 2016;87:922-935. https://doi.org/10.1016/j.renene.2015.07.039. [10] K. Zhou L. Cheng L. Wen X. Lu T. Ding A coordinated charging scheduling method for electric vehicles considering different charging demands Energy 213 2020 118882 10.1016/j.energy.2020.118882 Zhou K, Cheng L, Wen L, Lu X, Ding T. A coordinated charging scheduling method for electric vehicles considering different charging demands. Energy 2020;213:118882. https://doi.org/10.1016/j.energy.2020.118882. [11] T. Chang M. Alizadeh A. Scaglione Real-time power balancing via decentralized coordinated home energy scheduling IEEE Trans Smart Grid 4 2013 1490 1504 10.1109/TSG.2013.2250532 Chang T, Alizadeh M, Scaglione A. Real-time power balancing via decentralized coordinated home energy scheduling. IEEE Trans Smart Grid 2013;4:1490-1504. https://doi.org/10.1109/TSG.2013.2250532. [12] G. Martinopoulos K.T. Papakostas A.M. Papadopoulos A comparative review of heating systems in EU countries, based on efficiency and fuel cost Renew Sustain Energy Rev 90 2018 687 699 10.1016/j.rser.2018.03.060 Martinopoulos G, Papakostas KT, Papadopoulos AM. A comparative review of heating systems in EU countries, based on efficiency and fuel cost. Renew Sustain Energy Rev 2018;90:687-699. https://doi.org/10.1016/j.rser.2018.03.060. [13] A. Afram F. Janabi-Sharifi Theory and applications of HVAC control systems - a review of model predictive control (MPC) Build Environ 72 2014 343 355 10.1016/j.buildenv.2013.11.016 Afram A, Janabi-Sharifi F. Theory and applications of HVAC control systems - a review of model predictive control (MPC). Build Environ 2014;72:343-355. https://doi.org/10.1016/j.buildenv.2013.11.016. [14] G. Serale M. Fiorentini A. Capozzoli D. Bernardini A. Bemporad Model Predictive Control (MPC) for enhancing building and HVAC system energy efficiency: problem formulation, applications and opportunities Energies 11 2018 10.3390/en11030631 Serale G, Fiorentini M, Capozzoli A, Bernardini D, Bemporad A. Model Predictive Control (MPC) for enhancing building and HVAC system energy efficiency: problem formulation, applications and opportunities. Energies 2018;11. https://doi.org/10.3390/en11030631. [15] S. Gonzato J. Chimento E. O'Dwyer G. Bustos-Turu S. Acha N. Shah Hierarchical price coordination of heat pumps in a building network controlled using model predictive control Energy Build 202 2019 109421 10.1016/j.enbuild.2019.109421 Gonzato S, Chimento J, O'Dwyer E, Bustos-Turu G, Acha S, Shah N. Hierarchical price coordination of heat pumps in a building network controlled using model predictive control. Energy Build 2019;202:109421. https://doi.org/10.1016/j.enbuild.2019.109421. [16] Z. Wang T. Hong Reinforcement learning for building controls: the opportunities and challenges Appl Energy 269 2020 115036 10.1016/j.apenergy.2020.115036 Wang Z, Hong T. Reinforcement learning for building controls: the opportunities and challenges. Appl Energy 2020;269:115036. https://doi.org/10.1016/j.apenergy.2020.115036. [17] K. Mason S. Grijalva A review of reinforcement learning for autonomous building energy management Comput Electr Eng 78 2019 300 312 10.1016/j.compeleceng.2019.07.019 Mason K, Grijalva S. A review of reinforcement learning for autonomous building energy management. Comput Electr Eng 2019;78:300-312. https://doi.org/10.1016/j.compeleceng.2019.07.019. [18] S. Brandi M.S. Piscitelli M. Martellacci A. Capozzoli Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings Energy Build 224 2020 110225 10.1016/j.enbuild.2020.110225 Brandi S, Piscitelli MS, Martellacci M, Capozzoli A. Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings. Energy Build 2020;224:110225. https://doi.org/10.1016/j.enbuild.2020.110225. [19] J.R. Vázquez-canteli Z. Nagy Reinforcement learning for demand response: a review of algorithms and modeling techniques Appl Energy 235 2019 1072 1089 10.1016/j.apenergy.2018.11.002 Vazquez-canteli JR, Nagy Z. Reinforcement learning for demand response: a review of algorithms and modeling techniques. Appl Energy 2019;235:1072-1089. https://doi.org/10.1016/j.apenergy.2018.11.002. [20] H. Kazmi F. Mehmood S. Lodeweyckx J. Driesen Gigawatt-hour scale savings on a budget of zero: deep reinforcement learning based optimal control of hot water systems Energy 144 2018 159 168 10.1016/j.energy.2017.12.019 Kazmi H, Mehmood F, Lodeweyckx S, Driesen J. Gigawatt-hour scale savings on a budget of zero: deep reinforcement learning based optimal control of hot water systems. Energy 2018;144:159-168. https://doi.org/10.1016/j.energy.2017.12.019. [21] R. Lu S.H. Hong Incentive-based demand response for smart grid with reinforcement learning and deep neural network Appl Energy 236 2019 937 949 10.1016/j.apenergy.2018.12.061 Lu R, Hong SH. Incentive-based demand response for smart grid with reinforcement learning and deep neural network. Appl Energy 2019;236:937-949. https://doi.org/10.1016/j.apenergy.2018.12.061. [22] F. Ruelens B.J. Claessens S. Vandael B. De Schutter R. Babuska R. Belmans Residential demand response of thermostatically controlled loads using batch reinforcement learning IEEE Trans Smart Grid 8 2017 2149 2159 10.1109/TSG.2016.2517211 Ruelens F, Claessens BJ, Vandael S, De Schutter B, Babuska R, Belmans R. Residential demand response of thermostatically controlled loads using batch reinforcement learning. IEEE Trans Smart Grid 2017;8:2149-2159. https://doi.org/10.1109/TSG.2016.2517211. [23] J. Vázquez-Canteli J. Kämpf Z. Nagy Balancing comfort and energy consumption of a heat pump using batch reinforcement learning with fitted Q-iteration Energy Procedia 122 2017 415 420 10.1016/j.egypro.2017.07.429 Vazquez-Canteli J, Kampf J, Nagy Z. Balancing comfort and energy consumption of a heat pump using batch reinforcement learning with fitted Q-iteration. Energy Procedia 2017;122:415-420. https://doi.org/10.1016/j.egypro.2017.07.429. [24] H. Kazmi S. D'Oca C. Delmastro S. Lodeweyckx S.P. Corgnati Generalizable occupant-driven optimization model for domestic hot water production in NZEB Appl Energy 175 2016 1 15 10.1016/j.apenergy.2016.04.108 Kazmi H, D'Oca S, Delmastro C, Lodeweyckx S, Corgnati SP. Generalizable occupant-driven optimization model for domestic hot water production in NZEB. Appl Energy 2016;175:1-15. https://doi.org/10.1016/j.apenergy.2016.04.108. [25] F. Ruelens B.J. Claessens S. Quaiyum B De Schutter R. Babuška R. Belmans Reinforcement learning applied to an electric water heater: from Theor Pract 9 2018 3792 3800 10.1109/TSG.2016.2640184 Ruelens F, Claessens BJ, Quaiyum S, Schutter B De, Babuska R, Belmans R. Reinforcement learning applied to an electric water heater: from Theor Pract 2018;9:3792-3800. https://doi.org/10.1109/TSG.2016.2640184. [26] E. Lam F. de Nijs P.J. Stuckey D. Azuatalam A. Liebman Large neighborhood search for temperature control with demand response H. Simonis Princ. Pract. Constraint program 2020 Springer International Publishing Cham 603 619 Lam E, de Nijs F, Stuckey PJ, Azuatalam D, Liebman A. Large neighborhood search for temperature control with demand response. In: Simonis H, editor. Princ. Pract. Constraint program., Cham: Springer International Publishing; 2020, p. 603-619. [27] J.R. Vázquez-Canteli S. Ulyanin J. Kämpf Z. Nagy Fusing TensorFlow with building energy simulation for intelligent energy management in smart cities Sustain Cities Soc (under Rev 2018 Vazquez-Canteli JR, Ulyanin S, Kampf J, Nagy Z. Fusing TensorFlow with building energy simulation for intelligent energy management in smart cities. Sustain Cities Soc (under Rev 2018. [28] O. De Somer A. Soares T. Kuijpers K. Vossen K. Vanthournout F. Spiessens Using reinforcement learning for demand response of domestic hot water buffers: a real-life demonstration vols. 1\u20136 2017 De Somer O, Soares A, Kuijpers T, Vossen K, Vanthournout K, Spiessens F. Using reinforcement learning for demand response of domestic hot water buffers: a real-life demonstration 2017:vols. 1-6. [29] P. Kofinas A.I. Dounis G.A. Vouros Fuzzy Q-Learning for multi-agent decentralized energy management in microgrids Appl Energy 219 2018 53 67 10.1016/j.apenergy.2018.03.017 Kofinas P, Dounis AI, Vouros GA. Fuzzy Q-Learning for multi-agent decentralized energy management in microgrids. Appl Energy 2018;219:53-67. https://doi.org/10.1016/j.apenergy.2018.03.017. [30] J.R. Vazquez-Canteli G. Henze Z. Nagy Marlisa Multi-agent reinforcement learning with iterative sequential action selection for load shaping of grid-interactive connected buildings ISBN BuildSys \u201920 2020 Association for Computing Machinery Yokohama, Japan 10.1145/3408308.3427604 Vazquez-Canteli JR, Henze G, Nagy Z. Marlisa: Multi-agent reinforcement learning with iterative sequential action selection for load shaping of grid-interactive connected buildings. In: ISBN, editor. BuildSys \u201920, Yokohama, Japan: Association for Computing Machinery; 2020. https://doi.org/10.1145/3408308.3427604. [31] A. Marinescu I. Dusparic A. Taylor V. Canili S. Clarke P.- Marl Prediction-based multi-agent reinforcement learning for non-stationary environments Proc Int Jt Conf Auton Agents Multiagent Syst AAMAS 3 2015 1897 1898 Marinescu A, Dusparic I, Taylor A, Canili V, Clarke S. P-Marl: Prediction-based multi-agent reinforcement learning for non-stationary environments. Proc Int Jt Conf Auton Agents Multiagent Syst AAMAS 2015;3:1897-1898. [32] A. Marinescu I. Dusparic S. Clarke Prediction-based multi-agent reinforcement learning in inherently non-stationary environments vol. 12 2017 10.1145/3070861 Marinescu A, Dusparic I, Clarke S. Prediction-based multi-agent reinforcement learning in inherently non-stationary environments 2017;vol. 12. https://doi.org/https://doi.org/10.1145/3070861. [33] P. Huang C. Fan X. Zhang J. Wang A hierarchical coordinated demand response control for buildings with improved performances at building group Appl Energy 242 2019 684 694 10.1016/j.apenergy.2019.03.148 Huang P, Fan C, Zhang X, Wang J. A hierarchical coordinated demand response control for buildings with improved performances at building group. Appl Energy 2019;242:684-694. https://doi.org/10.1016/j.apenergy.2019.03.148. [34] J. Vazquez-Canteli T. Detjeen G. Henze J. Kämpf Z. Nagy Multi-agent reinforcement learning for adaptive demand response in smart cities J Phys Conf Ser 1343 2019 10.1088/1742-6596/1343/1/012058 Vazquez-Canteli J, Detjeen T, Henze G, Kampf J, Nagy Z. Multi-agent reinforcement learning for adaptive demand response in smart cities. J Phys Conf Ser 2019;1343. https://doi.org/10.1088/1742-6596/1343/1/012058. [35] J.R. Vázquez-Canteli J. Kämpf G. Henze Z. Nagy CityLearn v1.0: an OpenAI gym environment for demand response with deep reinforcement learning Proc. 6th ACM int. Conf. Syst. Energy-efficient build. Cities, transp 2019 Association for Computing Machinery New York, NY, USA 356 357 10.1145/3360322.3360998 Vazquez-Canteli JR, Kampf J, Henze G, Nagy Z. CityLearn v1.0: an OpenAI gym environment for demand response with deep reinforcement learning. Proc. 6th ACM int. Conf. Syst. Energy-efficient build. Cities, transp., New York, NY, USA: Association for Computing Machinery; 2019, p. 356-357. https://doi.org/10.1145/3360322.3360998. [36] R.S. Sutton A.G. Barto Reinforcement learning: an introduction 1998 MIT Press Cambridge 10.1016/S0140-6736(51)92942-X Sutton RS, Barto AG. Reinforcement learning: an introduction. MIT Press Cambridge 1998. https://doi.org/10.1016/S0140-6736(51)92942-X. [37] C.J.C.H. Watkins P. Dayan Q-learning. Mach learn vol. 8 1992 279 292 10.1007/BF00992698 Watkins CJCH, Dayan P. Q-learning. Mach learn 1992;vol. 8:279-292. https://doi.org/10.1007/BF00992698. [38] V. Mnih K. Kavukcuoglu D. Silver A. Graves I. Antonoglou D. Wierstra Playing atari with deep reinforcement learning vols. 1\u20139 2013 Mnih V, Kavukcuoglu K, Silver D, Graves A, Antonoglou I, Wierstra D, et al. Playing atari with deep reinforcement learning 2013:vols. 1-9. [39] T. Haarnoja A. Zhou P. Abbeel S. Levine Soft actor-critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor 35th int conf mach learn ICML 2018 vol. 5 2018 2976 2989 Haarnoja T, Zhou A, Abbeel P, Levine S. Soft actor-critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor. 35th int conf mach learn ICML 2018 2018;vol. 5:2976-2989. [40] T. Haarnoja A. Zhou K. Hartikainen G. Tucker S. Ha J. Tan Soft actor-critic algorithms and applications 2018 Haarnoja T, Zhou A, Hartikainen K, Tucker G, Ha S, Tan J, et al. Soft actor-critic algorithms and applications 2018. [41] J.R. Vázquez-Canteli J. Kämpf G.P. Henze Z. Nagy CityLearn - GitHub repository 2019 https://github.com/intelligent-environments-lab/CityLearn Vazquez-Canteli JR, Kampf J, Henze GP, Nagy Z. CityLearn - GitHub repository 2019. https://github.com/intelligent-environments-lab/CityLearn. [42] G.P. Henze J. Schoenmann Evaluation of reinforcement learning control for thermal energy storage systems HVAC R Res 9 2003 259 275 10.1080/10789669.2003.10391069 Henze GP, Schoenmann J. Evaluation of reinforcement learning control for thermal energy storage systems. HVAC R Res 2003;9:259-275. https://doi.org/10.1080/10789669.2003.10391069. [43] Austin Energy. Electricity Tariff Pilot Programs n.d. https://austinenergy.com/ae/. [44] UNI EN 14825:2019 Condizionatori d\u2019aria, refrigeratori di liquido e pompe di calore, con compressore elettrico, per il riscaldamento e il raffrescamento degli ambienti - metodi di prova e valutazione a carico parziale e calcolo del rendimento stagionale 2019 Italy UNI EN 14825:2019 \u201cCondizionatori d\u2019aria, refrigeratori di liquido e pompe di calore, con compressore elettrico, per il riscaldamento e il raffrescamento degli ambienti - metodi di prova e valutazione a carico parziale e calcolo del rendimento stagionale.\u201d Italy: 2019.",
    "scopus-id": "85105332601",
    "coredata": {
        "eid": "1-s2.0-S0360544221009737",
        "dc:description": "Advanced control strategies can enable energy flexibility in buildings by enhancing on-site renewable energy exploitation and storage operation, significantly reducing both energy costs and emissions. However, when the energy management is faced shifting from a single building to a cluster of buildings, uncoordinated strategies may have negative effects on the grid reliability, causing undesirable new peaks. To overcome these limitations, the paper explores the opportunity to enhance energy flexibility of a cluster of buildings, taking advantage from the mutual collaboration between single buildings by pursuing a coordinated approach in energy management. This is achieved using Deep Reinforcement Learning (DRL), an adaptive model-free control algorithm, employed to manage the thermal storages of a cluster of four buildings equipped with different energy systems. The controller was designed to flatten the cluster load profile while optimizing energy consumption of each building. The coordinated energy management controller is tested and compared against a manually optimised rule-based one. Results shows a reduction of operational costs of about 4%, together with a decrease of peak demand up to 12%. Furthermore, the control strategy allows to reduce the average daily peak and average peak-to-average ratio by 10 and 6% respectively, highlighting the benefits of a coordinated approach.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2021-08-15",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S0360544221009737",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Pinto, Giuseppe"
            },
            {
                "@_fa": "true",
                "$": "Piscitelli, Marco Savino"
            },
            {
                "@_fa": "true",
                "$": "Vázquez-Canteli, José Ramón"
            },
            {
                "@_fa": "true",
                "$": "Nagy, Zoltán"
            },
            {
                "@_fa": "true",
                "$": "Capozzoli, Alfonso"
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S0360544221009737"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S0360544221009737"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S0360-5442(21)00973-7",
        "prism:volume": "229",
        "articleNumber": "120725",
        "prism:publisher": "Elsevier Ltd.",
        "dc:title": "Coordinated energy management for a cluster of buildings through deep reinforcement learning",
        "prism:copyright": "© 2021 Elsevier Ltd. All rights reserved.",
        "openaccess": "0",
        "prism:issn": "03605442",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "Coordinated energy management"
            },
            {
                "@_fa": "true",
                "$": "Deep reinforcement learning"
            },
            {
                "@_fa": "true",
                "$": "Building energy flexibility"
            },
            {
                "@_fa": "true",
                "$": "Peak demand reduction"
            },
            {
                "@_fa": "true",
                "$": "Grid interaction"
            }
        ],
        "openaccessArticle": "false",
        "prism:publicationName": "Energy",
        "openaccessSponsorType": null,
        "prism:pageRange": "120725",
        "pubType": "fla",
        "prism:coverDisplayDate": "15 August 2021",
        "prism:doi": "10.1016/j.energy.2021.120725",
        "prism:startingPage": "120725",
        "dc:identifier": "doi:10.1016/j.energy.2021.120725",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "207",
            "@width": "669",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "47636",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "201",
            "@width": "389",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "15047",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "304",
            "@width": "388",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr10.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "31636",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "386",
            "@width": "535",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr11.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "50106",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "288",
            "@width": "389",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "20557",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "344",
            "@width": "624",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "53058",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "220",
            "@width": "669",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "52895",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "160",
            "@width": "624",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "38966",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "308",
            "@width": "624",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "53300",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "459",
            "@width": "691",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr8.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "89495",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "340",
            "@width": "387",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr9.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "33640",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "68",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7745",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "113",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "4419",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "209",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr10.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7590",
            "@ref": "gr10",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "158",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr11.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "8823",
            "@ref": "gr11",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "162",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "5733",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "121",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7332",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "72",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7049",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "56",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "4750",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "108",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7766",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "145",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr8.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "10449",
            "@ref": "gr8",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "186",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr9.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "8678",
            "@ref": "gr9",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "915",
            "@width": "2962",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "338638",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "889",
            "@width": "1722",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "111208",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1348",
            "@width": "1720",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr10_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "242405",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1712",
            "@width": "2370",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr11_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "433862",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1274",
            "@width": "1722",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "138082",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1522",
            "@width": "2764",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "378177",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "972",
            "@width": "2961",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "359227",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "708",
            "@width": "2763",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "322847",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1366",
            "@width": "2764",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "439198",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2031",
            "@width": "3059",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr8_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "634482",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1507",
            "@width": "1713",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-gr9_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "232987",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si9.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5809",
            "@ref": "si9",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si27.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8669",
            "@ref": "si27",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si46.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4456",
            "@ref": "si46",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si6.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2867",
            "@ref": "si6",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si42.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5137",
            "@ref": "si42",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si15.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1522",
            "@ref": "si15",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si16.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2877",
            "@ref": "si16",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si5.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5524",
            "@ref": "si5",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si2.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "18920",
            "@ref": "si2",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si20.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2793",
            "@ref": "si20",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si34.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4564",
            "@ref": "si34",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si48.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1311",
            "@ref": "si48",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si1.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9073",
            "@ref": "si1",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si4.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1664",
            "@ref": "si4",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si3.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "15217",
            "@ref": "si3",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si26.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "10311",
            "@ref": "si26",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si37.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "17440",
            "@ref": "si37",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si45.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4644",
            "@ref": "si45",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si49.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2502",
            "@ref": "si49",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si31.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1334",
            "@ref": "si31",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si33.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5589",
            "@ref": "si33",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si38.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "24849",
            "@ref": "si38",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si29.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4025",
            "@ref": "si29",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si13.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "19907",
            "@ref": "si13",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si43.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "54671",
            "@ref": "si43",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si28.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2290",
            "@ref": "si28",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si22.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1146",
            "@ref": "si22",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si14.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "23239",
            "@ref": "si14",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si23.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1429",
            "@ref": "si23",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si47.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1344",
            "@ref": "si47",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si10.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1239",
            "@ref": "si10",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si40.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8491",
            "@ref": "si40",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si36.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "13081",
            "@ref": "si36",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si17.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3217",
            "@ref": "si17",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si21.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1207",
            "@ref": "si21",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si12.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8984",
            "@ref": "si12",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si32.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5137",
            "@ref": "si32",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si7.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6258",
            "@ref": "si7",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si44.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "20487",
            "@ref": "si44",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si19.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5897",
            "@ref": "si19",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si41.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "19714",
            "@ref": "si41",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si8.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "14579",
            "@ref": "si8",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si30.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "47925",
            "@ref": "si30",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si39.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11326",
            "@ref": "si39",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si24.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "43465",
            "@ref": "si24",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si35.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "10456",
            "@ref": "si35",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si18.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "24972",
            "@ref": "si18",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si50.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1814",
            "@ref": "si50",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si25.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "49267",
            "@ref": "si25",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-si11.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6198",
            "@ref": "si11",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360544221009737-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "1483935",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85105332601"
    }
}}