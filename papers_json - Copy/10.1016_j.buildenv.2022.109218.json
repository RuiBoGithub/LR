{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85133897005",
    "originalText": "serial JL 271434 291210 291731 291800 291881 31 Building and Environment BUILDINGENVIRONMENT 2022-05-26 2022-05-26 2022-07-08 2022-07-08 2022-11-11T10:57:48 1-s2.0-S0360132322004541 S0360-1323(22)00454-1 S0360132322004541 10.1016/j.buildenv.2022.109218 S300 S300.1 FULL-TEXT 1-s2.0-S0360132322X00127 2022-11-11T11:21:31.801246Z 0 0 20220815 2022 2022-05-26T15:24:00.429526Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref 0360-1323 03601323 true 222 222 C Volume 222 82 109218 109218 109218 20220815 15 August 2022 2022-08-15 2022 VSI: AI&IoT for Smart Building article fla © 2022 Elsevier Ltd. All rights reserved. UAVASSISTEDTASKOFFLOADINGFORIOTINSMARTBUILDINGSENVIRONMENTVIADEEPREINFORCEMENTLEARNING XU J 1 Introduction 2 System model and problem formulation 2.1 Network model for smart buildings and environment 2.2 UAV-assisted offloading model for IoT 2.2.1 Task model 2.2.2 Local computation model 2.2.3 Offloading model 2.3 Energy model 2.4 Problem formulation 3 UAV-assisted offloading algorithm for smart buildings and environments via deep reinforcement learning 3.1 MDP formulation of UAV-assisted offloading problem for smart buildings and environments 3.2 DRL-based offloading algorithm for IoT 4 Performance evaluation 4.1 Experiment settings 4.2 Convergence analysis 4.3 Comparison experiments 5 Related work 6 Conclusion CRediT authorship contribution statement Acknowledgements References GRAHAM 2021 108014 C YAN 2021 107982 K HE 2019 L YAN 2020 109689 K BRIK 2021 108056 B SHAHINMOGHADAM 2021 107905 M ZHOU 2021 171 178 X QI 2021 4159 4167 L WANG 2021 F ZHOU 2021 X LIANG 2021 W CHEN 2021 Y HUANG 2022 1964 1973 J ZHOU 2021 X QI 2021 L CHEN 2022 Y WANG 2018 28 34 H YAN 2022 387 395 K CHEN 2021 1659 1692 W CHEN 2021 4925 4934 Y YAN 2020 106698 K HUANG 2020 2581 2593 L WANG 2017 4924 4938 C MAO 2016 3590 3605 Y TANG 2020 M XIONG 2020 1133 1146 X FRAGKOS 2021 G YU 2020 3147 3159 Z WU 2020 556 562 G YAO 2021 3395 3410 Z LAHMERI 2021 1015 M CHEN 2021 108249 X CUI 2020 729 743 J XUX2022X109218 XUX2022X109218XJ 2024-07-08T00:00:00.000Z 2024-07-08T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2022 Elsevier Ltd. All rights reserved. 2022-07-17T05:28:14.896Z http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/aggregated-refined 0 https://doi.org/10.15223/policy-017 https://doi.org/10.15223/policy-037 https://doi.org/10.15223/policy-012 https://doi.org/10.15223/policy-029 https://doi.org/10.15223/policy-004 item S0360-1323(22)00454-1 S0360132322004541 1-s2.0-S0360132322004541 10.1016/j.buildenv.2022.109218 271434 2022-11-11T11:21:31.801246Z 2022-08-15 1-s2.0-S0360132322004541-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/MAIN/application/pdf/887992675ead3755c19f566663691d1e/main.pdf main.pdf pdf true 5150815 MAIN 11 1-s2.0-S0360132322004541-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/PREVIEW/image/png/de9998030c66efe2db51204263d63c27/main_1.png main_1.png png 58823 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0360132322004541-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr1/DOWNSAMPLED/image/jpeg/3d0089c98c2fce8e6f5b9bb89b07e914/gr1.jpg gr1 gr1.jpg jpg 40962 204 337 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322004541-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr3/DOWNSAMPLED/image/jpeg/ec3828e41f0b251a458d0bf5ce6f20c1/gr3.jpg gr3 gr3.jpg jpg 34088 247 337 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322004541-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr2/DOWNSAMPLED/image/jpeg/fec2a1a0dc97081948d99ece07520e0f/gr2.jpg gr2 gr2.jpg jpg 142674 764 711 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322004541-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr9/DOWNSAMPLED/image/jpeg/255b6ef2c0334edf3e045164d8fbe11c/gr9.jpg gr9 gr9.jpg jpg 46671 336 337 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322004541-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr8/DOWNSAMPLED/image/jpeg/23bd274bfb8d973641afb1e80f6fd677/gr8.jpg gr8 gr8.jpg jpg 33839 247 337 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322004541-gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr10/DOWNSAMPLED/image/jpeg/02a69689c385358343a4710f95761dc4/gr10.jpg gr10 gr10.jpg jpg 44069 337 337 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322004541-fx1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/fx1/DOWNSAMPLED/image/jpeg/b0b285154aa0f084d6f8dff217b62241/fx1.jpg fx1 fx1.jpg jpg 139168 454 387 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322004541-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr5/DOWNSAMPLED/image/jpeg/b51ce540b1a7c76b9e86c11e91efa4e9/gr5.jpg gr5 gr5.jpg jpg 32826 259 337 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322004541-gr11.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr11/DOWNSAMPLED/image/jpeg/d9be1e7865a314164027dd600eb2d839/gr11.jpg gr11 gr11.jpg jpg 43447 315 333 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322004541-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr4/DOWNSAMPLED/image/jpeg/6d729de11df98ddcbddbc2619e66c84f/gr4.jpg gr4 gr4.jpg jpg 33337 247 337 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322004541-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr7/DOWNSAMPLED/image/jpeg/ae9c6b8dff7b977c2f889b88e39e1a50/gr7.jpg gr7 gr7.jpg jpg 41786 245 337 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322004541-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr6/DOWNSAMPLED/image/jpeg/77bbf12a1dba3d1c31547e0f6bf2cc17/gr6.jpg gr6 gr6.jpg jpg 38842 265 337 IMAGE-DOWNSAMPLED 1-s2.0-S0360132322004541-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr1/THUMBNAIL/image/gif/b6e064139fa2fdade5b02be5504d5900/gr1.sml gr1 gr1.sml sml 17922 133 219 IMAGE-THUMBNAIL 1-s2.0-S0360132322004541-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr3/THUMBNAIL/image/gif/afc47993089736fbc7abb0bc49543026/gr3.sml gr3 gr3.sml sml 16199 161 219 IMAGE-THUMBNAIL 1-s2.0-S0360132322004541-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr2/THUMBNAIL/image/gif/228a9c02bee680ac5c38ed3ad3fc8244/gr2.sml gr2 gr2.sml sml 17297 163 152 IMAGE-THUMBNAIL 1-s2.0-S0360132322004541-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr9/THUMBNAIL/image/gif/7dcfec6ea392c05b2ac8337283b0b1a6/gr9.sml gr9 gr9.sml sml 16607 163 164 IMAGE-THUMBNAIL 1-s2.0-S0360132322004541-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr8/THUMBNAIL/image/gif/4ff8e80ab58d237dcf0604f25577318f/gr8.sml gr8 gr8.sml sml 15136 161 219 IMAGE-THUMBNAIL 1-s2.0-S0360132322004541-gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr10/THUMBNAIL/image/gif/b8c38114de4c8a2ae81901c5cf053620/gr10.sml gr10 gr10.sml sml 15373 164 164 IMAGE-THUMBNAIL 1-s2.0-S0360132322004541-fx1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/fx1/THUMBNAIL/image/gif/4754274cee889b1c3ba57308023d5e6c/fx1.sml fx1 fx1.sml sml 78073 164 140 IMAGE-THUMBNAIL 1-s2.0-S0360132322004541-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr5/THUMBNAIL/image/gif/f614b6090f10967a4ef089038c0276f0/gr5.sml gr5 gr5.sml sml 14795 164 213 IMAGE-THUMBNAIL 1-s2.0-S0360132322004541-gr11.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr11/THUMBNAIL/image/gif/6e39b643163dd53a1260da2fddb80f9f/gr11.sml gr11 gr11.sml sml 16023 164 173 IMAGE-THUMBNAIL 1-s2.0-S0360132322004541-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr4/THUMBNAIL/image/gif/2b0d8c2943ca5bc2120048d248810442/gr4.sml gr4 gr4.sml sml 14809 161 219 IMAGE-THUMBNAIL 1-s2.0-S0360132322004541-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr7/THUMBNAIL/image/gif/4bded73ab71738af958275585f55d182/gr7.sml gr7 gr7.sml sml 17852 159 219 IMAGE-THUMBNAIL 1-s2.0-S0360132322004541-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr6/THUMBNAIL/image/gif/d145296e3bbb079ffe893a2c9e2d3545/gr6.sml gr6 gr6.sml sml 16395 164 209 IMAGE-THUMBNAIL 1-s2.0-S0360132322004541-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr1/HIGHRES/image/jpeg/7bbf5ec67ba53ee84ee9a0a58868c745/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 250276 906 1495 IMAGE-HIGH-RES 1-s2.0-S0360132322004541-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr3/HIGHRES/image/jpeg/b47d77ab728d64ab74c32cac59baaada/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 187679 1096 1494 IMAGE-HIGH-RES 1-s2.0-S0360132322004541-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr2/HIGHRES/image/jpeg/860cc0e37360490cd1730319062c9179/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 1002413 3384 3150 IMAGE-HIGH-RES 1-s2.0-S0360132322004541-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr9/HIGHRES/image/jpeg/bbd7db7c8619565cef482343495b92fb/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 237961 1489 1494 IMAGE-HIGH-RES 1-s2.0-S0360132322004541-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr8/HIGHRES/image/jpeg/f45192569411228c26750600ed5cea12/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 186524 1096 1494 IMAGE-HIGH-RES 1-s2.0-S0360132322004541-gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr10/HIGHRES/image/jpeg/92ec6cd206afd154ddfbcd37669bd6b8/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 217609 1496 1494 IMAGE-HIGH-RES 1-s2.0-S0360132322004541-fx1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/fx1/HIGHRES/image/jpeg/3957abe177b9b5f041aa2a16ce44e5a7/fx1_lrg.jpg fx1 fx1_lrg.jpg jpg 541436 2009 1713 IMAGE-HIGH-RES 1-s2.0-S0360132322004541-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr5/HIGHRES/image/jpeg/bf6e30bef446b0f3edafcc38864be59f/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 178483 1149 1494 IMAGE-HIGH-RES 1-s2.0-S0360132322004541-gr11_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr11/HIGHRES/image/jpeg/3f5b4a925d2ff4807b65d5e9b2d2eac7/gr11_lrg.jpg gr11 gr11_lrg.jpg jpg 210068 1397 1476 IMAGE-HIGH-RES 1-s2.0-S0360132322004541-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr4/HIGHRES/image/jpeg/4f2ae1fa6a598d557b4ca6c2375aad17/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 181654 1097 1494 IMAGE-HIGH-RES 1-s2.0-S0360132322004541-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr7/HIGHRES/image/jpeg/83bcab475cc2b57009422cd12afe655b/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 248866 1088 1494 IMAGE-HIGH-RES 1-s2.0-S0360132322004541-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0360132322004541/gr6/HIGHRES/image/jpeg/764a2c8a35b6f0a721fa05cd020ab021/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 206549 1173 1494 IMAGE-HIGH-RES 1-s2.0-S0360132322004541-am.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:104B5W96W76/MAIN/application/pdf/725106f64a932454fbdce95c1478e439/am.pdf am am.pdf pdf false 2010032 AAM-PDF BAE 109218 109218 S0360-1323(22)00454-1 10.1016/j.buildenv.2022.109218 Elsevier Ltd Fig. 1 Framework of UAV-assisted task offloading for IoT in smart buildings and environment. Fig. 1 Fig. 2 Framework of our UTO approach. Fig. 2 Fig. 3 Impact of learning rate on energy consumption. Fig. 3 Fig. 4 Impact of discount factor on energy consumption. Fig. 4 Fig. 5 Comparison of reward of different algorithm. Fig. 5 Fig. 6 Comparison of queue length of different algorithm. Fig. 6 Fig. 7 Change of queue length within one episode. Fig. 7 Fig. 8 Comparison of energy consumption of different algorithm. Fig. 8 Fig. 9 Impact of the number IoT devices in energy consumption. Fig. 9 Fig. 10 Impact of bandwidth of UAV in energy consumption. Fig. 10 Fig. 11 Impact of the transmission power of UAV in energy consumption. Fig. 11 Table 1 Notations. Table 1 Symbol Description M The set of UAVs N The set of IoT devices N m The set of IoT devices connected to the UAV m τ The duration time of each time slot B u The offloading bandwidth of each UAV B m The bandwidth UAV m allocates to the IoT devices N m A n k The size of task that gennerated by IoT device n in time slot k Q n , l k The local computation queue length of IoT device n in time slot k D n , l k The local computation size of IoT device n in time slot k Q n , m k The local offloading queue length of IoT device n in time slot k D n , m k The local offloading size of IoT device n in time slot k Q n , b k The UAV offloading queue length of IoT device n in time slot k D n , b k The UAV offloading size of IoT device n in time slot k f n The CPU frequency of IoT device n p n The transmission power of IoT device n p m The transmission power of UAV m h m,n The channel gain between UAV m and IoT device n φ The number of cycles required to process one bit data of task p m The transmission power of UAV m Δ u The Gaussian noise power between UAV and IoT devices Δ b The Gaussian noise power between BS and UAV x n k The offloading indicator of IoT device n in time slot k B n , m k The bandwidth of UAV m allocated to IoT device n in time slot k a n , m k The transmission time ratio of UAV m allocated to IoT devices n in time slot k δ The computation energy efficiency coefficient Table 2 Hyper parameters in the experience. Table 2 Parameters Value The transmission power of IoT devices [10,100] mw The CPU frequency of IoT devices [1,2] GHz The noise power between IoT devices and UAV 10−14 w The needed CPU cycles to process one bit task 1000 Bandwidth allocated by UAV to IoT devices 10 MHz The allocated bandwidth of BS to UAV 7 MHz The size of new tasks [1,3] Mb The size of reply buffer 2000 Number of time slots 2000 The duration time of a time slot 1 s UAV-assisted task offloading for IoT in smart buildings and environment via deep reinforcement learning Jiajie Xu Writing \u2013 review & editing Writing \u2013 original draft Visualization Validation Supervision Software Resources Project administration Methodology Investigation Funding acquisition Formal analysis Data curation Conceptualization a Dejuan Li Visualization Validation Project administration Investigation Formal analysis Data curation b Wei Gu Writing \u2013 review & editing Visualization Validation Software Resources Project administration Formal analysis Data curation Conceptualization a Ying Chen Writing \u2013 review & editing Writing \u2013 original draft Validation Supervision Resources Project administration Funding acquisition Data curation Conceptualization a ∗ a School of Computer, Beijing Information Science and Technology University, Beijing, 100101, China School of Computer Beijing Information Science and Technology University Beijing 100101 China School of Compute, Beijing Information Science and Technology University, BeiJing, 100101, China b School of Agronomy and Environment, WeiFang University of Science and Technology, Weifang, 262700, China School of Agronomy and Environment WeiFang University of Science and Technology Weifang 262700 China School of Agronomy and Environment, WeiFang University of Science and Technology, Weifang, 262700, China ∗ Corresponding author. With the rapid development of Internet of Things (IoT) techniques, IoT devices with sensors have been widely deployed and used in smart buildings and environment, and the application scenarios of IoT in smart buildings and environment have been further extended. However, due to the limitations of computation, storage and battery capacity, IoT devices cannot process all the tasks locally by themselves and need to offload some tasks to edge servers typically deployed in base stations (BSs). Besides, unmanned aerial vehicles (UAVs) with controllable mobility and flexibility have been recognized as a promising solution to assist communication in emergency scenarios. In this paper, we investigate UAV-assisted offloading for IoT in smart buildings and environment. We formulate the offloading problem with the goal of minimizing the long-term energy consumption and minimizing the queue length at the same time. As the solution space size is extremely large and the offloading problem focuses on the long-term optimization goal, solving this problem faces several challenges. To address these challenges, we reformulate it as a Markov decision process (MDP)-based offloading problem and propose the UAV-assisted task offloading (UTO) approach based on deep reinforcement learning (DRL) techniques. Our UTO approach can cope well with the challenges brought by high-dimensional and consecutive state and action space. We carry out a series of comparison experiments with both DRL and non-DRL algorithms, and the results validate the performance of our proposed UTO approach. Keywords Smart buildings and environment Internet of things (IoT) Task offloading Unmanned aerial vehicle (UAV) Deep reinforcement learning (DRL) 1 Introduction In recent years, the applications and services of the Internet of Things (IoT) have been widely used in smart buildings and environment [1]. For example, IoT devices with sensors can be used for monitoring the quality of indoor and outdoor environment [2\u20135]. In addition, another widely adopted application of the IoT in smart buildings is collecting thermal comfort data to analyze the demand of people for adjusting the temperature [6\u20139]. With the rapid development of IoT techniques, the application scenarios of IoT in smart buildings and environment have been further expanded. Therefore, the study of IoT applications in smart buildings and environment has attracted extensive attention from both academia and industry [10]. However, due to the limitation of computation and storage capacity, IoT devices in smart buildings and environment cannot process all the tasks locally by themselves [11\u201313]. Thus, finding a solution to offload tasks to computing servers for processing is important. With the emergence of the mobile edge computing (MEC) paradigm, the tasks of IoT devices in smart buildings and environment can be offloaded to the edge servers typically deployed in base stations (BSs) [14]. In this way, the latency of tasks can be reduced, and the battery life of IoT devices can be extended [15]. Hence, designing and implementing a reasonable and reliable task offloading strategy is an important research problem. In some scenarios, the BS or relay deployed on the ground may be damaged due to some uncontrollable factors. With the development of unmanned aerial vehicles (UAVs), UAVs with controllable mobility and flexibility have been adopted in emergency scenarios to improve the coverage, reliability and capacity of wireless networks [16,17]. UAVs can be used as caching devices, relays or BSs because of their high agility [18,19]. Therefore, taking advantage of UAVs to assist the task offloading of IoT devices in smart buildings and environment is a promising solution. Nevertheless, solving the problem of UAV-assisted task offloading in smart buildings and environment is a great challenge [20]. First, with the increase in the number of IoT devices, the scale of the solution space increases dramatically. It is difficult to obtain an effective task offloading and resource allocation strategy from a large solution space through traditional heuristic algorithms. In addition, traditional algorithms usually focus on one-shot optimization, while the task offloading problem seeks long-term optimization. Moreover, traditional reinforcement learning (RL) algorithms, such as Q-Learning, which is based on value estimation with a table, will be limited in storage capacity or computational efficiency. The deep reinforcement learning (DRL) approach can be a practicable method for dealing with the UAV-assisted task offloading problem in smart buildings and environment [21]. DRL, which combines neural network and RL, can solve the problem of state space explosion [22\u201324]. In this paper, we investigate the problem of UAV-assisted task offloading in smart buildings and environments. First, we integrate UAVs to assist communication in emergency scenarios. Then, we formulate the UAV-assisted offloading problem for IoT in smart buildings and environment. Our optimization goal is minimizing the long-term energy consumption and minimizing the queue length. Moreover, we reformulate it as a Markov decision process (MDP)-based task offloading problem, and design the UAV-assisted Task Offloading (UTO) algorithm based on DRL to solve the problem. Finally, we conduct a series of comparative experiments by comparing our UTO algorithm with the deep deterministic policy gradient (DDPG) algorithm and non-DRL algorithms to verify its performance. The main contributions of this paper can be summarized as follows: 1) We investigate the problem of UAV-assisted task offloading for IoT in smart buildings and environment. The UAV is used as a temporary relay to meet the demands of task offloading from IoT devices to the BS in response to an emergency scenario. We formulate the UAV-assisted task offloading problem, with the goal of minimizing the energy consumption and minimizing the queue length. Our control decisions include determining whether the tasks generated by IoT devices should be processed locally or offloaded, and allocating the resources for the IoT devices connected with the UAV. 2) We adopt MDP theory to reformulate the UAV-assisted offloading problem. To balance the two conflicting optimization goals of energy consumption and queue length, we combine and coordinate the two goals into the reward function. Then, we propose the UTO algorithm to solve the offloading problem. Our UTO algorithm can cope well with the challenges caused by high-dimensional and consecutive state and action space. It can also realize the long-term optimization of energy consumption and queue length without prior knowledge of statistical future information of the task generation process. 3) We perform extensive experiments to evaluate the performance of the UTO algorithm. We compare our UTO algorithm with the DDPG algorithm and non-DRL algorithms. The experimental results demonstrate that the UTO algorithm can effectively decrease energy consumption and queue length. The remainder of this paper is organized as follows. The system model of UAV-assisted task offloading in smart buildings and environment and problem formulation are presented in Section 2. The MDP-based problem reformulation and the proposed UTO approach are given in Section 3. We present the performance evaluation in Section 4. The related work and conclusion are presented in Section 5 and Section 6, respectively. 2 System model and problem formulation 2.1 Network model for smart buildings and environment In this paper, we consider a system of UAV-assisted IoT devices in smart buildings and environment. As shown in Fig. 1 , the system consists of a set of IoT devices, a set of UAVs and a BS. There are N IoT devices denoted by N = { 1 , 2 , \u2026 , n , \u2026 N } and M UAVs denoted by M = { 1 , 2 , \u2026 , m , \u2026 , M } . N m represents the set of IoT devices connected to UAV m. We adopt a time-slotted model, and there is a set of time slots denoted by K = { 1 , 2 , \u2026 , k , \u2026 K } , where the duration time of each time slot is τ seconds. The BS communicates with UAVs and UAVs communicate with IoT devices through wireless transmission. The BS allocates B u channel bandwidth to each UAV for transmission, and each UAV m provides B m channel bandwidth for the set of IoT devices N m . The main symbols used in the paper are given in Table 1 . 2.2 UAV-assisted offloading model for IoT 2.2.1 Task model At the beginning of each time slot k, each IoT device n generates a task. The size of the task is denoted as A n k . Whether to process the task locally or offload the task to UAV must be decided. If the IoT device decides to process it locally, then the task will be placed into the local computation queue Q n , l k . Otherwise, it will be placed into the local offloading queue Q n , m k . The tasks in the local offloading queue are offloaded to the BS through the UAV. Thus, Q n , m k has a corresponding offloading queue in the UAV, which is denoted as Q n , b k . Inspired by Ref. [25], we consider a binary offloading policy. We let x n k ∈ { 0 , 1 } be an indicator, where x n k = 0 denotes task A n k of IoT device n placed into the local computation queue, and x n k = 1 denotes the task of IoT device n placed into the local offloading queue. After making this decision, the update of the local computation queue and local offloading queue can be formulated by (1) Q n , l k = Q n , l k + ( 1 − x n k ) A n k , (2) Q n , m k = Q n , m k + x n k A n k . 2.2.2 Local computation model The tasks in the local computation queue of IoT device n need to be processed locally. The local computation size D n , l k can be calculated by (3) D n , l k = min Q n , l k , f n τ φ , where f n denotes the CPU frequency of IoT device n, τ denotes the duration time of a time slot, and φ denotes the number of cycles required to process one bit of task data. After the local computation, the local computation queue length in the beginning of time slot k + 1 is given by (4) Q n , l k + 1 = Q n , l k − D n , l k . 2.2.3 Offloading model 1) Offloading to UAV: We consider that devices adopt the frequency-division multiplexing (FDM) protocol to communicate with UAV. The transmission rate v m , n k between IoT device n and UAV m can be formulated as (5) v n , m k = B n , m k log 2 1 + p n h n , m Δ u , where B n , m k represents the bandwidth that UAV m allocates to IoT device n in time slot k, p n represents the transmission power of IoT device n, h n,m represents the wireless channel gain between IoT device n and UAV m, and Δ u represents the Gaussian noise power when IoT devices communicate with the UAV. Thus, the transmission size of IoT device n to UAV m can be calculated by (6) D n , m k = min { Q n , m k , v n , m k τ } . The update of the local offloading queue consists of two parts, the size of task A n k in time slot k according to (1) and the offloading size D n , m k . Therefore, the local offloading queue of IoT device n in the beginning of time slot k + 1 is given as (7) Q n , m k + 1 = Q n , m k − D n , m k . 2) Offloading to BS: The UAV m receives the offloading task of IoT devices and allocates resources of offloading time to the UAV offloading queue of each IoT device. The transmission rate v m between UAV m and the BS can be calculated by (8) v m = B u log 2 1 + p m Δ b , where B u represents the bandwidth between UAV m and the BS, p m represents the transmission power of the UAV and Δ b represents the Gaussian noise power. The offloading size D n , b k of IoT device n from UAV m to the BS can be formulated by (9) D n , b k = min { Q n , b k , c n , m k τ v m } , where c n , m k denotes the transmission time ratio that UAV m allocates to the offloading queue of IoT device n in N m within time slot k. Similarly, the UAV offloading queue Q n , b k + 1 of IoT device n in UAV m in the beginning of time slot k + 1 can be formulated by (10) Q n , b k + 1 = Q n , b k − D n , b k + D n , m k . The difference in the local computation queue Q n , l , d k of IoT device n in time slot k can be expressed by (11) Q n , l , d k = A n , l k − D n , l k . The difference in the local offloading queue Q n , m , d k of IoT device n in time slot k can be expressed by (12) Q n , m , d k = A n , m k − D n , m k . The difference in the offloading queue Q n , b , d k of IoT device n in UAV m can be expressed by (13) Q n , b , d k = D n , m k − D n , b k . The total queue difference of IoT device n in time slot k consists of three parts, including the difference in the local computation queue, the difference in the local offloading queue and the difference in the UAV offloading queue. Therefore, the calculation of the total queue difference is (14) Q n , d k = Q n , l , d k + Q n , m , d k + Q n , b , d k . 2.3 Energy model The BS has sufficient power supply. Therefore, we focus on optimizing the energy consumption, including the energy of local computation, the energy of offloading the task to the UAV and the energy of offloading the task from the UAV to the BS. 1) Local computation energy: The local computing time t n , l k of IoT device n in time slot k is given by (15) t n , l k = D n , l k φ f n , and the local computation energy E n , l k of IoT device n in time slot k can be calculated by (16) E n , l k = δ f n 3 t n , l k , where δ represents the computation energy efficiency coefficient [25]. 2) Local offloading energy: The offloading time t n , m k of IoT device n to UAV m in time slot k can be calculated by (17) t n , m k = D n , m k v n , m k . The energy E n , m k of IoT device n offloading task to UAV m in time slot k can be formulated by (18) E n , m k = p n t n , m k . 3) UAV offloading energy: The offloading time from UAV m to the BS for the queue of IoT device n in time slot k can be calculated by (19) t n , b k = D n , b k v m . The offloading energy of nth IoT device in time slot k can be calculated by (20) E n , b k = p m t n , b k , where p m denotes the transmission power of UAV m. The total energy for IoT device n in time slot k consists of local computing energy, local offloading energy and UAV offloading energy, which is given by (21) E n k = E n , l k + E n , m k + E n , b k . 2.4 Problem formulation In each time slot k, we make the decision for the task, i.e., to place it into the local computing queue or the local offloading queue, and allocate the bandwidth resource to the local offloading queue and the offloading time to the UAV offloading queue. Our goal is to minimize the long-term energy consumption for all IoT devices in N m . Hence, the total energy consumption of all IoT devices connected to UAV m can be expressed by (22) E total = ∑ k = 1 K ∑ n = 1 | N m | E n k . The UAV-assisted task offloading problem for IoT in smart buildings and environment is formulated as (23) min E total s.t. C 1 : p min ≤ p n ≤ p max , n ∈ N C 2 : f min ≤ f n ≤ f max , n ∈ N C 3 : x n k ∈ { 0 , 1 } , n ∈ N C 4 : ∑ n = 1 | N m | B n , m k = B m , n ∈ N m , m ∈ M C 5 : ∑ n = 1 | N m | c n , m k = 1 , n ∈ N m , m ∈ M , 0 ≤ c n , m k ≤ 1 . C1 represents the range of transmission power of IoT devices. p min is the minimum transmission power of IoT devices, and p max is the maximum transmission power of IoT devices. Similarly, C2 represents the range of CPU frequency of IoT devices. f min is the minimum CPU frequency of IoT devices, and f max is the maximum CPU frequency of IoT devices. C3 represents the task offloading decision of IoT devices. C4 and C5 are the constraints of the allocation of bandwidth and offloading time, respectively. 3 UAV-assisted offloading algorithm for smart buildings and environments via deep reinforcement learning 3.1 MDP formulation of UAV-assisted offloading problem for smart buildings and environments At the beginning of each time slot k, the system obtains the state, which includes the size of the task and the queue length of IoT devices n ( n ∈ N m ) . The system makes decisions for the tasks of IoT devices, allocates channel bandwidth for the IoT devices to offload tasks to UAV and allocates the offloading time for the queue to offload task from UAV to BS. The action-based state will result in a reward and a new state. Thus, we formulate our considered problem as a Markov decision process (MDP) problem defined as a 5-tuple { S , A , P , R , γ } , where S ≜ { s k } denotes the set of states, A ≜ { a k } denotes the set of actions, and P : S × A → S denotes the transition function from s k to s k+1. S × A → R denotes the immediate reward from s k to s k+1, and γ denotes the discount of reward. The MDP model for the UAV-assisted offloading model is given as follows. 1) State: At each time slot k ∈ K , the agent obtains the states of IoT devices N m , consisting of the size of the new tasks and the length of all queues. Since the new tasks and queue lengths are variable, we use k to distinguish the new tasks and state of the queue in different time slot k. The state space of the | N m | IoT devices can be formulated by (24) S m k = { s n k = { A n k , Q n , l k , Q n , m k , Q n , b k } , n ∈ N m } , where A n k represents the task size of IoT device n, Q n , l k reflects the queue length of the local computation in time slot k, Q n , m k reflects the queue length of the local offloading at the beginning of time slot k and Q n , b k reflects the offloading queue length of IoT device n in UAV m. 2) Action: In our model, IoT device n ∈ N m generates a task at the beginning of time slot k, and the size of the task is denoted by A n k . We use the binary offloading decision x n k for each IoT device. Similarly, UAV m allocates the channel bandwidth and offloading time for the queue in UAV m of IoT device n. Thus, the action is described as (25) A m k = { a n k = { x n k , B n , m k , c n , m k } , n ∈ N m } , where x n k = { 0 , 1 } represents the offloading decision of the new task A n k , x n k = 0 denotes that the new task will be placed into the local computation queue and x n k = 1 denotes that the new task will be placed into the local offloading queue. B n , m k represents the allocated channel bandwidth of IoT device n in time slot k, and c n , m k represents the offloading time ratio for the queue of IoT device n in UAV m. 3) Reward: It is significant to propose a suitable reward function for achieving our goal. In our system, the energy consumption is one of the vital metrics to evaluate the system. Meanwhile, we add the queue difference to avoid the queue continuing to lengthen. In summery, our goal is to optimize the energy consumption and queue length. The reward function is given by (26) R k = α ∑ n = 1 | N m | E n k − β ∑ n = 1 | N m | Q n , d k , where ∑ n = 1 | N m | E n k represents the energy consumption of the set of IoT devices N m in time slot k, and the calculation of E n k is based on (21). ∑ n = 1 | N m | Q n , d k represents the total queue difference of the set of IoT devices N m , and the calculation of Q n , d k according to (14). The parameters α and β correspond to the weights of the total energy and queue difference, respectively. 3.2 DRL-based offloading algorithm for IoT As one of the significant machine learning methods, reinforcement learning (RL) has been recognized as a conventional scheme for resolving the MDP problem. Nevertheless, the RL-based method has defects when perceiving complex environmental states and high-dimensional actions. The RL-based method leads to inefficiency because of the calculation of value function for the possible action spaces of the state. The recent advanced deep reinforcement learning (DRL), which uses deep neural network (DNN) to gradually approximate the value function and policy function, has been recognized as a prospected approach to deal with large-scale complicated MDP problems [22]. Our problem is a complicated MDP problem with high-dimensional state space and action space, and the state space and action space are consecutive. In this paper, we adopt the proximal policy optimization (PPO) algorithm to cope with the MDP problem, which is a DRL algorithm based the actor-critic algorithm. The PPO algorithm is an improvement of the policy gradient algorithm and can perform well in the problem of continuous action compared with other DRL algorithms such as deep Q-learning (DQN) algorithm. The PPO algorithm uses an actor network and a critic network for learning. Specifically, the PPO algorithm generates the probability distribution of actions through DNN. In detail, after inputting the state into the actor network, two values of normal distribution can be obtained, which are μ and σ. The PPO algorithm takes the normal distribution function as the probability density function. When an action is selected according to the probability, it only needs to be sampled according to the probability density function. Furthermore, the critic network is used to approximate the value function for calculating the discount reward. The detailed description of the training process is as follows. The policy gradient algorithm is an on-policy algorithm. On-policy denotes that the agent to be learned is the same as the agent interacting with the environment. Policy gradient algorithm will take considerable time in sample data. After the agent interacts with the environment, it needs to update the parameters. We can only update the parameter once. Next, we must collect the data again before we can update the parameter. Thus, the time efficiency of the on-policy algorithm is very low, and PPO converts on-policy to off-policy. PPO uses two policy with parameters θ and θ old . The actor with the parameter θ old interacts with the environment to obtain the experiences. We can train θ many times through gradient ascent with the experience collected by θ old . The transition from on-policy to off-policy is through importance sampling. The meaning of importance sampling is that we use policy π with parameter θ old to interact with the environment and obtain trajectory tr. Then, we take the demonstration of policy θ old to update the parameter θ of other policy. Because of the difference of θ old and θ, we need to add an importance weight ζ. The definition of ζ is ζ = π θ ( t r ) π θ old ( t r ) , where π θ (tr) represents the probability of the trajectory in the policy with parameter θ; similarly π θ old ( t r ) represents the probability with the parameter θ old . There are three neural networks: actor-new, actor-old and critic. The parameter of actor-new is θ, the parameter of actor-old is θ old and the parameter of the critic network is ω. At the beginning of each time slot k, a k is generated through the actor-new neural network with the sample from the probability density function. The parameter of actor-new is θ. Then, the environment returns the immediate reward r k of s k and the next state s k+1. As a feature of the off-policy algorithm, the reply buffer R is needed to store the tuple of { s k , a k , π θ old ( a k , s k ) , r k , s k + 1 } . The above step is repeated until the number of tuples in the reply buffer reaches a certain number. For this step and the following steps, we let state s k+1 in the last time slot k to input the critic network to achieve the state value V k+1. The discount reward of each step is obtained through the discount reward function, where the discount function is expressed by (27) y k = r ( k ) + γ r ( k + 1 ) + ⋯ + γ K − k + 1 r ( K − 1 ) + γ K − k V K + 1 . where the calculation of V K+1 is obtained through the critic neural network. Then, we push the set of states into the critic network to obtain the set of state values. We update the parameter of the critic network to minimize the loss L(ω), where the loss function is define as (28) L ( ω ) = E [ y k − V ( s k | ω ) ] 2 . For the actor network, the performance objective function is expressed by (29) J ( θ ) = E [ min ( ζ k θ G k θ ( s k , a k ) , c l i p ( ζ k θ , 1 − ε , 1 + ε ) G k θ ( s k , a k ) ) ] , where ζ k θ represents π θ ( a k | s k ) π θ old ( a k | s k ) and ε is the a hyper parameter. The clip function means that if the first term ζ k θ is less than the second term 1 − ε, the output is ζ k θ , and if it is greater than the third term 1 + ε, the output is ε. G k θ ( s k , a k ) is the advantage function, which can be denoted by (30) G k θ ( s k , a k ) = y k − V ( s k | ω ) . Algorithm1 UAV-assisted Task Offloading (UTO) Algorithm Image 1 We propose a UAV-assisted task offloading (UTO) algorithm based on the DRL method. The UTO algorithm is an off-policy, which means that the agent learning and interacting with the environment are different. We build an actor network with parameters θ, which is regarded as policy π, to sample an action through the probability density function. The input of the actor network is the state denoted by S m k = { s n k = { A n k , Q n , l k , Q n , m k , Q n , b k } , n ∈ N m } , where the state includes the size of tasks and the length of queues. The output includes two values of the normal distribution function. The values of the two parameters mean and variance are denoted an μ and σ, respectively. Through the function, we can obtain the action denoted by A m k = { a n k = { x n k , B n , m k , c n , m k } , n ∈ N m } . The action includes the decision of task whether placed into local computation queue or local offloading queue. Similarly to the actor network, we build a critic network. The input of the network is the state the same as the actor critic, and the output of the network is the action value corresponding to the sate. actor-new and actor-old have the same network structure as the actor network. For each episode l ∈ L, the agent collects the information of the environment as a state. The information consist of the size of the generated tasks, the length of all queues including the local computation queue, local offloading queue and the UAV offloading queue of the set of IoT devices n ∈ N m . Then, the agent will make a decision for the generated tasks whether the new task to be placed into the local computation queue for local calculation or the local offloading queue for offloading at the beginning of time slot k. In addition, the agent will allocates the transmission resource of the UAV for the queue of the set of IoT devices to offload. The resource includes the bandwidth resource for queues locally and the time resource for queues in UAV. At the end of the task, the agent obtains the energy consumption of local computation and two parts of energy for offloading, which include the local offloading and the UAV offloading. In addition, to ensure that the queue is not overstocked, the agent also needs to collect the queue difference of the set of IoT devices locally and in the UAV. After that, the agent can calculate the immediate reward r k in time slot k with energy consumption and queue difference according to (26). The environment enters the next state s k+1. Then, the tuple { s k , a k , l o g π θ old ( a k , s k ) , r k , s k + 1 } can be obtained, and the tuple can be saved into the reply buffer. Upon the count of the experience reaches a certain amount, we can train the network of actor neural network and critic neural network for updating the parameters of the θ and ω. To update the parameter ω, we use (28) to decrease the loss. Similarly, we use (29) to update the parameter θ of the actor-new neural network to increase the loss. The UTO framework is shown in Fig. 2 , and the details of the UTO algorithm are given by Algorithm 1. 4 Performance evaluation In this section, we conduct a serials of simulations to evaluate the performance of the proposed UTO algorithm. First, we consider the impact of the learning rate and discount rate on the performance of the UTO algorithm. In the next part, we compare our UTO algorithm with three other algorithms to further evaluate its performance. The results are as follows. 4.1 Experiment settings The system consists of a BS, multiple UAVs and IoT devices. The required CPU cycles to compute one bit of task data are 1000. The lower limit transmission power of IoT devices p min is 10 mW and the upper limit transmission power p max is 100 mW. The CPU frequency of IoT devices ranges from 1 GHz to 2 GHz. For each new task, the maximum size of the task is 3 Mb. The Gaussian noise power between IoT devices and UAV is 10−14 W. For the UAV, the transmission power is 2 w. The bandwidth allocated to IoT devices by each UAV is 10 MHz, and the bandwidth of BS allocated to each UAV is 7 MHz. The special simulation parameters in this experiments are given in Table 2 . 4.2 Convergence analysis First, we investigate the convergence of the UTO algorithm of the learning rate and the discount factor. 1) Impact of the learning rate: The learning rate is a significant parameter in DRL-based algorithms. We need to conduct a set of experiments to determine the learning rate in the DRL-based algorithm. If the learning rate approaches 1, the convergence speed will be too fast to make the decision correctly. Similarly, when the learning rate approaches 0, it will lead to the extension of iteration time and the slowdown of convergence speed. Fig. 3 depicts the influence of different learning rates on the convergence performance of energy consumption. We set the learning rate as 0.0001, 0.0005 and 0.001. As shown in Fig. 3, when the learning rate is 0.001, the energy consumption curve converges after 4 episodes. Thus, in the following experience, we choose 0.001 as the value of the learning rate. 2) Impact of the discount factor: Fig. 4 depicts the influence of the discount factors on the convergence performance of energy consumption. In the simulation, the discount is set to be 0.1, 0.5 and 0.9. As shown in Fig. 4, when the discount factor is 0.9, the energy consumption is the lowest after convergence compared to the values of 0.1 and 0.5. While the discount factor approaches 0, we only focus on the current reward. In contrast, when the discount factor approaches 1, the future reward is close to the current reward. Thus, when the discount is 0.9, we obtain a greater long-term reward, that is, lower energy consumption. 4.3 Comparison experiments In this part, we compared the UTO algorithm with four other baseline algorithms to verify the performance of our algorithm in terms of the queue length and energy consumption. ● Stochastic algorithm: The decision of the task generated in each time slot is randomly placed into the local computation queue or local offloading queue. The bandwidth allocated by UAV to IoT devices is random. Similarly, the proportion of the offloading time for each IoT device is also stochastic. ● All-Offload algorithm: The decision of task is placed into the local offloading queue of IoT devices. The bandwidth allocated by UAV to IoT devices is even and the offloading time ratio for the UAV offloading of each IoT device is also average. ● All-Local algorithm: Contrary to the All-Offload algorithm, the All-Local algorithm places all task into the local computation queue waiting for calculation by the local CPU. Neither the bandwidth resource for the local offloading queue nor the offloading time for IoT devices in the UAV need to be allocated because there is no task offloading. ● Deep deterministic policy gradient (DDPG) algorithm: The DDPG algorithm is another DRL-based algorithm. The DDPG algorithm combines the policy gradient and deep Q-network (DQN). The DDPG algorithm can update the policy through the off policy method and predict the deterministic policy. Fig. 5 depicts the reward comparison of different algorithms, where the weight of the energy consumption and queue length is 5 to 1 according to (26). The convergence speed of the UTO algorithm is much better than that of the DDPG algorithm. After several episodes of training of the neural network, the UTO algorithm can adjust the offloading decision for the task generated in each time slot and the allocation of resources. Our algorithm has the highest reward after convergence of the 8th episode compared with the Stochastic algorithm, All-Offload algorithm, All-Local algorithm and DDPG algorithm. In particular, DDPG converged after 9 episodes, but the result of convergence is not satisfactory. A previous study proved that the DDPG algorithm performs relatively poorly in complex environments with high action dimensions. Queue length is an important metric of our experience. As shown in Fig. 6 , we compare the queue lengths of different algorithms. Our algorithm has a relatively lower queue length after convergence. Compared to the All-Offload algorithm and Stochastic algorithm, the queue length of our algorithm is larger. This situation is allowed while the allocation of resources of the Stochastic algorithm is equally distributed in the long term. To prove this, We remove the change in queue length after convergence separately. Fig. 7 demonstrates the change in queue in one episode that contains 2000 time slots. We can see that the update of the queue length of DDPG algorithm and All-Local algorithm is obviously linear. The update of the queue length of the UTO algorithm in the first 10 time slots is linear, while it fluctuates in a stable trend in the subsequent time slots. Fig. 8 plots the energy consumption of four different algorithms. The energy consumption of the DDPG algorithm and All-Local algorithm is lowest compared to the other three algorithms because the task cannot be processed completely. Although the queue length is greater than that of the Stochastic algorithm and All-Offload algorithm, as shown in Fig. 8, the energy consumption of the UTO algorithm is the lowest compared with the Stochastic algorithm and All-Offload algorithm. This demonstrates that the UTO algorithm can decrease the energy consumption of UAV and the set of IoT devices. The UTO algorithm can adjust the task offloading decision and resource allocation to make full use of the local computing capability of IoT devices. Because the DDPG algorithm and All-Local algorithm have problems dealing with the queue length, we only compare the UTO algorithm, All-Offload algorithm and Stochastic algorithm in the following comparative experiment. 1) Impact of the number of IoT devices: As shown in Fig. 9 , we compare the energy consumption subject to the number of IoT devices, where the number of IoT devices ranges from 15 to 40. Fig. 9 shows that as the number of IoT devices increases, the energy consumption of the UTO algorithm also increases, but compared with the All-Offload algorithm and Stochastic algorithm, the energy consumption of the UTO algorithm is always the lowest of the three algorithms. The UTO algorithm shows a logarithmic growth trend compared to the All-Offload algorithm and Stochastic algorithm with the increase in the number of IoT devices. When the number of IoT devices is 40, compared with the All-Offload algorithm and stochastic algorithm, the energy consumption of the UTO algorithm is reduced by 60% and 40%, respectively. 2) Impact of bandwidth of the UAV: We compare the impact of different bandwidths on energy consumption. Fig. 10 depicts the influence of the difference in the total bandwidth allocated by UAV to IoT devices. It is obvious from the figure that with the increase in bandwidth, the energy consumption gradually decreases. The reason for the reduction of energy consumption is that the increase of bandwidth will affect the transmission rate, then affect the time of tasks offloading, and finally lead to the reduction of energy consumption. As shown in Fig. 10, the energy consumption of the UTO algorithm is lowest regardless of whether the bandwidth is 4, 6, 8, 10 or 12 MHz. On average, the UTO algorithm can reduce 60% of the energy consumption compared to the All-Offload algorithm and reduce 35% of energy consumption compared to the Stochastic algorithm. 3) Impact of the transmission power of the UAV: Fig. 11 shows the energy consumption for the UTO algorithm, Stochastic algorithm and All-Offload algorithm at different transmission powers of the UAV. It is obvious from Fig. 11 that the energy consumption of the UTO algorithm is always the lowest regardless of whether the transmission power is 1, 1.5, 2, 2.5 or 3 W. Fig. 11 also illustrates that the UTO algorithm performs better when the transmission power is limited. For example, when the transmission power is 1 W, the energy consumption of the Stochastic algorithm is more than twice of the energy consumption of the UTO algorithm, and the energy consumption of All-Offload algorithm is nearly three times the energy consumption of the UTO algorithm. 5 Related work With the rapid development of IoT devices, a number of IoT devices are widely used in smart buildings and environment. In particular, the IoT devices with sensors are used to monitor indoor and outdoor environments. Graham et al. [2] introduced IoT enabled multi modal devices based micro-electromechanical systems that could monitor indoor environment quality. They designed and implemented IoT and low-cost devices for monitoring the local environment. Bouziane et al. [6] aimed to use IoT devices for monitoring the thermal comfort of building occupants. Through collecting and analysing the data, they could predict the thermal comfort of different people for decreasing the energy consumption. Research on mobile edge computing and fog computing has attracted extensive attention in recent decades, especially research on task offloading and resource allocation. Wang et al. [26] converted the problem that combined task offloading and resource allocation in edge computing to a convex optimization problem. They proposed an alternating direction method based on a multiplier algorithm for solving the problem. Mao et al. [27] developed an offloading policy based on Lyapunov optimization. The algorithm can efficiently diminish the delay of processing tasks. In addition, reinforcement learning, as one of the machine learning approaches, has been used to solve the problem. Especially after the emergence of DRL, which adds the neural network into reinforcement learning, it can cope with the deficiency of RL in dimension catastrophe. Liang et al. [25] proposed an algorithm named DROO, which was based on the DQN algorithm to deal with task offloading and resource allocation under the time-varying wireless channel. The system framework consisted of an AP and a set of wireless devices. Ming et al. [28] investigated whether the task generated by devices was offloaded and to which edge node. The proposed a distributed algorithm based on model-free deep reinforcement learning, in which each device can determine its offloading decision without knowing the offloading decision of other devices. To improve the estimation of long-term cost in the algorithm, They combined LSTM and double DQN. Xiong et al. [29] proposed an resource allocation policy for IoT edge computing systems to optimize the resource utilization. The goal was to minimize the sum of the average completion time and the average amount of resources required with weight in the long term. He et al. [4] addressed the problem of task offloading under architecture including cloud, fog and crowd sensing devices and proposed an algorithm based on DQN. The mobile crowd sensing devices could send the request to the cloud and send the data to the corresponding fog node according to the decision of the cloud. In recent years, research based on UAV has also attracted many researchers due to the mobility of UAV. UAV-assisted emergency communication has become possible with the acceleration of UAV succession. Pavlos et al. [30] designed an offloading decision framework in which users can choose to offload data to a complex multi-access edge computing environment under resource uncertainty. The environment consisted of the ground and UAV. Zhe et al. [31] considered a UAV-enabled system for solving the problem that users offload computation tasks to UAVs. The role of the UAVs is to act as a relay and data processing node to meet the communication and computing needs of devices in the ground. One of the goal of optimization is to minimize the latency of devices in the ground. Another optimization goal is to determine the location of the UAVs and the allocation of communication and computation resources. Wu et al. [32] investigated how to conserve energy of UAV. In this study, the problem combined the optimization of the UAV trajectory and the server in the ground based on LSTM. Zhuo et al. [33] proposed a heterogeneous composite fading channel model for an emergency wireless communication network. The adaptive resource allocation scheme of the composite fading channel could obtain higher capacity and energy efficiency to provide better communication services for disaster areas. Similarly, a number of studies combine the UAVs and artificial intelligence (AI) to process communication and task offloading problems. Mohamed et al. [34] introduced the application of UAVs in different scenarios with artificial intelligence in detail, for instance, wireless connectivity, disaster management, traffic control and so on. Xin et al. [35] compared the Q-learning algorithm and DQN algorithm in the problem of resource allocation with UAV-assisted Ultra-Dense network. They transformed resource allocation into power allocation to maximize energy efficiency. Cui et al. [36] investigated resource allocation with multiple UAV-assisted. Each UAV learned the best strategy through the observations of itself based on multi agent reinforcement learning (MARL) framework. In our study, we consider UAV-assisted task offloading for IoT in smart buildings and environment. We formulate the task offloading problem to minimize the long-term energy consumption. Then, we transform the task offloading problem into an MDP model. Finally, we design the UTO algorithm based on the DRL method to solve the problem. 6 Conclusion In this paper, we study the UAV-assisted offloading problem for IoT in smart buildings and environment. We formulate the offloading optimization problem with the goal of minimizing the energy consumption and the queue length together. The control decisions are to determine whether the tasks of IoT devices should be processed locally or offloaded and to determine the resource allocation of bandwidth and time for IoT devices connected to the UAV. We reformulate it as a MDP-based offloading problem. To balance the two conflicting optimization goals of energy consumption and queue length, we integrate the two goals into the unified reward function. We design the UTO approach to solve the UAV-assisted offloading problem. Our UTO approach can cope well with the challenges caused by high-dimensional and consecutive state and action space. Comparison experiments with DDPG and other non-DRL algorithms are conducted, and the results validate the effectiveness of our UTO approach in reducing both the energy consumption and queue length. CRediT authorship contribution statement Jiajie Xu: Writing \u2013 review & editing, Writing \u2013 original draft, Visualization, Validation, Supervision, Software, Resources, Project administration, Methodology, Investigation, Funding acquisition, Formal analysis, Data curation, Conceptualization. Dejuan Li: Visualization, Validation, Project administration, Investigation, Formal analysis, Data curation. Wei Gu: Writing \u2013 review & editing, Visualization, Validation, Software, Resources, Project administration, Formal analysis, Data curation, Conceptualization. Ying Chen: Writing \u2013 review & editing, Writing \u2013 original draft, Validation, Supervision, Resources, Project administration, Funding acquisition, Data curation, Conceptualization. Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements This work was partly supported by the National Natural Science Foundation of China (61902029, 61973161 and 61991404), R&D Program of Beijing Municipal Education Commission (No KM202011232015). References [1] Jiwei Huang, Zeyu Tong, and Zihan Feng. Geographical poi recommendation for internet of things: a federated learning approach using matrix factorization. Int. J. Commun. Syst., pages pp.1\u20131. doi:10.1002/dac.5161. [2] Coulby Graham K. Adrian Clear, Oliver Jones, and Alan Godfrey Low-cost, multimodal environmental monitoring based on the internet of things Build. Environ. 203 2021 108014 10.1016/j.buildenv.2021.108014 Graham Coulby, Adrian K. Clear, Oliver Jones, and Alan Godfrey. Low-cost, multimodal environmental monitoring based on the internet of things. Building and Environment, 203:108014, 2021. doi:10.1016/j.buildenv.2021.108014. [3] Ke Yan Chiller fault detection and diagnosis with anomaly detective generative adversarial network Build. Environ. 201 2021 107982 10.1016/j.buildenv.2021.107982 Ke Yan. Chiller fault detection and diagnosis with anomaly detective generative adversarial network. Building and Environment, 201:107982, 2021. doi:10.1016/j.buildenv.2021.107982. [4] Li He Kaoru Ota Mianxiong Dong Deep reinforcement scheduling for mobile crowdsensing in fog computing ACM Trans. Internet Technol. 19 2 apr 2019 10.1145/3234463 He Li, Kaoru Ota, and Mianxiong Dong. Deep reinforcement scheduling for mobile crowdsensing in fog computing. ACM Trans. Internet Technol., 19(2), apr 2019. doi:10.1145/3234463. [5] Ke Yan Jing Huang Wen Shen Zhiwei Ji Unsupervised learning for fault detection and diagnosis of air handling units Energy Build. 210 2020 109689 10.1016/j.enbuild.2019.109689 Ke Yan, Jing Huang, Wen Shen, and Zhiwei Ji. Unsupervised learning for fault detection and diagnosis of air handling units. Energy and Buildings, 210:109689, 2020. doi:10.1016/j.enbuild.2019.109689. [6] Bouziane Brik Moez Esseghir Leila Merghem-Boulahia Hichem Snoussi An iot-based deep learning approach to analyse indoor thermal comfort of disabled people Build. Environ. 203 2021 108056 10.1016/j.buildenv.2021.108056 Bouziane Brik, Moez Esseghir, Leila Merghem-Boulahia, and Hichem Snoussi. An iot-based deep learning approach to analyse indoor thermal comfort of disabled people. Building and Environment, 203:108056, 2021. doi:10.1016/j.buildenv.2021.108056. [7] Mehrzad Shahinmoghadam Worawan Natephra Motamedi Ali Bim- and iot-based virtual reality tool for real-time thermal comfort assessment in building enclosures Build. Environ. 199 2021 107905 10.1016/j.buildenv.2021.107905 Mehrzad Shahinmoghadam, Worawan Natephra, and Ali Motamedi. Bim- and iot-based virtual reality tool for real-time thermal comfort assessment in building enclosures. Building and Environment, 199:107905, 2021. doi:10.1016/j.buildenv.2021.107905. [8] Xiaokang Zhou Wei Liang Kevin I-Kai Wang T. Laurence Yang Deep correlation mining based on hierarchical hybrid networks for heterogeneous big data recommendations IEEE.Trans Comput Soc. Syst. 8 1 2021 171 178 10.1109/TCSS.2020.2987846 Xiaokang Zhou, Wei Liang, Kevin I-Kai Wang, and Laurence T. Yang. Deep correlation mining based on hierarchical hybrid networks for heterogeneous big data recommendations. IEEE Transactions on Computational Social Systems, 8(1):171-178, 2021. doi:10.1109/TCSS.2020.2987846. [9] Lianyong Qi Chunhua Hu Xuyun Zhang Mohammad R. Khosravi Suraj Sharma Shaoning Pang Tian Wang Privacy-aware data fusion and prediction with spatial-temporal context for smart city industrial environment IEEE Trans. Ind. Inf. 17 6 2021 4159 4167 10.1109/TII.2020.3012157 Lianyong Qi, Chunhua Hu, Xuyun Zhang, Mohammad R. Khosravi, Suraj Sharma, Shaoning Pang, and Tian Wang. Privacy-aware data fusion and prediction with spatial-temporal context for smart city industrial environment. IEEE Transactions on Industrial Informatics, 17(6):4159-4167, 2021. doi:10.1109/TII.2020.3012157. [10] Fan Wang Haibin Zhu Gautam Srivastava Shancang Li Mohammad Khosravi Lianyong Qi Robust collaborative filtering recommendation with user-item-trust records IEEE.Trans Comput Soc. Syst. 03 2021 10.1109/TCSS.2021.3064213 , PP:1\u201311 Fan Wang, Haibin Zhu, Gautam Srivastava, Shancang Li, Mohammad Khosravi, and Lianyong Qi. Robust collaborative filtering recommendation with user-item-trust records. IEEE Transactions on Computational Social Systems, PP:1-11, 03 2021. doi:10.1109/TCSS.2021.3064213. [11] Chen Y, Zhao F, Lu Y, and X. Chen. Dynamic task offloading for mobile edge computing with hybrid energy supply. Tsinghua Sci. Technol., 10, 2021. doi:10.26599/TST.2021.9010050. [12] Xiaokang Zhou Wei Liang Weimin Li Ke Yan Shohei Shimizu Kevin I-Kai Wang Hierarchical adversarial attacks against graph neural network based iot network intrusion detection system IEEE Internet Things J. \u20131 2021 10.1109/JIOT.2021.3130434 pages 1 Xiaokang Zhou, Wei Liang, Weimin Li, Ke Yan, Shohei Shimizu, and Kevin I-Kai Wang. Hierarchical adversarial attacks against graph neural network based iot network intrusion detection system. IEEE Internet of Things Journal, pages 1-1, 2021. doi:10.1109/JIOT.2021.3130434. [13] Wei Liang Yiyong Hu Xiaokang Zhou Yi Pan Kevin I-Kai Wang Variational few-shot learning for microservice-oriented intrusion detection in distributed industrial iot IEEE Trans. Ind. Inf. 2021 10.1109/TII.2021.3116085 pages 1\u20131 Wei Liang, Yiyong Hu, Xiaokang Zhou, Yi Pan, and Kevin I-Kai Wang. Variational few-shot learning for microservice-oriented intrusion detection in distributed industrial iot. IEEE Transactions on Industrial Informatics, pages 1-1, 2021. doi:10.1109/TII.2021.3116085. [14] Ying Chen Fengjun Zhao Xin Chen Yuan Wu Efficient multi-vehicle task offloading for mobile edge computing in 6g networks IEEE Trans. Veh. Technol. 2021 10.1109/TVT.2021.3133586 Ying Chen, Fengjun Zhao, Xin Chen, and Yuan Wu. Efficient multi-vehicle task offloading for mobile edge computing in 6g networks. IEEE Transactions on Vehicular Technology, 2021. doi:10.1109/TVT.2021.3133586. [15] Jiwei Huang Bofeng Lv Yuan Wu Dynamic admission control and resource allocation for mobile edge computing enabled small cell network IEEE Trans. Veh. Technol. 71 2 2022 1964 1973 Jiwei Huang, Bofeng Lv, Yuan Wu, and et al. Dynamic admission control and resource allocation for mobile edge computing enabled small cell network. IEEE Transactions on Vehicular Technology, 71(2):1964-1973, 2022. [16] Xiaokang Zhou Xiang Yang Jianhua Ma Kevin Wang Energy efficient smart routing based on link correlation mining for wireless edge computing in iot IEEE Internet Things J. 05 2021 10.1109/JIOT.2021.3077937 , PP:1\u20131 Xiaokang Zhou, Xiang Yang, Jianhua Ma, and Kevin Wang. Energy efficient smart routing based on link correlation mining for wireless edge computing in iot. IEEE Internet of Things Journal, PP:1-1, 05 2021. doi:10.1109/JIOT.2021.3077937. [17] Lianyong Qi Houbing Song Xuyun Zhang Gautam Srivastava Xiaolong Xu Shui Yu Compatibility-aware web api recommendation for mashup creation via textual description mining ACM Trans. Multimed Comput. Commun. Appl 17 1s mar 2021 10.1145/3417293 Lianyong Qi, Houbing Song, Xuyun Zhang, Gautam Srivastava, Xiaolong Xu, and Shui Yu. Compatibility-aware web api recommendation for mashup creation via textual description mining. ACM Trans. Multimedia Comput. Commun. Appl., 17(1s), mar 2021. doi:10.1145/3417293. [18] Ying Chen Hua Xing Zhuo Ma Xin Chen Jiwei Huang Cost-efficient edge caching for noma-enabled iot services China Communications 2022 Ying Chen, Hua Xing, Zhuo Ma, Xin Chen, and Jiwei Huang. Cost-efficient edge caching for noma-enabled iot services. China Communications, 2022. [19] Haichao Wang Guoru Ding Feifei Gao Jin Chen Jinlong Wang Le Wang Power control in uav-supported ultra dense networks: communications, caching, and energy transfer IEEE Commun. Mag. 56 6 2018 28 34 10.1109/MCOM.2018.1700431 Haichao Wang, Guoru Ding, Feifei Gao, Jin Chen, Jinlong Wang, and Le Wang. Power control in uav-supported ultra dense networks: Communications, caching, and energy transfer. IEEE Communications Magazine, 56(6):28-34, 2018. doi:10.1109/MCOM.2018.1700431. [20] Ying Chen, Wei Gu, and Kaixin Li. Dynamic task offloading for internet of things in mobile edge computing via deep reinforcement learning. Int. J. Commun. Syst., pages pp.1\u20131. dio:10.1002/dac.5154. [21] Ke Yan Jianye Su Jing Huang Yuchang Mo Chiller fault diagnosis based on vae-enabled generative adversarial networks IEEE Trans. Autom. Sci. Eng. 19 1 2022 387 395 10.1109/TASE.2020.3035620 Ke Yan, Jianye Su, Jing Huang, and Yuchang Mo. Chiller fault diagnosis based on vae-enabled generative adversarial networks. IEEE Transactions on Automation Science and Engineering, 19(1):387-395, 2022. doi:10.1109/TASE.2020.3035620. [22] Wuhui Chen Xiaoyu Qiu Ting Cai Hong-Ning Dai Zibin Zheng Yan Zhang Deep reinforcement learning for internet of things: a comprehensive survey IEEE Commun. Surv. Tutorials 23 3 2021 1659 1692 10.1109/COMST.2021.3073036 Wuhui Chen, Xiaoyu Qiu, Ting Cai, Hong-Ning Dai, Zibin Zheng, and Yan Zhang. Deep reinforcement learning for internet of things: A comprehensive survey. IEEE Communications Surveys Tutorials, 23(3):1659-1692, 2021. doi:10.1109/COMST.2021.3073036. [23] Ying Chen Zhiyong Liu Yongchao Zhang Yuan Wu Xin Chen Lian Zhao Deep reinforcement learning-based dynamic resource management for mobile edge computing in industrial internet of things IEEE Trans. Ind. Inf. 17 7 2021 4925 4934 10.1109/TII.2020.3028963 Ying Chen, Zhiyong Liu, Yongchao Zhang, Yuan Wu, Xin Chen, and Lian Zhao. Deep reinforcement learning-based dynamic resource management for mobile edge computing in industrial internet of things. IEEE Transactions on Industrial Informatics, 17(7):4925-4934, 2021. doi:10.1109/TII.2020.3028963. [24] Ke Yan Adrian Chong Yuchang Mo Generative adversarial network for fault detection diagnosis of chillers Build. Environ. 172 2020 106698 10.1016/j.buildenv.2020.106698 Ke Yan, Adrian Chong, and Yuchang Mo. Generative adversarial network for fault detection diagnosis of chillers. Building and Environment, 172:106698, 2020. doi:10.1016/j.buildenv.2020.106698. [25] Liang Huang Suzhi Bi Ying-Jun Angela Zhang Deep reinforcement learning for online computation offloading in wireless powered mobile-edge computing networks IEEE Trans. Mobile Comput. 19 11 2020 2581 2593 10.1109/TMC.2019.2928811 Liang Huang, Suzhi Bi, and Ying-Jun Angela Zhang. Deep reinforcement learning for online computation offloading in wireless powered mobile-edge computing networks. IEEE Transactions on Mobile Computing, 19(11):2581-2593, 2020. doi:10.1109/TMC.2019.2928811. [26] Chenmeng Wang Chengchao Liang F. Richard Yu Qianbin Chen Lun Tang Computation offloading and resource allocation in wireless cellular networks with mobile edge computing IEEE Trans. Wireless Commun. 16 8 2017 4924 4938 10.1109/TWC.2017.2703901 Chenmeng Wang, Chengchao Liang, F. Richard Yu, Qianbin Chen, and Lun Tang. Computation offloading and resource allocation in wireless cellular networks with mobile edge computing. IEEE Transactions on Wireless Communications, 16(8):4924-4938, 2017. doi:10.1109/TWC.2017.2703901. [27] Yuyi Mao Jun Zhang B. Khaled Letaief. Dynamic computation offloading for mobile-edge computing with energy harvesting devices IEEE J. Sel. Area. Commun. 34 12 2016 3590 3605 10.1109/JSAC.2016.2611964 Yuyi Mao, Jun Zhang, and Khaled B. Letaief. Dynamic computation offloading for mobile-edge computing with energy harvesting devices. IEEE Journal on Selected Areas in Communications, 34(12):3590-3605, 2016. doi:10.1109/JSAC.2016.2611964. [28] Ming Tang Vincent W.S. Wong Deep reinforcement learning for task offloading in mobile edge computing systems IEEE Trans. Mobile Comput. 2020 10.1109/TMC.2020.3036871 pages 1\u20131 Ming Tang and Vincent W.S. Wong. Deep reinforcement learning for task offloading in mobile edge computing systems. IEEE Transactions on Mobile Computing, pages 1-1, 2020. doi:10.1109/TMC.2020.3036871. [29] Xiong Xiong Kan Zheng Lei Lei Lu Hou Resource allocation based on deep reinforcement learning in iot edge computing IEEE J. Sel. Area. Commun. 38 6 2020 1133 1146 10.1109/JSAC.2020.2986615 Xiong Xiong, Kan Zheng, Lei Lei, and Lu Hou. Resource allocation based on deep reinforcement learning in iot edge computing. IEEE Journal on Selected Areas in Communications, 38(6):1133-1146, 2020. doi:10.1109/JSAC.2020.2986615. [30] Pavlos Athanasios Apostolopoulos Georgios Fragkos Eirini Eleni Tsiropoulou Symeon Papavassiliou Data offloading in uav-assisted multi-access edge computing systems under resource uncertainty IEEE Trans. Mobile Comput. 2021 10.1109/TMC.2021.3069911 1\u20131 Pavlos Athanasios Apostolopoulos, Georgios Fragkos, Eirini Eleni Tsiropoulou, and Symeon Papavassiliou. Data offloading in uav-assisted multi-access edge computing systems under resource uncertainty. IEEE Transactions on Mobile Computing, pages 1-1, 2021. doi:10.1109/TMC.2021.3069911. [31] Zhe Yu Yanmin Gong Shimin Gong Yuanxiong Guo Joint task offloading and resource allocation in uav-enabled mobile edge computing IEEE Internet Things J. 7 4 2020 3147 3159 10.1109/JIOT.2020.2965898 Zhe Yu, Yanmin Gong, Shimin Gong, and Yuanxiong Guo. Joint task offloading and resource allocation in uav-enabled mobile edge computing. IEEE Internet of Things Journal, 7(4):3147-3159, 2020. doi:10.1109/JIOT.2020.2965898. [32] Gaoxiang Wu Yiming Miao Yu Zhang Barnawi Ahmed Energy efficient for uav-enabled mobile edge computing networks: intelligent task prediction and offloading Comput. Commun. 150 2020 556 562 10.1016/j.comcom.2019.11.037 Gaoxiang Wu, Yiming Miao, Yu Zhang, and Ahmed Barnawi. Energy efficient for uav-enabled mobile edge computing networks: Intelligent task prediction and offloading. Computer Communications, 150:556-562, 2020. doi:10.1016/j.comcom.2019.11.037. [33] Zhuohui Yao Wenchi Cheng Wei Zhang Hailin Zhang Resource allocation for 5g-uav-based emergency wireless communications IEEE J. Sel. Area. Commun. 39 11 2021 3395 3410 10.1109/JSAC.2021.3088684 Zhuohui Yao, Wenchi Cheng, Wei Zhang, and Hailin Zhang. Resource allocation for 5g-uav-based emergency wireless communications. IEEE Journal on Selected Areas in Communications, 39(11):3395-3410, 2021. doi:10.1109/JSAC.2021.3088684. [34] Mohamed-Amine Lahmeri Mustafa A. Kishk Mohamed-Slim Alouini Artificial intelligence for uav-enabled wireless networks: a survey IEEE Open J. Commun Soc 2 2021 1015 10.1109/OJCOMS.2021.3075201 \u20131040 Mohamed-Amine Lahmeri, Mustafa A. Kishk, and Mohamed-Slim Alouini. Artificial intelligence for uav-enabled wireless networks: A survey. IEEE Open Journal of the Communications Society, 2:1015-1040, 2021. doi:10.1109/OJCOMS.2021.3075201. [35] Xin Chen Xu Liu Ying Chen Libo Jiao Geyong Min Deep q-network based resource allocation for uav-assisted ultra-dense networks Comput. Network. 196 2021 108249 Xin Chen, Xu Liu, Ying Chen, Libo Jiao, and Geyong Min. Deep q-network based resource allocation for uav-assisted ultra-dense networks. Computer Networks, 196:108249, 2021. [36] Jingjing Cui Yuanwei Liu Arumugam Nallanathan Multi-agent reinforcement learning-based resource allocation for uav networks IEEE Trans. Wireless Commun. 19 2 2020 729 743 10.1109/TWC.2019.2935201 Jingjing Cui, Yuanwei Liu, and Arumugam Nallanathan. Multi-agent reinforcement learning-based resource allocation for uav networks. IEEE Transactions on Wireless Communications, 19(2):729-743, 2020. doi:10.1109/TWC.2019.2935201.",
    "scopus-id": "85133897005",
    "coredata": {
        "eid": "1-s2.0-S0360132322004541",
        "dc:description": "With the rapid development of Internet of Things (IoT) techniques, IoT devices with sensors have been widely deployed and used in smart buildings and environment, and the application scenarios of IoT in smart buildings and environment have been further extended. However, due to the limitations of computation, storage and battery capacity, IoT devices cannot process all the tasks locally by themselves and need to offload some tasks to edge servers typically deployed in base stations (BSs). Besides, unmanned aerial vehicles (UAVs) with controllable mobility and flexibility have been recognized as a promising solution to assist communication in emergency scenarios. In this paper, we investigate UAV-assisted offloading for IoT in smart buildings and environment. We formulate the offloading problem with the goal of minimizing the long-term energy consumption and minimizing the queue length at the same time. As the solution space size is extremely large and the offloading problem focuses on the long-term optimization goal, solving this problem faces several challenges. To address these challenges, we reformulate it as a Markov decision process (MDP)-based offloading problem and propose the UAV-assisted task offloading (UTO) approach based on deep reinforcement learning (DRL) techniques. Our UTO approach can cope well with the challenges brought by high-dimensional and consecutive state and action space. We carry out a series of comparison experiments with both DRL and non-DRL algorithms, and the results validate the performance of our proposed UTO approach.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2022-08-15",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S0360132322004541",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Xu, Jiajie"
            },
            {
                "@_fa": "true",
                "$": "Li, Dejuan"
            },
            {
                "@_fa": "true",
                "$": "Gu, Wei"
            },
            {
                "@_fa": "true",
                "$": "Chen, Ying"
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S0360132322004541"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S0360132322004541"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S0360-1323(22)00454-1",
        "prism:volume": "222",
        "articleNumber": "109218",
        "prism:publisher": "Elsevier Ltd.",
        "dc:title": "UAV-assisted task offloading for IoT in smart buildings and environment via deep reinforcement learning",
        "prism:copyright": "© 2022 Elsevier Ltd. All rights reserved.",
        "openaccess": "0",
        "prism:issn": "03601323",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "Smart buildings and environment"
            },
            {
                "@_fa": "true",
                "$": "Internet of things (IoT)"
            },
            {
                "@_fa": "true",
                "$": "Task offloading"
            },
            {
                "@_fa": "true",
                "$": "Unmanned aerial vehicle (UAV)"
            },
            {
                "@_fa": "true",
                "$": "Deep reinforcement learning (DRL)"
            }
        ],
        "openaccessArticle": "false",
        "prism:publicationName": "Building and Environment",
        "openaccessSponsorType": null,
        "prism:pageRange": "109218",
        "pubType": "fla",
        "prism:coverDisplayDate": "15 August 2022",
        "prism:doi": "10.1016/j.buildenv.2022.109218",
        "prism:startingPage": "109218",
        "dc:identifier": "doi:10.1016/j.buildenv.2022.109218",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "204",
            "@width": "337",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "40962",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "247",
            "@width": "337",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "34088",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "764",
            "@width": "711",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "142674",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "336",
            "@width": "337",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr9.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "46671",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "247",
            "@width": "337",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr8.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "33839",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "337",
            "@width": "337",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr10.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "44069",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "454",
            "@width": "387",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-fx1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "139168",
            "@ref": "fx1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "259",
            "@width": "337",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "32826",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "315",
            "@width": "333",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr11.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "43447",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "247",
            "@width": "337",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "33337",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "245",
            "@width": "337",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "41786",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "265",
            "@width": "337",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "38842",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "133",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "17922",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "161",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "16199",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "152",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "17297",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "164",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr9.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "16607",
            "@ref": "gr9",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "161",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr8.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "15136",
            "@ref": "gr8",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "164",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr10.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "15373",
            "@ref": "gr10",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "140",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-fx1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "78073",
            "@ref": "fx1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "213",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "14795",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "173",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr11.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "16023",
            "@ref": "gr11",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "161",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "14809",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "159",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "17852",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "209",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "16395",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "906",
            "@width": "1495",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "250276",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1096",
            "@width": "1494",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "187679",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "3384",
            "@width": "3150",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "1002413",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1489",
            "@width": "1494",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr9_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "237961",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1096",
            "@width": "1494",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr8_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "186524",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1496",
            "@width": "1494",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr10_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "217609",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2009",
            "@width": "1713",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-fx1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "541436",
            "@ref": "fx1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1149",
            "@width": "1494",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "178483",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1397",
            "@width": "1476",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr11_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "210068",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1097",
            "@width": "1494",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "181654",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1088",
            "@width": "1494",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "248866",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1173",
            "@width": "1494",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "206549",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0360132322004541-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "2010032",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85133897005"
    }
}}