{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85084440917",
    "originalText": "serial JL 271429 291210 291702 291731 291787 291877 291878 291881 31 Applied Energy APPLIEDENERGY 2020-05-12 2020-05-12 2020-05-12 2020-05-12 2020-09-21T14:55:46 1-s2.0-S0306261920305481 S0306-2619(20)30548-1 S0306261920305481 10.1016/j.apenergy.2020.115036 S300 S300.3 FULL-TEXT 1-s2.0-S0306261920X00092 2024-01-01T13:23:58.349315Z 0 0 20200701 2020 2020-05-12T16:02:40.798252Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table e-component body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast footnotes grantsponsor grantsponsorid highlightsabst primabst ref 0306-2619 03062619 true 269 269 C Volume 269 20 115036 115036 115036 20200701 1 July 2020 2020-07-01 2020 Research Papers article fla © 2020 Elsevier Ltd. All rights reserved. REINFORCEMENTLEARNINGFORBUILDINGCONTROLSOPPORTUNITIESCHALLENGES WANG Z 1 Introduction 2 Methods and objectives 2.1 Reinforcement learning for building controls 2.2 Literature search 2.3 Previous reviews 2.4 Research gaps and objectives 3 Survey on reinforcement learning for building controls 3.1 Algorithms 3.2 States 3.3 Actions 3.4 Rewards 3.5 Environment 3.6 Application in real buildings 3.7 Discount factor 4 Discussion 4.1 Accelerate training 4.2 Control security/safety/robustness 4.3 Multi-agent problem 4.4 Performance evaluation 4.5 Contribution and implication 5 Conclusion Acknowledgments Appendix Appendix A Supplementary material References KLEPEIS 2001 231 252 N MORARI 1999 667 682 M PRIVARA 2011 564 572 S KARLSSON 2011 556 569 H HAZYUK 2012 388 394 I YUAN 2006 1248 1261 S MA 2009 392 397 Y PROCEEDINGS48HIEEECONFERENCEDECISIONCONTROLCDCHELDJOINTLY200928THCHINESECONTROLCONFERENCE MODELPREDICTIVECONTROLTHERMALENERGYSTORAGEINBUILDINGCOOLINGSYSTEMS PARIS 2010 1908 1917 B KONTES 2018 3376 G HONG 2020 T SILVER 2018 1140 1144 D LEVINE 2018 421 436 S DALAMAGKIDIS 2007 2686 2698 K WEI 2015 2509 2518 Q LIU 2006 148 161 S JIANG 2015 3 13 B CHENG 2016 43 55 Z HAN 2019 101748 M TAYLOR 2009 1633 1685 M CHEN 2020 119866 Y VAZQUEZCANTELI 2019 1072 1089 J BLUM 2019 410 425 D CHEN 2019 1141 1152 Y GUNE 2018 1 43 A ZHANG 2017 348 365 X FUSELLI 2013 148 160 D RUELENS 2015 8300 8318 F RUELENS 2017 2149 2159 F DEGRACIA 2015 234 242 A COSTANZO 2016 81 90 G RUELENS 2018 3792 3800 F DESOMER 2017 1 7 O 2017IEEEPESINNOVATIVESMARTGRIDTECHNOLOGIESCONFERENCEEUROPEISGTEUROPE USINGREINFORCEMENTLEARNINGFORDEMANDRESPONSEDOMESTICHOTWATERBUFFERSAREALLIFEDEMONSTRATION YU 2010 532 539 Z KAZMI 2018 159 168 H VAZQUEZCANTELI 2017 415 420 J HENZE 2002 139 148 G PRESENTEDASMESOLARINTERNATIONALSOLARENERGYCONFERENCE2009 ADAPTIVEOPTIMALCONTROLAGRIDINDEPENDENTPHOTOVOLTAICSYSTEM YANG 2015 577 586 L CHENXIAOGUAN 2015 637 642 Y 201512THANNUALIEEECONSUMERCOMMUNICATIONSNETWORKINGCONFERENCECCNC REINFORCEMENTLEARNINGBASEDCONTROLRESIDENTIALENERGYSTORAGESYSTEMSFORELECTRICBILLMINIMIZATION ZHOU 2019 1 10 S YOON 2019 109420 Y KIM 2016 2187 2198 B SUN 2015 1396 1406 B JIA 2019 6158 6163 R WANG 2016 77 86 Y HURTADO 2018 127 136 L RAJU 2015 231 239 L ALJABERY 2017 775 788 K MOCANU 2019 3698 3708 E BARRETT 2015 3 19 E MACHINELEARNINGKNOWLEDGEDISCOVERYINDATABASESCHAM AUTONOMOUSHVACCONTROLAREINFORCEMENTLEARNINGAPPROACH WANG 2017 46 Y CHEN 2018 195 205 Y KAZMI 2019 1022 1035 H AHN 2019 1 14 K WETTER 2018 775 782 M PROCBUILDINGPERFORMANCEMODELINGCONFERENCESIMBUILDCHICAGOILUSA OPENBUILDINGCONTROLMODELINGFEEDBACKCONTROLASTEPTOWARDSFORMALDESIGNSPECIFICATIONDEPLOYMENTVERIFICATIONBUILDINGCONTROLSEQUENCES VAZQUEZCANTELI 2019 356 357 J PROCEEDINGS6THACMINTERNATIONALCONFERENCESYSTEMSFORENERGYEFFICIENTBUILDINGSCITIESTRANSPORTATIONNEWYORKNYUSA CITYLEARNV10OPENAIGYMENVIRONMENTFORDEMANDRESPONSEDEEPREINFORCEMENTLEARNING ANDERSON 1997 421 429 C HENZE 2003 259 275 G LIU 2006 142 147 S LIU 2007 215 225 S DU 2008 26 36 D JIANG 2011 76 90 B LIANG 2013 2296 2308 Y KALIAPPAN 2013 342 347 A IEEECONFERENCECLEANENERGYTECHNOLOGYCEAT2013 FLEXIBLEPOWERCONSUMPTIONMANAGEMENTUSINGQLEARNINGTECHNIQUESINASMARTHOME LI 2014 1 6 D 2014IEEEGREENENERGYSYSTEMSCONFERENCEIGESC REINFORCEMENTLEARNINGAIDEDSMARTHOMEDECISIONMAKINGININTERACTIVESMARTGRID WEI 2014 1 7 Q 2014IEEESYMPOSIUMADAPTIVEDYNAMICPROGRAMMINGREINFORCEMENTLEARNINGADPRL OPTIMALSELFLEARNINGBATTERYCONTROLINSMARTRESIDENTIALGRIDSBYITERATIVEQLEARNINGALGORITHM LI 2015 1529 1540 D FAZENDA 2014 675 690 P WEN 2015 2312 2324 Z RAYATI 2015 1 5 M 2015IEEEPOWERENERGYSOCIETYINNOVATIVESMARTGRIDTECHNOLOGIESCONFERENCEISGT APPLYINGREINFORCEMENTLEARNINGMETHODOPTIMIZEENERGYHUBOPERATIONINSMARTGRID BERLINK 2015 331 354 H QIU 2016 1453 1461 X SEKIZAKI 2015 9 14 S 2015IEEE8THINTERNATIONALWORKSHOPCOMPUTATIONALINTELLIGENCEAPPLICATIONSIWCIA INTELLIGENTHOMEENERGYMANAGEMENTSYSTEMCLASSIFIERSYSTEM SUN 2015 2912 2917 Y 2015AMERICANCONTROLCONFERENCEACC LEARNINGBASEDBIDDINGSTRATEGYFORHVACSYSTEMSINDOUBLEAUCTIONRETAILENERGYMARKETS SHEIKHI 2016 63 77 A KAZMI 2016 1 15 H BAHRAMI 2018 4712 4725 S MBUWIR 2017 1846 B SCHMIDT 2017 257 279 M REMANI 2019 3283 3294 T CLAESSENS 2018 1 10 B ZHANG 2018 2575 Z ODONKOR 2019 P ZHANG 2019 472 490 Z LU 2019 137 146 S PARK 2019 397 414 J VAZQUEZCANTELI 2019 243 257 J WANGX2020X115036 WANGX2020X115036XZ https://vtw.elsevier.com/content/oragreement/10131 CHU_DOE publishAcceptedManuscriptIndexable http://www.elsevier.com/open-access/userlicense/1.0/ 2021-05-12T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2020 Elsevier Ltd. All rights reserved. 2020-05-04T18:15:02.956Z http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/eoas USDOE U.S. Department of Energy http://data.elsevier.com/vocabulary/SciValFunders/100000015 http://sws.geonames.org/6252001 item S0306-2619(20)30548-1 S0306261920305481 1-s2.0-S0306261920305481 10.1016/j.apenergy.2020.115036 271429 2024-01-01T13:23:58.349315Z 2020-07-01 1-s2.0-S0306261920305481-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/MAIN/application/pdf/cafea1895161d5aa5470599ffe150be2/main.pdf main.pdf pdf true 3401265 MAIN 18 1-s2.0-S0306261920305481-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/PREVIEW/image/png/f8ca1a274de064ed1845260dde61b66d/main_1.png main_1.png png 47956 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0306261920305481-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr1/DOWNSAMPLED/image/jpeg/6553c76e0537b25b1214d042b1128382/gr1.jpg gr1 gr1.jpg jpg 15971 169 364 IMAGE-DOWNSAMPLED 1-s2.0-S0306261920305481-gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr10/DOWNSAMPLED/image/jpeg/1d3dc394156f5be41b2ab14f229d08e2/gr10.jpg gr10 gr10.jpg jpg 22242 303 734 IMAGE-DOWNSAMPLED 1-s2.0-S0306261920305481-gr11.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr11/DOWNSAMPLED/image/jpeg/78bdbde59d779cee1cf105cdf3c3aba4/gr11.jpg gr11 gr11.jpg jpg 28318 303 734 IMAGE-DOWNSAMPLED 1-s2.0-S0306261920305481-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr2/DOWNSAMPLED/image/jpeg/6d40b1d16993d22f8089a9184e04d7d8/gr2.jpg gr2 gr2.jpg jpg 58976 357 690 IMAGE-DOWNSAMPLED 1-s2.0-S0306261920305481-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr3/DOWNSAMPLED/image/jpeg/1478cd2e3b049e5b020b56b5cc780ead/gr3.jpg gr3 gr3.jpg jpg 75992 689 724 IMAGE-DOWNSAMPLED 1-s2.0-S0306261920305481-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr4/DOWNSAMPLED/image/jpeg/b8674bce43a6fe717a75f24a3e5a74fe/gr4.jpg gr4 gr4.jpg jpg 55379 675 734 IMAGE-DOWNSAMPLED 1-s2.0-S0306261920305481-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr5/DOWNSAMPLED/image/jpeg/4ddbf6bd33c17579f04af6a575f3d307/gr5.jpg gr5 gr5.jpg jpg 26144 302 734 IMAGE-DOWNSAMPLED 1-s2.0-S0306261920305481-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr6/DOWNSAMPLED/image/jpeg/4cae05db77f41bca222c76bd4ada5326/gr6.jpg gr6 gr6.jpg jpg 27449 302 734 IMAGE-DOWNSAMPLED 1-s2.0-S0306261920305481-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr7/DOWNSAMPLED/image/jpeg/3c872fa95949827e08c4c44fa7210cad/gr7.jpg gr7 gr7.jpg jpg 49850 675 734 IMAGE-DOWNSAMPLED 1-s2.0-S0306261920305481-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr8/DOWNSAMPLED/image/jpeg/e0c7a81b8fa1ec2eae9baa8ffcbad1ea/gr8.jpg gr8 gr8.jpg jpg 25457 302 734 IMAGE-DOWNSAMPLED 1-s2.0-S0306261920305481-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr9/DOWNSAMPLED/image/jpeg/4a4ec2bee46088f2c37ee53f1ee5a41e/gr9.jpg gr9 gr9.jpg jpg 66242 630 600 IMAGE-DOWNSAMPLED 1-s2.0-S0306261920305481-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr1/THUMBNAIL/image/gif/a60c0423009949ff767b6d987e36affd/gr1.sml gr1 gr1.sml sml 5188 102 219 IMAGE-THUMBNAIL 1-s2.0-S0306261920305481-gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr10/THUMBNAIL/image/gif/ab36ba5f823370360197a571b5358b51/gr10.sml gr10 gr10.sml sml 4063 90 219 IMAGE-THUMBNAIL 1-s2.0-S0306261920305481-gr11.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr11/THUMBNAIL/image/gif/b7c203c8e92826403664bc0eb39a476b/gr11.sml gr11 gr11.sml sml 5078 90 219 IMAGE-THUMBNAIL 1-s2.0-S0306261920305481-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr2/THUMBNAIL/image/gif/fe27b3f96308bf8d3d29a2a700189859/gr2.sml gr2 gr2.sml sml 8802 113 219 IMAGE-THUMBNAIL 1-s2.0-S0306261920305481-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr3/THUMBNAIL/image/gif/d70cc575082f8612ec347cb12eed228d/gr3.sml gr3 gr3.sml sml 8340 164 172 IMAGE-THUMBNAIL 1-s2.0-S0306261920305481-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr4/THUMBNAIL/image/gif/80199da7e2d7345e6ec6bb0088fccc5e/gr4.sml gr4 gr4.sml sml 6150 164 178 IMAGE-THUMBNAIL 1-s2.0-S0306261920305481-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr5/THUMBNAIL/image/gif/dada7855df48bdafbbf4075426dc57f0/gr5.sml gr5 gr5.sml sml 4910 90 219 IMAGE-THUMBNAIL 1-s2.0-S0306261920305481-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr6/THUMBNAIL/image/gif/bde9340d1fe18f0543f6b7c356476e5b/gr6.sml gr6 gr6.sml sml 4615 90 219 IMAGE-THUMBNAIL 1-s2.0-S0306261920305481-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr7/THUMBNAIL/image/gif/8765bc05fb2aa5c700c97fc44f608986/gr7.sml gr7 gr7.sml sml 5730 164 178 IMAGE-THUMBNAIL 1-s2.0-S0306261920305481-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr8/THUMBNAIL/image/gif/386e1723a50b0c3466342b0dddb42164/gr8.sml gr8 gr8.sml sml 4696 90 219 IMAGE-THUMBNAIL 1-s2.0-S0306261920305481-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr9/THUMBNAIL/image/gif/807cd94aa8fd60861103b65ac8b3eed9/gr9.sml gr9 gr9.sml sml 7931 164 156 IMAGE-THUMBNAIL 1-s2.0-S0306261920305481-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr1/HIGHRES/image/jpeg/d824f5e801557645eb1468711c13c517/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 85438 750 1614 IMAGE-HIGH-RES 1-s2.0-S0306261920305481-gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr10/HIGHRES/image/jpeg/730d81c27437c29d144bef35f2824db7/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 135076 1339 3248 IMAGE-HIGH-RES 1-s2.0-S0306261920305481-gr11_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr11/HIGHRES/image/jpeg/24b4981db1737b4c570d178aad737051/gr11_lrg.jpg gr11 gr11_lrg.jpg jpg 170032 1339 3248 IMAGE-HIGH-RES 1-s2.0-S0306261920305481-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr2/HIGHRES/image/jpeg/59ff43f10414c81162d6ac773b5a361d/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 443409 1580 3055 IMAGE-HIGH-RES 1-s2.0-S0306261920305481-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr3/HIGHRES/image/jpeg/f7cf468adbcb3b0c990af3d67fc0af91/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 461367 3050 3207 IMAGE-HIGH-RES 1-s2.0-S0306261920305481-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr4/HIGHRES/image/jpeg/7f1db6a88f88d7277c18de8281daacab/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 337338 2985 3248 IMAGE-HIGH-RES 1-s2.0-S0306261920305481-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr5/HIGHRES/image/jpeg/742f0b2e6de65d93726007b92a95fce1/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 162195 1336 3248 IMAGE-HIGH-RES 1-s2.0-S0306261920305481-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr6/HIGHRES/image/jpeg/8a90953a6a7ba10c143ea9a21ddf3500/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 156832 1335 3248 IMAGE-HIGH-RES 1-s2.0-S0306261920305481-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr7/HIGHRES/image/jpeg/26a45e9cdee148fa0f9ba46c759ac845/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 307466 2988 3248 IMAGE-HIGH-RES 1-s2.0-S0306261920305481-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr8/HIGHRES/image/jpeg/1bb2cf63d5afcaeca78f37aff16f6b0a/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 140512 1338 3248 IMAGE-HIGH-RES 1-s2.0-S0306261920305481-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/gr9/HIGHRES/image/jpeg/4f25a7e3166637e89da48a67f7dd1e9e/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 407770 2793 2659 IMAGE-HIGH-RES 1-s2.0-S0306261920305481-mmc1.docx https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/mmc1/MAIN/application/vnd.openxmlformats-officedocument.wordprocessingml.document/8ad18af4023cac1cd2c2fc3fb078e7db/mmc1.docx mmc1 mmc1.docx docx 98089 APPLICATION 1-s2.0-S0306261920305481-si1.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/375ad605a47b8c5c9ce7366834370e74/si1.svg si1 si1.svg svg 8177 ALTIMG 1-s2.0-S0306261920305481-si10.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/9afb7402e10e23ca49745032344465f1/si10.svg si10 si10.svg svg 6906 ALTIMG 1-s2.0-S0306261920305481-si11.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/538bc75bca5828354f23c3a41f30a3ee/si11.svg si11 si11.svg svg 14995 ALTIMG 1-s2.0-S0306261920305481-si12.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/f42743fdd906490b71e6edd83a944319/si12.svg si12 si12.svg svg 6522 ALTIMG 1-s2.0-S0306261920305481-si13.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/8ad5d20a1dd96feb6c97da001eda5710/si13.svg si13 si13.svg svg 17988 ALTIMG 1-s2.0-S0306261920305481-si14.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/db75512f2a8a4e43f55032e1e34a809f/si14.svg si14 si14.svg svg 20597 ALTIMG 1-s2.0-S0306261920305481-si15.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/4b20674b67d67d9213b7b92f3b9be5a7/si15.svg si15 si15.svg svg 13914 ALTIMG 1-s2.0-S0306261920305481-si16.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/d258b06674a4d1cbb348d2182ee16b61/si16.svg si16 si16.svg svg 2414 ALTIMG 1-s2.0-S0306261920305481-si17.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/ba1d1d3c2666f08d604cac6d27948723/si17.svg si17 si17.svg svg 1412 ALTIMG 1-s2.0-S0306261920305481-si18.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/12dc78bc6870411af5c2a0e39a3f3ed1/si18.svg si18 si18.svg svg 4903 ALTIMG 1-s2.0-S0306261920305481-si19.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/67006fcc54508554e5c3bf86739fdbfb/si19.svg si19 si19.svg svg 3493 ALTIMG 1-s2.0-S0306261920305481-si2.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/8153adb61459c14ad5963911659d6854/si2.svg si2 si2.svg svg 5752 ALTIMG 1-s2.0-S0306261920305481-si20.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/76f360a0a035f112bb948e0862936af9/si20.svg si20 si20.svg svg 12929 ALTIMG 1-s2.0-S0306261920305481-si21.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/c11f569ffd3295078fdaa4a0bda24714/si21.svg si21 si21.svg svg 8910 ALTIMG 1-s2.0-S0306261920305481-si22.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/da040c4a84da787e2aa158e482e2f6ba/si22.svg si22 si22.svg svg 28705 ALTIMG 1-s2.0-S0306261920305481-si23.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/2fd716ab6a5849b1c71cd953cd6b2c18/si23.svg si23 si23.svg svg 7374 ALTIMG 1-s2.0-S0306261920305481-si24.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/2f99030961edda26bc9e8315348305ed/si24.svg si24 si24.svg svg 9803 ALTIMG 1-s2.0-S0306261920305481-si25.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/7513bcef3a18e315d36295cd2ae4bc16/si25.svg si25 si25.svg svg 8414 ALTIMG 1-s2.0-S0306261920305481-si26.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/b2af52a6efd0c07794289a787b6233c2/si26.svg si26 si26.svg svg 1808 ALTIMG 1-s2.0-S0306261920305481-si27.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/aa90a6ef96e1f38f8a2b6fae0676faa3/si27.svg si27 si27.svg svg 2796 ALTIMG 1-s2.0-S0306261920305481-si28.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/66f449dcd509ede89fe0490108076021/si28.svg si28 si28.svg svg 9312 ALTIMG 1-s2.0-S0306261920305481-si29.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/61c27559c5f6f239613a3de83c96c5b6/si29.svg si29 si29.svg svg 9866 ALTIMG 1-s2.0-S0306261920305481-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/e13128342d1ffc9acb74b390451598d2/si3.svg si3 si3.svg svg 8575 ALTIMG 1-s2.0-S0306261920305481-si30.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/725c0f8a818b2b3fd61042309c154e8d/si30.svg si30 si30.svg svg 5967 ALTIMG 1-s2.0-S0306261920305481-si31.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/7ba5145363f18500a48fc827f7f36ee3/si31.svg si31 si31.svg svg 7374 ALTIMG 1-s2.0-S0306261920305481-si32.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/f7d3ca7485f87e59526295237a24f56b/si32.svg si32 si32.svg svg 34290 ALTIMG 1-s2.0-S0306261920305481-si33.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/25c03d93bacf4f53021b66c879fe0958/si33.svg si33 si33.svg svg 7636 ALTIMG 1-s2.0-S0306261920305481-si34.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/483b750d5b64bb145b75907921cd0184/si34.svg si34 si34.svg svg 7840 ALTIMG 1-s2.0-S0306261920305481-si35.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/466287e48b148c3ea1aaabe74731dd7f/si35.svg si35 si35.svg svg 7744 ALTIMG 1-s2.0-S0306261920305481-si36.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/612ffdc520c206ee5aac7061a6f46909/si36.svg si36 si36.svg svg 2998 ALTIMG 1-s2.0-S0306261920305481-si37.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/0207a15037ebab3edc7444187edfe24e/si37.svg si37 si37.svg svg 1712 ALTIMG 1-s2.0-S0306261920305481-si38.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/d85311fda2527c55b5bf2f6148225d19/si38.svg si38 si38.svg svg 17713 ALTIMG 1-s2.0-S0306261920305481-si39.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/38b0b31dbf654ba44d30d8d0f22d9e77/si39.svg si39 si39.svg svg 13530 ALTIMG 1-s2.0-S0306261920305481-si4.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/e023aaaf5726cccae38db4712904679f/si4.svg si4 si4.svg svg 2951 ALTIMG 1-s2.0-S0306261920305481-si40.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/80e6d0b4339631a48c9a715756d81498/si40.svg si40 si40.svg svg 5824 ALTIMG 1-s2.0-S0306261920305481-si41.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/329745e01ad1821ea89f7074012a40a9/si41.svg si41 si41.svg svg 1397 ALTIMG 1-s2.0-S0306261920305481-si42.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/bb418195d2c9d43e70ac4843eaf44dab/si42.svg si42 si42.svg svg 20870 ALTIMG 1-s2.0-S0306261920305481-si43.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/96ca24243b77ab70434090756fc04640/si43.svg si43 si43.svg svg 1771 ALTIMG 1-s2.0-S0306261920305481-si44.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/4ef0eed8ef086685daef99397ef281c2/si44.svg si44 si44.svg svg 19534 ALTIMG 1-s2.0-S0306261920305481-si45.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/fe3df8b72617602e6401acd4247bf496/si45.svg si45 si45.svg svg 1748 ALTIMG 1-s2.0-S0306261920305481-si46.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/54070240a78f062263566d182b8b1e1e/si46.svg si46 si46.svg svg 16411 ALTIMG 1-s2.0-S0306261920305481-si5.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/a75e4fad20144c39329424372eae624c/si5.svg si5 si5.svg svg 2775 ALTIMG 1-s2.0-S0306261920305481-si6.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/e35fe20f8ce1a70388bbaa3fe131b588/si6.svg si6 si6.svg svg 1702 ALTIMG 1-s2.0-S0306261920305481-si7.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/2020f6f7e7167ef1bca985afc66da2d2/si7.svg si7 si7.svg svg 40959 ALTIMG 1-s2.0-S0306261920305481-si8.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/b0bcf466b7ed735c208f1c369289e69e/si8.svg si8 si8.svg svg 7114 ALTIMG 1-s2.0-S0306261920305481-si9.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306261920305481/STRIPIN/image/svg+xml/3edbb78942f20c0ccdefa16ef4428709/si9.svg si9 si9.svg svg 9736 ALTIMG 1-s2.0-S0306261920305481-am.pdf am am.pdf pdf 1974108 AAM-PDF https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:10N8NVMVL9P/MAIN/application/pdf/ebeedd92a9f0b0d728ec8b5e9e46118f/am.pdf APEN 115036 115036 S0306-2619(20)30548-1 10.1016/j.apenergy.2020.115036 Elsevier Ltd Fig. 1 Three types of machine learning problems. Fig. 2 Reinforcement learning for building controls. Fig. 3 Summary of articles searched. As studies using reinforcement learning for building controls were published in too many journals to be exhaustively presented, we only list those journals that published at least two papers and group the remaining into a small number of combined categories. The combined categories are noted with a star. An article might be counted twice if it controls multiple building components. Fig. 4 Algorithms of RL for building controls. A paper might be counted twice if it controls more than one building system. Fig. 5 Value functions of RL for building controls. Fig. 6 Exploration method of RL for building controls. Fig. 7 States used in RL controller for building controls. Fig. 8 Number of control points of RL for building controls. Fig. 9 Control points of RL for HVAC control: temp is short for temperature, sp is short for set-point. Fig. 10 Optimization goal of RL controller for buildings. Fig. 11 Environment to train the RL controller. Table 1 Highly cited papers in each subject. Subject Number of Citations Most Highly Cited Paper Journal/Conference of Publication HVAC 114 Dalamagkidis et al. (2007) [19] Building and Environment Batteries 108 Wei et al. (2014) [20] IEEE Transactions on Industrial Electronics Appliances 250 O'Neill et al. (2010) [18] IEEE Conference on Smart Grid Communications Domestic Hot Water 48 Ruelens et al. (2014) [21] IEEE Conference on Power Systems Computation Thermal Energy Storage 107 Liu and Henze (2006) [22] Energy and Buildings Combined Heat and Power 87 Jiang and Fei (2014) [23] IEEE Transactions on Smart Grid Windows 114 Dalamagkidis et al. (2007) [19] Building and Environment Lighting 37 Cheng et al. (2016) [24] Energy and Buildings Table 2 RL algorithms used for building controls. Algorithm π θ a t | s t V ϕ ( s t ) or Q ϕ ( s t , a t ) Popularity Model-free Policy Gradient √ × 3 out of 73 studies Value-Based × √ 56 out of 73 studies Actor-Critic √ √ 11 out of 73 studies Model-based 3 out of 73 studies Table A1 Summary of existing studies on reinforcement learning for building controls. Control objectives Control subject Algorithm Exploration Simulation environment Length of data for training Implementation in real buildings Anderson et al. (1997) [73] Energy & Comfort HVAC Value iteration ɛ-greedy Not introduced in detail No Henze and Dodier (2003) [47] Flexibility & Comfort battery, PV Tabular Q-learning Boltzmann, ɛ-greedy Not introduced in detail 30 years No Henze and Schoenmann (2003) [74] Flexibility & Comfort TES Tabular Q-learning ɛ-greedy Not introduced in detail No Liu and Henze (2006) [75,22] Flexibility & Comfort HVAC, TES Tabular Q-learning Boltzmann, ɛ-greedy Matlab/Simulink 3000\u20136000 days No Liu and Henze (2007) [76] Flexibility & Comfort HVAC, TES Tabular Q-learning Boltzmann Matlab/Simulink implementing RC (2R3C) model 6000 days No Dalamagkidis et al. (2007) [19] Energy & Comfort HVAC, window ɛ-greedy Matlab/Simulink 4 years No Du and Fei (2008) [77] Energy & Comfort HVAC Not introduced in detail No O'Neill et al. (2010) [18] Flexibility & Comfort appliances Tabular Q-learning ɛ-greedy Not introduced in detail No Yu and Dexter (2010) [44] Energy & Comfort HVAC Fuzzy Q-learning ɛ-greedy 30 days No Jiang and Fei (2011) [78] Flexibility & Comfort CHP, battery Tabular Q-learning ɛ-greedy Not introduced in detail No Liang et al. (2013) [79] Flexibility & Comfort appliances Temporal Difference Learning boltzmann No Kaliappan and Sathiakumar (2013) [80] Flexibility & Comfort appliances Matlab No Fuselli and De Angelis (2013) [36] Flexibility & Comfort battery Random perturbation Not introduced in detail No Sun et al. (2013) [55] Flexibility & Comfort HVAC Tabular Q-learning Matlab No Li and Jayaweera (2013) [81] Flexibility & Comfort appliances Tabular Q-learning ɛ-greedy 10,000 time steps No Wei and Liu (2014) [82] Flexibility & Comfort battery Not introduced in detail No Zhang and van der Schaar (2014) [52] Flexibility & Comfort battery Not introduced in detail No Wei and Liu (2014) [20] Flexibility & Comfort battery No Li and Jayaweera (2014) [83] Flexibility & Comfort battery ɛ-greedy 10,000 time steps No Jiang and Fei (2014) [23] Flexibility & Comfort CHP, appliances, battery ɛ-greedy self-coded in Java No Ruelens et al. (2014) [21] Flexibility & Comfort domestic hot water Fitted Q-iteration boltzmann 40\u201345 days No Fazenda and Veeramachaneni (2014) [84] Energy & Comfort HVAC Wire fitted neural network Matlab 50 days No Wen et al. (2015) [85] Flexibility & Comfort appliances Tabular Q-learning No Kim et al. (2015) [53] Flexibility & Comfort appliances Not used No Rayati et al. (2015) [86] Flexibility & Comfort appliances, CHP, domestic hot water ɛ-greedy Not introduced in detail No Wang et al. (2015) [59] Flexibility & Comfort battery Fitted Q-iteration Matlab No Raju et al. (2015) [61] Flexibility & Comfort battery Coordinated Q-learning self-coded in Python No Berlink et al. (2015) [87] Flexibility & Comfort battery Tabular Q-learning ɛ-greedy Simulation with historical data No Guan et al. (2015) [49] Flexibility & Comfort battery Temperoral Difference ɛ-greedy Simulation with historical data No Qiu et al. (2015) [88] Flexibility & Comfort battery Tabular Q-learning ɛ-greedy Simulation with historical data No Sekizaki et al. (2015) [89] Flexibility & Comfort battery, domestic hot water ɛ-greedy Simulation with historical data No Yang et al. (2015) [48] Energy & Comfort HVAC Batch Q-learning with Memory Replay ɛ-greedy Matlab/Simulink 3 years No Ruelens et al. (2015) [37] Energy & Comfort HVAC Fitted Q-iteration Boltzmann RC model (1R1C for air and the building envelope) No Barrett and Linder (2015) [64] Energy & Comfort HVAC Tabular Q-learning ɛ-greedy Not introduced in detail No Li and Xia (2015) [54] Energy & Comfort HVAC Tabular Q-learning ɛ-greedy Matlab, Energyplus No Sun et al. (2015) [90] Flexibility & Comfort HVAC Tabular Q-learning ɛ-greedy Matlab No Sun et al. (2015) [56] Energy & Comfort HVAC Tabular Q-learning ɛ-greedy Matlab No de Gracia et al. (2015) [39] Energy & Comfort TES SARSA ɛ-greedy Self-coded numerical equation No Sheikhi et al. (2016) [91] Flexibility & Comfort appliances Tabular Q-learning ɛ-greedy Matlab No Kazmi et al. (2016) [92] Energy & Comfort domestic hot water Hybrid Ant-Colony Optimization was used to find the optimal control solution Yes Al-Jabery et al. (2016) [62] Flexibility & Comfort domestic hot water Random selection Matlab No Ruelens et al. (2016) [42] Flexibility & Comfort domestic hot water Fitted Q-iteration Boltzmann 40 days Yes Leurs et al. (2016) [40] Flexibility & Comfort HVAC Fitted Q-iteration Boltzmann RC model (2R2C) No Ruelens et al. (2016) [38] Flexibility & Comfort HVAC Fitted Q-iteration ɛ-greedy RC Model (Second order) No Costanzo et al. (2016) [41] Flexibility & Comfort HVAC Fitted Q-iteration ɛ-greedy RC model 20 days Yes Cheng et al. (2016) [24] Energy & Comfort lighting Tabular Q-learning ɛ-greedy Not used Yes Bahrami et al. (2017) [93] Flexibility & Comfort appliances Boltzmann Not introduced in detail No Mbuwir et al. (2017) [94] Flexibility & Comfort battery Fitted Q-iteration Simulation with historical data No De Somer et al. (2017) [43] Flexibility & Comfort domestic hot water Fitted Q-iteration Not introduced in detail Simulation with historical data 2 months No Wang et al. (2017) [65] Energy & Comfort HVAC EnergyPlus No Schmidt et al. (2017) [95] Energy & Comfort HVAC Fitted Q-iteration Gaussian noise No Vázquez-Canteli et al. (2017) [46] Energy & Comfort TES Fitted Q-iteration Boltzmann CitySim No Hurtado et al. (2017) [60] Flexibility & Comfort Matlab/Simulink No Zhang et al. (2017) [35] Flexibility & Comfort ɛ-greedy Not introduced in detail No Remani et al. (2018) [96] Flexibility & Comfort appliances Fitted Q-iteration ɛ-greedy Not introduced in detail No Kazmi et al. (2018) [45] Energy & Comfort domestic hot water as part of the reward function Not used Yes Claessens et al. (2018) [97] Energy & Flexibility & Comfort district heating Fitted Q-iteration Boltzmann Not introduced in detail 60 days No Kontes et al. (2018) [14] Energy & Comfort HVAC Not introduced in detail EnergyPlus No Zhang et al. (2018) [98] Energy & Comfort HVAC Not introduced in detail Not used No Jia et al. (2018) [57] Energy & Comfort HVAC Not used EnergyPlus No Mocanu et al. (2018) [63] Flexibility & Comfort HVAC, appliances Deep Q-learning, Deep policy gradient Not used Not used Yes Chen et al. (2018) [66] Energy & Comfort HVAC, window Q-learning ɛ-greedy Not introduced in detail No Zhou et al. (2019) [50] Flexibility & Comfort battery Fuzzy Q-learning ɛ-greedy Not introduced in detail No Odonkor et al. (2019) [99] Flexibility & Comfort battery Deep Deterministic Policy Gradients with experience replay Not introduced in detail No Kazmi et al. (2019) [67] Energy & Comfort domestic hot water Monte Carlo with Exploring Starts ɛ-greedy Not introduced in detail No Ahn and Park (2019) [68] Energy & Comfort HVAC ɛ-greedy EnergyPlus on DOE reference model No Zhang et al. (2019) [100] Energy & Comfort HVAC Asynchronous advantage actor- critic ɛ-greedy EnergyPlus on real buildings No Lu et al. (2019) [101] Comfort HVAC Tabular Q-learning ɛ-greedy ASHRAE database No Yoon and Moon (2019) [51] Energy & Comfort HVAC Double Deep Q-learning with experience replay Not introduced in detail EnergyPlus No Chen et al. (2019) [32] Energy & Comfort HVAC Differentiable MPC; REINFORCE Not used EnergyPlus Yes Park et al. (2019) [102] Energy & Comfort lighting Value iteration limited state and action, explore all of them Not used Yes Vázquez-Canteli et al. (2019) [103] Energy & Flexibility & Comfort TES Fitted Q-iteration Boltzmann CitySim 40 days No May (2019) [4] Comfort window SARSA Not used Not used Yes Reinforcement learning for building controls: The opportunities and challenges Zhe Wang Tianzhen Hong \u204e Building Technology and Urban Systems Division, Lawrence Berkeley National Laboratory, One Cyclotron Road, Berkeley, CA 94720, USA Building Technology and Urban Systems Division Lawrence Berkeley National Laboratory One Cyclotron Road Berkeley CA 94720 USA Building Technology and Urban Systems Division, Lawrence Berkeley National Laboratory, One Cyclotron Road, Berkeley, CA 94720, USA \u204e Corresponding author. Building controls are becoming more important and complicated due to the dynamic and stochastic energy demand, on-site intermittent energy supply, as well as energy storage, making it difficult for them to be optimized by conventional control techniques. Reinforcement Learning (RL), as an emerging control technique, has attracted growing research interest and demonstrated its potential to enhance building performance while addressing some limitations of other advanced control techniques, such as model predictive control. This study conducted a comprehensive review of existing studies that applied RL for building controls. It provided a detailed breakdown of the existing RL studies that use a specific variation of each major component of the Reinforcement Learning: algorithm, state, action, reward, and environment. We found RL for building controls is still in the research stage with limited applications (11%) in real buildings. Three significant barriers prevent the adoption of RL controllers in actual building controls: (1) the training process is time consuming and data demanding, (2) the control security and robustness need to be enhanced, and (3) the generalization capabilities of RL controllers need to be improved using approaches such as transfer learning. Future research may focus on developing RL controllers that could be used in real buildings, addressing current RL challenges, such as accelerating training and enhancing control robustness, as well as developing an open-source testbed and dataset for performance benchmarking of RL controllers. Keywords Building controls Reinforcement learning Machine learning Optimization Building performance 1 Introduction People spend more than 85% of their time in buildings [1]. At the same time, buildings consume about 40% of total primary energy in countries like the United States [2]. Well-performing building controls are capable of delivering a healthy and comfortable indoor environment in an energy- and carbon-efficient way. However, building controls are becoming complicated because in addition to traditional services such as lighting and HVAC, modern building energy systems must respond to on-site intermittent renewables, energy storage, electric vehicle charging, and more. Furthermore, buildings need to respond to grid signals by shifting the load to improve grid stability and security, adding a layer of complexity to building controls. The U.S. Department of Energy launched an initiative on Grid-interactive Efficient Buildings (GEB), which aims to develop and integrate technologies for grid responsive buildings to achieve lower energy use (energy efficiency), flexible loads (demand flexibility), and resilience (e.g., running in low power mode under constrained conditions such as heatwaves). Smart building controls play a critical role in GEB [3]. May [4] argued that advanced building controls need to function well in the following three aspects to make buildings smart and intelligent: first, they must balance the trade-off between multiple goals, such as occupant comfort, energy conservation, grid flexibility and carbon emission reduction; second, they must adapt autonomously to the environment and its occupants; and third, they must feed occupant feedback into the control logic (human-in-the-loop). Unfortunately, those functions are difficult to achieve using conventional building control techniques. The most conventional building control is rule-based feedback control, which includes two steps: (1) rely on some pre-determined schedules to select the setpoints (e.g., temperature setpoint), and (2) track the setpoints using techniques such as Proportional-Integral-Derivative (PID) control [5]. The rule-based prescriptive approach can maintain occupant comfort by maintaining a comfort range. Additionally, it is possible to reduce energy consumption and carbon emissions by adjusting the setpoints based on heuristic rules; for example, relaxing the temperature setpoint band during unoccupied hours or demand response events. ASHRAE Guideline 36 summarized those rules [6], which could represent the state of the art of this approach adopted by industry. The prescriptive and feedback-based reactive control strategy is simple and effective, but not optimal, for two reasons. First, predictive information is not taken into consideration, leading to sub-optimal performance. For instance, if the coming day is predicted to be hot, it might be more energy efficient to pre-cool the building in advance. Second, the control sequence (such as those parameters in the PID controller) is fixed and predetermined, so it is not customized to a specific building and climate condition. To improve building control performance, Model Predictive Control (MPC) has been explored. The three words in the Model Predictive Control correspond to its three critical steps. \u201cModel\u201d corresponds to the development and identification of models that characterize the thermal and energy dynamics of buildings and systems. \u201cPredictive\u201d corresponds to the disturbance prediction, such as weather or occupancy prediction in the building context. \u201cControl\u201d corresponds to solving the optimization problem by feeding the predictive information into the developed model. Since it was initially proposed in the 1970s in the chemical and petrochemical industries, MPC has been successfully applied in many fields [7]. In the building industry, MPC has been used to control radiant ceiling heating [8], floor heating [9], intermittent heating [10] and ventilation [11], and to optimize cold water thermal storage systems [12]. MPC has proved its potential to save energy in both simulation [13] and experimental tests on real buildings [8]. The major challenge of MPC is that it is labor-intensive and requires expertise to use. It might be cost-effective to develop and calibrate a model for a car or an airplane that can be generalized and used for many cars and airplanes. Still, every building and its energy systems are unique, so it is difficult to generalize a standard building energy model for various buildings. As a result, despite the promising results, MPC has not yet been widely adopted by the building industry [14]. Empowered by big data, powerful computing, and algorithm advancement, Machine Learning (ML) has been used in almost every stage of the building lifecycle and has demonstrated its potential to enhance building performance [15]. As a branch of machine learning specifically for control problems, Reinforcement Learning (RL) is becoming a promising method to revolutionize building controls. RL is data-driven, which could help users avoid the tedious work of developing and calibrating a detailed model, as is required by MPC. Additionally, RL could leverage the recent and rapid developments in the machine learning field, such as deep learning and feature encoding, to make better control decisions. RL has been successfully applied in other areas, ranging from gaming [16] to robotics [17]. It is the time to explore whether or not RL could be used to optimize building controls to achieve energy efficiency, demand flexibility, and resiliency, which is a new but rapidly developing area. The objectives of this paper are threefold. First, we introduce the general framework of RL and how this framework would fit into the building control field. Secondly, we provide a detailed breakdown of the existing studies that use a specific variation of each major component of the Reinforcement Learning: algorithm, state, action, reward, and environment. Last, we discuss the current challenges and future research opportunities of RL for building controls. 2 Methods and objectives 2.1 Reinforcement learning for building controls Reinforcement learning is a branch of machine learning that is specialized in solving control, or sequential decision making, problems. As shown in Fig. 1 , the three categories of machine learning problems differentiate from each other in terms of the kinds of feedback the agent/algorithm will receive after they make a decision/prediction. For supervised learning, the agent will immediately know how accurate its prediction is compared with the ground truth given by the label data. And this information will be used to update and improve the predictor. For unsupervised learning, no feedback is provided as the dataset is unlabeled. Reinforcement learning lies in the middle between the two scenarios, which receives delayed feedback. To better understand the concept of delayed feedback, we need to dive deep into the Markov Decision Process (MDP), which is the mathematical foundation of RL. MDP is formed by a tuple (S, A, P, R), as shown in Fig. 2 . \u2022 S: State The state is a mathematical description of the environment that is relevant and informative to the decision to be made. States in RL are similar to features in supervised or unsupervised learning. Taking HVAC controls as an example, current room temperature could be the state the HVAC controller wants to consider. Additionally, the predicted outdoor temperature of the next time step might also be another state variable, as this information could inform the controller to make a better decision. \u2022 A: Action Action is the decision made by the controller in terms of how to control the environment. In the example of HVAC control, the action could be adjusting indoor temperature setpoint, supply air temperature, fan speed, etc. \u2022 Environment Environment is the target of the control, which is mathematically represented by the following two functions: o P: Transition Probability The transition probability predicts how the environment will evolve if we take action a t at state s t , i.e., mapping the state and action of the current time step to the state of the next time step. o R: Reward Function The reward function predicts the immediate rewards of taking action a t at state s t , i.e., mapping the state and action to the rewards. \u2022 Controller/Agent The goal of the controller is to find the optimal Policy ( π ), which outputs an optimal action for each state. There are primarily two approaches to achieve this goal: o Model-based RL If the characteristic of the environment is known to the controller, i.e., the transition probability and the reward function are known, we could use value iteration or policy iteration to find the optimal policy. o Model-free RL In most scenarios, the behaviors of the environment are unknown to the agent. The controller needs to find out the optimal policy without modeling the environment. Model-free RL is similar to the concept of end-to-end machine learning, as they both skip some intermediate steps (modeling the environment in RL context) and achieve the goal directly. Given the RL framework, we can better understand the concept of delayed feedback. Because the feedback is delayed, the control problem becomes complicated. Under the context of RL, any action leads to two consequences, receiving an immediate reward and arriving at a new state. The control agent could not simply select the action corresponding to the highest reward; instead, it needs to consider the delayed future rewards corresponding to the new state. For instance, the action of pre-cooling might lead to higher immediate energy consumption, but in the long term, the new state saves utility costs. The strength of RL lies in its ability to optimize the trade-off between short-term and long-term benefits. To differentiate the long-term benefits from the short-term ones, the concept of Value is introduced. Value is defined as the accumulated \u2018benefits\u2019 of future multiple steps. On the contrary, reward is defined as the immediate \u2018benefits\u2019 of taking the selected action at the current time step. In other words, value is the accumulated rewards of multiple future steps until the end. As observed in Fig. 2, there are five major components in RL settings: controller, states, actions, rewards, and the environment. Varieties in the five components (such as different algorithms or different states to represent the environment) lead to different RL implementation, which results in different control performance. The ultimate goal of this study is to conduct a tutorial survey and a comprehensive review of existing studies using RL for building controls. By surveying how current researchers select state and action variables, determine reward function, and choose algorithms, we aim to present an overview of the current applications of RL for building controls. 2.2 Literature search We conducted a literature search on the academic search platform Web of Science using the topic structure and keywords shown in Equation (1), where the symbol \u201c*\u201d is used to search for terms in both singular and plural forms. The Web of Science platform could retrieve papers from both the traditional built environment field and the computer science field. We did not down select or filter out any papers that applied Reinforcement Learning in the buildings field. (1) topic = ( r e i n f o r c e m e n t l e a r n i n g ) A N D [ b u i l d i n g ∗ O R h o u s e ∗ O R h o m e O R r e s i d e n t i a l ∗ A N D c o n t r o l ] The literature search was conducted in December of 2019. With the search structure and keywords listed in Eq. (1), 77 articles on this topic were found and reviewed. The 77 studies examined in this paper are listed in Table A1 of the Appendix. Fig. 3 summarizes the papers based on their publication journals and the control subjects. As shown in Fig. 3, the publication on this topic jumped between 2014 and 2015, and then stayed stable. Applying RL for building controls is an interdisciplinary field: half of the papers are from journals focused on computer science, artificial intelligence, and controls. The remaining publications are from journals focused on buildings, energy, and the environment. To follow the latest progress, researchers need to watch journals from both fields. In terms of the control subject, 35% of studies used RL to control HVAC, and the proportion increased to 50% after 2015. Other popular subjects include the charging/discharging of batteries and scheduling of home appliances. In addition to the number of articles published, another important index for estimating the quality and influence of those publications is the number of citations, 1 1 The number of citations were retrieved from Google Scholar until December 23, 2019. which was listed in Fig. 3b. Articles in this field, in general, are cited between 20 and 70 times per paper. The most cited paper in this field was published in 2010 using tabular Q-learning for home appliance scheduling [18]. This article was published in First IEEE International Conference on Smart Grid Communications and has been cited 250 times. Papers using RL for HVAC controls were not cited as many times as those focused on other fields. A possible reason is those papers were published more recently, between 2015 and 2019. Table 1 lists the most highly cited papers in each subject, so hopefully, that list can direct readers to the most influential papers on each subject. 2.3 Previous reviews Three literature review studies were found in the literature search. They summarized the applications of reinforcement learning in building controls for three specific purposes: occupant comfort, energy savings, and demand response. Han et al. (2019) [25] reviewed the application of reinforcement learning for occupant comfort management. Thirty-three empirical studies on this topic have been identified and reviewed. Among the papers reviewed, value-based Q-learning was found to dominate the learning algorithms. The majority of papers sought to maintain comfortable indoor temperature, while other important aspects of occupant comfort, such as indoor air quality and visual comfort, are rarely studied. Another interesting finding is how the occupant comfort should be defined, as only 5 out of 33 studies include occupant feedback in the control loop. At the end of the paper, the authors proposed some future research trends. First, multi-agent reinforcement learning needs to be further explored because there might be multiple occupants present in the environment. Additionally, because reinforcement learning is computationally demanding, how to better integrate the computation platforms with the building management system is also important for the application of RL in buildings. Mason and Grijalva (2019) [26] reviewed the application of reinforcement learning for building energy management, including HVAC, water heater, appliances, lighting, photovoltaics (PV), batteries and the electrical grid. It was found that RL can typically provide savings of about 10% for HVAC and about 20% for water heaters. However, the vast majority of current studies are in simulation only. Several future research trends on this topic have been identified. First, Deep Reinforcement Learning was believed to be promising due to its capability to learn more complex policy under sophisticated environments. Second, as building operation has multiple goals (e.g., energy, comfort, cost), multi-objective RL demands further investigation, such as Pareto Q learning. Third, in the scenario of controlling a community of homes or in the campus/urban scale, multi-agent RL is needed. Last, transfer learning is crucial for the large adoption of RL for building controls, as it is time-consuming and computationally demanding, if not totally impossible, to train an RL controller for each building. Transfer Learning is defined as the process of applying the knowledge learned from one task to a related, but different, task [27]. Transfer learning is important because training the controller is a time-consuming process and requires expertise. Rather than training the RL controller on every individual building, it would be more efficient and scalable if we could train the RL controller on a small number of buildings and then apply them to larger building stocks. Transfer learning technique has been used in MPC based building controls [28], however, no successful application of transfer learning is found in the RL-based building controls. Vázquez-Canteli and Nagy (2019) [29] reviewed the use of reinforcement learning for demand response applications. In total, 105 articles were reviewed. Some common research gaps were identified; for example, only a small fraction of studies reviewed have been tested in physical systems, and very few studies include occupant feedback into the control loop. Additionally, Vázquez-Canteli and Nagy (2019) pointed out that most of the studies are not easily reproducible, and the performance of controllers are not comparable due to the different thermal dynamics and properties of different testbeds. Based on those gaps, two future research needs were identified. First, standardized control problems\u2014as well as integrated software tools that include both building simulation and machine learning features\u2014are needed to help researchers investigate their control approaches and compare them directly to other approaches. And second, the applicability of reinforcement learning in multi-agent systems needs to be further explored, especially for grid operation and optimization. 2.4 Research gaps and objectives As introduced in Section 2.1, \u201creinforcement learning\u201d is a broad and ambiguous term. A careful selection of states, actions, and algorithms is crucial for the performance of RL controllers. Meanwhile, different environmental settings make the comparison between various studies very challenging, if not impossible. As RL attracts increasing research and practical attention, it is necessary to comprehensively review and summarize which states, actions, and algorithms were selected, and how the environment was set up in existing studies. Such a review could help new researchers better understand the progress and identify research gaps in existing studies. In this study, we aim to provide a detailed breakdown of the existing studies that use a specific variation of each major component of the Reinforcement Learning, from the selection of algorithm, state, action, value approximation to the design of environment. Such a comprehensive breakdown has never been done before. Even though three review studies were found on similar topics, they only focus on a specific topic of building controls, which might not be able to provide a comprehensive overview of this topic. The goal of this study was to conduct a comprehensive survey of studies that applied RL for building controls. When designing an RL controller, many decisions need to be made; among them, how the state and action space is determined, how the reward function is designed, which algorithm is used, and where the training data come from. The object of this study was to dive deep into those subtle but essential variations. By reviewing the existing studies, we aim to present a whole picture of: (1) which areas/approaches have been extensively studied and which have not, and (2) which approach works and which does not, and why. We believe this work could help researchers learn from existing studies, get inspiration from current research trends, choose which research gaps to address, and improve the design of their own RL controller. We organize our review around five topics: algorithms, states, actions, rewards, and the environment\u2014each corresponding to a key component in the RL framework, as presented in Fig. 2. The result of this survey will be presented in Section 3. A full list of reviewed papers is shown in Table A1, so readers can easily retrieve key information as needed. In Section 4, we discuss some advanced topics of RL controllers, including how to speed up training, how to guarantee security, and how to evaluate performance. Conclusions are presented in Section 5. 3 Survey on reinforcement learning for building controls Before diving deep into the survey results, we started with the mathematical formulation of the RL problem, which is shown in Eqs. (2) and (3). The sequence of states and actions s 1 , a 1 , ⋯ s T , a T is called trajectory г, which is determined by the transition probability p s t + 1 | s t , a t and the policy π θ a t | s t . The transition probability p s t + 1 | s t , a t and the reward function r s t , a t are the characteristics of the environment. Given p s t + 1 | s t , a t and r s t , a t , the goal of the agent is to find the optimal control policy π θ a t | s t that could result in the trajectory s 1 , a 1 , ⋯ s T , a T with highest accumulative rewards E r p θ Γ [ ∑ t r ( s t , a t ) ] . The expectation operator E Γ p θ ( Γ ) is introduced because both the environment and the policy could be stochastic. (2) p θ s 1 , a 1 , ⋯ s T , a T = p s 1 ∏ t = 1 T π θ a t | s t p s t + 1 | s t , a t (3) max θ { E Γ p θ ( Γ ) ∑ t r s t , a t } 3.1 Algorithms As introduced in Section 2.1, there are two major categories of RL algorithms: model-based RLs and model-free RLs. A model-based RL learns the characteristics of the environment p s t + 1 | s t , a t and r s t , a t first, if they are not known in advance. Then the learned p s t + 1 | s t , a t and r s t , a t can be used to find the optimal policy. This approach is called \u201cmodel-based RL\u201d because the process of learning p s t + 1 | s t , a t and r s t , a t is primarily developing a model for the environment. The model could be a data-driven model, such as a deep neural network, or a physics-based model, such as thermal resistance\u2013thermal capacity model. In this regard, a model-based RL is similar to the MPC technique discussed in the Introduction section, as MPC could be developed from not only physics-based or reduced-order models [30], but also pure data-driven models [31]. However, learning an accurate model is time-consuming and requires expertise. And a more accurate model might not necessarily lead to better control [32]. The model-free RL skips the process of having to learn a model. Instead, it explores the optimal control policy by learning from the interaction with the environment. There are three approaches to find the optimal control policy without learning the model: policy gradient, actor-critic, and value-based. The Policy Gradient method directly differentiates the accumulated rewards E Γ p θ Γ ∑ t r s t , a t (referred to as J θ ) with respect to θ . After some mathematical tricks, the gradient of accumulated rewards could be rewritten as Eq. (4) [33]. Once ∇ θ J θ is calculated, we could update θ using Eq. (5) to increase the accumulated reward J θ . This approach is named \u201cpolicy gradient\u201d because it uses the gradient of policy ∇ θ l o g π θ a t | s t to update the policy. After the policy is updated, we run the new policy with the environment to collect a new trajectory and rewards s 1 , a 1 , r 1 , ⋯ s T , a T , r T , and use the new trajectory to calculate ∇ θ J θ . Thanks to the advancement of deep learning, the implementation of the policy gradient is convenient, using automatic differentiation packages [34] such as TensorFlow, Pytorch, and others. (4) ∇ θ J θ ≈ 1 N ∑ i = 1 N [ ∑ t = 1 T ∇ θ l o g π θ a t | s t ∑ t = 1 T r ( s t , a t ) ] (5) θ ← θ + α ∇ θ J θ As the log-likelihood term of ∇ θ l o g π θ a t | s t essentially quantifies how likely the action a t will be selected given the current s t , the policy gradient algorithm could be interpreted as: increasing the chance of taking action a t if a t will result in a higher accumulated reward ∑ t = 1 T r ( s t , a t ) . The Actor-Critic algorithm enhances the policy gradient approach by replacing the accumulated rewards ∑ t = 1 T r ( s t , a t ) with a value approximation function. We introduced ∑ t T r ( s t , a t ) because we need to evaluate the policy π , and use this evaluation to improve the policy. However, this evaluation has a high variance. Because the environment is stochastic in most cases, the trajectory s 1 , a 1 , r 1 , ⋯ s T , a T , r T is just one of many outcomes. Therefore using ∑ t = 1 T r ( s t , a t ) in Eq. (4) is essentially using only one sample of trajectory to estimate the performance of the policy π θ . Though the single-sample estimator is unbiased, it has a very high variance. To address this issue, Actor-Critic algorithm introduces a value estimator Q π ( s t , a t ) to replace the single sample estimator ∑ t = 1 T r ( s t , a t ) as the evaluation of the policy; and then use Q π ( s t , a t ) to update and improve the policy. Q φ π ( s t , a t ) is fitted with the sampled reward sums. This approach is named \u201cActor-Critic\u201d because in addition to the policy function π θ a t | s t (called actor), the value estimation function Q φ π ( s t , a t ) (called critic) is introduced. The third model-free RL algorithm type is Value-Based. A value-based RL learns the value function without explicitly representing the policies. The idea behind the value-based approach is: once we can evaluate every action-state pair ( s t , a t ) at any time step, there is no need to calculate ∇ θ J θ to improve the policy ( θ ← θ + α ∇ θ J θ ). Instead, we can directly use ( s t , a t ) to select our action by using an argmax operation as shown in Eq. (6). In this way, we do not need to explicitly represent the policy function. The value function of action-state pairs is called the Q function. This approach is known as Q-learning. (6) π ' a t | s t = 1 i f a l = argmax a t Q ( s t , a t ) 0 o t h e r w i s e We summarized which approach is most widely used for the purpose of building controls in Table 2 and Fig. 4 . Table 2 excluded three reviews and one tutorial from the 77 studies. It is clear the value-based approach dominates our field. The reason is that the value approximation function is needed because it significantly reduces the variance, compared with the single-sample estimation ∑ t r s t , a t . However, the policy function is not necessary, as we could directly use the argmax operation to represent and improve the policy. Therefore, the value-based approach balances well the trade-off of performance and simplicity. However, shown in Fig. 4, the policy gradient and actor-critic approaches have become increasingly popular in recent years, especially since 2017. The reason behind this trend is researchers gradually realized an explicitly represented policy function π θ a t | s t could help to transfer the knowledge learned from one building to another, i.e., facilitate transfer learning. To persuade the industry to adopt RL, it is critical to convince users that what a controller learned from one building can be generalized to another. The value function (amapping from state-action pairs to value) is not suitable to transfer, because different clients might have different control goals and utility structures. However, the policy function (a mapping from state to action) is more transferable; for instance, no matter what the goal is, turning on the heating when the indoor temperature is low remains the same for almost every building. Therefore, increasingly studies are starting to explore the possibility of using policy gradient [32] or actor-critic [35] methods to facilitate transfer learning. If an actor-critic or value-based approach is selected, the next question is how to represent the value function. The simplest idea is to use a table to record the value associated with each action-state pair. As shown in Fig. 5 , about 42% of studies adopted this simple solution. However, when the number of action/state variables increase, or if the state/action variables are continuous rather than discrete, storing the value of each action-state pair in a table becomes infeasible. To solve this problem, value function estimators have been proposed. The most widely used estimator is a deep neural network, thanks to the rapid development of deep learning. As observed in Fig. 5, using a deep neural network as a value function approximation accounts for more than 50% of studies in this field since 2018. Another important aspect RL practitioners need to consider is the balance between exploration and exploitation. RL implements the trial-and-error approach to find the optimal control policy. It tries different control policies, evaluates them, and selects the most rewarding one. However, if the controller only focuses on improving itself using either policy gradient (Eq. (5)) or argmax (Eq. (6)), the controller might be locked in local optimal if it fails to explore the entire action space. Utilizing the currently known knowledge is called exploitation, and exploring the new action space is called exploration. A good controller needs to guarantee that it has explored the whole action space and avoided the local optimal. Two exploration strategies are popular in the RL field: ε - g r e e d y and Boltzmann Exploration (a.k.a. softmax exploration). The ε - g r e e d y approach, as presented in Eqs. (7) and (8), selects the currently known optimal action with the probability of 1 - ε and selects a random action with the probability of ε . The controller with a higher ε explores more. (7) P a i = a r g m a x ( Q a i ) = 1 - ε (8) P a i = r a n d o m = ε The Boltzmann approach, as presented in Equation (9), selects the action based on the action performance Q a i and τ . τ is also called the \u201ctemperature\u201d factor, specifying how random the selection is. When τ is high, all possible actions will be explored almost equally. When τ is small, actions with high Q a i value are more likely to be selected. (9) P a i = e x p Q a i τ ∑ i = 1 n e x p Q a i τ In practice, the controller tends to explore more at the beginning of training and exploit more when the majority of action space has been explored already. This strategy could be easily implemented by reducing ε or τ for ε - g r e e d y and Boltzmann Exploration, respectively. Fig. 6 surveyed the exploration methods used in the RL controller. About 60% of studies selected ε - g r e e d y , which is three times as popular as Boltzmann Exploration. ε - g r e e d y is more popular because its simplicity does not sacrifice its performance. 3.2 States The selection of states is another crucial step for RL learning. If unnecessary states are selected, the RL controller suffers from the curse of dimensionality. Contrarily, if some important states are not selected as inputs of the controller, it is impossible for the controller to make optimal decisions, regardless of how good the algorithm is. As introduced in Section 2, RL is mathematically formed as an MDP, in which the Markovian Property must be held. Markovian Property represents the behavior that future states purely depend on the current states, which, unfortunately, does not hold for building thermal dynamics, because of the thermal mass. To solve this problem, historical states need to be included in the MDP, especially if thermal dynamics are involved. As shown in Fig. 7 a, only one out of six studies of RL for HVAC control considered historical states. The remaining studies might be problematic because they train their controllers using RL but cannot guarantee that the Markovian Property holds. One reason that the majority of studies do not consider historical states is the curse of dimensionality. For instance, in the Fuselli and De Angelis (2013) study, states of the previous two time steps were considered in the critic network, markedly increasing the number of inputs [36]. Some solutions have been proposed to address the curse of dimensionality; for example, Ruelens et al. (2015) [37] used an auto-encoder to compress the previous ten indoor temperatures and control signals into six hidden states. They then used the six hidden states to develop their RL controller. The auto-encoder is a deep neural network-based dimension reduction technique. In addition to the historical states, predicted states could also help to improve controller performance. For instance, a weather forecast could be used to inform the operation of pre-cooling or pre-heating. Integrating predicted information into control is a crucial idea introduced by MPC, and this also can be used in an RL controller. As shown in Fig. 7b, only about 16% of studies use predicted information. Ruelens et al. (2016) [38] found that including weather forecasts as states could improve the performance of RL controllers by 27%. Similarly, de Gracia et al. (2015) [39] used weather forecasts to improve the energy performance of a ventilated double skin façade controller. However, energy performance is very sensitive to the accuracy of the weather forecast. Using real weather data could save 18% energy than using predicted weather data. Actual weather forecasts unavoidably have some prediction errors; how those forecast errors influence the RL performance demands further investigation. 3.3 Actions The selection of control variables is the third decision RL practitioners need to make. Too many control points is problematic due to the curse of dimensionality. As shown in Fig. 8 , 70% of existing studies control fewer than four points. More than ten control points are included in 13.8% of studies, mostly because those controllers are designed for multi-building optimization. HVAC control is more complicated than that for other building components, including batteries or lighting. Fig. 9 a illustrates why it is challenging to design an HVAC controller that could be used in every building. HVAC control is complex for two reasons. First, there are different components: a terminal, an air handling unit, a heating/cooling source, and a condenser; and for each component, there are different device types; for instance, the terminal could be a variable air volume (VAV) box or baseboard radiator. Second, for each device, there are different levels of controls. The controller could directly control the actuator level, or the setpoint (aka supervisory control). If the supervisory control is selected, conventional controllers are needed to control the actuator to track the setpoint. Fig. 9b shows the control variables surveyed from existing studies. The majority of studies were controlling the HVAC terminals at a high level, i.e., the room temperature setpoint. At this level, pre-cooling or pre-heating strategies are optimized to shift the load. Additionally, six studies are about medium-level control of the terminal, such as the supply air temperature or flow rate of the VAV box. Very few current studies are controlling the actuator directly. 3.4 Rewards The reward function is designed based on the optimization goal. Fig. 10 shows results from surveys of the optimization goal of existing studies. The surveys found three major goals for building controls: occupant comfort, energy conservation, and load flexibility. In this study, we consider load flexibility and cost reduction to be the same goals because cost reduction is achieved by shifting the load from the periods with high utility prices to periods with lower prices. As observed in Fig. 10, occupant comfort is a prerequisite of load flexibility and energy conservation, and all studies list occupant comfort as at least one of their control goals. Additionally, more than 60% of studies aim to enhance load flexibility. All RL controllers for combined heat and power, appliance scheduling and battery operation aim to improve load flexibility. Energy conservation is another common goal, and this goal is mostly achieved by controlling HVAC. Some other important goals, such as carbon reduction, are missing in current studies using RL for building controls. When multiple goals exist, the next question is how the reward function should be formulated so that different goals can be considered simultaneously. The first approach is to use the weighted sum of different optimization goals. Chen et al. (2019) [32] integrated the comfort and energy targets by formulating the cost function as Eq. (10), where the weight η is tunable: η has a higher value during occupied hours than non-occupied hours. (10) m i n ( η C comfort + C energy ) A second approach is to form the multi-objective optimization as a constrained optimization problem. Leurs et al. (2016) [40] set a lower and upper temperature boundary to guarantee occupant comfort. To make sure the comfort constraint would be satisfied, a conventional controller that could overwrite the RL controller when the temperature is close to or beyond the boundary was set up as a backup. The idea of a backup controller enhances the reliability of the RL controller. It also was used in Costanzo et al. (2016) [41], Ruelens et al. (2016) [42], and De Somer et al. (2017) [43]. In addition to the hard constraint, Yu and Dexter (2010) [44] implemented a soft constraint to co-optimize the comfort and energy goal by posing a penalty if the indoor temperature is outside the comfort range. 3.5 Environment An RL controller is trained through trial-and-error approaches, which means an RL controller tries different policies, evaluates their performances, and then uses the evaluation to improve its policy. The trial-and-error method requires that the environment run the policy generated by the controller. This type of learning is called on-policy learning, i.e., the policy output of the controller is being carried out by the environment. However, on-policy training is challenging to implement in real buildings. A building operator would not allow an RL controller to test some random policy on an actual building because those random policies might mess up the built environment. Therefore, the idea of off-policy learning has been proposed. In off-policy training, controllers learn by observing the trajectory s 1 , a 1 , r 1 , ⋯ s T , a T , r T generated from other polices. Policy gradient and actor-critic methods require on-policy learning, while some value-based algorithms allow off-policy learning. 2 2 SARSA (State-Action-Reward-State-Action) is an on-policy value-based RL, while Q-learning is an off-policy value-based RL. This is one reason why a value-based approach is more popular than a policy gradient or actor-critic approach, because off-policy learning is more flexible than on-policy learning. Though off-policy learning is more flexible, it is not as effective as on-policy learning, because the action space cannot be fully explored, and the optimal policy might be overlooked by the current policy. Additionally, training an RL controller demands a huge amount of data. Using measured data alone might be inadequate. Therefore, some researchers use simulation to create a virtual environment. The RL controller learns by interacting with the virtual environment. Fig. 11 shows results from surveys of the simulation platform used to train an RL controller. MATLAB and EnergyPlus are the most popular simulation platforms for this purpose. 3.6 Application in real buildings Whether the RL controllers have been implemented in actual buildings and how they perform compared with conventional controllers are two key performance indicators of RL. Among the 77 studies reviewed in this paper, only nine controllers were implemented in real buildings: 3 domestic hot water controllers, 3 HVAC controllers, 2 lighting controllers and 1 window controller. Three studies reported energy/cost savings or comfort improvements compared with other controllers: In De Somer et al. (2017), the hot water controller [38] saved operational cost by 15% after 40 days of training. In Kazmi et al. (2018), the hot water controllers were implemented in 32 Dutch houses [45] and found, compared with the fixed schedule or fixed setpoint control, the RL controller reduced energy consumption by almost 20% while maintaining occupant comfort. Extrapolated to a year, the RL controller has the potential to reduce household energy consumption by up to 200 kW-hours (kWh). May (2019) [4] compared an RL controller with the manual occupant control of windows and found the RL controller could significantly improve thermal comfort and indoor air quality by 90%. However, how the improvements were quantified was not described in detail. 3.7 Discount factor Another component of a typical MDP that was not discussed in Section 2 is the discount factor γ . The discount factor will revise the accumulated rewards to E Γ p θ Γ ∑ t γ t - 1 r s t , a t . As γ is usually less than 1, future rewards are not as valuable as current rewards. The discount factor is introduced mostly to guarantee convergence of MDP. Though primarily for mathematical purposes, the discount factor could be explained in two ways: (1) immediate rewards are more valuable because immediate rewards can generate interest if the reward is in a monetary form, and (2) due to the existence of uncertainty, future benefits are associated with higher risks, and accordingly need to be discounted. Because if you are uncertain about what will happen in the future, it is not a bad idea to discount future potential rewards a bit. Even though the discount factor seems to make sense in some way, determining the proper value of γ remains a question. Vázquez-Canteli et al. (2017) [46] discussed how different values of γ would influence the behaviors of an RL controller for a heat pump. Higher γ values assign greater importance to achieve long-term rewards and accordingly lead to more frequent operation of heat pumps when the outdoor temperature is high (higher coefficient of performance [COP]). Pre-heating during the periods with a high COP consumes more energy at the current time step, while saving energy for upcoming time steps. If γ is small, the discounted future savings could not justify the current costs. Therefore, pre-heating could happen more only when γ is adequately high. 4 Discussion 4.1 Accelerate training As introduced in the previous section, training an RL controller is data- and time-demanding. An early study conducted by Henze and Dodier (2003) [47] showed 30 years of data was required to train the RL controller. With the advancement of the RL algorithm, the size of the training data has reduced significantly. The Yang et al. (2015) study found three years of training data to be adequate to guarantee that RL controllers outperform rule-based controllers [48]. How to use fewer data to achieve high performance with less training time is a crucial research question in this field. The first approach researchers have proposed to accelerate training is to reduce the dimension of state-action space. The optimization problem becomes more complicated when the number of state and action variables increases, which demands more data and longer training time. Guan et al. (2015) [49] used one state variable, the net power (defined as the difference between the electricity load and PV generation), to replace two state variables, load and generation. Additionally, auto-encoder, a neural network-based dimension reduction technique, was employed in the Ruelens et al. (2016) [42] study. Zhou et al. (2019) [50] implemented Fuzzy Q-learning, which used fuzzy rules to discretize continuous states-actions and to reduce dimensions. Yoon and Moon (2019) [51] used Gaussian Process Regression (GPR) to compress six states into two. The second approach is to decouple a complicated problem into multiple simpler problems. Ruelens et al. (2014) [21] decoupled a complicated problem with multiple action variables into multiple sub-problems, where each problem contained only one action variable. The multiple sub-problems communicate and collaborate in a multi-agent system. The authors claimed this approach brought in two benefits. First, the problem is more tractable and faster to train, as the number of state and action variables decreases for each sub-problem. Second, it provides a realistic decentralized solution with good scalability qualities. We will discuss the decentralized controller in more detail in Section 4.4. Zhang and van der Schaar (2014) [52] proposed the idea of decoupling the system dynamics into the known part and the unknown part, to speed the training. By decomposing the transition dynamics into these two parts, only the unknown part of the dynamics needs to be learned. By exploiting the partial information about the system dynamics that is already known, the convergence speed was increased by 30% compared to conventional Q-learning. Similarly, Kim et al. (2015) [53] also defined and used post-decision state (PDS) to better utilize the known dynamics, using RL to only learn unknown dynamics. The third way to decouple a complicated problem is to leverage supervisor control, i.e., controlling the setpoint only and leaving the setpoint tracking to conventional controllers such as a PID. In this way, the complexity of controlling the actuator could be left aside. As a result, fewer data and time are needed to train the controller. Li and Xia (2015) [54] used a multi-stage approach to speed the training. The idea is to first discretize the state-action space at low grid density, once the RL controller converges at the coarse discretization, then to implement a finer discretization. The simulation result shows that the multi-grid method helps accelerate the convergence of Q-learning. It is suggested to be applied to other problems where Q-learning of a single grid density is unsatisfying. Sun et al. (2013) [55] proposed an event-based approach to accelerate training. The event-based approach only updates decision variables when certain events happen. The events are defined as a set of transitions of disturbance variables such as occupancy, outdoor weather, and energy price. The authors claim this approach is effective and that it can save 70% of computational time for a building with 24 rooms. However, the event-based approach could only find the suboptimal solution, which consumes 0.5% more energy than the traditional RL controller. The event-based approach was also adopted in the Sun et al. (2015) study to accelerate training [56]. 4.2 Control security/safety/robustness As RL controllers learn the optimal policy by testing new policies and evaluating the outcomes, it is possible that some tested policies might lead to an undesirable outcome, such as too cold or too hot temperatures. The control security/safety/robustness in this section refers to minimizing or even eliminating the chance of generating control signals that might lead to undesirable outcomes during the training, testing, or implementation phases. How to avoid those undesirable outcomes and guarantee control security is a key challenge of implementing RL in real buildings. A commonly used approach to enhance control security is to set up a backup controller, like in studies [40\u201343]. When the temperature is close to or about to go beyond the comfort boundary, the backup controller is activated to overwrite the RL controller. The second approach is to pre-train the controller to make it safe enough to be implemented in real buildings. We could use simulators rather than real buildings to pre-train the controllers. Or, we could use some \u201cexpert\u201d knowledge to pre-train the controller. The \u201cexpert\u201d knowledge could be a common practice or industry standard in this field. The Jia et al. (2018) study found that, with some guidance from \u201cexpert\u201d policy, the RL controller\u2019s performance could be significantly improved [57]. Additionally, we could also use the optimal policy resulting from other optimization methods to pre-train the controller. Fuselli and De Angelis (2013) [36] used the optimal solution found from Particle Swarm Optimization (PSO) 3 3 PSO is a technique developed by Eberhard and Kennedy and inspired by certain social behaviors exhibited in bird and fish groups that is used to explore a solutions space for finding parameters that are required to optimize a specific aspect of the problem [58]. to pre-train the actor network. Wang et al. (2015) [59] first optimized the charging/discharging of the storage system by solving a model-based convex optimization problem, and then used the optimized results to pre-train the RL. Chen et al. (2019) [32] used the optimization result of MPC to pre-train the actor network. As the simulator or the model-based optimization is used only for pre-training, either the simulator or the model does not need to be very accurate. It is okay if the optimal policy found from other optimization methods is not the global optimal solution because RL will further improve the policy by fine-tuning it to better adapt to the environment. 4.3 Multi-agent problem Multi-agent systems are common for building- or campus-level control problems. Based on the information availability and optimization goal, there are four different types of multi-agent optimization problems [60]: \u2022 Centralized: One agent with available information about the whole environment makes decisions. \u2022 Decentralized: Multiple agents who only perceive their environments make decisions. \u2022 Cooperative: Agents are allowed to share their observations about the environment, and this type aims to maximize the rewards of all agents. \u2022 Non-cooperative: Agents do not share observations, and only consider their interests. Ruelens et al. (2014) [21] proposed a decentralized multi-agent solution to coordinate the operation of domestic hot water heating of multiple households. Raju et al. (2015) [61] proposed Coordinated Q-Learning to optimize the micro-grid operation. In the Raju et al. (2015) framework, the agent first learns optimal single-agent policy when acting alone in the environment using conventional Q-learning. Then the coordinated Q-learning algorithm detects if any other agents\u2019 operation would lead to any reward changes for the selected state-action pair. If no reward changes occur, then this state-action pair is marked as a \u201csafe\u201d state, and the Q-value does not need to be updated. If any changes are detected in the rewards, then this state-action pair is marked as a \u201cdangerous\u201d state, where the Q-value and corresponding optimal action will depend on other agents. For \u201cdangerous\u201d states, any interference from other agents will be reflected in the rewards. The Q-values of \u201cdangerous\u201d states need to be updated accordingly. The authors argued that distinguishing between \u201csafe\u201d and \u201cdangerous\u201d states could save computation by avoiding recalculating the Q-value of \u201csafe\u201d state-action pairs. Sun et al. (2015) explored the possibility of using the Lagrangian Relaxation (LR)-based method to co-optimize the fresh air unit (FAU) at the building level and the fan coil unit (FCU) at the room level. The Lagrangian multipliers were introduced to decouple the two-level problem into sub-problems of FAU control and FCU control. The Lagrangian multipliers would be updated in a building-level dual problem to make sure the chiller capacity constraint is being considered, and finally, be forced with iteration. 4.4 Performance evaluation Given that there are so many approaches, as introduced in Section 3, a natural question is which performs better under the context of building controls. Al-Jabery et al. (2016) [62] compared the actor-critic approach with the value-based approach for domestic hot water control and found Q-learning performs better ($466 annual savings compared with $367). Al-Jabery et al. (2016) [62] claimed that Q-learning is simpler, more robust, and more easily deployable. Mocanu et al. (2018) [63] found both Deep Q-learning and Deep Policy Gradient performs better than the tabular Q-learning, and Deep Policy Gradient is more suited to perform scheduling of energy resources than Deep Q-learning. In terms of cross-study comparison, different studies use different benchmarks to evaluate the performance of their RL controller, making cross-study comparison difficult. Yang et al. (2015) [48] compared the RL controller with a \u201crule-based controller.\u201d Barrett and Linder (2015) [64] selected the \u201calways on\u201d and \u201cprogrammable control\u201d as comparison benchmarks. Wang et al. (2017) [65] compared their RL controller with a \u201cfixed setpoint\u201d controller. Similarly, the Kazmi et al. (2018) [45] controller is compared with a \u201cfixed schedule, fixed setpoint\u201d control. Chen et al. (2018) [66] benchmarked their controller with a \u201crule-based heuristic\u201d control strategy. Kazmi et al. (2019) [67] used a rule-based dead-band controller as the benchmark. Ahn and Park (2019) [68] claimed their controller saved 15.7% energy compared with the fixed pre-determined schedule on OA damper position and temperature setpoint. Different studies use different comparison baselines, and some of those baselines are too simple to justify the performance of an RL controller. To enable the selection and performance comparison of a different RL controller, an open-sourced and well-recognized control testbed is needed. One reason why RL developed fast in the past decade is that OpenAI Gym provides a common benchmark with a wide variety of different environments [69]. A similar platform in the building controls area is needed. The good news is some efforts have been taken. For example, the Open Building controls project is targeting this goal [70]. Additionally, the OpenAI Gym environment, CityLearn, was developed and open-sourced by the UT Austin team, for the easy implementation of reinforcement learning controllers in a multi-agent demand response setting [71]. The CityLearn Challenge has been announced and aims to compare different RL algorithms that are capable of coordinating multiple buildings to maximize demand response potential [72]. 4.5 Contribution and implication In this study, we conducted a comprehensive review of studies applying RL for building controls. We dove deep into subtle but important choices researchers need to make to develop an RL controller, such as how the state and action space is determined, how the reward function is designed, which algorithm is used, where the training data come from, and other factors. This comprehensive review helps researchers better understand the general RL framework, and more importantly, the progress and challenges of applying RL for building controls, as well as helps identify the research gap and design specific RL controllers. 5 Conclusion This article reviewed a broad set of studies using reinforcement learning for building controls. The number of papers published on this topic jumped in 2015 and has remained stable since then. Reinforcement learning has demonstrated its potential to enhance building controls, although some key challenges remain to be addressed. We surveyed existing studies from five perspectives: algorithms, states, actions, rewards, and the environment, each corresponding to one of the five key components of an RL controller. Significant findings include (1) Algorithm: 77% of existing studies used value-based RL algorithms, among which Q-learning is the most popular. Actor-critic and policy gradient approaches have become more frequently used since 2017 due to their ability to facilitate transfer learning. (2) States: 91% of studies did not include historical states. As the Markovian property might not hold in building thermal dynamics, the RL controller might fail to converge to the optimal. 83% of the studies did not include predicted states, even though the predicted states (e.g., weather forecast) might provide valuable information for optimization. (3) Actions: Fewer than four variables were controlled in 69% of the studies. A majority of HVAC controllers adopted supervisory control, i.e., controlling the setpoint rather than controlling the actuator directly. In this case, conventional controllers are still needed to track the setpoint. (4) Rewards: 97% of studies have multiple objectives: either enhancing flexibility (61%) or conserving energy (34%) or both (3%) while maintaining occupant comfort. (5) Environments: 90% of studies used simulators to generate data for training the RL controller. Even though RL-based building controls have attracted increasing research interest, the RL controller is still in the research and development stage, with limited adoption in actual buildings. Among the 77 studies we surveyed, only 11% of RL controllers were implemented and tested in an actual building. Significant barriers limiting the applications of RL controller for real building controls include (1) the training process is time-consuming and data-demanding, (2) the security of controls needs to be addressed, i.e., making sure the RL controller would not mess up the building controls, especially during the training stage, (3) it is yet unknown how to implement the transfer learning so that controllers trained by a small number of buildings could be generalized and used for other buildings, and (4) a data-rich, open-sourced, and interoperable virtual testbed is needed to facilitate cross-study validations and benchmarking of performance of RL controllers. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments This work was supported by Laboratory Directed Research and Development (LDRD) funding from Lawrence Berkeley National Laboratory, provided by the Director, Office of Science, of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. Appendix See Table A1. Appendix A Supplementary material Supplementary data to this article can be found online at https://doi.org/10.1016/j.apenergy.2020.115036. Appendix A Supplementary material The following are the Supplementary data to this article: Supplementary data 1 References [1] N.E. Klepeis The National Human Activity Pattern Survey (NHAPS): a resource for assessing exposure to environmental pollutants J Expo Sci Environ Epidemiol 11 3 2001 231 252 10.1038/sj.jea.7500165 N. E. Klepeis et al., \u201cThe National Human Activity Pattern Survey (NHAPS): a resource for assessing exposure to environmental pollutants,\u201d J. Expo. Sci. Environ. Epidemiol., vol. 11, no. 3, pp. 231\u2013252, Jul. 2001, doi: 10.1038/sj.jea.7500165. [2] U. S. Energy Information Administration. Monthly Energy Review November 2019. US EIA; Nov-2019, [Online]. Available: https://www.eia.gov/totalenergy/data/monthly/pdf/sec2_3.pdf. [3] Roth A, Reyna J. Grid-interactive efficient buildings technical report series: whole-building controls, sensors, modeling, and analytics. NREL/TP-5500-75478, DOE/GO-102019-5230, 1580329; Dec. 2019. doi: 10.2172/1580329. [4] May R. The reinforcement learning method : A feasible and sustainable control strategy for efficient occupant-centred building operation in smart cities; 2019. Accessed: 23-Dec-2019. [Online]. Available: http://urn.kb.se/resolve?urn=urn:nbn:se:du-30613. [5] Geng Guang, Geary GM. On performance and tuning of PID controllers in HVAC systems. In: Proceedings of IEEE international conference on control and applications, vol. 2; 1993. p. 819\u201324. doi: 10.1109/CCA.1993.348229. [6] The American Society of Heating, Refrigerating and Air-Conditioning Engineers. Guideline 36-2018. High performance sequences of operation for HVAC systems. A.S.H.R.A.E.; 2018. [7] M. Morari J.H. Lee Model predictive control: past, present and future Comput Chem Eng 23 4 1999 667 682 10.1016/S0098-1354(98)00301-9 M. Morari and J. H. Lee, \u201cModel predictive control: past, present and future,\u201d Comput. Chem. Eng., vol. 23, no. 4, pp. 667\u2013682, May 1999, doi: 10.1016/S0098-1354(98)00301-9. [8] S. Prívara J. Široký L. Ferkl J. Cigler Model predictive control of a building heating system: The first experience Energy Build 43 2 2011 564 572 10.1016/j.enbuild.2010.10.022 S. Prívara, J. Široký, L. Ferkl, and J. Cigler, \u201cModel predictive control of a building heating system: The first experience,\u201d Energy Build., vol. 43, no. 2, pp. 564\u2013572, Feb. 2011, doi: 10.1016/j.enbuild.2010.10.022. [9] H. Karlsson C.-E. Hagentoft Application of model based predictive control for water-based floor heating in low energy residential buildings Build Environ 46 3 2011 556 569 10.1016/j.buildenv.2010.08.014 H. Karlsson and C.-E. Hagentoft, \u201cApplication of model based predictive control for water-based floor heating in low energy residential buildings,\u201d Build. Environ., vol. 46, no. 3, pp. 556\u2013569, Mar. 2011, doi: 10.1016/j.buildenv.2010.08.014. [10] I. Hazyuk C. Ghiaus D. Penhouet Optimal temperature control of intermittently heated buildings using Model Predictive Control: Part II \u2013 Control algorithm Build Environ 51 2012 388 394 10.1016/j.buildenv.2011.11.008 I. Hazyuk, C. Ghiaus, and D. Penhouet, \u201cOptimal temperature control of intermittently heated buildings using Model Predictive Control: Part II \u2013 Control algorithm,\u201d Build. Environ., vol. 51, pp. 388\u2013394, May 2012, doi: 10.1016/j.buildenv.2011.11.008. [11] S. Yuan R. Perez Multiple-zone ventilation and temperature control of a single-duct VAV system using model predictive strategy Energy Build 38 10 2006 1248 1261 10.1016/j.enbuild.2006.03.007 S. Yuan and R. Perez, \u201cMultiple-zone ventilation and temperature control of a single-duct VAV system using model predictive strategy,\u201d Energy Build., vol. 38, no. 10, pp. 1248\u20131261, Oct. 2006, doi: 10.1016/j.enbuild.2006.03.007. [12] Y. Ma F. Borrelli B. Hencey A. Packard S. Bortoff Model predictive control of thermal energy storage in building cooling systems Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference 2009 392 397 10.1109/CDC.2009.5400677 Y. Ma, F. Borrelli, B. Hencey, A. Packard, and S. Bortoff, \u201cModel Predictive Control of thermal energy storage in building cooling systems,\u201d in Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference, 2009, pp. 392\u2013397, doi: 10.1109/CDC.2009.5400677. [13] B. Paris J. Eynard S. Grieu T. Talbert M. Polit Heating control schemes for energy management in buildings Energy Build 42 10 2010 1908 1917 10.1016/j.enbuild.2010.05.027 B. Paris, J. Eynard, S. Grieu, T. Talbert, and M. Polit, \u201cHeating control schemes for energy management in buildings,\u201d Energy Build., vol. 42, no. 10, pp. 1908\u20131917, Oct. 2010, doi: 10.1016/j.enbuild.2010.05.027. [14] G.D. Kontes Simulation-based evaluation and optimization of control strategies in buildings Energies 11 12 2018 3376 10.3390/en11123376 G. D. Kontes et al., \u201cSimulation-Based Evaluation and Optimization of Control Strategies in Buildings,\u201d Energies, vol. 11, no. 12, p. 3376, Dec. 2018, doi: 10.3390/en11123376. [15] T. Hong Z. Wang X. Luo W. Zhang State-of-the-art on research and applications of machine learning in the building life cycle Energy Build 2020 10.1016/j.enbuild.2020.109831 p. 109831 T. Hong, Z. Wang, X. Luo, and W. Zhang, \u201cState-of-the-Art on Research and Applications of Machine Learning in the Building Life Cycle,\u201d Energy Build., p. 109831, Feb. 2020, doi: 10.1016/j.enbuild.2020.109831. [16] D. Silver A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play Science 362 6419 2018 1140 1144 10.1126/science.aar6404 D. Silver et al., \u201cA general reinforcement learning algorithm that masters chess, shogi, and Go through self-play,\u201d Science, vol. 362, no. 6419, pp. 1140\u20131144, Dec. 2018, doi: 10.1126/science.aar6404. [17] S. Levine P. Pastor A. Krizhevsky J. Ibarz D. Quillen Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection Int J Robot Res 37 4\u20135 2018 421 436 10.1177/0278364917710318 S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen, \u201cLearning hand-eye coordination for robotic grasping with deep learning and large-scale data collection,\u201d Int. J. Robot. Res., vol. 37, no. 4\u20135, pp. 421\u2013436, Apr. 2018, doi: 10.1177/0278364917710318. [18] O\u2019Neill D, Levorato M, Goldsmith A, Mitra U. Residential demand response using reinforcement learning. In: 2010 First IEEE international conference on smart grid communications; 2010. p. 409\u201314. doi: 10.1109/SMARTGRID.2010.5622078. [19] K. Dalamagkidis D. Kolokotsa K. Kalaitzakis G.S. Stavrakakis Reinforcement learning for energy conservation and comfort in buildings Build Environ 42 7 2007 2686 2698 10.1016/j.buildenv.2006.07.010 K. Dalamagkidis, D. Kolokotsa, K. Kalaitzakis, and G. S. Stavrakakis, \u201cReinforcement learning for energy conservation and comfort in buildings,\u201d Build. Environ., vol. 42, no. 7, pp. 2686\u20132698, Jul. 2007, doi: 10.1016/j.buildenv.2006.07.010. [20] Q. Wei D. Liu G. Shi A novel dual iterative Q-learning method for optimal battery management in smart residential environments IEEE Trans Ind Electron 62 4 2015 2509 2518 10.1109/TIE.2014.2361485 Q. Wei, D. Liu, and G. Shi, \u201cA novel dual iterative Q-learning method for optimal battery management in smart residential environments,\u201d IEEE Trans. Ind. Electron., vol. 62, no. 4, pp. 2509\u20132518, Apr. 2015, doi: 10.1109/TIE.2014.2361485. [21] Ruelens F, Claessens BJ, Vandael S, Iacovella S, Vingerhoets P, Belmans R. Demand response of a heterogeneous cluster of electric water heaters using batch reinforcement learning. In: 2014 Power systems computation conference; 2014. p. 1\u20137, doi: 10.1109/PSCC.2014.7038106. [22] S. Liu G.P. Henze Experimental analysis of simulated reinforcement learning control for active and passive building thermal storage inventory: Part 2: Results and analysis Energy Build 38 2 2006 148 161 10.1016/j.enbuild.2005.06.001 S. Liu and G. P. Henze, \u201cExperimental analysis of simulated reinforcement learning control for active and passive building thermal storage inventory: Part 2: Results and analysis,\u201d Energy Build., vol. 38, no. 2, pp. 148\u2013161, Feb. 2006, doi: 10.1016/j.enbuild.2005.06.001. [23] B. Jiang Y. Fei Smart home in smart microgrid: a cost-effective energy ecosystem with intelligent hierarchical agents IEEE Trans Smart Grid 6 1 2015 3 13 10.1109/TSG.2014.2347043 B. Jiang and Y. Fei, \u201cSmart Home in Smart Microgrid: A Cost-Effective Energy Ecosystem With Intelligent Hierarchical Agents,\u201d IEEE Trans. Smart Grid, vol. 6, no. 1, pp. 3\u201313, Jan. 2015, doi: 10.1109/TSG.2014.2347043. [24] Z. Cheng Q. Zhao F. Wang Y. Jiang L. Xia J. Ding Satisfaction based Q-learning for integrated lighting and blind control Energy Build 127 2016 43 55 10.1016/j.enbuild.2016.05.067 Z. Cheng, Q. Zhao, F. Wang, Y. Jiang, L. Xia, and J. Ding, \u201cSatisfaction based Q-learning for integrated lighting and blind control,\u201d Energy Build., vol. 127, pp. 43\u201355, Sep. 2016, doi: 10.1016/j.enbuild.2016.05.067. [25] M. Han A review of reinforcement learning methodologies for controlling occupant comfort in buildings Sustain Cities Soc 51 2019 101748 10.1016/j.scs.2019.101748 M. Han et al., \u201cA review of reinforcement learning methodologies for controlling occupant comfort in buildings,\u201d Sustain. Cities Soc., vol. 51, p. 101748, Nov. 2019, doi: 10.1016/j.scs.2019.101748. [26] Mason K, Grijalva S. A review of reinforcement learning for autonomous building energy management. ArXiv190305196 Cs Stat; Mar. 2019. Accessed: 26-Nov-2019. [Online]. Available: http://arxiv.org/abs/1903.05196. [27] M.E. Taylor P. Stone Transfer learning for reinforcement learning domains: a survey J Mach Learn Res 10 1 2009 1633 1685 M. E. Taylor and P. Stone, \u201cTransfer Learning for Reinforcement Learning Domains: A Survey,\u201d J. Mach. Learn. Res., vol. 10, no. Jul, pp. 1633\u20131685, 2009. [28] Y. Chen Z. Tong Y. Zheng H. Samuelson L. Norford Transfer learning with deep neural networks for model predictive control of HVAC and natural ventilation in smart buildings J Clean Prod 254 2020 119866 10.1016/j.jclepro.2019.119866 Y. Chen, Z. Tong, Y. Zheng, H. Samuelson, and L. Norford, \u201cTransfer learning with deep neural networks for model predictive control of HVAC and natural ventilation in smart buildings,\u201d J. Clean. Prod., vol. 254, p. 119866, May 2020, doi: 10.1016/j.jclepro.2019.119866. [29] J.R. Vázquez-Canteli Z. Nagy Reinforcement learning for demand response: A review of algorithms and modeling techniques Appl Energy 235 2019 1072 1089 10.1016/j.apenergy.2018.11.002 J. R. Vázquez-Canteli and Z. Nagy, \u201cReinforcement learning for demand response: A review of algorithms and modeling techniques,\u201d Appl. Energy, vol. 235, pp. 1072\u20131089, Feb. 2019, doi: 10.1016/j.apenergy.2018.11.002. [30] D.H. Blum K. Arendt L. Rivalin M.A. Piette M. Wetter C.T. Veje Practical factors of envelope model setup and their effects on the performance of model predictive control for building heating, ventilating, and air conditioning systems Appl Energy 236 2019 410 425 10.1016/j.apenergy.2018.11.093 D. H. Blum, K. Arendt, L. Rivalin, M. A. Piette, M. Wetter, and C. T. Veje, \u201cPractical factors of envelope model setup and their effects on the performance of model predictive control for building heating, ventilating, and air conditioning systems,\u201d Appl. Energy, vol. 236, pp. 410\u2013425, Feb. 2019, doi: 10.1016/j.apenergy.2018.11.093. [31] Y. Chen Z. Tong W. Wu H. Samuelson A. Malkawi L. Norford Achieving natural ventilation potential in practice: Control schemes and levels of automation Appl Energy 235 2019 1141 1152 10.1016/j.apenergy.2018.11.016 Y. Chen, Z. Tong, W. Wu, H. Samuelson, A. Malkawi, and L. Norford, \u201cAchieving natural ventilation potential in practice: Control schemes and levels of automation,\u201d Appl. Energy, vol. 235, pp. 1141\u20131152, Feb. 2019, doi: 10.1016/j.apenergy.2018.11.016. [32] Chen B, Cai Z, Bergés M. Gnu-RL: A precocial reinforcement learning solution for building HVAC control using a differentiable MPC policy. In: Proceedings of the 6th ACM international conference on systems for energy-efficient buildings, cities, and transportation, New York, NY, USA; 2019. p. 316\u201325, doi: 10.1145/3360322.3360849. [33] Levine S. CS 285: Deep reinforcement learning. CS 285 at UC Berkeley: Deep Reinforcement Learning. http://rail.eecs.berkeley.edu/deeprlcourse/ (accessed Jan. 02, 2020). [34] A. Güne G. Baydin B.A. Pearlmutter J.M. Siskind Automatic differentiation in machine learning: a survey J Mach Learn Res 18 2018 1 43 A. Güne¸, G. Baydin, B. A. Pearlmutter, and J. M. Siskind, \u201cAutomatic Differentiation in Machine Learning: a Survey,\u201d J. Mach. Learn. Res., vol. 18, pp. 1\u201343, 2018. [35] X. Zhang T. Bao T. Yu B. Yang C. Han Deep transfer Q-learning with virtual leader-follower for supply-demand Stackelberg game of smart grid Energy 133 2017 348 365 10.1016/j.energy.2017.05.114 X. Zhang, T. Bao, T. Yu, B. Yang, and C. Han, \u201cDeep transfer Q-learning with virtual leader-follower for supply-demand Stackelberg game of smart grid,\u201d Energy, vol. 133, pp. 348\u2013365, Aug. 2017, doi: 10.1016/j.energy.2017.05.114. [36] D. Fuselli Action dependent heuristic dynamic programming for home energy resource scheduling Int J Electr Power Energy Syst 48 2013 148 160 10.1016/j.ijepes.2012.11.023 D. Fuselli et al., \u201cAction dependent heuristic dynamic programming for home energy resource scheduling,\u201d Int. J. Electr. Power Energy Syst., vol. 48, pp. 148\u2013160, Jun. 2013, doi: 10.1016/j.ijepes.2012.11.023. [37] F. Ruelens S. Iacovella B.J. Claessens R. Belmans Learning agent for a heat-pump thermostat with a set-back strategy using model-free reinforcement learning Energies 8 8 2015 8300 8318 10.3390/en8088300 F. Ruelens, S. Iacovella, B. J. Claessens, and R. Belmans, \u201cLearning Agent for a Heat-Pump Thermostat with a Set-Back Strategy Using Model-Free Reinforcement Learning,\u201d Energies, vol. 8, no. 8, pp. 8300\u20138318, Aug. 2015, doi: 10.3390/en8088300. [38] F. Ruelens B.J. Claessens S. Vandael B. De Schutter R. Babuška R. Belmans Residential demand response of thermostatically controlled loads using batch reinforcement learning IEEE Trans Smart Grid 8 5 2017 2149 2159 10.1109/TSG.2016.2517211 F. Ruelens, B. J. Claessens, S. Vandael, B. De Schutter, R. Babuška, and R. Belmans, \u201cResidential Demand Response of Thermostatically Controlled Loads Using Batch Reinforcement Learning,\u201d IEEE Trans. Smart Grid, vol. 8, no. 5, pp. 2149\u20132159, Sep. 2017, doi: 10.1109/TSG.2016.2517211. [39] A. de Gracia C. Fernández A. Castell C. Mateu L.F. Cabeza Control of a PCM ventilated facade using reinforcement learning techniques Energy Build 106 2015 234 242 10.1016/j.enbuild.2015.06.045 A. de Gracia, C. Fernández, A. Castell, C. Mateu, and L. F. Cabeza, \u201cControl of a PCM ventilated facade using reinforcement learning techniques,\u201d Energy Build., vol. 106, pp. 234\u2013242, Nov. 2015, doi: 10.1016/j.enbuild.2015.06.045. [40] Leurs T, Claessens BJ, Ruelens F, Weckx S, Deconinck G. Beyond theory: experimental results of a self-learning air conditioning unit. In: 2016 IEEE International Energy Conference (ENERGYCON); 2016. p. 1\u20136. doi: 10.1109/ENERGYCON.2016.7513916. [41] G.T. Costanzo S. Iacovella F. Ruelens T. Leurs B.J. Claessens Experimental analysis of data-driven control for a building heating system Sustain Energy Grids Netw 6 2016 81 90 10.1016/j.segan.2016.02.002 G. T. Costanzo, S. Iacovella, F. Ruelens, T. Leurs, and B. J. Claessens, \u201cExperimental analysis of data-driven control for a building heating system,\u201d Sustain. Energy Grids Netw., vol. 6, pp. 81\u201390, Jun. 2016, doi: 10.1016/j.segan.2016.02.002. [42] F. Ruelens B.J. Claessens S. Quaiyum B. De Schutter R. Babuška R. Belmans Reinforcement learning applied to an electric water heater: from theory to practice IEEE Trans Smart Grid 9 4 2018 3792 3800 10.1109/TSG.2016.2640184 F. Ruelens, B. J. Claessens, S. Quaiyum, B. De Schutter, R. Babuška, and R. Belmans, \u201cReinforcement Learning Applied to an Electric Water Heater: From Theory to Practice,\u201d IEEE Trans. Smart Grid, vol. 9, no. 4, pp. 3792\u20133800, Jul. 2018, doi: 10.1109/TSG.2016.2640184. [43] O. De Somer A. Soares K. Vanthournout F. Spiessens T. Kuijpers K. Vossen \u201cUsing reinforcement learning for demand response of domestic hot water buffers: A real-life demonstration 2017 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe) 2017 1 7 10.1109/ISGTEurope.2017.8260152 O. De Somer, A. Soares, K. Vanthournout, F. Spiessens, T. Kuijpers, and K. Vossen, \u201cUsing reinforcement learning for demand response of domestic hot water buffers: A real-life demonstration,\u201d in 2017 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe), 2017, pp. 1\u20137, doi: 10.1109/ISGTEurope.2017.8260152. [44] Z. Yu A. Dexter Online tuning of a supervisory fuzzy controller for low-energy building system using reinforcement learning Control Eng Pract 18 5 2010 532 539 10.1016/j.conengprac.2010.01.018 Z. Yu and A. Dexter, \u201cOnline tuning of a supervisory fuzzy controller for low-energy building system using reinforcement learning,\u201d Control Eng. Pract., vol. 18, no. 5, pp. 532\u2013539, May 2010, doi: 10.1016/j.conengprac.2010.01.018. [45] H. Kazmi F. Mehmood S. Lodeweyckx J. Driesen Gigawatt-hour scale savings on a budget of zero: Deep reinforcement learning based optimal control of hot water systems Energy 144 2018 159 168 10.1016/j.energy.2017.12.019 H. Kazmi, F. Mehmood, S. Lodeweyckx, and J. Driesen, \u201cGigawatt-hour scale savings on a budget of zero: Deep reinforcement learning based optimal control of hot water systems,\u201d Energy, vol. 144, pp. 159\u2013168, Feb. 2018, doi: 10.1016/j.energy.2017.12.019. [46] J. Vázquez-Canteli J. Kämpf Z. Nagy Balancing comfort and energy consumption of a heat pump using batch reinforcement learning with fitted Q-iteration Energy Procedia 122 2017 415 420 10.1016/j.egypro.2017.07.429 J. Vázquez-Canteli, J. Kämpf, and Z. Nagy, \u201cBalancing comfort and energy consumption of a heat pump using batch reinforcement learning with fitted Q-iteration,\u201d Energy Procedia, vol. 122, pp. 415\u2013420, Sep. 2017, doi: 10.1016/j.egypro.2017.07.429. [47] G.P. Henze R.H. Dodier Adaptive optimal control of a grid-independent photovoltaic system Presented at the ASME Solar International Solar Energy Conference 2009 2002 139 148 10.1115/SED2002-1045 G. P. Henze and R. H. Dodier, \u201cAdaptive Optimal Control of a Grid-Independent Photovoltaic System,\u201d presented at the ASME Solar 2002: International Solar Energy Conference, 2009, pp. 139\u2013148, doi: 10.1115/SED2002-1045. [48] L. Yang Z. Nagy P. Goffin A. Schlueter Reinforcement learning for optimal control of low exergy buildings Appl Energy 156 2015 577 586 10.1016/j.apenergy.2015.07.050 L. Yang, Z. Nagy, P. Goffin, and A. Schlueter, \u201cReinforcement learning for optimal control of low exergy buildings,\u201d Appl. Energy, vol. 156, pp. 577\u2013586, Oct. 2015, doi: 10.1016/j.apenergy.2015.07.050. [49] Y. Chenxiao Guan Xue Lin Wang S. Nazarian M. Pedram Reinforcement learning-based control of residential energy storage systems for electric bill minimization 2015 12th Annual IEEE Consumer Communications and Networking Conference (CCNC) 2015 637 642 10.1109/CCNC.2015.7158054 Chenxiao Guan, Y. Wang, Xue Lin, S. Nazarian, and M. Pedram, \u201cReinforcement learning-based control of residential energy storage systems for electric bill minimization,\u201d in 2015 12th Annual IEEE Consumer Communications and Networking Conference (CCNC), 2015, pp. 637\u2013642, doi: 10.1109/CCNC.2015.7158054. [50] S. Zhou Z. Hu W. Gu M. Jiang X.-P. Zhang Artificial intelligence based smart energy community management: A reinforcement learning approach CSEE J Power Energy Syst 5 1 2019 1 10 10.17775/CSEEJPES.2018.00840 S. Zhou, Z. Hu, W. Gu, M. Jiang, and X.-P. Zhang, \u201cArtificial intelligence based smart energy community management: A reinforcement learning approach,\u201d CSEE J. Power Energy Syst., vol. 5, no. 1, pp. 1\u201310, Mar. 2019, doi: 10.17775/CSEEJPES.2018.00840. [51] Y.R. Yoon H.J. Moon Performance based thermal comfort control (PTCC) using deep reinforcement learning for space cooling Energy Build 203 2019 109420 10.1016/j.enbuild.2019.109420 Y. R. Yoon and H. J. Moon, \u201cPerformance based thermal comfort control (PTCC) using deep reinforcement learning for space cooling,\u201d Energy Build., vol. 203, p. 109420, Nov. 2019, doi: 10.1016/j.enbuild.2019.109420. [52] Zhang Y, van der Schaar M. Structure-aware stochastic load management in smart grids. In: IEEE INFOCOM 2014 \u2013 IEEE conference on computer communications; 2014. p. 2643\u201351. doi: 10.1109/INFOCOM.2014.6848212. [53] B.-G. Kim Y. Zhang M. van der Schaar J.-W. Lee Dynamic pricing and energy consumption scheduling with reinforcement learning IEEE Trans Smart Grid 7 5 2016 2187 2198 10.1109/TSG.2015.2495145 B.-G. Kim, Y. Zhang, M. van der Schaar, and J.-W. Lee, \u201cDynamic Pricing and Energy Consumption Scheduling With Reinforcement Learning,\u201d IEEE Trans. Smart Grid, vol. 7, no. 5, pp. 2187\u20132198, Sep. 2016, doi: 10.1109/TSG.2015.2495145. [54] Li B, Xia L. A multi-grid reinforcement learning method for energy conservation and comfort of HVAC in buildings. In: 2015 IEEE International Conference on Automation Science and Engineering (CASE); 2015. p. 444\u20139, doi: 10.1109/CoASE.2015.7294119. [55] Sun B, Luh PB, Jia Q-S, Yan B. Event-based optimization with non-stationary uncertainties to save energy costs of HVAC systems in buildings. In: 2013 IEEE International Conference on Automation Science and Engineering (CASE), 2013, pp. 436\u2013441, doi: 10.1109/CoASE.2013.6654055. [56] B. Sun P.B. Luh Q.-S. Jia B. Yan Event-based optimization within the lagrangian relaxation framework for energy savings in HVAC systems IEEE Trans Autom Sci Eng 12 4 2015 1396 1406 10.1109/TASE.2015.2455419 B. Sun, P. B. Luh, Q.-S. Jia, and B. Yan, \u201cEvent-Based Optimization Within the Lagrangian Relaxation Framework for Energy Savings in HVAC Systems,\u201d IEEE Trans. Autom. Sci. Eng., vol. 12, no. 4, pp. 1396\u20131406, Oct. 2015, doi: 10.1109/TASE.2015.2455419. [57] R. Jia M. Jin K. Sun T. Hong C. Spanos Advanced building control via deep reinforcement learning Energy Procedia 158 2019 6158 6163 10.1016/j.egypro.2019.01.494 R. Jia, M. Jin, K. Sun, T. Hong, and C. Spanos, \u201cAdvanced Building Control via Deep Reinforcement Learning,\u201d Energy Procedia, vol. 158, pp. 6158\u20136163, Feb. 2019, doi: 10.1016/j.egypro.2019.01.494. [58] Eberhart, Shi Y. Particle swarm optimization: developments, applications and resources. In: Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546), vol. 1; 2001. p. 81\u20136. doi: 10.1109/CEC.2001.934374. [59] Y. Wang X. Lin M. Pedram A near-optimal model-based control algorithm for households equipped with residential photovoltaic power generation and energy storage systems IEEE Trans Sustain Energy 7 1 2016 77 86 10.1109/TSTE.2015.2467190 Y. Wang, X. Lin, and M. Pedram, \u201cA Near-Optimal Model-Based Control Algorithm for Households Equipped With Residential Photovoltaic Power Generation and Energy Storage Systems,\u201d IEEE Trans. Sustain. Energy, vol. 7, no. 1, pp. 77\u201386, Jan. 2016, doi: 10.1109/TSTE.2015.2467190. [60] L.A. Hurtado E. Mocanu P.H. Nguyen M. Gibescu R.I.G. Kamphuis Enabling cooperative behavior for building demand response based on extended joint action learning IEEE Trans Ind Inform 14 1 2018 127 136 10.1109/TII.2017.2753408 L. A. Hurtado, E. Mocanu, P. H. Nguyen, M. Gibescu, and R. I. G. Kamphuis, \u201cEnabling Cooperative Behavior for Building Demand Response Based on Extended Joint Action Learning,\u201d IEEE Trans. Ind. Inform., vol. 14, no. 1, pp. 127\u2013136, Jan. 2018, doi: 10.1109/TII.2017.2753408. [61] L. Raju S. Sankar R.S. Milton Distributed optimization of solar micro-grid using multi agent reinforcement learning Procedia Comput Sci 46 2015 231 239 10.1016/j.procs.2015.02.016 L. Raju, S. Sankar, and R. S. Milton, \u201cDistributed Optimization of Solar Micro-grid Using Multi Agent Reinforcement Learning,\u201d Procedia Comput. Sci., vol. 46, pp. 231\u2013239, Jan. 2015, doi: 10.1016/j.procs.2015.02.016. [62] K. Al-jabery Z. Xu W. Yu D.C. Wunsch J. Xiong Y. Shi Demand-side management of domestic electric water heaters using approximate dynamic programming IEEE Trans Comput-Aided Des Integr Circuits Syst 36 5 2017 775 788 10.1109/TCAD.2016.2598563 K. Al-jabery, Z. Xu, W. Yu, D. C. Wunsch, J. Xiong, and Y. Shi, \u201cDemand-Side Management of Domestic Electric Water Heaters Using Approximate Dynamic Programming,\u201d IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst., vol. 36, no. 5, pp. 775\u2013788, May 2017, doi: 10.1109/TCAD.2016.2598563. [63] E. Mocanu On-line building energy optimization using deep reinforcement learning IEEE Trans Smart Grid 10 4 2019 3698 3708 10.1109/TSG.2018.2834219 E. Mocanu et al., \u201cOn-Line Building Energy Optimization Using Deep Reinforcement Learning,\u201d IEEE Trans. Smart Grid, vol. 10, no. 4, pp. 3698\u20133708, Jul. 2019, doi: 10.1109/TSG.2018.2834219. [64] E. Barrett S. Linder Autonomous HVAC Control, a reinforcement learning approach Machine learning and knowledge discovery in databases, Cham 2015 3 19 10.1007/978-3-319-23461-8_1 E. Barrett and S. Linder, \u201cAutonomous HVAC Control, A Reinforcement Learning Approach,\u201d in Machine Learning and Knowledge Discovery in Databases, Cham, 2015, pp. 3\u201319, doi: 10.1007/978-3-319-23461-8_1. [65] Y. Wang K. Velswamy B. Huang A long-short term memory recurrent neural network based reinforcement learning controller for office heating ventilation and air conditioning systems Processes 5 3 2017 46 10.3390/pr5030046 Y. Wang, K. Velswamy, and B. Huang, \u201cA Long-Short Term Memory Recurrent Neural Network Based Reinforcement Learning Controller for Office Heating Ventilation and Air Conditioning Systems,\u201d Processes, vol. 5, no. 3, p. 46, Sep. 2017, doi: 10.3390/pr5030046. [66] Y. Chen L.K. Norford H.W. Samuelson A. Malkawi Optimal control of HVAC and window systems for natural ventilation through reinforcement learning Energy Build 169 2018 195 205 10.1016/j.enbuild.2018.03.051 Y. Chen, L. K. Norford, H. W. Samuelson, and A. Malkawi, \u201cOptimal control of HVAC and window systems for natural ventilation through reinforcement learning,\u201d Energy Build., vol. 169, pp. 195\u2013205, Jun. 2018, doi: 10.1016/j.enbuild.2018.03.051. [67] H. Kazmi J. Suykens A. Balint J. Driesen Multi-agent reinforcement learning for modeling and control of thermostatically controlled loads Appl Energy 238 2019 1022 1035 10.1016/j.apenergy.2019.01.140 H. Kazmi, J. Suykens, A. Balint, and J. Driesen, \u201cMulti-agent reinforcement learning for modeling and control of thermostatically controlled loads,\u201d Appl. Energy, vol. 238, pp. 1022\u20131035, Mar. 2019, doi: 10.1016/j.apenergy.2019.01.140. [68] K.U. Ahn C.S. Park Application of deep Q-networks for model-free optimal control balancing between different HVAC systems Sci Technol Built Environ 2019 1 14 10.1080/23744731.2019.1680234 K. U. Ahn and C. S. Park, \u201cApplication of deep Q-networks for model-free optimal control balancing between different HVAC systems,\u201d Sci. Technol. Built Environ., vol. 0, no. 0, pp. 1\u201314, Oct. 2019, doi: 10.1080/23744731.2019.1680234. [69] Brockman G et al. OpenAI Gym; Jun. 2016. Accessed: 02-Jan-2020. [Online]. Available: https://arxiv.org/abs/1606.01540v1. [70] M. Wetter J. Hu M. Grahovac B. Eubanks P. Haves OpenBuildingControl: Modeling feedback control as a step towards formal design, specification, deployment and verification of building control sequences Proc of building performance modeling conference and SimBuild, Chicago, IL, USA 2018 775 782 M. Wetter, J. Hu, M. Grahovac, B. Eubanks, and P. Haves, \u201cOpenBuildingControl: Modeling feedback control as a step towards formal design, specification, deployment and verification of building control sequences,\u201d in Proc. of Building Performance Modeling Conference and SimBuild, Chicago, IL, USA, 2018, pp. 775\u2013782. [71] J.R. Vázquez-Canteli J. Kämpf G. Henze Z. Nagy CityLearn v1.0: An OpenAI gym environment for demand response with deep reinforcement learning Proceedings of the 6th ACM international conference on systems for energy-efficient buildings, cities, and transportation, New York, NY, USA 2019 356 357 10.1145/3360322.3360998 J. R. Vázquez-Canteli, J. Kämpf, G. Henze, and Z. Nagy, \u201cCityLearn v1.0: An OpenAI Gym Environment for Demand Response with Deep Reinforcement Learning,\u201d in Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, New York, NY, USA, 2019, pp. 356\u2013357, doi: 10.1145/3360322.3360998. [72] www.citylearn.net. https://sites.google.com/view/citylearnchallenge (accessed Mar. 27, 2020). [73] C.W. Anderson D.C. Hittle A.D. Katz R.M. Kretchmar Synthesis of reinforcement learning, neural networks and PI control applied to a simulated heating coil Artif Intell Eng 11 4 1997 421 429 10.1016/S0954-1810(97)00004-6 C. W. Anderson, D. C. Hittle, A. D. Katz, and R. M. Kretchmar, \u201cSynthesis of reinforcement learning, neural networks and PI control applied to a simulated heating coil,\u201d Artif. Intell. Eng., vol. 11, no. 4, pp. 421\u2013429, Oct. 1997, doi: 10.1016/S0954-1810(97)00004-6. [74] G.P. Henze J. Schoenmann Evaluation of reinforcement learning control for thermal energy storage systems HVACR Res 9 3 2003 259 275 10.1080/10789669.2003.10391069 G. P. Henze and J. Schoenmann, \u201cEvaluation of Reinforcement Learning Control for Thermal Energy Storage Systems,\u201d HVACR Res., vol. 9, no. 3, pp. 259\u2013275, Jul. 2003, doi: 10.1080/10789669.2003.10391069. [75] S. Liu G.P. Henze Experimental analysis of simulated reinforcement learning control for active and passive building thermal storage inventory: Part 1. Theoretical foundation Energy Build 38 2 2006 142 147 10.1016/j.enbuild.2005.06.002 S. Liu and G. P. Henze, \u201cExperimental analysis of simulated reinforcement learning control for active and passive building thermal storage inventory: Part 1. Theoretical foundation,\u201d Energy Build., vol. 38, no. 2, pp. 142\u2013147, Feb. 2006, doi: 10.1016/j.enbuild.2005.06.002. [76] S. Liu G.P. Henze Evaluation of reinforcement learning for optimal control of building active and passive thermal storage inventory J Sol Energy Eng 129 2 2007 215 225 10.1115/1.2710491 S. Liu and G. P. Henze, \u201cEvaluation of Reinforcement Learning for Optimal Control of Building Active and Passive Thermal Storage Inventory,\u201d J. Sol. Energy Eng., vol. 129, no. 2, pp. 215\u2013225, May 2007, doi: 10.1115/1.2710491. [77] D. Du M. Fei A two-layer networked learning control system using actor\u2013critic neural network Appl Math Comput 205 1 2008 26 36 10.1016/j.amc.2008.05.062 D. Du and M. Fei, \u201cA two-layer networked learning control system using actor\u2013critic neural network,\u201d Appl. Math. Comput., vol. 205, no. 1, pp. 26\u201336, Nov. 2008, doi: 10.1016/j.amc.2008.05.062. [78] B. Jiang Y. Fei Dynamic residential demand response and distributed generation management in smart microgrid with hierarchical agents Energy Procedia 12 2011 76 90 10.1016/j.egypro.2011.10.012 B. Jiang and Y. Fei, \u201cDynamic Residential Demand Response and Distributed Generation Management in Smart Microgrid with Hierarchical Agents,\u201d Energy Procedia, vol. 12, pp. 76\u201390, Jan. 2011, doi: 10.1016/j.egypro.2011.10.012. [79] Y. Liang L. He X. Cao Z.-J. Shen Stochastic control for smart grid users with flexible demand IEEE Trans Smart Grid 4 4 2013 2296 2308 10.1109/TSG.2013.2263201 Y. Liang, L. He, X. Cao, and Z.-J. Shen, \u201cStochastic Control for Smart Grid Users With Flexible Demand,\u201d IEEE Trans. Smart Grid, vol. 4, no. 4, pp. 2296\u20132308, Dec. 2013, doi: 10.1109/TSG.2013.2263201. [80] A.T. Kaliappan S. Sathiakumar N. Parameswaran Flexible power consumption management using Q learning techniques in a smart home IEEE Conference on Clean Energy and Technology (CEAT), 2013 2013 342 347 10.1109/CEAT.2013.6775653 A. T. Kaliappan, S. Sathiakumar, and N. Parameswaran, \u201cFlexible power consumption management using Q learning techniques in a smart home,\u201d in 2013 IEEE Conference on Clean Energy and Technology (CEAT), 2013, pp. 342\u2013347, doi: 10.1109/CEAT.2013.6775653. [81] D. Li S.K. Jayaweera Reinforcement learning aided smart-home decision-making in an interactive smart grid 2014 IEEE Green Energy and Systems Conference (IGESC) 2014 1 6 10.1109/IGESC.2014.7018632 D. Li and S. K. Jayaweera, \u201cReinforcement learning aided smart-home decision-making in an interactive smart grid,\u201d in 2014 IEEE Green Energy and Systems Conference (IGESC), 2014, pp. 1\u20136, doi: 10.1109/IGESC.2014.7018632. [82] Q. Wei D. Liu G. Shi Y. Liu Q. Guan Optimal self-learning battery control in smart residential grids by iterative Q-learning algorithm 2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL) 2014 1 7 10.1109/ADPRL.2014.7010630 Q. Wei, D. Liu, G. Shi, Y. Liu, and Q. Guan, \u201cOptimal self-learning battery control in smart residential grids by iterative Q-learning algorithm,\u201d in 2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), 2014, pp. 1\u20137, doi: 10.1109/ADPRL.2014.7010630. [83] D. Li S.K. Jayaweera Machine-learning aided optimal customer decisions for an interactive smart grid IEEE Syst J 9 4 2015 1529 1540 10.1109/JSYST.2014.2334637 D. Li and S. K. Jayaweera, \u201cMachine-Learning Aided Optimal Customer Decisions for an Interactive Smart Grid,\u201d IEEE Syst. J., vol. 9, no. 4, pp. 1529\u20131540, Dec. 2015, doi: 10.1109/JSYST.2014.2334637. [84] P. Fazenda K. Veeramachaneni P. Lima U.-M. O\u2019Reilly Using reinforcement learning to optimize occupant comfort and energy usage in HVAC systems J Ambient Intell Smart Environ 6 6 2014 675 690 10.3233/AIS-140288 P. Fazenda, K. Veeramachaneni, P. Lima, and U.-M. O\u2019Reilly, \u201cUsing reinforcement learning to optimize occupant comfort and energy usage in HVAC systems,\u201d J. Ambient Intell. Smart Environ., vol. 6, no. 6, pp. 675\u2013690, Jan. 2014, doi: 10.3233/AIS-140288. [85] Z. Wen D. O\u2019Neill H. Maei Optimal demand response using device-based reinforcement learning IEEE Trans Smart Grid 6 5 2015 2312 2324 10.1109/TSG.2015.2396993 Z. Wen, D. O\u2019Neill, and H. Maei, \u201cOptimal Demand Response Using Device-Based Reinforcement Learning,\u201d IEEE Trans. Smart Grid, vol. 6, no. 5, pp. 2312\u20132324, Sep. 2015, doi: 10.1109/TSG.2015.2396993. [86] M. Rayati A. Sheikhi A.M. Ranjbar Applying reinforcement learning method to optimize an Energy Hub operation in the smart grid 2015 IEEE Power Energy Society Innovative Smart Grid Technologies Conference (ISGT) 2015 1 5 10.1109/ISGT.2015.7131906 M. Rayati, A. Sheikhi, and A. M. Ranjbar, \u201cApplying reinforcement learning method to optimize an Energy Hub operation in the smart grid,\u201d in 2015 IEEE Power Energy Society Innovative Smart Grid Technologies Conference (ISGT), 2015, pp. 1\u20135, doi: 10.1109/ISGT.2015.7131906. [87] H. Berlink N. Kagan A.H. Reali Costa Intelligent decision-making for smart home energy management J Intell Robot Syst 80 1 2015 331 354 10.1007/s10846-014-0169-8 H. Berlink, N. Kagan, and A. H. Reali Costa, \u201cIntelligent Decision-Making for Smart Home Energy Management,\u201d J. Intell. Robot. Syst., vol. 80, no. 1, pp. 331\u2013354, Dec. 2015, doi: 10.1007/s10846-014-0169-8. [88] X. Qiu T.A. Nguyen M.L. Crow Heterogeneous energy storage optimization for microgrids IEEE Trans Smart Grid 7 3 2016 1453 1461 10.1109/TSG.2015.2461134 X. Qiu, T. A. Nguyen, and M. L. Crow, \u201cHeterogeneous Energy Storage Optimization for Microgrids,\u201d IEEE Trans. Smart Grid, vol. 7, no. 3, pp. 1453\u20131461, May 2016, doi: 10.1109/TSG.2015.2461134. [89] S. Sekizaki T. Hayashida I. Nishizaki An intelligent home energy management system with classifier system 2015 IEEE 8th International Workshop on Computational Intelligence and Applications (IWCIA) 2015 9 14 10.1109/IWCIA.2015.7449452 S. Sekizaki, T. Hayashida, and I. Nishizaki, \u201cAn intelligent Home Energy Management System with classifier system,\u201d in 2015 IEEE 8th International Workshop on Computational Intelligence and Applications (IWCIA), 2015, pp. 9\u201314, doi: 10.1109/IWCIA.2015.7449452. [90] Y. Sun A. Somani T.E. Carroll Learning based bidding strategy for HVAC systems in double auction retail energy markets 2015 American Control Conference (ACC) 2015 2912 2917 10.1109/ACC.2015.7171177 Y. Sun, A. Somani, and T. E. Carroll, \u201cLearning based bidding strategy for HVAC systems in double auction retail energy markets,\u201d in 2015 American Control Conference (ACC), 2015, pp. 2912\u20132917, doi: 10.1109/ACC.2015.7171177. [91] A. Sheikhi M. Rayati A.M. Ranjbar Demand side management for a residential customer in multi-energy systems Sustain Cities Soc 22 2016 63 77 10.1016/j.scs.2016.01.010 A. Sheikhi, M. Rayati, and A. M. Ranjbar, \u201cDemand side management for a residential customer in multi-energy systems,\u201d Sustain. Cities Soc., vol. 22, pp. 63\u201377, Apr. 2016, doi: 10.1016/j.scs.2016.01.010. [92] H. Kazmi S. D\u2019Oca C. Delmastro S. Lodeweyckx S.P. Corgnati Generalizable occupant-driven optimization model for domestic hot water production in NZEB Appl Energy 175 2016 1 15 10.1016/j.apenergy.2016.04.108 H. Kazmi, S. D\u2019Oca, C. Delmastro, S. Lodeweyckx, and S. P. Corgnati, \u201cGeneralizable occupant-driven optimization model for domestic hot water production in NZEB,\u201d Appl. Energy, vol. 175, pp. 1\u201315, Aug. 2016, doi: 10.1016/j.apenergy.2016.04.108. [93] S. Bahrami V.W.S. Wong J. Huang An online learning algorithm for demand response in smart grid IEEE Trans Smart Grid 9 5 2018 4712 4725 10.1109/TSG.2017.2667599 S. Bahrami, V. W. S. Wong, and J. Huang, \u201cAn Online Learning Algorithm for Demand Response in Smart Grid,\u201d IEEE Trans. Smart Grid, vol. 9, no. 5, pp. 4712\u20134725, Sep. 2018, doi: 10.1109/TSG.2017.2667599. [94] B.V. Mbuwir F. Ruelens F. Spiessens G. Deconinck Battery energy management in a microgrid using batch reinforcement learning Energies 10 11 2017 1846 10.3390/en10111846 B. V. Mbuwir, F. Ruelens, F. Spiessens, and G. Deconinck, \u201cBattery Energy Management in a Microgrid Using Batch Reinforcement Learning,\u201d Energies, vol. 10, no. 11, p. 1846, Nov. 2017, doi: 10.3390/en10111846. [95] M. Schmidt M.V. Moreno A. Schülke K. Macek K. Mařík A.G. Pastor Optimizing legacy building operation: The evolution into data-driven predictive cyber-physical systems Energy Build 148 2017 257 279 10.1016/j.enbuild.2017.05.002 M. Schmidt, M. V. Moreno, A. Schülke, K. Macek, K. Mařík, and A. G. Pastor, \u201cOptimizing legacy building operation: The evolution into data-driven predictive cyber-physical systems,\u201d Energy Build., vol. 148, pp. 257\u2013279, Aug. 2017, doi: 10.1016/j.enbuild.2017.05.002. [96] T. Remani E.A. Jasmin T.P.I. Ahamed Residential load scheduling with renewable generation in the smart grid: a reinforcement learning approach IEEE Syst J 13 3 2019 3283 3294 10.1109/JSYST.2018.2855689 T. Remani, E. A. Jasmin, and T. P. I. Ahamed, \u201cResidential Load Scheduling With Renewable Generation in the Smart Grid: A Reinforcement Learning Approach,\u201d IEEE Syst. J., vol. 13, no. 3, pp. 3283\u20133294, Sep. 2019, doi: 10.1109/JSYST.2018.2855689. [97] B.J. Claessens D. Vanhoudt J. Desmedt F. Ruelens Model-free control of thermostatically controlled loads connected to a district heating network Energy Build 159 2018 1 10 10.1016/j.enbuild.2017.08.052 B. J. Claessens, D. Vanhoudt, J. Desmedt, and F. Ruelens, \u201cModel-free control of thermostatically controlled loads connected to a district heating network,\u201d Energy Build., vol. 159, pp. 1\u201310, Jan. 2018, doi: 10.1016/j.enbuild.2017.08.052. [98] Z. Zhang C. Ma R. Zhu Thermal and energy management based on bimodal airflow-temperature sensing and reinforcement learning Energies 11 10 2018 2575 10.3390/en11102575 Z. Zhang, C. Ma, and R. Zhu, \u201cThermal and Energy Management Based on Bimodal Airflow-Temperature Sensing and Reinforcement Learning,\u201d Energies, vol. 11, no. 10, p. 2575, Oct. 2018, doi: 10.3390/en11102575. [99] P. Odonkor K. Lewis Automated design of energy efficient control strategies for building clusters using reinforcement learning J Mech Des 141 2 2019 10.1115/1.4041629 P. Odonkor and K. Lewis, \u201cAutomated Design of Energy Efficient Control Strategies for Building Clusters Using Reinforcement Learning,\u201d J. Mech. Des., vol. 141, no. 2, Feb. 2019, doi: 10.1115/1.4041629. [100] Z. Zhang A. Chong Y. Pan C. Zhang K.P. Lam Whole building energy model for HVAC optimal control: A practical framework based on deep reinforcement learning Energy Build 199 2019 472 490 10.1016/j.enbuild.2019.07.029 Z. Zhang, A. Chong, Y. Pan, C. Zhang, and K. P. Lam, \u201cWhole building energy model for HVAC optimal control: A practical framework based on deep reinforcement learning,\u201d Energy Build., vol. 199, pp. 472\u2013490, Sep. 2019, doi: 10.1016/j.enbuild.2019.07.029. [101] S. Lu W. Wang C. Lin E.C. Hameen Data-driven simulation of a thermal comfort-based temperature set-point control with ASHRAE RP884 Build Environ 156 2019 137 146 10.1016/j.buildenv.2019.03.010 S. Lu, W. Wang, C. Lin, and E. C. Hameen, \u201cData-driven simulation of a thermal comfort-based temperature set-point control with ASHRAE RP884,\u201d Build. Environ., vol. 156, pp. 137\u2013146, Jun. 2019, doi: 10.1016/j.buildenv.2019.03.010. [102] J.Y. Park T. Dougherty H. Fritz Z. Nagy LightLearn: An adaptive and occupant centered controller for lighting based on reinforcement learning Build Environ 147 2019 397 414 10.1016/j.buildenv.2018.10.028 J. Y. Park, T. Dougherty, H. Fritz, and Z. Nagy, \u201cLightLearn: An adaptive and occupant centered controller for lighting based on reinforcement learning,\u201d Build. Environ., vol. 147, pp. 397\u2013414, Jan. 2019, doi: 10.1016/j.buildenv.2018.10.028. [103] J.R. Vázquez-Canteli S. Ulyanin J. Kämpf Z. Nagy Fusing TensorFlow with building energy simulation for intelligent energy management in smart cities Sustain Cities Soc 45 2019 243 257 10.1016/j.scs.2018.11.021 J. R. Vázquez-Canteli, S. Ulyanin, J. Kämpf, and Z. Nagy, \u201cFusing TensorFlow with building energy simulation for intelligent energy management in smart cities,\u201d Sustain. Cities Soc., vol. 45, pp. 243\u2013257, Feb. 2019, doi: 10.1016/j.scs.2018.11.021.",
    "scopus-id": "85084440917",
    "coredata": {
        "eid": "1-s2.0-S0306261920305481",
        "dc:description": "Building controls are becoming more important and complicated due to the dynamic and stochastic energy demand, on-site intermittent energy supply, as well as energy storage, making it difficult for them to be optimized by conventional control techniques. Reinforcement Learning (RL), as an emerging control technique, has attracted growing research interest and demonstrated its potential to enhance building performance while addressing some limitations of other advanced control techniques, such as model predictive control. This study conducted a comprehensive review of existing studies that applied RL for building controls. It provided a detailed breakdown of the existing RL studies that use a specific variation of each major component of the Reinforcement Learning: algorithm, state, action, reward, and environment. We found RL for building controls is still in the research stage with limited applications (11%) in real buildings. Three significant barriers prevent the adoption of RL controllers in actual building controls: (1) the training process is time consuming and data demanding, (2) the control security and robustness need to be enhanced, and (3) the generalization capabilities of RL controllers need to be improved using approaches such as transfer learning. Future research may focus on developing RL controllers that could be used in real buildings, addressing current RL challenges, such as accelerating training and enhancing control robustness, as well as developing an open-source testbed and dataset for performance benchmarking of RL controllers.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2020-07-01",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S0306261920305481",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Wang, Zhe"
            },
            {
                "@_fa": "true",
                "$": "Hong, Tianzhen"
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S0306261920305481"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S0306261920305481"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S0306-2619(20)30548-1",
        "prism:volume": "269",
        "articleNumber": "115036",
        "prism:publisher": "Elsevier Ltd.",
        "dc:title": "Reinforcement learning for building controls: The opportunities and challenges",
        "prism:copyright": "© 2020 Elsevier Ltd. All rights reserved.",
        "openaccess": "0",
        "prism:issn": "03062619",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "Building controls"
            },
            {
                "@_fa": "true",
                "$": "Reinforcement learning"
            },
            {
                "@_fa": "true",
                "$": "Machine learning"
            },
            {
                "@_fa": "true",
                "$": "Optimization"
            },
            {
                "@_fa": "true",
                "$": "Building performance"
            }
        ],
        "openaccessArticle": "false",
        "prism:publicationName": "Applied Energy",
        "openaccessSponsorType": null,
        "prism:pageRange": "115036",
        "pubType": "fla",
        "prism:coverDisplayDate": "1 July 2020",
        "prism:doi": "10.1016/j.apenergy.2020.115036",
        "prism:startingPage": "115036",
        "dc:identifier": "doi:10.1016/j.apenergy.2020.115036",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "169",
            "@width": "364",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "15971",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "303",
            "@width": "734",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr10.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "22242",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "303",
            "@width": "734",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr11.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "28318",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "357",
            "@width": "690",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "58976",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "689",
            "@width": "724",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "75992",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "675",
            "@width": "734",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "55379",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "302",
            "@width": "734",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "26144",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "302",
            "@width": "734",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "27449",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "675",
            "@width": "734",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "49850",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "302",
            "@width": "734",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr8.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "25457",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "630",
            "@width": "600",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr9.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "66242",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "102",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "5188",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "90",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr10.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "4063",
            "@ref": "gr10",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "90",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr11.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "5078",
            "@ref": "gr11",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "113",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "8802",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "172",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "8340",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "178",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "6150",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "90",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "4910",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "90",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "4615",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "178",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "5730",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "90",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr8.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "4696",
            "@ref": "gr8",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "164",
            "@width": "156",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr9.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7931",
            "@ref": "gr9",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "750",
            "@width": "1614",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "85438",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1339",
            "@width": "3248",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr10_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "135076",
            "@ref": "gr10",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1339",
            "@width": "3248",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr11_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "170032",
            "@ref": "gr11",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1580",
            "@width": "3055",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "443409",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "3050",
            "@width": "3207",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "461367",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2985",
            "@width": "3248",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "337338",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1336",
            "@width": "3248",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "162195",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1335",
            "@width": "3248",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "156832",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2988",
            "@width": "3248",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "307466",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1338",
            "@width": "3248",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr8_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "140512",
            "@ref": "gr8",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2793",
            "@width": "2659",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-gr9_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "407770",
            "@ref": "gr9",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-mmc1.docx?httpAccept=%2A%2F%2A",
            "@multimediatype": "Microsoft Word file",
            "@type": "APPLICATION",
            "@size": "98089",
            "@ref": "mmc1",
            "@mimetype": "application/word"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si1.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8177",
            "@ref": "si1",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si10.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6906",
            "@ref": "si10",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si11.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "14995",
            "@ref": "si11",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si12.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6522",
            "@ref": "si12",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si13.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "17988",
            "@ref": "si13",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si14.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "20597",
            "@ref": "si14",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si15.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "13914",
            "@ref": "si15",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si16.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2414",
            "@ref": "si16",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si17.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1412",
            "@ref": "si17",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si18.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "4903",
            "@ref": "si18",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si19.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3493",
            "@ref": "si19",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si2.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5752",
            "@ref": "si2",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si20.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "12929",
            "@ref": "si20",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si21.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8910",
            "@ref": "si21",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si22.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "28705",
            "@ref": "si22",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si23.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7374",
            "@ref": "si23",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si24.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9803",
            "@ref": "si24",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si25.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8414",
            "@ref": "si25",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si26.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1808",
            "@ref": "si26",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si27.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2796",
            "@ref": "si27",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si28.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9312",
            "@ref": "si28",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si29.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9866",
            "@ref": "si29",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si3.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8575",
            "@ref": "si3",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si30.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5967",
            "@ref": "si30",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si31.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7374",
            "@ref": "si31",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si32.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "34290",
            "@ref": "si32",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si33.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7636",
            "@ref": "si33",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si34.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7840",
            "@ref": "si34",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si35.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7744",
            "@ref": "si35",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si36.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2998",
            "@ref": "si36",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si37.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1712",
            "@ref": "si37",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si38.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "17713",
            "@ref": "si38",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si39.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "13530",
            "@ref": "si39",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si4.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2951",
            "@ref": "si4",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si40.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5824",
            "@ref": "si40",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si41.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1397",
            "@ref": "si41",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si42.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "20870",
            "@ref": "si42",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si43.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1771",
            "@ref": "si43",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si44.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "19534",
            "@ref": "si44",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si45.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1748",
            "@ref": "si45",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si46.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "16411",
            "@ref": "si46",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si5.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "2775",
            "@ref": "si5",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si6.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "1702",
            "@ref": "si6",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si7.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "40959",
            "@ref": "si7",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si8.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7114",
            "@ref": "si8",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-si9.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9736",
            "@ref": "si9",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "standard",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0306261920305481-am.pdf?httpAccept=%2A%2F%2A",
            "@multimediatype": "Acrobat PDF file",
            "@type": "AAM-PDF",
            "@size": "1974108",
            "@ref": "am",
            "@mimetype": "application/pdf"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85084440917"
    }
}}