{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba466e6a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78902a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Anchor-based, single-CSV pipeline (YAML-faithful + acronym-safe) ===\n",
    "\n",
    "import os, re, json, yaml, string, difflib, bisect\n",
    "from typing import Dict, Iterable, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------- NLTK bootstrap ----------------\n",
    "import nltk\n",
    "def _ensure_nltk():\n",
    "    for res, pkg in [\n",
    "        (\"tokenizers/punkt\",\"punkt\"),\n",
    "        (\"corpora/wordnet\",\"wordnet\"),\n",
    "        (\"corpora/omw-1.4\",\"omw-1.4\"),\n",
    "    ]:\n",
    "        try: nltk.data.find(res)\n",
    "        except LookupError: nltk.download(pkg, quiet=True)\n",
    "_ensure_nltk()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "_lem = WordNetLemmatizer()\n",
    "\n",
    "# ---------------- Normalisation & compounding ----------------\n",
    "def load_cpwords(py_path: str) -> Dict[str,str]:\n",
    "    env = {}\n",
    "    with open(py_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        code = f.read()\n",
    "    exec(compile(code, py_path, \"exec\"), env, env)\n",
    "    if \"compound_keywords\" not in env or not isinstance(env[\"compound_keywords\"], dict):\n",
    "        raise ValueError(\"compound_keywords not found in cpwords file\")\n",
    "    out = {}\n",
    "    for k, v in env[\"compound_keywords\"].items():\n",
    "        kk = str(k).strip().lower()\n",
    "        vv = str(v).strip().lower().replace(\"-\", \" \").replace(\" \", \"_\")\n",
    "        if kk: out[kk] = vv\n",
    "    return out\n",
    "\n",
    "def _collect_multiword_phrases_from_ontology(ont: dict) -> List[str]:\n",
    "    out = set()\n",
    "    def add_if_multi(s):\n",
    "        s = str(s).strip()\n",
    "        if s and (\" \" in s or \"-\" in s):\n",
    "            out.add(s.lower())\n",
    "    for v in (ont or {}).values():\n",
    "        if isinstance(v, dict):\n",
    "            for vv in v.values():\n",
    "                if isinstance(vv, (list, tuple, set)):\n",
    "                    for t in vv: add_if_multi(t)\n",
    "                else:\n",
    "                    add_if_multi(vv)\n",
    "        elif isinstance(v, (list, tuple, set)):\n",
    "            for t in v: add_if_multi(t)\n",
    "        else:\n",
    "            add_if_multi(v)\n",
    "    return sorted(out, key=len, reverse=True)\n",
    "\n",
    "def compile_compounds_regex(phrases: Iterable[str], cpwords_map: Dict[str,str]) -> re.Pattern:\n",
    "    base = set(p.strip().lower() for p in phrases if p and p.strip())\n",
    "    base |= set(k.strip().lower() for k in (cpwords_map or {}).keys())\n",
    "    if not base: return re.compile(r\"(?!x)x\")\n",
    "    toks = sorted(base, key=len, reverse=True)\n",
    "    pat = r\"\\b(\" + \"|\".join(re.escape(p) for p in toks) + r\")\\b\"\n",
    "    return re.compile(pat, re.I)\n",
    "\n",
    "def apply_compounds(text: str, compounds_rx: re.Pattern, cpwords_map: Dict[str,str]) -> str:\n",
    "    def repl(m):\n",
    "        key = m.group(1).lower()\n",
    "        return cpwords_map.get(key, key.replace(\"-\", \"_\").replace(\" \", \"_\"))\n",
    "    return compounds_rx.sub(repl, text)\n",
    "\n",
    "def canonicalise_term(term: str) -> str:\n",
    "    t = term.lower().strip().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    return re.sub(r\"_+\", \"_\", t)\n",
    "\n",
    "def normalize(text: str, compounds_rx: Optional[re.Pattern], cpwords_map: Dict[str,str]) -> str:\n",
    "    if not text: return \"\"\n",
    "    t = text.lower()\n",
    "    if compounds_rx is not None:\n",
    "        t = apply_compounds(t, compounds_rx, cpwords_map)\n",
    "    keep_us = string.punctuation.replace(\"_\",\"\")\n",
    "    t = re.sub(rf\"[{re.escape(keep_us)}]+\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    t = \" \".join(_lem.lemmatize(w) for w in t.split())\n",
    "    return t\n",
    "\n",
    "# ---------------- Sentence split & token windows ----------------\n",
    "_SENT_SPLIT = re.compile(r'(?<=[\\.\\?\\!])\\s+')\n",
    "def sents(text: str) -> List[str]:\n",
    "    try:\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "        return sent_tokenize(text or \"\")\n",
    "    except Exception:\n",
    "        return _SENT_SPLIT.split(text or \"\")\n",
    "\n",
    "def tokenize(text: str):\n",
    "    tokens, starts = [], []\n",
    "    for m in re.finditer(r\"\\S+\", text):\n",
    "        tokens.append(m.group(0))\n",
    "        starts.append(m.start())\n",
    "    return tokens, starts\n",
    "\n",
    "def window_from_charspan(text: str, tokens: List[str], starts: List[int], span: Tuple[int,int], k: int) -> str:\n",
    "    i = bisect.bisect_right(starts, span[0]) - 1\n",
    "    if i < 0: i = 0\n",
    "    lo = max(0, i - k); hi = min(len(tokens)-1, i + k)\n",
    "    return \" \".join(tokens[lo:hi+1])\n",
    "\n",
    "# ---------------- Regex helpers ----------------\n",
    "SEPARATOR_FLEX = r\"[ _-]+\"\n",
    "def _flexify(term: str) -> str:\n",
    "    t = (term or \"\").strip()\n",
    "    t = re.escape(t)\n",
    "    t = re.sub(r\"\\\\\\s+\", SEPARATOR_FLEX, t)\n",
    "    return t + r\"\\w*\"\n",
    "\n",
    "def build_regexes_from_terms(terms: Iterable[str], require_min_ngram=None) -> List[re.Pattern]:\n",
    "    regs, seen = [], set()\n",
    "    for t in (terms or []):\n",
    "        v = canonicalise_term(t)\n",
    "        if not v: continue\n",
    "        if require_min_ngram and len(re.split(r\"[ _-]+\", v)) < require_min_ngram:\n",
    "            continue\n",
    "        key = (\"rx\", v)\n",
    "        if key in seen: continue\n",
    "        seen.add(key)\n",
    "        regs.append(re.compile(r\"\\b\" + _flexify(v) + r\"\\b\", re.I))\n",
    "    return regs\n",
    "\n",
    "# ---------------- Action/performance gates ----------------\n",
    "VERBS_RX     = re.compile(r\"\\b(model(?:ing|led|ed|s)?|estimate(?:d|s|r)?|predict(?:ed|s|ive)?|analy[sz]e(?:d|s|r)?|simulate(?:d|s)?)\\b\", re.I)\n",
    "FAVOURED_RX  = re.compile(r\"\\b(performance|accuracy|efficienc\\w*|load|consumption|demand|baseline|benchmark(?:ing)?|error|mae|rmse|mape)\\b\", re.I)\n",
    "MODEL_WORD_RX= re.compile(r\"\\bmodel(?:s|ing|led|ed)?\\b\", re.I)\n",
    "GENERIC_OPT_RX = re.compile(r\"\\b(optimi[sz]e?(?:d|s|r)?|optimi[sz]ation|multi-?objective)\\b\", re.I)\n",
    "\n",
    "# ---------------- Gappy “<term> … model” support ----------------\n",
    "GAP_MAX   = 3   # words allowed between term and \"model\"\n",
    "WORD_WINDOW = 15# ± words around anchor\n",
    "MERGE_SIM = 0.90# similarity threshold for merging\n",
    "\n",
    "TERM_SUFFIX_RX = r\"(?:-?(?:based|driven|guided|assisted|enabled|oriented|like|alike))?\"\n",
    "\n",
    "def _sep_flexify(s: str) -> str:\n",
    "    s = canonicalise_term(s)\n",
    "    parts = [re.escape(p) for p in s.split(\"_\") if p]\n",
    "    if not parts: return \"\"\n",
    "    return r\"(?:%s)\" % r\"[ _-]+\".join(parts)\n",
    "\n",
    "def build_gappy_model_regexes(terms: Iterable[str], gap_max: int = GAP_MAX) -> List[re.Pattern]:\n",
    "    regs = []\n",
    "    for t in set(canonicalise_term(x) for x in terms if str(x).strip()):\n",
    "        core = _sep_flexify(t)\n",
    "        if not core: continue\n",
    "        pat = r\"\\b\" + core + TERM_SUFFIX_RX + r\"(?:\\W+\\w+){0,\" + str(gap_max) + r\"}\\W+model\\w*\\b\"\n",
    "        regs.append(re.compile(pat, re.I))\n",
    "    return regs\n",
    "\n",
    "def any_regex_hits(text: str, regs: List[re.Pattern]) -> List[str]:\n",
    "    out = []\n",
    "    for rx in regs:\n",
    "        out += [m.group(0) for m in rx.finditer(text)]\n",
    "    return out\n",
    "\n",
    "def extract_term_from_gappy_hit(hit: str) -> str:\n",
    "    s = hit.lower()\n",
    "    s = s.split(\" model\", 1)[0]\n",
    "    s = re.sub(r\"(?:\\W+\\w+){0,\" + str(GAP_MAX) + r\"}$\", \"\", s)  # trim trailing gap words\n",
    "    s = re.sub(TERM_SUFFIX_RX + r\"$\", \"\", s)                    # strip suffix\n",
    "    return canonicalise_term(s)\n",
    "\n",
    "# ---------------- 90% merge for near-duplicates ----------------\n",
    "def _sim(a: str, b: str) -> float:\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def merge_counter_by_similarity(counter: \"Counter[str]\", threshold: float = MERGE_SIM) -> \"Counter[str]\":\n",
    "    items = [(canonicalise_term(k), v) for k, v in counter.items()]\n",
    "    items.sort(key=lambda kv: len(kv[0]))\n",
    "    reps, used = [], set()\n",
    "    for i, (k, v) in enumerate(items):\n",
    "        if i in used: continue\n",
    "        cluster = [(k, v)]; used.add(i)\n",
    "        for j in range(i+1, len(items)):\n",
    "            if j in used: continue\n",
    "            k2, v2 = items[j]\n",
    "            if _sim(k, k2) >= threshold or k in k2 or k2 in k:\n",
    "                cluster.append((k2, v2)); used.add(j)\n",
    "        rep = min((t for t,_ in cluster), key=len)\n",
    "        reps.append((rep, sum(c for _, c in cluster)))\n",
    "    out = Counter()\n",
    "    for k, v in reps: out[k] += v\n",
    "    return out\n",
    "\n",
    "def merge_set_by_similarity(terms: Iterable[str], threshold: float = MERGE_SIM) -> List[str]:\n",
    "    cnt = Counter(canonicalise_term(t) for t in terms if str(t).strip())\n",
    "    merged = merge_counter_by_similarity(cnt, threshold)\n",
    "    return sorted(merged.keys())\n",
    "\n",
    "# ---------------- Optional Word2Vec expansion (scale only) ----------------\n",
    "try:\n",
    "    from gensim.models import Word2Vec as GensimWord2Vec\n",
    "except Exception:\n",
    "    GensimWord2Vec = None\n",
    "\n",
    "@dataclass\n",
    "class NLPConfig:\n",
    "    mode: str = \"auto\"   # off|light|full|auto\n",
    "    model_path: str = \"\"\n",
    "    exp_topn: int = 40\n",
    "    exp_thresh: float = 0.70\n",
    "    exp_min_count: int = 10\n",
    "    context_k: int = 1\n",
    "\n",
    "def load_nlp_config(path_or_ontology_yaml: Optional[str]) -> NLPConfig:\n",
    "    cfg = NLPConfig()\n",
    "    if path_or_ontology_yaml and os.path.exists(path_or_ontology_yaml):\n",
    "        with open(path_or_ontology_yaml, \"r\", encoding=\"utf-8\") as f:\n",
    "            y = yaml.safe_load(f) or {}\n",
    "        y = y.get(\"_nlp\", y)\n",
    "        cfg.mode = str(y.get(\"mode\", cfg.mode)).lower()\n",
    "        cfg.model_path = str(y.get(\"model_path\", cfg.model_path))\n",
    "        cfg.exp_topn = int(y.get(\"exp_topn\", cfg.exp_topn))\n",
    "        cfg.exp_thresh = float(y.get(\"exp_thresh\", cfg.exp_thresh))\n",
    "        cfg.exp_min_count = int(y.get(\"exp_min_count\", cfg.exp_min_count))\n",
    "        cfg.context_k = int(y.get(\"context_k\", cfg.context_k))\n",
    "    return cfg\n",
    "\n",
    "def resolve_nlp_mode(cfg: NLPConfig) -> str:\n",
    "    if cfg.mode in {\"off\",\"light\",\"full\"}: return cfg.mode\n",
    "    if GensimWord2Vec is None: return \"light\"\n",
    "    candidates = [cfg.model_path] if cfg.model_path else []\n",
    "    candidates += [\"model.word2vec\",\"model.w2v\",\"word2vec.model\", os.path.join(os.getcwd(),\"word2vec.model\")]\n",
    "    for p in candidates:\n",
    "        if p and os.path.exists(p):\n",
    "            try:\n",
    "                GensimWord2Vec.load(p); return \"full\"\n",
    "            except Exception:\n",
    "                continue\n",
    "    return \"light\"\n",
    "\n",
    "def expand_terms_w2v(model, seeds: List[str], topn=40, thresh=0.70, min_count=10) -> List[str]:\n",
    "    out = set(canonicalise_term(s) for s in seeds if s)\n",
    "    if not model: return sorted(out)\n",
    "    for t in list(out):\n",
    "        if t in model.wv:\n",
    "            for cand, sim in model.wv.most_similar(t, topn=topn):\n",
    "                if sim < thresh: continue\n",
    "                try:\n",
    "                    if model.wv.get_vecattr(cand,\"count\") < min_count: continue\n",
    "                except Exception: pass\n",
    "                if any(ch.isdigit() for ch in cand): continue\n",
    "                out.add(canonicalise_term(cand))\n",
    "    return sorted(out)\n",
    "\n",
    "# ---------------- IO ----------------\n",
    "def load_jsons(dirpath):\n",
    "    for fn in os.listdir(dirpath):\n",
    "        if fn.endswith(\".json\"):\n",
    "            p = os.path.join(dirpath, fn)\n",
    "            try:\n",
    "                with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                    yield fn, json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] failed to read {fn}: {e}\")\n",
    "\n",
    "# ---------------- Acronym-safe matchers (use RAW text) ----------------\n",
    "_ACRO_LEFT  = r\"(?<![A-Za-z0-9_])\"\n",
    "_ACRO_RIGHT = r\"(?![A-Za-z0-9_])\"\n",
    "\n",
    "def pick_acronyms(terms: Iterable[str]) -> List[str]:\n",
    "    acr = []\n",
    "    for t in (terms or []):\n",
    "        s = str(t).strip()\n",
    "        if not s: continue\n",
    "        if \" \" in s: continue          # spaces → phrase, not acronym\n",
    "        # keep shortish tokens or mixed with digits/dots (e.g., PM2.5)\n",
    "        if len(s) <= 6 or re.search(r\"[0-9.]\", s):\n",
    "            acr.append(s)\n",
    "    return acr\n",
    "\n",
    "def build_acronym_regexes(terms: Iterable[str]) -> List[re.Pattern]:\n",
    "    regs = []\n",
    "    for t in pick_acronyms(terms):\n",
    "        pat = _ACRO_LEFT + re.escape(t.upper()) + _ACRO_RIGHT\n",
    "        regs.append(re.compile(pat))    # case-sensitive on raw text\n",
    "    return regs\n",
    "\n",
    "def count_yaml_category(raw_text: str, norm_text: str, terms: Iterable[str]) -> Counter:\n",
    "    \"\"\"\n",
    "    Count matches for a YAML term list by combining:\n",
    "      - phrase tolerant matches on normalized text (lowercased)\n",
    "      - acronym strict matches on raw text (case-sensitive, whole-token)\n",
    "    \"\"\"\n",
    "    cnt = Counter()\n",
    "    # phrases on normalized text\n",
    "    phrase_rx = build_regexes_from_terms(terms, require_min_ngram=1)\n",
    "    for rx in phrase_rx:\n",
    "        for m in rx.finditer(norm_text):\n",
    "            cnt[canonicalise_term(m.group(0))] += 1\n",
    "    # acronyms on raw text\n",
    "    acro_rx = build_acronym_regexes(terms)\n",
    "    for rx in acro_rx:\n",
    "        for m in rx.finditer(raw_text):\n",
    "            cnt[canonicalise_term(m.group(0))] += 1\n",
    "    return merge_counter_by_similarity(cnt, threshold=MERGE_SIM)\n",
    "\n",
    "# ---------------- Paradigm term-only regex builder ----------------\n",
    "def build_simple_paradigm_term_rx(paradigms: dict) -> dict:\n",
    "    out = {}\n",
    "    for pname, terms in (paradigms or {}).items():\n",
    "        canon_terms = [canonicalise_term(t) for t in (terms or [])]\n",
    "        out[pname] = build_regexes_from_terms(canon_terms, require_min_ngram=1)\n",
    "    return out\n",
    "\n",
    "# ---------------- Core analysis (anchor-based + YAML-faithful cats) ----------------\n",
    "def analyze_anchor_based_single(doc: dict,\n",
    "                                ont: dict,\n",
    "                                compounds_rx: Optional[re.Pattern],\n",
    "                                cp_map: Dict[str,str],\n",
    "                                cfg: NLPConfig,\n",
    "                                w2v) -> dict:\n",
    "    raw = (doc.get(\"full-text-retrieval-response\", {}) or {}).get(\"originalText\",\"\") or \"\"\n",
    "    text = normalize(raw, compounds_rx, cp_map)  # normalized (lowercased)\n",
    "    S = sents(text)\n",
    "    tokens, starts = tokenize(text)\n",
    "\n",
    "    # Ontology buckets\n",
    "    scale       = ont.get(\"scale\", {}) or {}\n",
    "    paradigms   = ont.get(\"model_paradigm\", {}) or {}\n",
    "    optim_cats  = ont.get(\"optimization_methods\", {}) or {}\n",
    "    app_cats    = ont.get(\"applications\", {}) or {}\n",
    "    data_cats   = ont.get(\"data_types\", {}) or {}\n",
    "\n",
    "    # Canonicalised seeds (+ optional W2V) for scale only\n",
    "    def seeds(key):\n",
    "        base = [canonicalise_term(x) for x in (scale.get(key, []) or [])]\n",
    "        return expand_terms_w2v(w2v, base, cfg.exp_topn, cfg.exp_thresh, cfg.exp_min_count) if w2v else base\n",
    "\n",
    "    building_terms = seeds(\"building_model\")\n",
    "    system_terms   = seeds(\"system_model\")\n",
    "    climate_terms  = seeds(\"climate_model\")\n",
    "    occup_terms    = seeds(\"occupancy_model\")\n",
    "\n",
    "    # Finders for scale terms (need spans)\n",
    "    def rx_find_terms(terms):\n",
    "        if not terms: return None\n",
    "        pats = [r\"\\b\" + _flexify(t) + r\"\\b\" for t in set(terms)]\n",
    "        return re.compile(\"|\".join(pats), re.I)\n",
    "\n",
    "    RX_FIND = {\n",
    "        \"building\": rx_find_terms(building_terms),\n",
    "        \"system\":   rx_find_terms(system_terms),\n",
    "        \"weather\":  rx_find_terms(climate_terms),\n",
    "        \"occupancy\":rx_find_terms(occup_terms),\n",
    "    }\n",
    "\n",
    "    # Paradigm & optimisation matchers\n",
    "    par_rx        = {k: build_regexes_from_terms(v, require_min_ngram=1) for k, v in (paradigms or {}).items()}\n",
    "    par_gappy_rx  = {k: build_gappy_model_regexes(v, gap_max=GAP_MAX)   for k, v in (paradigms or {}).items()}\n",
    "    par_term_rx   = build_simple_paradigm_term_rx(paradigms)\n",
    "    opt_rx        = {k: build_regexes_from_terms(v, require_min_ngram=1) for k, v in (optim_cats or {}).items()}\n",
    "\n",
    "    # Containers\n",
    "    term_counts = {c: Counter() for c in RX_FIND.keys()}\n",
    "    par_found   = {c: set() for c in RX_FIND.keys()}\n",
    "    par_terms   = {c: set() for c in RX_FIND.keys()}\n",
    "    opt_methods, opt_terms = set(), set()\n",
    "\n",
    "    # PRIMARY path — scale-term windows\n",
    "    for cat, rx in RX_FIND.items():\n",
    "        if rx is None: continue\n",
    "        for m in rx.finditer(text):\n",
    "            w = window_from_charspan(text, tokens, starts, m.span(), k=WORD_WINDOW)\n",
    "            if not (VERBS_RX.search(w) or FAVOURED_RX.search(w)):\n",
    "                continue\n",
    "            term_counts[cat][canonicalise_term(m.group(0))] += 1\n",
    "\n",
    "            # Paradigms in this window → ONLY this category\n",
    "            for pname in par_rx.keys():\n",
    "                plain_hits = []\n",
    "                for rxx in par_rx[pname]:\n",
    "                    plain_hits += [canonicalise_term(h.group(0)) for h in rxx.finditer(w)]\n",
    "                gappy_strings = any_regex_hits(w, par_gappy_rx.get(pname, []))\n",
    "                gappy_terms   = [extract_term_from_gappy_hit(h) for h in gappy_strings]\n",
    "                belongs_via_gappy = False\n",
    "                if gappy_terms and par_term_rx.get(pname):\n",
    "                    for t_ in gappy_terms:\n",
    "                        if any(r.search(t_) for r in par_term_rx[pname]):\n",
    "                            belongs_via_gappy = True; break\n",
    "                if plain_hits or belongs_via_gappy:\n",
    "                    par_found[cat].add(pname)\n",
    "                    par_terms[cat].update(plain_hits)\n",
    "                    par_terms[cat].update(gappy_terms)\n",
    "\n",
    "            # Optimisation phrases in this window (global list)\n",
    "            for oname, regs in opt_rx.items():\n",
    "                found = []\n",
    "                for rxx in regs:\n",
    "                    found += [canonicalise_term(h.group(0)) for h in rxx.finditer(w)]\n",
    "                if found:\n",
    "                    opt_methods.add(oname); opt_terms.update(found)\n",
    "            g = GENERIC_OPT_RX.search(w)\n",
    "            if g:\n",
    "                opt_methods.add(\"generic\"); opt_terms.add(canonicalise_term(g.group(0)))\n",
    "\n",
    "    # FLEX path — paradigm anchored; map to categories if window shows 'model' + category hints\n",
    "    CATEGORY_HINT = {\n",
    "        \"building\":  re.compile(r\"\\b(building|facility|premise|asset|whole[_ ]building|zone|envelope|bem|ubem)\\b\", re.I),\n",
    "        \"system\":    re.compile(r\"\\b(system|hvac|ahu|doas|vav|fcu|coil|chiller|boiler|pump|tower|vr[fb]|heat[_ ]pump)\\b\", re.I),\n",
    "        \"occupancy\": re.compile(r\"\\b(occupanc\\w*|occupant\\w*|people|tenant|user)\\b\", re.I),\n",
    "        \"weather\":   re.compile(r\"\\b(weather|climate|outdoor|ambient|meteorolog\\w*)\\b\", re.I),\n",
    "    }\n",
    "    ALL_PAR_RXS = [r for regs in par_rx.values() for r in regs]\n",
    "\n",
    "    for rx in ALL_PAR_RXS:\n",
    "        for m in rx.finditer(text):\n",
    "            w = window_from_charspan(text, tokens, starts, m.span(), k=WORD_WINDOW)\n",
    "            if not (MODEL_WORD_RX.search(w) and (VERBS_RX.search(w) or FAVOURED_RX.search(w))):\n",
    "                continue\n",
    "            for cat in [\"building\",\"system\",\"occupancy\",\"weather\"]:\n",
    "                hint_ok  = CATEGORY_HINT[cat].search(w) is not None\n",
    "                scale_ok = (RX_FIND[cat] is not None) and (RX_FIND[cat].search(w) is not None)\n",
    "                if not (hint_ok or scale_ok): continue\n",
    "\n",
    "                pstr = canonicalise_term(m.group(0))\n",
    "                all_gappy_strings = any_regex_hits(w, [rgx for lst in par_gappy_rx.values() for rgx in lst])\n",
    "                ext_terms = [extract_term_from_gappy_hit(h) for h in all_gappy_strings]\n",
    "\n",
    "                matched_buckets = set()\n",
    "                for pname, regs in par_rx.items():\n",
    "                    if any(r.search(m.group(0)) for r in regs):\n",
    "                        matched_buckets.add(pname)\n",
    "                for pname, regs in par_term_rx.items():\n",
    "                    if any(r.search(t_) for r in regs for t_ in ([pstr] + ext_terms)):\n",
    "                        matched_buckets.add(pname)\n",
    "\n",
    "                for pname in matched_buckets:\n",
    "                    par_found[cat].add(pname)\n",
    "                par_terms[cat].add(pstr)\n",
    "                par_terms[cat].update(ext_terms)\n",
    "\n",
    "    # Merge near-duplicates (scale/paradigm terms)\n",
    "    for cat in term_counts.keys():\n",
    "        term_counts[cat] = merge_counter_by_similarity(term_counts[cat], threshold=MERGE_SIM)\n",
    "        par_terms[cat]   = set(merge_set_by_similarity(par_terms[cat], threshold=MERGE_SIM))\n",
    "\n",
    "    # ---------------- YAML-faithful counts for optimization/applications/data_types ----------------\n",
    "    # We do these at document-level (no window), with acronym-safe matching.\n",
    "    opt_ranked_cols = {}\n",
    "    for cat_name, terms in (optim_cats or {}).items():\n",
    "        c = count_yaml_category(raw, text, terms)     # raw=acronyms; text=phrases\n",
    "        opt_ranked_cols[f\"optimization_{canonicalise_term(cat_name)}_ranked\"] = (\n",
    "            \"NM\" if not c else \";\".join(f\"{t} ({n})\" for t, n in sorted(c.items(), key=lambda kv:(-kv[1], kv[0])))\n",
    "        )\n",
    "\n",
    "    app_ranked_cols = {}\n",
    "    for cat_name, terms in (app_cats or {}).items():\n",
    "        c = count_yaml_category(raw, text, terms)\n",
    "        app_ranked_cols[f\"applications_{canonicalise_term(cat_name)}_ranked\"] = (\n",
    "            \"NM\" if not c else \";\".join(f\"{t} ({n})\" for t, n in sorted(c.items(), key=lambda kv:(-kv[1], kv[0])))\n",
    "        )\n",
    "\n",
    "    data_ranked_cols = {}\n",
    "    for cat_name, terms in (data_cats or {}).items():\n",
    "        c = count_yaml_category(raw, text, terms)\n",
    "        data_ranked_cols[f\"data_{canonicalise_term(cat_name)}_ranked\"] = (\n",
    "            \"NM\" if not c else \";\".join(f\"{t} ({n})\" for t, n in sorted(c.items(), key=lambda kv:(-kv[1], kv[0])))\n",
    "        )\n",
    "\n",
    "    # Assemble row\n",
    "    def ranked_pairs(counter: Counter) -> str:\n",
    "        if not counter: return \"NM\"\n",
    "        items = sorted(counter.items(), key=lambda kv: (-kv[1], kv[0]))\n",
    "        return \";\".join(f\"{t} ({c})\" for t, c in items)\n",
    "\n",
    "    def dedup_join(xs: Iterable[str]) -> str:\n",
    "        vals = [canonicalise_term(x) for x in (xs or []) if str(x).strip()]\n",
    "        return \";\".join(sorted(set(vals))) if vals else \"NM\"\n",
    "\n",
    "    row = {\n",
    "        \"building_terms_ranked\":  ranked_pairs(term_counts[\"building\"]),\n",
    "        \"system_terms_ranked\":    ranked_pairs(term_counts[\"system\"]),\n",
    "        \"weather_terms_ranked\":   ranked_pairs(term_counts[\"weather\"]),\n",
    "        \"occupancy_terms_ranked\": ranked_pairs(term_counts[\"occupancy\"]),\n",
    "        \"building_paradigms\":       dedup_join(par_found[\"building\"]),\n",
    "        \"system_paradigms\":         dedup_join(par_found[\"system\"]),\n",
    "        \"weather_paradigms\":        dedup_join(par_found[\"weather\"]),\n",
    "        \"occupancy_paradigms\":      dedup_join(par_found[\"occupancy\"]),\n",
    "        \"building_paradigm_terms\":  dedup_join(par_terms[\"building\"]),\n",
    "        \"system_paradigm_terms\":    dedup_join(par_terms[\"system\"]),\n",
    "        \"weather_paradigm_terms\":   dedup_join(par_terms[\"weather\"]),\n",
    "        \"occupancy_paradigm_terms\": dedup_join(par_terms[\"occupancy\"]),\n",
    "        \"optimization_methods\":     dedup_join(opt_methods),\n",
    "        \"optimization_method_terms\":dedup_join(opt_terms),\n",
    "    }\n",
    "    row.update(opt_ranked_cols)\n",
    "    row.update(app_ranked_cols)\n",
    "    row.update(data_ranked_cols)\n",
    "    return row\n",
    "\n",
    "# ---------------- Runner (writes a single CSV) ----------------\n",
    "def run_anchor_based(\n",
    "    input_dir: str,\n",
    "    ontology_path: str,\n",
    "    cpwords_path: Optional[str] = None,\n",
    "    anchor_csv: str = \"nlp_output/anchor_based.csv\",\n",
    "    nlp: str = \"auto\",\n",
    "    config_path: Optional[str] = None,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    with open(ontology_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ont = yaml.safe_load(f) or {}\n",
    "    cfg = load_nlp_config(config_path or ontology_path)\n",
    "    if nlp: cfg.mode = nlp\n",
    "    mode = resolve_nlp_mode(cfg)\n",
    "    if verbose: print(f\"[NLP] mode = {mode}\")\n",
    "\n",
    "    # Word2Vec (full)\n",
    "    w2v = None\n",
    "    if mode == \"full\" and GensimWord2Vec is not None:\n",
    "        candidates = [cfg.model_path] if cfg.model_path else []\n",
    "        candidates += [\"model.word2vec\",\"model.w2v\",\"word2vec.model\", os.path.join(os.getcwd(),\"word2vec.model\")]\n",
    "        for p in candidates:\n",
    "            if p and os.path.exists(p):\n",
    "                try:\n",
    "                    w2v = GensimWord2Vec.load(p)\n",
    "                    if verbose: print(f\"[NLP] loaded Word2Vec: {p}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if verbose: print(f\"[WARN] could not load {p}: {e}\")\n",
    "        if w2v is None:\n",
    "            if verbose: print(\"[WARN] falling back to light mode\")\n",
    "            mode = \"light\"\n",
    "\n",
    "    # Compounding sources\n",
    "    cp_map = {}\n",
    "    if cpwords_path and os.path.exists(cpwords_path):\n",
    "        try: cp_map = load_cpwords(cpwords_path)\n",
    "        except Exception as e:\n",
    "            if verbose: print(f\"[WARN] cpwords not loaded: {e}\")\n",
    "    ont_multi = _collect_multiword_phrases_from_ontology(ont)\n",
    "    compounds_rx = compile_compounds_regex(ont_multi, cp_map)\n",
    "\n",
    "    # Process\n",
    "    rows = []\n",
    "    os.makedirs(os.path.dirname(anchor_csv) or \".\", exist_ok=True)\n",
    "    for fname, doc in tqdm(list(load_jsons(input_dir)), desc=\"Anchor-based\"):\n",
    "        r = analyze_anchor_based_single(doc, ont, compounds_rx, cp_map, cfg, w2v)\n",
    "        r[\"file\"] = fname\n",
    "        rows.append(r)\n",
    "\n",
    "    # Collect dynamic YAML-driven columns\n",
    "    dyn_cols = []\n",
    "    for grp_key in (\"optimization_methods\",\"applications\",\"data_types\"):\n",
    "        for cat_name in (ont.get(grp_key, {}) or {}).keys():\n",
    "            prefix = \"optimization\" if grp_key==\"optimization_methods\" else (\"applications\" if grp_key==\"applications\" else \"data\")\n",
    "            dyn_cols.append(f\"{prefix}_{canonicalise_term(cat_name)}_ranked\")\n",
    "\n",
    "    cols_fixed = [\n",
    "        \"file\",\n",
    "        \"building_terms_ranked\",\"system_terms_ranked\",\"weather_terms_ranked\",\"occupancy_terms_ranked\",\n",
    "        \"building_paradigms\",\"system_paradigms\",\"weather_paradigms\",\"occupancy_paradigms\",\n",
    "        \"building_paradigm_terms\",\"system_paradigm_terms\",\"weather_paradigm_terms\",\"occupancy_paradigm_terms\",\n",
    "        \"optimization_methods\",\"optimization_method_terms\",\n",
    "    ]\n",
    "    cols = cols_fixed + dyn_cols\n",
    "    df = pd.DataFrame(rows)\n",
    "    # ensure all columns exist even if empty\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"NM\"\n",
    "    df = df[cols].sort_values(\"file\")\n",
    "    df.to_csv(anchor_csv, index=False)\n",
    "    if verbose: print(f\"[OK] wrote {anchor_csv} ({len(df)} rows)\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e8a5ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NLP] mode = full\n",
      "[NLP] loaded Word2Vec: word2vec.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Anchor-based: 100%|██████████| 14/14 [00:05<00:00,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] wrote nlp_output/anchor_summary.csv (14 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_summary = run_anchor_based(\n",
    "    input_dir=\"elsevier_out/papers_ftrr\",\n",
    "    ontology_path=\"nlp_input/ontology.yaml\",\n",
    "    cpwords_path=\"nlp_input/_cpwords.py\",\n",
    "    anchor_csv=\"nlp_output/anchor_summary.csv\",\n",
    "    nlp=\"auto\",                              # off | light | full | auto\n",
    "    config_path=\"nlp_input/ontology.yaml\",                        # or a YAML with an _nlp block\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0adfdbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>building_terms_ranked</th>\n",
       "      <th>system_terms_ranked</th>\n",
       "      <th>weather_terms_ranked</th>\n",
       "      <th>occupancy_terms_ranked</th>\n",
       "      <th>building_paradigms</th>\n",
       "      <th>system_paradigms</th>\n",
       "      <th>weather_paradigms</th>\n",
       "      <th>occupancy_paradigms</th>\n",
       "      <th>building_paradigm_terms</th>\n",
       "      <th>...</th>\n",
       "      <th>applications_energy_flexibility_ranked</th>\n",
       "      <th>applications_performance_tracking_ranked</th>\n",
       "      <th>applications_performance_analysis_ranked</th>\n",
       "      <th>data_metering_ranked</th>\n",
       "      <th>data_air_temperature_ranked</th>\n",
       "      <th>data_air_quality_ranked</th>\n",
       "      <th>data_environment_ranked</th>\n",
       "      <th>data_occupancy_ranked</th>\n",
       "      <th>data_hvac_signals_ranked</th>\n",
       "      <th>data_thermal_comfort_ranked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1016_j.apenergy.2019.113920.json</td>\n",
       "      <td>room (21);building (10);temperature (4)</td>\n",
       "      <td>acmv (19);ahu (17);coil (17);doas (11);chiller...</td>\n",
       "      <td>NM</td>\n",
       "      <td>internal (4);occupancy (3)</td>\n",
       "      <td>blackbox;greybox</td>\n",
       "      <td>blackbox;greybox</td>\n",
       "      <td>NM</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>kalman_filter;linear;rc_model;state_space</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>air_temperature (10)</td>\n",
       "      <td>concentration (6)</td>\n",
       "      <td>humidity (16)</td>\n",
       "      <td>occupancy (5)</td>\n",
       "      <td>return_air (17);supply_air (12);water_flow (1)</td>\n",
       "      <td>pmv (92)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1016_j.apenergy.2020.115147.json</td>\n",
       "      <td>building (60);temperature (11);room (9);zone (1)</td>\n",
       "      <td>acmv (14);accuracy (7);achieve (6);chiller (6)...</td>\n",
       "      <td>NM</td>\n",
       "      <td>occupancy (6);internal (5);heat_loads (1)</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>ann;linearized</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>performance_analysis (1)</td>\n",
       "      <td>kw (12)</td>\n",
       "      <td>air_temperature (35)</td>\n",
       "      <td>concentration (5)</td>\n",
       "      <td>humidity (13)</td>\n",
       "      <td>occupancy (7)</td>\n",
       "      <td>supply_air (21);damper (3);return_air (3)</td>\n",
       "      <td>pmv (150)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1016_j.apenergy.2020.115426.json</td>\n",
       "      <td>room (21);temperature (9)</td>\n",
       "      <td>ac (11)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>bcnn;convolutional_neural_networks</td>\n",
       "      <td>...</td>\n",
       "      <td>demand_response (2)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1016_j.apenergy.2021.117276.json</td>\n",
       "      <td>building (9);temperature (2)</td>\n",
       "      <td>achieved (26);actual (22);accurate (11);accura...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>data_driven_approach</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>performance_evaluation (2)</td>\n",
       "      <td>kwh (62)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>humidity (3);wind_speed (3);weather_station (2)</td>\n",
       "      <td>presence (1)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1016_j.apenergy.2021.117987.json</td>\n",
       "      <td>room (46);temperature (7);air_quality (3);buil...</td>\n",
       "      <td>accuracy (36);coil (11);hvac (8);according (2)...</td>\n",
       "      <td>outdoor_temperature (8)</td>\n",
       "      <td>occupancy (46);internal (7)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>commissioning (4)</td>\n",
       "      <td>performance_evaluation (2)</td>\n",
       "      <td>kw (2)</td>\n",
       "      <td>air_temperature (19)</td>\n",
       "      <td>NM</td>\n",
       "      <td>outdoor_temperature (13);humidity (6)</td>\n",
       "      <td>occupancy (107)</td>\n",
       "      <td>return_air (12);damper (1)</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.1016_j.apenergy.2022.119580.json</td>\n",
       "      <td>zone (61);temperature (40);room (31);building ...</td>\n",
       "      <td>achieved (8);according (7);actual (7);accuracy...</td>\n",
       "      <td>outdoor_temperature (2)</td>\n",
       "      <td>internal (20);occupant_number (4);infiltration...</td>\n",
       "      <td>greybox</td>\n",
       "      <td>greybox</td>\n",
       "      <td>NM</td>\n",
       "      <td>greybox</td>\n",
       "      <td>rc_model</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>co2 (32)</td>\n",
       "      <td>outdoor_temperature (4);humidity (2)</td>\n",
       "      <td>occupancy (1)</td>\n",
       "      <td>supply_air (2)</td>\n",
       "      <td>pmv (32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.1016_j.buildenv.2020.107089.json</td>\n",
       "      <td>room (17);temperature (8);zone (4);building (2)</td>\n",
       "      <td>actuation (4);vav (3);according (2);accuracy (...</td>\n",
       "      <td>NM</td>\n",
       "      <td>occupancy_patterns (2);height (1)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>kwh (2)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>solar_irradiance (2);humidity (1)</td>\n",
       "      <td>occupancy (6)</td>\n",
       "      <td>air_flow (14);supply_air (1)</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.1016_j.enbuild.2020.109792.json</td>\n",
       "      <td>temperature (4);building (3);room (3)</td>\n",
       "      <td>accuracy (1);accurate (1);hvac (1);valve (1)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>kwh (2)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.1016_j.enbuild.2020.110271.json</td>\n",
       "      <td>building (49);ieq (12);temperature (12);zone (...</td>\n",
       "      <td>actual (28);across (6);account (5);accuracy (2...</td>\n",
       "      <td>NM</td>\n",
       "      <td>occupancy (2)</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>statistical</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>commissioning (5)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>air_temperature (2)</td>\n",
       "      <td>concentration (4)</td>\n",
       "      <td>NM</td>\n",
       "      <td>occupancy (14)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.1016_j.enbuild.2020.110276.json</td>\n",
       "      <td>building (71)</td>\n",
       "      <td>actual (51);pump (26);chiller (24);ahu (16);hv...</td>\n",
       "      <td>NM</td>\n",
       "      <td>occupancy (1)</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>statistical_analysis</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>commissioning (6)</td>\n",
       "      <td>NM</td>\n",
       "      <td>kw (14)</td>\n",
       "      <td>air_temperature (4)</td>\n",
       "      <td>NM</td>\n",
       "      <td>outdoor_temperature (6)</td>\n",
       "      <td>occupancy (11)</td>\n",
       "      <td>supply_air (10);return_air (6);air_flow (2);da...</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.1016_j.enbuild.2021.111255.json</td>\n",
       "      <td>zone (21);building (20);temperature (15);room (4)</td>\n",
       "      <td>ahu (20);chiller (7);coil (6);acceptable (3);h...</td>\n",
       "      <td>NM</td>\n",
       "      <td>occupancy (7)</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>artificial_neural_networks;data_driven</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>air_temperature (24)</td>\n",
       "      <td>NM</td>\n",
       "      <td>outdoor_temperature (2)</td>\n",
       "      <td>occupancy (24);presence (2)</td>\n",
       "      <td>supply_air (17);damper (14);valve_position (4)...</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.1016_j.enbuild.2023.113747.json</td>\n",
       "      <td>temperature (11);building (4)</td>\n",
       "      <td>boiler (34);accuracy (13);actual (13);accurate...</td>\n",
       "      <td>NM</td>\n",
       "      <td>occupancy (2)</td>\n",
       "      <td>NM</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>kw (1)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>humidity (2);wind_speed (1)</td>\n",
       "      <td>occupancy (7)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.1016_j.enbuild.2024.114215.json</td>\n",
       "      <td>temperature (13);building (10);zone (9)</td>\n",
       "      <td>chiller (43);exchanger (24);hvacs (15);accumul...</td>\n",
       "      <td>NM</td>\n",
       "      <td>internal (8)</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>annual</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>commissioning (6)</td>\n",
       "      <td>NM</td>\n",
       "      <td>kw (46)</td>\n",
       "      <td>dry_bulb (4)</td>\n",
       "      <td>NM</td>\n",
       "      <td>humidity (2)</td>\n",
       "      <td>occupancy (2)</td>\n",
       "      <td>water_flow (26);return_air (7);air_flow (3)</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.1016_j.jobe.2022.104498.json</td>\n",
       "      <td>zone (13);temperature (7);building_heating (1)</td>\n",
       "      <td>hvac (11);ahu (10);accuracy (8);achieves (7);a...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>kwh (50)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>air_flow (11);supply_air (6)</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   file  \\\n",
       "0   10.1016_j.apenergy.2019.113920.json   \n",
       "1   10.1016_j.apenergy.2020.115147.json   \n",
       "2   10.1016_j.apenergy.2020.115426.json   \n",
       "3   10.1016_j.apenergy.2021.117276.json   \n",
       "4   10.1016_j.apenergy.2021.117987.json   \n",
       "5   10.1016_j.apenergy.2022.119580.json   \n",
       "6   10.1016_j.buildenv.2020.107089.json   \n",
       "7    10.1016_j.enbuild.2020.109792.json   \n",
       "8    10.1016_j.enbuild.2020.110271.json   \n",
       "9    10.1016_j.enbuild.2020.110276.json   \n",
       "10   10.1016_j.enbuild.2021.111255.json   \n",
       "11   10.1016_j.enbuild.2023.113747.json   \n",
       "12   10.1016_j.enbuild.2024.114215.json   \n",
       "13      10.1016_j.jobe.2022.104498.json   \n",
       "\n",
       "                                building_terms_ranked  \\\n",
       "0             room (21);building (10);temperature (4)   \n",
       "1    building (60);temperature (11);room (9);zone (1)   \n",
       "2                           room (21);temperature (9)   \n",
       "3                        building (9);temperature (2)   \n",
       "4   room (46);temperature (7);air_quality (3);buil...   \n",
       "5   zone (61);temperature (40);room (31);building ...   \n",
       "6     room (17);temperature (8);zone (4);building (2)   \n",
       "7               temperature (4);building (3);room (3)   \n",
       "8   building (49);ieq (12);temperature (12);zone (...   \n",
       "9                                       building (71)   \n",
       "10  zone (21);building (20);temperature (15);room (4)   \n",
       "11                      temperature (11);building (4)   \n",
       "12            temperature (13);building (10);zone (9)   \n",
       "13     zone (13);temperature (7);building_heating (1)   \n",
       "\n",
       "                                  system_terms_ranked  \\\n",
       "0   acmv (19);ahu (17);coil (17);doas (11);chiller...   \n",
       "1   acmv (14);accuracy (7);achieve (6);chiller (6)...   \n",
       "2                                             ac (11)   \n",
       "3   achieved (26);actual (22);accurate (11);accura...   \n",
       "4   accuracy (36);coil (11);hvac (8);according (2)...   \n",
       "5   achieved (8);according (7);actual (7);accuracy...   \n",
       "6   actuation (4);vav (3);according (2);accuracy (...   \n",
       "7        accuracy (1);accurate (1);hvac (1);valve (1)   \n",
       "8   actual (28);across (6);account (5);accuracy (2...   \n",
       "9   actual (51);pump (26);chiller (24);ahu (16);hv...   \n",
       "10  ahu (20);chiller (7);coil (6);acceptable (3);h...   \n",
       "11  boiler (34);accuracy (13);actual (13);accurate...   \n",
       "12  chiller (43);exchanger (24);hvacs (15);accumul...   \n",
       "13  hvac (11);ahu (10);accuracy (8);achieves (7);a...   \n",
       "\n",
       "       weather_terms_ranked  \\\n",
       "0                        NM   \n",
       "1                        NM   \n",
       "2                        NM   \n",
       "3                        NM   \n",
       "4   outdoor_temperature (8)   \n",
       "5   outdoor_temperature (2)   \n",
       "6                        NM   \n",
       "7                        NM   \n",
       "8                        NM   \n",
       "9                        NM   \n",
       "10                       NM   \n",
       "11                       NM   \n",
       "12                       NM   \n",
       "13                       NM   \n",
       "\n",
       "                               occupancy_terms_ranked building_paradigms  \\\n",
       "0                          internal (4);occupancy (3)   blackbox;greybox   \n",
       "1           occupancy (6);internal (5);heat_loads (1)           blackbox   \n",
       "2                                                  NM           blackbox   \n",
       "3                                                  NM           blackbox   \n",
       "4                         occupancy (46);internal (7)                 NM   \n",
       "5   internal (20);occupant_number (4);infiltration...            greybox   \n",
       "6                   occupancy_patterns (2);height (1)                 NM   \n",
       "7                                                  NM                 NM   \n",
       "8                                       occupancy (2)           blackbox   \n",
       "9                                       occupancy (1)           blackbox   \n",
       "10                                      occupancy (7)           blackbox   \n",
       "11                                      occupancy (2)                 NM   \n",
       "12                                       internal (8)           blackbox   \n",
       "13                                                 NM                 NM   \n",
       "\n",
       "    system_paradigms weather_paradigms occupancy_paradigms  \\\n",
       "0   blackbox;greybox                NM            blackbox   \n",
       "1           blackbox          blackbox            blackbox   \n",
       "2           blackbox                NM                  NM   \n",
       "3           blackbox          blackbox            blackbox   \n",
       "4                 NM                NM                  NM   \n",
       "5            greybox                NM             greybox   \n",
       "6                 NM                NM                  NM   \n",
       "7                 NM                NM                  NM   \n",
       "8           blackbox                NM                  NM   \n",
       "9                 NM                NM                  NM   \n",
       "10          blackbox                NM                  NM   \n",
       "11          blackbox                NM                  NM   \n",
       "12          blackbox                NM                  NM   \n",
       "13                NM                NM                  NM   \n",
       "\n",
       "                      building_paradigm_terms  ...  \\\n",
       "0   kalman_filter;linear;rc_model;state_space  ...   \n",
       "1                              ann;linearized  ...   \n",
       "2          bcnn;convolutional_neural_networks  ...   \n",
       "3                        data_driven_approach  ...   \n",
       "4                                          NM  ...   \n",
       "5                                    rc_model  ...   \n",
       "6                                          NM  ...   \n",
       "7                                          NM  ...   \n",
       "8                                 statistical  ...   \n",
       "9                        statistical_analysis  ...   \n",
       "10     artificial_neural_networks;data_driven  ...   \n",
       "11                                         NM  ...   \n",
       "12                                     annual  ...   \n",
       "13                                         NM  ...   \n",
       "\n",
       "   applications_energy_flexibility_ranked  \\\n",
       "0                                      NM   \n",
       "1                                      NM   \n",
       "2                     demand_response (2)   \n",
       "3                                      NM   \n",
       "4                                      NM   \n",
       "5                                      NM   \n",
       "6                                      NM   \n",
       "7                                      NM   \n",
       "8                                      NM   \n",
       "9                                      NM   \n",
       "10                                     NM   \n",
       "11                                     NM   \n",
       "12                                     NM   \n",
       "13                                     NM   \n",
       "\n",
       "   applications_performance_tracking_ranked  \\\n",
       "0                                        NM   \n",
       "1                                        NM   \n",
       "2                                        NM   \n",
       "3                                        NM   \n",
       "4                         commissioning (4)   \n",
       "5                                        NM   \n",
       "6                                        NM   \n",
       "7                                        NM   \n",
       "8                         commissioning (5)   \n",
       "9                         commissioning (6)   \n",
       "10                                       NM   \n",
       "11                                       NM   \n",
       "12                        commissioning (6)   \n",
       "13                                       NM   \n",
       "\n",
       "   applications_performance_analysis_ranked data_metering_ranked  \\\n",
       "0                                        NM                   NM   \n",
       "1                  performance_analysis (1)              kw (12)   \n",
       "2                                        NM                   NM   \n",
       "3                performance_evaluation (2)             kwh (62)   \n",
       "4                performance_evaluation (2)               kw (2)   \n",
       "5                                        NM                   NM   \n",
       "6                                        NM              kwh (2)   \n",
       "7                                        NM              kwh (2)   \n",
       "8                                        NM                   NM   \n",
       "9                                        NM              kw (14)   \n",
       "10                                       NM                   NM   \n",
       "11                                       NM               kw (1)   \n",
       "12                                       NM              kw (46)   \n",
       "13                                       NM             kwh (50)   \n",
       "\n",
       "   data_air_temperature_ranked data_air_quality_ranked  \\\n",
       "0         air_temperature (10)       concentration (6)   \n",
       "1         air_temperature (35)       concentration (5)   \n",
       "2                           NM                      NM   \n",
       "3                           NM                      NM   \n",
       "4         air_temperature (19)                      NM   \n",
       "5                           NM                co2 (32)   \n",
       "6                           NM                      NM   \n",
       "7                           NM                      NM   \n",
       "8          air_temperature (2)       concentration (4)   \n",
       "9          air_temperature (4)                      NM   \n",
       "10        air_temperature (24)                      NM   \n",
       "11                          NM                      NM   \n",
       "12                dry_bulb (4)                      NM   \n",
       "13                          NM                      NM   \n",
       "\n",
       "                            data_environment_ranked  \\\n",
       "0                                     humidity (16)   \n",
       "1                                     humidity (13)   \n",
       "2                                                NM   \n",
       "3   humidity (3);wind_speed (3);weather_station (2)   \n",
       "4             outdoor_temperature (13);humidity (6)   \n",
       "5              outdoor_temperature (4);humidity (2)   \n",
       "6                 solar_irradiance (2);humidity (1)   \n",
       "7                                                NM   \n",
       "8                                                NM   \n",
       "9                           outdoor_temperature (6)   \n",
       "10                          outdoor_temperature (2)   \n",
       "11                      humidity (2);wind_speed (1)   \n",
       "12                                     humidity (2)   \n",
       "13                                               NM   \n",
       "\n",
       "          data_occupancy_ranked  \\\n",
       "0                 occupancy (5)   \n",
       "1                 occupancy (7)   \n",
       "2                            NM   \n",
       "3                  presence (1)   \n",
       "4               occupancy (107)   \n",
       "5                 occupancy (1)   \n",
       "6                 occupancy (6)   \n",
       "7                            NM   \n",
       "8                occupancy (14)   \n",
       "9                occupancy (11)   \n",
       "10  occupancy (24);presence (2)   \n",
       "11                occupancy (7)   \n",
       "12                occupancy (2)   \n",
       "13                           NM   \n",
       "\n",
       "                             data_hvac_signals_ranked  \\\n",
       "0      return_air (17);supply_air (12);water_flow (1)   \n",
       "1           supply_air (21);damper (3);return_air (3)   \n",
       "2                                                  NM   \n",
       "3                                                  NM   \n",
       "4                          return_air (12);damper (1)   \n",
       "5                                      supply_air (2)   \n",
       "6                        air_flow (14);supply_air (1)   \n",
       "7                                                  NM   \n",
       "8                                                  NM   \n",
       "9   supply_air (10);return_air (6);air_flow (2);da...   \n",
       "10  supply_air (17);damper (14);valve_position (4)...   \n",
       "11                                                 NM   \n",
       "12        water_flow (26);return_air (7);air_flow (3)   \n",
       "13                       air_flow (11);supply_air (6)   \n",
       "\n",
       "   data_thermal_comfort_ranked  \n",
       "0                     pmv (92)  \n",
       "1                    pmv (150)  \n",
       "2                           NM  \n",
       "3                           NM  \n",
       "4                           NM  \n",
       "5                     pmv (32)  \n",
       "6                           NM  \n",
       "7                           NM  \n",
       "8                           NM  \n",
       "9                           NM  \n",
       "10                          NM  \n",
       "11                          NM  \n",
       "12                          NM  \n",
       "13                          NM  \n",
       "\n",
       "[14 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eplus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
