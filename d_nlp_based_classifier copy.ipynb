{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba466e6a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78902a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Anchor-based, single-CSV pipeline (YAML-faithful + acronym-safe) ===\n",
    "\n",
    "import os, re, json, yaml, string, difflib, bisect\n",
    "from typing import Dict, Iterable, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------- NLTK bootstrap ----------------\n",
    "import nltk\n",
    "def _ensure_nltk():\n",
    "    for res, pkg in [\n",
    "        (\"tokenizers/punkt\",\"punkt\"),\n",
    "        (\"corpora/wordnet\",\"wordnet\"),\n",
    "        (\"corpora/omw-1.4\",\"omw-1.4\"),\n",
    "    ]:\n",
    "        try: nltk.data.find(res)\n",
    "        except LookupError: nltk.download(pkg, quiet=True)\n",
    "_ensure_nltk()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "_lem = WordNetLemmatizer()\n",
    "\n",
    "# ---------------- Normalisation & compounding ----------------\n",
    "def load_cpwords(py_path: str) -> Dict[str,str]:\n",
    "    env = {}\n",
    "    with open(py_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        code = f.read()\n",
    "    exec(compile(code, py_path, \"exec\"), env, env)\n",
    "    if \"compound_keywords\" not in env or not isinstance(env[\"compound_keywords\"], dict):\n",
    "        raise ValueError(\"compound_keywords not found in cpwords file\")\n",
    "    out = {}\n",
    "    for k, v in env[\"compound_keywords\"].items():\n",
    "        kk = str(k).strip().lower()\n",
    "        vv = str(v).strip().lower().replace(\"-\", \" \").replace(\" \", \"_\")\n",
    "        if kk: out[kk] = vv\n",
    "    return out\n",
    "\n",
    "def _collect_multiword_phrases_from_ontology(ont: dict) -> List[str]:\n",
    "    out = set()\n",
    "    def add_if_multi(s):\n",
    "        s = str(s).strip()\n",
    "        if s and (\" \" in s or \"-\" in s):\n",
    "            out.add(s.lower())\n",
    "    for v in (ont or {}).values():\n",
    "        if isinstance(v, dict):\n",
    "            for vv in v.values():\n",
    "                if isinstance(vv, (list, tuple, set)):\n",
    "                    for t in vv: add_if_multi(t)\n",
    "                else:\n",
    "                    add_if_multi(vv)\n",
    "        elif isinstance(v, (list, tuple, set)):\n",
    "            for t in v: add_if_multi(t)\n",
    "        else:\n",
    "            add_if_multi(v)\n",
    "    return sorted(out, key=len, reverse=True)\n",
    "\n",
    "def compile_compounds_regex(phrases: Iterable[str], cpwords_map: Dict[str,str]) -> re.Pattern:\n",
    "    base = set(p.strip().lower() for p in phrases if p and p.strip())\n",
    "    base |= set(k.strip().lower() for k in (cpwords_map or {}).keys())\n",
    "    if not base: return re.compile(r\"(?!x)x\")\n",
    "    toks = sorted(base, key=len, reverse=True)\n",
    "    pat = r\"\\b(\" + \"|\".join(re.escape(p) for p in toks) + r\")\\b\"\n",
    "    return re.compile(pat, re.I)\n",
    "\n",
    "def apply_compounds(text: str, compounds_rx: re.Pattern, cpwords_map: Dict[str,str]) -> str:\n",
    "    def repl(m):\n",
    "        key = m.group(1).lower()\n",
    "        return cpwords_map.get(key, key.replace(\"-\", \"_\").replace(\" \", \"_\"))\n",
    "    return compounds_rx.sub(repl, text)\n",
    "\n",
    "def canonicalise_term(term: str) -> str:\n",
    "    t = term.lower().strip().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    return re.sub(r\"_+\", \"_\", t)\n",
    "\n",
    "def normalize(text: str, compounds_rx: Optional[re.Pattern], cpwords_map: Dict[str,str]) -> str:\n",
    "    if not text: return \"\"\n",
    "    t = text.lower()\n",
    "    if compounds_rx is not None:\n",
    "        t = apply_compounds(t, compounds_rx, cpwords_map)\n",
    "    keep_us = string.punctuation.replace(\"_\",\"\")\n",
    "    t = re.sub(rf\"[{re.escape(keep_us)}]+\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    t = \" \".join(_lem.lemmatize(w) for w in t.split())\n",
    "    return t\n",
    "\n",
    "\n",
    "def any_regex_hits(text: str, regs: List[re.Pattern]) -> List[str]:\n",
    "    \"\"\"collect raw matched strings for a list of regexes\"\"\"\n",
    "    out = []\n",
    "    for rx in regs:\n",
    "        out += [m.group(0) for m in rx.finditer(text)]\n",
    "    return out\n",
    "# ---------------- Sentence split & token windows ----------------\n",
    "_SENT_SPLIT = re.compile(r'(?<=[\\.\\?\\!])\\s+')\n",
    "def sents(text: str) -> List[str]:\n",
    "    try:\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "        return sent_tokenize(text or \"\")\n",
    "    except Exception:\n",
    "        return _SENT_SPLIT.split(text or \"\")\n",
    "\n",
    "def tokenize(text: str):\n",
    "    tokens, starts = [], []\n",
    "    for m in re.finditer(r\"\\S+\", text):\n",
    "        tokens.append(m.group(0))\n",
    "        starts.append(m.start())\n",
    "    return tokens, starts\n",
    "\n",
    "def window_from_charspan(text: str, tokens: List[str], starts: List[int], span: Tuple[int,int], k: int) -> str:\n",
    "    i = bisect.bisect_right(starts, span[0]) - 1\n",
    "    if i < 0: i = 0\n",
    "    lo = max(0, i - k); hi = min(len(tokens)-1, i + k)\n",
    "    return \" \".join(tokens[lo:hi+1])\n",
    "###\n",
    "# ---- STRICT token-boundary helpers (avoid 'ga*' → gap/gas/gauge/gain) ----\n",
    "_ACRO_LEFT  = r\"(?<![A-Za-z0-9_])\"\n",
    "_ACRO_RIGHT = r\"(?![A-Za-z0-9_])\"\n",
    "\n",
    "def build_phrase_regexes_strict(terms: Iterable[str]) -> List[re.Pattern]:\n",
    "    \"\"\"\n",
    "    Build regexes for phrases (multi-word or tokens len>=4).\n",
    "    Single short tokens are handled separately (acronym logic).\n",
    "    \"\"\"\n",
    "    regs, seen = [], set()\n",
    "    for t in (terms or []):\n",
    "        v = canonicalise_term(t)\n",
    "        if not v:\n",
    "            continue\n",
    "        # skip short single tokens (they'll be handled as acronyms)\n",
    "        if re.fullmatch(r\"[a-z0-9_]+\", v) and \"_\" not in v and len(v) <= 3:\n",
    "            continue\n",
    "        key = (\"strict\", v)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        regs.append(re.compile(r\"\\b\" + _flexify(v) + r\"\\b\", re.I))\n",
    "    return regs\n",
    "\n",
    "def build_acronym_regexes_exact(raw_terms: Iterable[str]) -> List[re.Pattern]:\n",
    "    \"\"\"\n",
    "    Acronyms on RAW text, case-sensitive exact token: GA, MPC, PID, RL, DRL, PSO, PMV, PPD, CO2, PM2.5, ...\n",
    "    \"\"\"\n",
    "    regs = []\n",
    "    for t in (raw_terms or []):\n",
    "        s = str(t).strip()\n",
    "        if not s or \" \" in s:\n",
    "            continue\n",
    "        # keep shortish tokens or with digits/dot\n",
    "        if len(s) <= 6 or re.search(r\"[0-9.]\", s):\n",
    "            pat = _ACRO_LEFT + re.escape(s.upper()) + _ACRO_RIGHT\n",
    "            regs.append(re.compile(pat))\n",
    "    return regs\n",
    "\n",
    "def build_lower_acronym_regexes_norm(norm_terms: Iterable[str]) -> List[re.Pattern]:\n",
    "    \"\"\"\n",
    "    Exact-token acronyms (len >= 3) on NORMALIZED text (case-insensitive),\n",
    "    to catch papers that write acronyms in lowercase (e.g., 'pso').\n",
    "    We *do not* include 2-letter tokens here to avoid noise.\n",
    "    \"\"\"\n",
    "    regs = []\n",
    "    for t in (norm_terms or []):\n",
    "        v = canonicalise_term(t)\n",
    "        if not v or \" \" in v or \"_\" in v:\n",
    "            continue\n",
    "        if len(v) >= 3:  # 3+ letters only\n",
    "            regs.append(re.compile(_ACRO_LEFT + re.escape(v) + _ACRO_RIGHT, re.I))\n",
    "    return regs\n",
    "\n",
    "# ---------------- Regex helpers ----------------\n",
    "SEPARATOR_FLEX = r\"[ _-]+\"\n",
    "def _flexify(term: str) -> str:\n",
    "    t = (term or \"\").strip()\n",
    "    t = re.escape(t)\n",
    "    t = re.sub(r\"\\\\\\s+\", SEPARATOR_FLEX, t)\n",
    "    return t + r\"\\w*\"\n",
    "\n",
    "def build_regexes_from_terms(terms: Iterable[str], require_min_ngram=None) -> List[re.Pattern]:\n",
    "    regs, seen = [], set()\n",
    "    for t in (terms or []):\n",
    "        v = canonicalise_term(t)\n",
    "        if not v: continue\n",
    "        if require_min_ngram and len(re.split(r\"[ _-]+\", v)) < require_min_ngram:\n",
    "            continue\n",
    "        key = (\"rx\", v)\n",
    "        if key in seen: continue\n",
    "        seen.add(key)\n",
    "        regs.append(re.compile(r\"\\b\" + _flexify(v) + r\"\\b\", re.I))\n",
    "    return regs\n",
    "\n",
    "# ---------------- Action/performance gates ----------------\n",
    "VERBS_RX     = re.compile(r\"\\b(model(?:ing|led|ed|s)?|estimate(?:d|s|r)?|predict(?:ed|s|ive)?|analy[sz]e(?:d|s|r)?|simulate(?:d|s)?)\\b\", re.I)\n",
    "FAVOURED_RX  = re.compile(r\"\\b(performance|accuracy|efficienc\\w*|load|consumption|demand|baseline|benchmark(?:ing)?|error|mae|rmse|mape)\\b\", re.I)\n",
    "MODEL_WORD_RX= re.compile(r\"\\bmodel(?:s|ing|led|ed)?\\b\", re.I)\n",
    "GENERIC_OPT_RX = re.compile(r\"\\b(optimi[sz]e?(?:d|s|r)?|optimi[sz]ation|multi-?objective)\\b\", re.I)\n",
    "\n",
    "# ---------------- Gappy “<term> … model” support ----------------\n",
    "GAP_MAX   = 3   # words allowed between term and \"model\"\n",
    "WORD_WINDOW = 15# ± words around anchor\n",
    "MERGE_SIM = 0.90# similarity threshold for merging\n",
    "\n",
    "# ----- General gappy \"<core> ... <SUFFIX>\" helpers -----\n",
    "\n",
    "TERM_SUFFIX_RX = r\"(?:-?(?:based|driven|guided|assisted|enabled|oriented|like|alike))?\"  # allowed after the *core*\n",
    "\n",
    "_SUFFIX_PATTERNS = {\n",
    "    \"model\": r\"model\\w*\",\n",
    "    \"optimization\": r\"optimi[sz]\\w*\",\n",
    "    \"control\": r\"control\\w*\",\n",
    "}\n",
    "\n",
    "def _sep_flexify(s: str) -> str:\n",
    "    s = canonicalise_term(s)\n",
    "    parts = [re.escape(p) for p in s.split(\"_\") if p]\n",
    "    if not parts: return \"\"\n",
    "    return r\"(?:%s)\" % r\"[ _-]+\".join(parts)\n",
    "\n",
    "def _split_core_and_suffix(term: str, suffix_key: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    If term already contains the suffix token (e.g. 'linear_model', 'multi_objective_optimization'),\n",
    "    split into (core, suffix). Otherwise, treat whole term as core (suffix empty).\n",
    "    \"\"\"\n",
    "    t = canonicalise_term(term)\n",
    "    toks = [p for p in t.split(\"_\") if p]\n",
    "    suf = suffix_key\n",
    "    # find last index where token equals the suffix (e.g., 'model', 'optimization', 'control')\n",
    "    idx = None\n",
    "    for i in range(len(toks)-1, -1, -1):\n",
    "        if toks[i] == suf:\n",
    "            idx = i; break\n",
    "    if idx is None:\n",
    "        return t, \"\"   # no explicit suffix present\n",
    "    core = \"_\".join(toks[:idx]) if idx > 0 else \"\"\n",
    "    return core, suf\n",
    "\n",
    "def build_gappy_suffix_regexes(terms: Iterable[str], suffix_key: str, gap_max: int = GAP_MAX) -> List[re.Pattern]:\n",
    "    \"\"\"\n",
    "    Build regexes that match:  <core>(-based/...)? (<=gap words) <suffix>\n",
    "    - If term already has the suffix, we treat tokens *before* suffix as <core>\n",
    "    - If term has no suffix, we use the *entire* term as <core> and still require suffix at the end\n",
    "    \"\"\"\n",
    "    regs = []\n",
    "    suffix_pat = _SUFFIX_PATTERNS[suffix_key]  # e.g., model\\w*\n",
    "    for t in set(canonicalise_term(x) for x in (terms or []) if str(x).strip()):\n",
    "        core, suf = _split_core_and_suffix(t, suffix_key)\n",
    "        if not core:\n",
    "            # fall back: if they wrote exactly 'model' alone, ignore; otherwise require suffix with gap\n",
    "            core = t if t != suffix_key else \"\"\n",
    "        if not core:\n",
    "            continue\n",
    "        core_pat = _sep_flexify(core)\n",
    "        pat = (\n",
    "            r\"\\b\" + core_pat + TERM_SUFFIX_RX +             # core + optional '-based' etc.\n",
    "            r\"(?:\\W+\\w+){0,\" + str(gap_max) + r\"}\" +        # up to N words between\n",
    "            r\"\\W+\" + suffix_pat + r\"\\b\"                     # required suffix token\n",
    "        )\n",
    "        regs.append(re.compile(pat, re.I))\n",
    "    return regs\n",
    "\n",
    "def extract_core_from_gappy_hit(hit: str, suffix_key: str) -> str:\n",
    "    \"\"\"\n",
    "    From a matched '<core>(-based)? ... <suffix>' return canonicalised <core>.\n",
    "    We trim everything from the suffix rightwards, then strip any trailing gap words/suffix-like endings.\n",
    "    \"\"\"\n",
    "    s = hit.lower()\n",
    "    # split at the *first* occurrence of the suffix family\n",
    "    suf_pat = re.compile(_SUFFIX_PATTERNS[suffix_key], re.I)\n",
    "    m = suf_pat.search(s)\n",
    "    if not m:\n",
    "        return canonicalise_term(s)\n",
    "    core = s[:m.start()]\n",
    "    # drop trailing gap words and core-suffix artefacts\n",
    "    core = re.sub(r\"(?:\\W+\\w+){0,\" + str(GAP_MAX) + r\"}$\", \"\", core)\n",
    "    core = re.sub(TERM_SUFFIX_RX + r\"$\", \"\", core)\n",
    "    return canonicalise_term(core)\n",
    "\n",
    "# ---------------- 90% merge for near-duplicates ----------------\n",
    "def _sim(a: str, b: str) -> float:\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def merge_counter_by_similarity(counter: \"Counter[str]\", threshold: float = MERGE_SIM) -> \"Counter[str]\":\n",
    "    items = [(canonicalise_term(k), v) for k, v in counter.items()]\n",
    "    \n",
    "    items.sort(key=lambda kv: len(kv[0]))\n",
    "    reps, used = [], set()\n",
    "    for i, (k, v) in enumerate(items):\n",
    "        if i in used: continue\n",
    "        cluster = [(k, v)]; used.add(i)\n",
    "        for j in range(i+1, len(items)):\n",
    "            if j in used: continue\n",
    "            k2, v2 = items[j]\n",
    "            if (len(k) > 6 or len(k2) > 6):\n",
    "                sim_thresh = 0.85\n",
    "            elif len(k) <= 3 or len(k2) <= 3:\n",
    "                sim_thresh = 0.95\n",
    "            else:\n",
    "                sim_thresh = threshold\n",
    "            if _sim(k, k2) >= sim_thresh or k in k2 or k2 in k:\n",
    "                cluster.append((k2, v2)); used.add(j)\n",
    "        rep = min((t for t,_ in cluster), key=len)\n",
    "        reps.append((rep, sum(c for _, c in cluster)))\n",
    "    out = Counter()\n",
    "    for k, v in reps: out[k] += v\n",
    "    return out\n",
    "\n",
    "def merge_set_by_similarity(terms: Iterable[str], threshold: float = MERGE_SIM) -> List[str]:\n",
    "    cnt = Counter(canonicalise_term(t) for t in terms if str(t).strip())\n",
    "    merged = merge_counter_by_similarity(cnt, threshold)\n",
    "    return sorted(merged.keys())\n",
    "\n",
    "# ---------------- Optional Word2Vec expansion (scale only) ----------------\n",
    "try:\n",
    "    from gensim.models import Word2Vec as GensimWord2Vec\n",
    "except Exception:\n",
    "    GensimWord2Vec = None\n",
    "\n",
    "@dataclass\n",
    "class NLPConfig:\n",
    "    mode: str = \"auto\"   # off|light|full|auto\n",
    "    model_path: str = \"\"\n",
    "    exp_topn: int = 40\n",
    "    exp_thresh: float = 0.70\n",
    "    exp_min_count: int = 10\n",
    "    context_k: int = 1\n",
    "\n",
    "def load_nlp_config(path_or_ontology_yaml: Optional[str]) -> NLPConfig:\n",
    "    cfg = NLPConfig()\n",
    "    if path_or_ontology_yaml and os.path.exists(path_or_ontology_yaml):\n",
    "        with open(path_or_ontology_yaml, \"r\", encoding=\"utf-8\") as f:\n",
    "            y = yaml.safe_load(f) or {}\n",
    "        y = y.get(\"_nlp\", y)\n",
    "        cfg.mode = str(y.get(\"mode\", cfg.mode)).lower()\n",
    "        cfg.model_path = str(y.get(\"model_path\", cfg.model_path))\n",
    "        cfg.exp_topn = int(y.get(\"exp_topn\", cfg.exp_topn))\n",
    "        cfg.exp_thresh = float(y.get(\"exp_thresh\", cfg.exp_thresh))\n",
    "        cfg.exp_min_count = int(y.get(\"exp_min_count\", cfg.exp_min_count))\n",
    "        cfg.context_k = int(y.get(\"context_k\", cfg.context_k))\n",
    "    return cfg\n",
    "\n",
    "def resolve_nlp_mode(cfg: NLPConfig) -> str:\n",
    "    if cfg.mode in {\"off\",\"light\",\"full\"}: return cfg.mode\n",
    "    if GensimWord2Vec is None: return \"light\"\n",
    "    candidates = [cfg.model_path] if cfg.model_path else []\n",
    "    candidates += [\"model.word2vec\",\"model.w2v\",\"word2vec.model\", os.path.join(os.getcwd(),\"word2vec.model\")]\n",
    "    for p in candidates:\n",
    "        if p and os.path.exists(p):\n",
    "            try:\n",
    "                GensimWord2Vec.load(p); return \"full\"\n",
    "            except Exception:\n",
    "                continue\n",
    "    return \"light\"\n",
    "\n",
    "def expand_terms_w2v(model, seeds: List[str], topn=40, thresh=0.70, min_count=10) -> List[str]:\n",
    "    out = set(canonicalise_term(s) for s in seeds if s)\n",
    "    if not model: return sorted(out)\n",
    "    for t in list(out):\n",
    "        if t in model.wv:\n",
    "            for cand, sim in model.wv.most_similar(t, topn=topn):\n",
    "                if sim < thresh: continue\n",
    "                try:\n",
    "                    if model.wv.get_vecattr(cand,\"count\") < min_count: continue\n",
    "                except Exception: pass\n",
    "                if any(ch.isdigit() for ch in cand): continue\n",
    "                out.add(canonicalise_term(cand))\n",
    "    return sorted(out)\n",
    "\n",
    "# ---------------- IO ----------------\n",
    "def load_jsons(dirpath):\n",
    "    for fn in os.listdir(dirpath):\n",
    "        if fn.endswith(\".json\"):\n",
    "            p = os.path.join(dirpath, fn)\n",
    "            try:\n",
    "                with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                    yield fn, json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] failed to read {fn}: {e}\")\n",
    "\n",
    "# ---------------- Acronym-safe matchers (use RAW text) ----------------\n",
    "_ACRO_LEFT  = r\"(?<![A-Za-z0-9_])\"\n",
    "_ACRO_RIGHT = r\"(?![A-Za-z0-9_])\"\n",
    "\n",
    "def pick_acronyms(terms: Iterable[str]) -> List[str]:\n",
    "    acr = []\n",
    "    for t in (terms or []):\n",
    "        s = str(t).strip()\n",
    "        if not s: continue\n",
    "        if \" \" in s: continue          # spaces → phrase, not acronym\n",
    "        # keep shortish tokens or mixed with digits/dots (e.g., PM2.5)\n",
    "        if len(s) <= 6 or re.search(r\"[0-9.]\", s):\n",
    "            acr.append(s)\n",
    "    return acr\n",
    "\n",
    "def build_acronym_regexes(terms: Iterable[str]) -> List[re.Pattern]:\n",
    "    regs = []\n",
    "    for t in pick_acronyms(terms):\n",
    "        pat = _ACRO_LEFT + re.escape(t.upper()) + _ACRO_RIGHT\n",
    "        regs.append(re.compile(pat))    # case-sensitive on raw text\n",
    "    return regs\n",
    "# ---------------- YAML-faithful category counting (acronym-safe) ----------------\n",
    "def count_yaml_category(raw_text: str, norm_text: str, terms: Iterable[str],\n",
    "                        suffix_hint: Optional[str] = None) -> Counter:\n",
    "    \"\"\"\n",
    "    YAML-faithful counting that avoids acronym noise:\n",
    "      - multi-word phrases & tokens len>=4 -> flexible phrase regex on normalized text\n",
    "      - acronyms (single tokens len<=3 or with digits) -> exact-token on RAW (case-sensitive)\n",
    "      - 3+ letter acronyms also matched exactly on normalized text (case-insensitive)\n",
    "      - optional gappy '<core> ... <suffix>' (e.g., optimization/control) on normalized text\n",
    "    \"\"\"\n",
    "    cnt = Counter()\n",
    "\n",
    "    # 1) phrases (multi-word or len>=4) on normalized text\n",
    "    phrase_rx = build_phrase_regexes_strict(terms)\n",
    "    for rx in phrase_rx:\n",
    "        for m in rx.finditer(norm_text):\n",
    "            cnt[canonicalise_term(m.group(0))] += 1\n",
    "\n",
    "    # 2) acronyms on raw text (2–6 chars or contains digits/dot), case-sensitive exact\n",
    "    acro_rx = build_acronym_regexes_exact(terms)\n",
    "    for rx in acro_rx:\n",
    "        for m in rx.finditer(raw_text):\n",
    "            cnt[canonicalise_term(m.group(0))] += 1\n",
    "\n",
    "    # 3) lower-case acronyms (len>=3) on normalized text, exact token, case-insensitive\n",
    "    acro_lower_rx = build_lower_acronym_regexes_norm(terms)\n",
    "    for rx in acro_lower_rx:\n",
    "        for m in rx.finditer(norm_text):\n",
    "            cnt[canonicalise_term(m.group(0))] += 1\n",
    "\n",
    "    # 4) optional gappy suffix (e.g., '... optimization' / '... control') on normalized text\n",
    "    if suffix_hint in _SUFFIX_PATTERNS:\n",
    "        gappy_rx = build_gappy_suffix_regexes(terms, suffix_key=suffix_hint, gap_max=GAP_MAX)\n",
    "        for rx in gappy_rx:\n",
    "            for m in rx.finditer(norm_text):\n",
    "                core = extract_core_from_gappy_hit(m.group(0), suffix_key=suffix_hint)\n",
    "                cnt[core] += 1\n",
    "\n",
    "    return merge_counter_by_similarity(cnt, threshold=MERGE_SIM)\n",
    "\n",
    "\n",
    "# ---------------- Paradigm term-only regex builder ----------------\n",
    "def build_simple_paradigm_term_rx(paradigms: dict) -> dict:\n",
    "    out = {}\n",
    "    for pname, terms in (paradigms or {}).items():\n",
    "        canon_terms = [canonicalise_term(t) for t in (terms or [])]\n",
    "        out[pname] = build_regexes_from_terms(canon_terms, require_min_ngram=1)\n",
    "    return out\n",
    "# ---------------- Merging ranked columns ----------------\n",
    "def _merge_ranked_cols(cols: dict) -> str:\n",
    "    \"\"\"\n",
    "    Merge several *_ranked strings like 'term_a (3);term_b (1)' into one,\n",
    "    summing counts and sorting by count desc, then name.\n",
    "    \"\"\"\n",
    "    merged = Counter()\n",
    "    for _, v in (cols or {}).items():\n",
    "        if not v or v == \"NM\":\n",
    "            continue\n",
    "        for token in v.split(\";\"):\n",
    "            token = token.strip()\n",
    "            if not token:\n",
    "                continue\n",
    "            if \"(\" in token and token.endswith(\")\"):\n",
    "                name, count = token.rsplit(\"(\", 1)\n",
    "                try:\n",
    "                    c = int(count.strip(\") \").strip())\n",
    "                except:\n",
    "                    c = 1\n",
    "                merged[name.strip()] += c\n",
    "            else:\n",
    "                merged[token] += 1\n",
    "    if not merged:\n",
    "        return \"NM\"\n",
    "    items = sorted(merged.items(), key=lambda kv: (-kv[1], kv[0]))\n",
    "    return \";\".join(f\"{t} ({n})\" for t, n in items)\n",
    "\n",
    "def load_alias_map(ont: dict) -> Dict[str, str]:\n",
    "    \"\"\"Load a global alias→canonical map; both sides are underscore-canonicalised.\"\"\"\n",
    "    amap = {}\n",
    "    for k, v in (ont.get(\"_alias_map\", {}) or {}).items():\n",
    "        if v is None:  # allow disabling an alias by mapping to null\n",
    "            continue\n",
    "        amap[canonicalise_term(k)] = canonicalise_term(v)\n",
    "    return amap\n",
    "\n",
    "def collapse_counter_aliases(counter: Counter, alias_map: Dict[str, str]) -> Counter:\n",
    "    \"\"\"\n",
    "    Move counts from alias keys to their canonical target per alias_map.\n",
    "    Then apply the usual 0.90 similarity merge.\n",
    "    \"\"\"\n",
    "    if not alias_map:\n",
    "        return merge_counter_by_similarity(counter, threshold=MERGE_SIM)\n",
    "    out = Counter(counter)\n",
    "    for k, c in list(out.items()):\n",
    "        can = alias_map.get(canonicalise_term(k))\n",
    "        if can and can != k:\n",
    "            out[can] += c\n",
    "            del out[k]\n",
    "    return merge_counter_by_similarity(out, threshold=MERGE_SIM)\n",
    "\n",
    "def map_terms_set(terms: Iterable[str], alias_map: Dict[str, str]) -> List[str]:\n",
    "    \"\"\"Apply alias_map to a set/list of strings (no counts).\"\"\"\n",
    "    mapped = []\n",
    "    for t in (terms or []):\n",
    "        ct = canonicalise_term(t)\n",
    "        mapped.append(alias_map.get(ct, ct))\n",
    "    # de-dup then apply similarity merge on names\n",
    "    return merge_set_by_similarity(mapped, threshold=MERGE_SIM)\n",
    "# ---------------- Core analysis (anchor-based + YAML-faithful cats) ----------------\n",
    "def analyze_anchor_based_single(doc: dict,\n",
    "                                ont: dict,\n",
    "                                compounds_rx: Optional[re.Pattern],\n",
    "                                cp_map: Dict[str,str],\n",
    "                                cfg: NLPConfig,\n",
    "                                w2v,\n",
    "                                alias_map: Dict[str, str]) -> dict:\n",
    "    raw = (doc.get(\"full-text-retrieval-response\", {}) or {}).get(\"originalText\",\"\") or \"\"\n",
    "    text = normalize(raw, compounds_rx, cp_map)  # normalized (lowercased)\n",
    "    S = sents(text)\n",
    "    tokens, starts = tokenize(text)\n",
    "\n",
    "    # Ontology buckets\n",
    "    scale       = ont.get(\"scale\", {}) or {}\n",
    "    paradigms   = ont.get(\"model_paradigm\", {}) or {}\n",
    "    optim_cats  = ont.get(\"optimization_methods\", {}) or {}\n",
    "    app_cats    = ont.get(\"applications\", {}) or {}\n",
    "    data_cats   = ont.get(\"data_types\", {}) or {}\n",
    "    \n",
    "    # Canonicalised seeds (+ optional W2V) for scale only\n",
    "    def seeds(key):\n",
    "        base = [canonicalise_term(x) for x in (scale.get(key, []) or [])]\n",
    "        return expand_terms_w2v(w2v, base, cfg.exp_topn, cfg.exp_thresh, cfg.exp_min_count) if w2v else base\n",
    "\n",
    "    building_terms = seeds(\"building_model\")\n",
    "    system_terms   = seeds(\"system_model\")\n",
    "    climate_terms  = seeds(\"climate_model\")\n",
    "    occup_terms    = seeds(\"occupancy_model\")\n",
    "\n",
    "    # Finders for scale terms (need spans)\n",
    "    def rx_find_terms(terms):\n",
    "        if not terms: return None\n",
    "        pats = [r\"\\b\" + _flexify(t) + r\"\\b\" for t in set(terms)]\n",
    "        return re.compile(\"|\".join(pats), re.I)\n",
    "\n",
    "    RX_FIND = {\n",
    "        \"building\": rx_find_terms(building_terms),\n",
    "        \"system\":   rx_find_terms(system_terms),\n",
    "        \"weather\":  rx_find_terms(climate_terms),\n",
    "        \"occupancy\":rx_find_terms(occup_terms),\n",
    "    }\n",
    "\n",
    "    # Paradigm & optimisation matchers\n",
    "    par_rx        = {k: build_regexes_from_terms(v, require_min_ngram=1) for k, v in (paradigms or {}).items()}\n",
    "    par_gappy_rx  = {k: build_gappy_suffix_regexes(v, suffix_key=\"model\", gap_max=GAP_MAX)\n",
    "                 for k, v in (paradigms or {}).items()}\n",
    "\n",
    "    par_term_rx   = build_simple_paradigm_term_rx(paradigms)\n",
    "    opt_rx        = {k: build_regexes_from_terms(v, require_min_ngram=1) for k, v in (optim_cats or {}).items()}\n",
    "\n",
    "    # Containers\n",
    "    term_counts = {c: Counter() for c in RX_FIND.keys()}\n",
    "    par_found   = {c: set() for c in RX_FIND.keys()}\n",
    "    par_terms   = {c: set() for c in RX_FIND.keys()}\n",
    "    opt_methods, opt_terms = set(), set()\n",
    "\n",
    "    # PRIMARY path — scale-term windows\n",
    "    for cat, rx in RX_FIND.items():\n",
    "        if rx is None: continue\n",
    "        for m in rx.finditer(text):\n",
    "            w = window_from_charspan(text, tokens, starts, m.span(), k=WORD_WINDOW)\n",
    "            if not (VERBS_RX.search(w) or FAVOURED_RX.search(w)):\n",
    "                continue\n",
    "            term_counts[cat][canonicalise_term(m.group(0))] += 1\n",
    "\n",
    "            # Paradigms in this window → ONLY this category\n",
    "            for pname in par_rx.keys():\n",
    "                plain_hits = []\n",
    "                for rxx in par_rx[pname]:\n",
    "                    plain_hits += [canonicalise_term(h.group(0)) for h in rxx.finditer(w)]\n",
    "                gappy_strings = any_regex_hits(w, par_gappy_rx.get(pname, []))\n",
    "                gappy_terms   = [extract_core_from_gappy_hit(h, suffix_key=\"model\") for h in gappy_strings]\n",
    "                belongs_via_gappy = False\n",
    "                if gappy_terms and par_term_rx.get(pname):\n",
    "                    for t_ in gappy_terms:\n",
    "                        if any(r.search(t_) for r in par_term_rx[pname]):\n",
    "                            belongs_via_gappy = True; break\n",
    "                if plain_hits or belongs_via_gappy:\n",
    "                    par_found[cat].add(pname)\n",
    "                    par_terms[cat].update(plain_hits)\n",
    "                    par_terms[cat].update(gappy_terms)\n",
    "\n",
    "            # Optimisation phrases in this window (global list)\n",
    "            for oname, regs in opt_rx.items():\n",
    "                found = []\n",
    "                for rxx in regs:\n",
    "                    found += [canonicalise_term(h.group(0)) for h in rxx.finditer(w)]\n",
    "                if found:\n",
    "                    opt_methods.add(oname); opt_terms.update(found)\n",
    "            g = GENERIC_OPT_RX.search(w)\n",
    "            if g:\n",
    "                opt_methods.add(\"generic\"); opt_terms.add(canonicalise_term(g.group(0)))\n",
    "\n",
    "    # FLEX path — paradigm anchored; map to categories if window shows 'model' + category hints\n",
    "    CATEGORY_HINT = {\n",
    "        \"building\":  re.compile(r\"\\b(building|facility|premise|asset|whole[_ ]building|zone|envelope|bem|ubem)\\b\", re.I),\n",
    "        \"system\":    re.compile(r\"\\b(system|hvac|ahu|doas|vav|fcu|coil|chiller|boiler|pump|tower|vr[fb]|heat[_ ]pump)\\b\", re.I),\n",
    "        \"occupancy\": re.compile(r\"\\b(occupanc\\w*|occupant\\w*|people|tenant|user)\\b\", re.I),\n",
    "        \"weather\":   re.compile(r\"\\b(weather|climate|outdoor|ambient|meteorolog\\w*)\\b\", re.I),\n",
    "    }\n",
    "    ALL_PAR_RXS = [r for regs in par_rx.values() for r in regs]\n",
    "\n",
    "    for rx in ALL_PAR_RXS:\n",
    "        for m in rx.finditer(text):\n",
    "            w = window_from_charspan(text, tokens, starts, m.span(), k=WORD_WINDOW)\n",
    "            if not (MODEL_WORD_RX.search(w) and (VERBS_RX.search(w) or FAVOURED_RX.search(w))):\n",
    "                continue\n",
    "            for cat in [\"building\",\"system\",\"occupancy\",\"weather\"]:\n",
    "                hint_ok  = CATEGORY_HINT[cat].search(w) is not None\n",
    "                scale_ok = (RX_FIND[cat] is not None) and (RX_FIND[cat].search(w) is not None)\n",
    "                if not (hint_ok or scale_ok): continue\n",
    "\n",
    "                pstr = canonicalise_term(m.group(0))\n",
    "                all_gappy_strings = any_regex_hits(w, [rgx for lst in par_gappy_rx.values() for rgx in lst])\n",
    "                ext_terms = [extract_core_from_gappy_hit(h, suffix_key=\"model\") for h in all_gappy_strings]\n",
    "\n",
    "\n",
    "                matched_buckets = set()\n",
    "                for pname, regs in par_rx.items():\n",
    "                    if any(r.search(m.group(0)) for r in regs):\n",
    "                        matched_buckets.add(pname)\n",
    "                for pname, regs in par_term_rx.items():\n",
    "                    if any(r.search(t_) for r in regs for t_ in ([pstr] + ext_terms)):\n",
    "                        matched_buckets.add(pname)\n",
    "\n",
    "                for pname in matched_buckets:\n",
    "                    par_found[cat].add(pname)\n",
    "                par_terms[cat].add(pstr)\n",
    "                par_terms[cat].update(ext_terms)\n",
    "\n",
    "    # Merge near-duplicates (scale/paradigm terms)\n",
    "    for cat in term_counts.keys():\n",
    "        term_counts[cat] = merge_counter_by_similarity(term_counts[cat], threshold=MERGE_SIM)\n",
    "        par_terms[cat]   = set(merge_set_by_similarity(par_terms[cat], threshold=MERGE_SIM))\n",
    "\n",
    "    # ---------------- YAML-faithful counts for optimization/applications/data_types ----------------\n",
    "    # We do these at document-level (no window), with acronym-safe matching.\n",
    "\n",
    "    opt_ranked_cols = {}\n",
    "    for cat_name, terms in (optim_cats or {}).items():\n",
    "        c = count_yaml_category(raw, text, terms, suffix_hint=\"optimization\")\n",
    "        opt_ranked_cols[f\"optimization_{canonicalise_term(cat_name)}_ranked\"] = (\n",
    "            \"NM\" if not c else \";\".join(f\"{t} ({n})\" for t, n in sorted(c.items(), key=lambda kv:(-kv[1], kv[0])))\n",
    "        )\n",
    "\n",
    "# ----- OPTIMIZATION (suffix = optimization) -----\n",
    "    opt_ranked_cols = {}\n",
    "    for cat_name, terms in (optim_cats or {}).items():\n",
    "        c = count_yaml_category(raw, text, terms, suffix_hint=\"optimization\")\n",
    "        c = collapse_counter_aliases(c, alias_map)  # <<< collapse aliases\n",
    "        opt_ranked_cols[f\"optimization_{canonicalise_term(cat_name)}_ranked\"] = (\n",
    "            \"NM\" if not c else \";\".join(f\"{t} ({n})\" for t, n in sorted(c.items(), key=lambda kv: (-kv[1], kv[0])))\n",
    "        )\n",
    "\n",
    "\n",
    "    app_ranked_cols = {}\n",
    "    for cat_name, terms in (app_cats or {}).items():\n",
    "        hint = \"control\" if (\"control\" in canonicalise_term(cat_name)\n",
    "                            or any(canonicalise_term(t).endswith(\"_control\") for t in (terms or []))) else None\n",
    "        c = count_yaml_category(raw, text, terms, suffix_hint=hint)\n",
    "        c = collapse_counter_aliases(c, alias_map)  # <<< collapse aliases\n",
    "        app_ranked_cols[f\"applications_{canonicalise_term(cat_name)}_ranked\"] = (\n",
    "            \"NM\" if not c else \";\".join(f\"{t} ({n})\" for t, n in sorted(c.items(), key=lambda kv: (-kv[1], kv[0])))\n",
    "        )\n",
    "\n",
    "\n",
    "    data_ranked_cols = {}\n",
    "    for cat_name, terms in (data_cats or {}).items():\n",
    "        c = count_yaml_category(raw, text, terms, suffix_hint=None)\n",
    "        c = collapse_counter_aliases(c, alias_map)  # <<< collapse aliases\n",
    "        data_ranked_cols[f\"data_{canonicalise_term(cat_name)}_ranked\"] = (\n",
    "            \"NM\" if not c else \";\".join(f\"{t} ({n})\" for t, n in sorted(c.items(), key=lambda kv: (-kv[1], kv[0])))\n",
    "        )\n",
    "\n",
    "    # Assemble row\n",
    "    def ranked_pairs(counter: Counter) -> str:\n",
    "        if not counter: return \"NM\"\n",
    "        items = sorted(counter.items(), key=lambda kv: (-kv[1], kv[0]))\n",
    "        return \";\".join(f\"{t} ({c})\" for t, c in items)\n",
    "\n",
    "    def dedup_join(xs: Iterable[str]) -> str:\n",
    "        vals = [canonicalise_term(x) for x in (xs or []) if str(x).strip()]\n",
    "        return \";\".join(sorted(set(vals))) if vals else \"NM\"\n",
    "\n",
    "    row = {\n",
    "    \"building_terms_ranked\":  ranked_pairs(term_counts[\"building\"]),\n",
    "    \"system_terms_ranked\":    ranked_pairs(term_counts[\"system\"]),\n",
    "    \"weather_terms_ranked\":   ranked_pairs(term_counts[\"weather\"]),\n",
    "    \"occupancy_terms_ranked\": ranked_pairs(term_counts[\"occupancy\"]),\n",
    "    \"building_paradigms\":       dedup_join(par_found[\"building\"]),\n",
    "    \"system_paradigms\":         dedup_join(par_found[\"system\"]),\n",
    "    \"weather_paradigms\":        dedup_join(par_found[\"weather\"]),\n",
    "    \"occupancy_paradigms\":      dedup_join(par_found[\"occupancy\"]),\n",
    "    \"building_paradigm_terms\":  dedup_join(par_terms[\"building\"]),\n",
    "    \"system_paradigm_terms\":    dedup_join(par_terms[\"system\"]),\n",
    "    \"weather_paradigm_terms\":   dedup_join(par_terms[\"weather\"]),\n",
    "    \"occupancy_paradigm_terms\": dedup_join(par_terms[\"occupancy\"]),\n",
    "    \"optimization_methods\":     dedup_join(opt_methods),\n",
    "    \"optimization_method_terms\":dedup_join(opt_terms),\n",
    "    # merged summaries the user wants\n",
    "    \"applications_merged\": _merge_ranked_cols(app_ranked_cols),\n",
    "    \"data_types_merged\":   _merge_ranked_cols(data_ranked_cols),\n",
    "    }\n",
    "    row.update(opt_ranked_cols)   # keep per-optimization-subgroup columns\n",
    "    # (If you *don’t* want the per-subgroup app/data columns, do NOT add app_ranked_cols / data_ranked_cols here)\n",
    "\n",
    "\n",
    "    row.update(opt_ranked_cols)\n",
    "    row.update(app_ranked_cols)\n",
    "    row.update(data_ranked_cols)\n",
    "    row[\"applications_merged\"] = _merge_ranked_cols(app_ranked_cols)\n",
    "    row[\"data_types_merged\"]   = _merge_ranked_cols(data_ranked_cols)\n",
    "\n",
    "    return row\n",
    "\n",
    "# ---------------- Runner (writes a single CSV) ----------------\n",
    "def run_anchor_based(\n",
    "    input_dir: str,\n",
    "    ontology_path: str,\n",
    "    cpwords_path: Optional[str] = None,\n",
    "    anchor_csv: str = \"nlp_output/anchor_based.csv\",\n",
    "    nlp: str = \"auto\",\n",
    "    config_path: Optional[str] = None,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    with open(ontology_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ont = yaml.safe_load(f) or {}\n",
    "    alias_map = load_alias_map(ont)\n",
    "    cfg = load_nlp_config(config_path or ontology_path)\n",
    "    if nlp: cfg.mode = nlp\n",
    "    mode = resolve_nlp_mode(cfg)\n",
    "    if verbose: print(f\"[NLP] mode = {mode}\")\n",
    "\n",
    "    # Word2Vec (full)\n",
    "    w2v = None\n",
    "    if mode == \"full\" and GensimWord2Vec is not None:\n",
    "        candidates = [cfg.model_path] if cfg.model_path else []\n",
    "        candidates += [\"model.word2vec\",\"model.w2v\",\"word2vec.model\", os.path.join(os.getcwd(),\"word2vec.model\")]\n",
    "        for p in candidates:\n",
    "            if p and os.path.exists(p):\n",
    "                try:\n",
    "                    w2v = GensimWord2Vec.load(p)\n",
    "                    if verbose: print(f\"[NLP] loaded Word2Vec: {p}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if verbose: print(f\"[WARN] could not load {p}: {e}\")\n",
    "        if w2v is None:\n",
    "            if verbose: print(\"[WARN] falling back to light mode\")\n",
    "            mode = \"light\"\n",
    "\n",
    "    # Compounding sources\n",
    "    cp_map = {}\n",
    "    if cpwords_path and os.path.exists(cpwords_path):\n",
    "        try: cp_map = load_cpwords(cpwords_path)\n",
    "        except Exception as e:\n",
    "            if verbose: print(f\"[WARN] cpwords not loaded: {e}\")\n",
    "    ont_multi = _collect_multiword_phrases_from_ontology(ont)\n",
    "    compounds_rx = compile_compounds_regex(ont_multi, cp_map)\n",
    "\n",
    "    # Process\n",
    "    rows = []\n",
    "    os.makedirs(os.path.dirname(anchor_csv) or \".\", exist_ok=True)\n",
    "    for fname, doc in tqdm(list(load_jsons(input_dir)), desc=\"Anchor-based\"):\n",
    "        r = analyze_anchor_based_single(doc, ont, compounds_rx, cp_map, cfg, w2v, alias_map)\n",
    "        r[\"file\"] = fname\n",
    "        rows.append(r)\n",
    "\n",
    "    # Collect dynamic YAML-driven optimization columns only\n",
    "    dyn_opt_cols = []\n",
    "    for cat_name in (ont.get(\"optimization_methods\", {}) or {}).keys():\n",
    "        dyn_opt_cols.append(f\"optimization_{canonicalise_term(cat_name)}_ranked\")\n",
    "\n",
    "    cols_fixed = [\n",
    "        \"file\",\n",
    "        \"building_terms_ranked\",\"system_terms_ranked\",\"weather_terms_ranked\",\"occupancy_terms_ranked\",\n",
    "        \"building_paradigms\",\"system_paradigms\",\"weather_paradigms\",\"occupancy_paradigms\",\n",
    "        \"building_paradigm_terms\",\"system_paradigm_terms\",\"weather_paradigm_terms\",\"occupancy_paradigm_terms\",\n",
    "        \"optimization_methods\",\"optimization_method_terms\",\n",
    "        \"applications_merged\",\"data_types_merged\",  # <-- the two merged columns\n",
    "    ]\n",
    "\n",
    "    cols = cols_fixed + dyn_opt_cols\n",
    "    df = pd.DataFrame(rows)\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"NM\"\n",
    "    df = df[cols].sort_values(\"file\")\n",
    "\n",
    "    # ensure all columns exist even if empty\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"NM\"\n",
    "    df = df[cols].sort_values(\"file\")\n",
    "    df.to_csv(anchor_csv, index=False)\n",
    "    if verbose: print(f\"[OK] wrote {anchor_csv} ({len(df)} rows)\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e8a5ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NLP] mode = full\n",
      "[NLP] loaded Word2Vec: word2vec.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Anchor-based: 100%|██████████| 14/14 [00:09<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] wrote nlp_output/anchor_summary_2.csv (14 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_summary = run_anchor_based(\n",
    "    input_dir=\"elsevier_out/papers_ftrr\",\n",
    "    ontology_path=\"nlp_input/ontology.yaml\",\n",
    "    cpwords_path=\"nlp_input/_cpwords.py\",\n",
    "    anchor_csv=\"nlp_output/anchor_summary_2.csv\",\n",
    "    nlp=\"auto\",                              # off | light | full | auto\n",
    "    config_path=\"nlp_input/ontology.yaml\",                        # or a YAML with an _nlp block\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0adfdbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>optimization_metaheuristics_and_optim_ranked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>particle_swarm_optimization (4);genetic_algorithm (2);multi_objective_function_that_could (1);quadratic_programming (1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>genetic_algorithm (9);multi_objective_occupant_behavior (2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>linear_programming (2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>convex_optimization (3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>genetic_algorithm (10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>genetic_algorithm (21)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show as HTML inside a Jupyter notebook\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(df_summary['optimization_metaheuristics_and_optim_ranked'].to_frame().to_html(escape=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0cca6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>building_terms_ranked</th>\n",
       "      <th>system_terms_ranked</th>\n",
       "      <th>weather_terms_ranked</th>\n",
       "      <th>occupancy_terms_ranked</th>\n",
       "      <th>building_paradigms</th>\n",
       "      <th>system_paradigms</th>\n",
       "      <th>weather_paradigms</th>\n",
       "      <th>occupancy_paradigms</th>\n",
       "      <th>building_paradigm_terms</th>\n",
       "      <th>system_paradigm_terms</th>\n",
       "      <th>weather_paradigm_terms</th>\n",
       "      <th>occupancy_paradigm_terms</th>\n",
       "      <th>optimization_methods</th>\n",
       "      <th>optimization_method_terms</th>\n",
       "      <th>applications_merged</th>\n",
       "      <th>data_types_merged</th>\n",
       "      <th>optimization_reinforcement_learning_ranked</th>\n",
       "      <th>optimization_classical_control_and_mpc_ranked</th>\n",
       "      <th>optimization_metaheuristics_and_optim_ranked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1016_j.apenergy.2019.113920.json</td>\n",
       "      <td>room (21);building (10);temperature (4)</td>\n",
       "      <td>acmv (19);ahu (17);coil (17);doas (11);chiller...</td>\n",
       "      <td>NM</td>\n",
       "      <td>internal (4);occupancy (3)</td>\n",
       "      <td>greybox</td>\n",
       "      <td>greybox</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>kalman_filter;rc;state_space</td>\n",
       "      <td>state_space</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>classical_control_and_mpc;generic;metaheuristi...</td>\n",
       "      <td>gain;model_predictive_control;mpc;optimise;opt...</td>\n",
       "      <td>model_predictive_control (214)</td>\n",
       "      <td>predicted_mean_vote (94);air_temperature (17);...</td>\n",
       "      <td>NM</td>\n",
       "      <td>model_predictive_control (190)</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1016_j.apenergy.2020.115147.json</td>\n",
       "      <td>building (60);room (9);temperature (1);zone (1)</td>\n",
       "      <td>acmv (14);accuracy (7);achieve (6);chiller (6)...</td>\n",
       "      <td>NM</td>\n",
       "      <td>occupancy (6);internal (5);heat_loads (1)</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>ann;artificial_neural_network;machine_learning...</td>\n",
       "      <td>ann;artificial_neural_network;machine_learning...</td>\n",
       "      <td>ann</td>\n",
       "      <td>ann</td>\n",
       "      <td>classical_control_and_mpc;generic</td>\n",
       "      <td>model_predictive_control;mpc;optimization;opti...</td>\n",
       "      <td>model_predictive_control (321);performance_ana...</td>\n",
       "      <td>predicted_mean_vote (139);air_temperature (45)...</td>\n",
       "      <td>NM</td>\n",
       "      <td>model_predictive_control (287);proportional_in...</td>\n",
       "      <td>particle_swarm_optimization (4);genetic_algori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1016_j.apenergy.2020.115426.json</td>\n",
       "      <td>room (21);temperature (3)</td>\n",
       "      <td>ac (11)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>bayesian_convolutional_neural_network;bcnn;con...</td>\n",
       "      <td>bcnns;data_driven</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>reinforcement_learning</td>\n",
       "      <td>q_learning;reinforcement_learning</td>\n",
       "      <td>demand_response (2)</td>\n",
       "      <td>setpoint (7);room_temperature (6);thermal_comf...</td>\n",
       "      <td>q_learning (7);reinforcement_learning (2)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1016_j.apenergy.2021.117276.json</td>\n",
       "      <td>building (9);temperature (2)</td>\n",
       "      <td>achieved (26);actual (22);accurate (11);accura...</td>\n",
       "      <td>NM</td>\n",
       "      <td>occupant_behavior (4)</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>data_driven_approach</td>\n",
       "      <td>ann;data_driven;knn;machine_learning;statistic...</td>\n",
       "      <td>knn;machine_learning;statistical;svm;svr</td>\n",
       "      <td>ann;data_driven;knn;machine_learning;svm;svr</td>\n",
       "      <td>generic;metaheuristics_and_optim</td>\n",
       "      <td>genetic_algorithm;optimization;optimizes</td>\n",
       "      <td>performance_evaluation (2)</td>\n",
       "      <td>thermal_comfort (22);setpoints (6);heater_stat...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>genetic_algorithm (9);multi_objective_occupant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1016_j.apenergy.2021.117987.json</td>\n",
       "      <td>room (46);air_quality (3);temperature (3);buil...</td>\n",
       "      <td>accuracy (36);coil (11);hvac (8);according (2)...</td>\n",
       "      <td>outdoor_temperature (8)</td>\n",
       "      <td>occupancy (46);internal (7);occupant_behavior (1)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>metaheuristics_and_optim</td>\n",
       "      <td>gave</td>\n",
       "      <td>commissioning (8);performance_evaluation (2)</td>\n",
       "      <td>occupancy (173);setpoint (24);thermal_comfort ...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.1016_j.apenergy.2022.119580.json</td>\n",
       "      <td>zone (63);room (31);temperature (20);building ...</td>\n",
       "      <td>achieved (8);actual (7);according (5);accuracy...</td>\n",
       "      <td>outdoor_temperature (2)</td>\n",
       "      <td>internal (20);occupant_number (4);infiltration...</td>\n",
       "      <td>greybox</td>\n",
       "      <td>greybox</td>\n",
       "      <td>NM</td>\n",
       "      <td>greybox</td>\n",
       "      <td>rc</td>\n",
       "      <td>kalman_filter;rc</td>\n",
       "      <td>NM</td>\n",
       "      <td>kalman_filter;rc_model</td>\n",
       "      <td>classical_control_and_mpc;generic;metaheuristi...</td>\n",
       "      <td>gain;model_predictive_control;mpc;optimization...</td>\n",
       "      <td>model_predictive_control (47);predictive_and (2)</td>\n",
       "      <td>setpoint (56);predicted_mean_vote (34);room_te...</td>\n",
       "      <td>NM</td>\n",
       "      <td>model_predictive_control (47);pi_controller (2)</td>\n",
       "      <td>linear_programming (2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.1016_j.buildenv.2020.107089.json</td>\n",
       "      <td>room (17);temperature (7);zone (4);building (2)</td>\n",
       "      <td>actuation (4);vav (3);according (2);accuracy (...</td>\n",
       "      <td>NM</td>\n",
       "      <td>occupancy_patterns (2);height (1)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>thermal_comfort (27);air_flow (14);setpoint (1...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.1016_j.enbuild.2020.109792.json</td>\n",
       "      <td>building (3);room (3);temperature (3)</td>\n",
       "      <td>accuracy (1);accurate (1);hvac (1);valve (1)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>classical_control_and_mpc</td>\n",
       "      <td>model_predictive_control</td>\n",
       "      <td>predictive (15)</td>\n",
       "      <td>room_temperature (11);ambient_temperature (8)</td>\n",
       "      <td>NM</td>\n",
       "      <td>model_predictive_control (13)</td>\n",
       "      <td>convex_optimization (3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.1016_j.enbuild.2020.110271.json</td>\n",
       "      <td>building (49);ieq (12);temperature (12);zone (...</td>\n",
       "      <td>actual (28);across (6);account (5);accuracy (2...</td>\n",
       "      <td>NM</td>\n",
       "      <td>occupancy (2)</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>statistical</td>\n",
       "      <td>annual;statistical</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>metaheuristics_and_optim</td>\n",
       "      <td>gap;gas</td>\n",
       "      <td>commissioning (10)</td>\n",
       "      <td>occupancy (28);air_temperature (2);metering (2)</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.1016_j.enbuild.2020.110276.json</td>\n",
       "      <td>building (71)</td>\n",
       "      <td>actual (51);pump (26);chiller (24);ahu (16);hv...</td>\n",
       "      <td>NM</td>\n",
       "      <td>occupancy (1)</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>statistical_analysis</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>metaheuristics_and_optim</td>\n",
       "      <td>gap;gas</td>\n",
       "      <td>commissioning (12)</td>\n",
       "      <td>occupancy (22);metering (12);air_temperature (...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.1016_j.enbuild.2021.111255.json</td>\n",
       "      <td>zone (21);building (20);temperature (11);room (4)</td>\n",
       "      <td>ahu (20);chiller (7);coil (6);acceptable (3);h...</td>\n",
       "      <td>NM</td>\n",
       "      <td>occupancy (7)</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>artificial_neural_networks;data_driven</td>\n",
       "      <td>data_driven_methods</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>metaheuristics_and_optim</td>\n",
       "      <td>ga;gaussian</td>\n",
       "      <td>predictive_control (1)</td>\n",
       "      <td>occupancy (47);setpoint (46);air_temperature (...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>genetic_algorithm (10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.1016_j.enbuild.2023.113747.json</td>\n",
       "      <td>temperature (11);building (4)</td>\n",
       "      <td>boiler (34);accuracy (13);actual (13);accurate...</td>\n",
       "      <td>NM</td>\n",
       "      <td>occupant_behavior (15);occupancy (2)</td>\n",
       "      <td>NM</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>NM</td>\n",
       "      <td>whitebox</td>\n",
       "      <td>NM</td>\n",
       "      <td>statistical</td>\n",
       "      <td>NM</td>\n",
       "      <td>energyplus</td>\n",
       "      <td>metaheuristics_and_optim</td>\n",
       "      <td>gap;gas;gather</td>\n",
       "      <td>NM</td>\n",
       "      <td>setpoint (29);occupancy (13);thermal_comfort (...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.1016_j.enbuild.2024.114215.json</td>\n",
       "      <td>building (10);temperature (10);zone (9)</td>\n",
       "      <td>chiller (43);exchanger (24);hvacs (15);accumul...</td>\n",
       "      <td>NM</td>\n",
       "      <td>internal (8)</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>blackbox</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>annual</td>\n",
       "      <td>annual</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>generic</td>\n",
       "      <td>optimization</td>\n",
       "      <td>commissioning (12)</td>\n",
       "      <td>supply_water_temperature (35);water_flow (26);...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.1016_j.jobe.2022.104498.json</td>\n",
       "      <td>zone (13);temperature (7);building_heating (1)</td>\n",
       "      <td>hvac (11);accuracy (8);ahu (8);achieves (7);ac...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>generic</td>\n",
       "      <td>optimization;optimizer;optimizes</td>\n",
       "      <td>NM</td>\n",
       "      <td>thermal_comfort (39);air_flow (15);indoor_air_...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NM</td>\n",
       "      <td>genetic_algorithm (21)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   file  \\\n",
       "0   10.1016_j.apenergy.2019.113920.json   \n",
       "1   10.1016_j.apenergy.2020.115147.json   \n",
       "2   10.1016_j.apenergy.2020.115426.json   \n",
       "3   10.1016_j.apenergy.2021.117276.json   \n",
       "4   10.1016_j.apenergy.2021.117987.json   \n",
       "5   10.1016_j.apenergy.2022.119580.json   \n",
       "6   10.1016_j.buildenv.2020.107089.json   \n",
       "7    10.1016_j.enbuild.2020.109792.json   \n",
       "8    10.1016_j.enbuild.2020.110271.json   \n",
       "9    10.1016_j.enbuild.2020.110276.json   \n",
       "10   10.1016_j.enbuild.2021.111255.json   \n",
       "11   10.1016_j.enbuild.2023.113747.json   \n",
       "12   10.1016_j.enbuild.2024.114215.json   \n",
       "13      10.1016_j.jobe.2022.104498.json   \n",
       "\n",
       "                                building_terms_ranked  \\\n",
       "0             room (21);building (10);temperature (4)   \n",
       "1     building (60);room (9);temperature (1);zone (1)   \n",
       "2                           room (21);temperature (3)   \n",
       "3                        building (9);temperature (2)   \n",
       "4   room (46);air_quality (3);temperature (3);buil...   \n",
       "5   zone (63);room (31);temperature (20);building ...   \n",
       "6     room (17);temperature (7);zone (4);building (2)   \n",
       "7               building (3);room (3);temperature (3)   \n",
       "8   building (49);ieq (12);temperature (12);zone (...   \n",
       "9                                       building (71)   \n",
       "10  zone (21);building (20);temperature (11);room (4)   \n",
       "11                      temperature (11);building (4)   \n",
       "12            building (10);temperature (10);zone (9)   \n",
       "13     zone (13);temperature (7);building_heating (1)   \n",
       "\n",
       "                                  system_terms_ranked  \\\n",
       "0   acmv (19);ahu (17);coil (17);doas (11);chiller...   \n",
       "1   acmv (14);accuracy (7);achieve (6);chiller (6)...   \n",
       "2                                             ac (11)   \n",
       "3   achieved (26);actual (22);accurate (11);accura...   \n",
       "4   accuracy (36);coil (11);hvac (8);according (2)...   \n",
       "5   achieved (8);actual (7);according (5);accuracy...   \n",
       "6   actuation (4);vav (3);according (2);accuracy (...   \n",
       "7        accuracy (1);accurate (1);hvac (1);valve (1)   \n",
       "8   actual (28);across (6);account (5);accuracy (2...   \n",
       "9   actual (51);pump (26);chiller (24);ahu (16);hv...   \n",
       "10  ahu (20);chiller (7);coil (6);acceptable (3);h...   \n",
       "11  boiler (34);accuracy (13);actual (13);accurate...   \n",
       "12  chiller (43);exchanger (24);hvacs (15);accumul...   \n",
       "13  hvac (11);accuracy (8);ahu (8);achieves (7);ac...   \n",
       "\n",
       "       weather_terms_ranked  \\\n",
       "0                        NM   \n",
       "1                        NM   \n",
       "2                        NM   \n",
       "3                        NM   \n",
       "4   outdoor_temperature (8)   \n",
       "5   outdoor_temperature (2)   \n",
       "6                        NM   \n",
       "7                        NM   \n",
       "8                        NM   \n",
       "9                        NM   \n",
       "10                       NM   \n",
       "11                       NM   \n",
       "12                       NM   \n",
       "13                       NM   \n",
       "\n",
       "                               occupancy_terms_ranked building_paradigms  \\\n",
       "0                          internal (4);occupancy (3)            greybox   \n",
       "1           occupancy (6);internal (5);heat_loads (1)           blackbox   \n",
       "2                                                  NM           blackbox   \n",
       "3                               occupant_behavior (4)           blackbox   \n",
       "4   occupancy (46);internal (7);occupant_behavior (1)                 NM   \n",
       "5   internal (20);occupant_number (4);infiltration...            greybox   \n",
       "6                   occupancy_patterns (2);height (1)                 NM   \n",
       "7                                                  NM                 NM   \n",
       "8                                       occupancy (2)           blackbox   \n",
       "9                                       occupancy (1)           blackbox   \n",
       "10                                      occupancy (7)           blackbox   \n",
       "11               occupant_behavior (15);occupancy (2)                 NM   \n",
       "12                                       internal (8)           blackbox   \n",
       "13                                                 NM                 NM   \n",
       "\n",
       "   system_paradigms weather_paradigms occupancy_paradigms  \\\n",
       "0           greybox                NM                  NM   \n",
       "1          blackbox          blackbox            blackbox   \n",
       "2          blackbox                NM                  NM   \n",
       "3          blackbox          blackbox            blackbox   \n",
       "4                NM                NM                  NM   \n",
       "5           greybox                NM             greybox   \n",
       "6                NM                NM                  NM   \n",
       "7                NM                NM                  NM   \n",
       "8          blackbox                NM                  NM   \n",
       "9                NM                NM                  NM   \n",
       "10         blackbox                NM                  NM   \n",
       "11         blackbox                NM            whitebox   \n",
       "12         blackbox                NM                  NM   \n",
       "13               NM                NM                  NM   \n",
       "\n",
       "                              building_paradigm_terms  \\\n",
       "0                        kalman_filter;rc;state_space   \n",
       "1   ann;artificial_neural_network;machine_learning...   \n",
       "2   bayesian_convolutional_neural_network;bcnn;con...   \n",
       "3                                data_driven_approach   \n",
       "4                                                  NM   \n",
       "5                                                  rc   \n",
       "6                                                  NM   \n",
       "7                                                  NM   \n",
       "8                                         statistical   \n",
       "9                                statistical_analysis   \n",
       "10             artificial_neural_networks;data_driven   \n",
       "11                                                 NM   \n",
       "12                                             annual   \n",
       "13                                                 NM   \n",
       "\n",
       "                                system_paradigm_terms  \\\n",
       "0                                         state_space   \n",
       "1   ann;artificial_neural_network;machine_learning...   \n",
       "2                                   bcnns;data_driven   \n",
       "3   ann;data_driven;knn;machine_learning;statistic...   \n",
       "4                                                  NM   \n",
       "5                                    kalman_filter;rc   \n",
       "6                                                  NM   \n",
       "7                                                  NM   \n",
       "8                                  annual;statistical   \n",
       "9                                                  NM   \n",
       "10                                data_driven_methods   \n",
       "11                                        statistical   \n",
       "12                                             annual   \n",
       "13                                                 NM   \n",
       "\n",
       "                      weather_paradigm_terms  \\\n",
       "0                                         NM   \n",
       "1                                        ann   \n",
       "2                                         NM   \n",
       "3   knn;machine_learning;statistical;svm;svr   \n",
       "4                                         NM   \n",
       "5                                         NM   \n",
       "6                                         NM   \n",
       "7                                         NM   \n",
       "8                                         NM   \n",
       "9                                         NM   \n",
       "10                                        NM   \n",
       "11                                        NM   \n",
       "12                                        NM   \n",
       "13                                        NM   \n",
       "\n",
       "                        occupancy_paradigm_terms  \\\n",
       "0                                             NM   \n",
       "1                                            ann   \n",
       "2                                             NM   \n",
       "3   ann;data_driven;knn;machine_learning;svm;svr   \n",
       "4                                             NM   \n",
       "5                         kalman_filter;rc_model   \n",
       "6                                             NM   \n",
       "7                                             NM   \n",
       "8                                             NM   \n",
       "9                                             NM   \n",
       "10                                            NM   \n",
       "11                                    energyplus   \n",
       "12                                            NM   \n",
       "13                                            NM   \n",
       "\n",
       "                                 optimization_methods  \\\n",
       "0   classical_control_and_mpc;generic;metaheuristi...   \n",
       "1                   classical_control_and_mpc;generic   \n",
       "2                              reinforcement_learning   \n",
       "3                    generic;metaheuristics_and_optim   \n",
       "4                            metaheuristics_and_optim   \n",
       "5   classical_control_and_mpc;generic;metaheuristi...   \n",
       "6                                                  NM   \n",
       "7                           classical_control_and_mpc   \n",
       "8                            metaheuristics_and_optim   \n",
       "9                            metaheuristics_and_optim   \n",
       "10                           metaheuristics_and_optim   \n",
       "11                           metaheuristics_and_optim   \n",
       "12                                            generic   \n",
       "13                                            generic   \n",
       "\n",
       "                            optimization_method_terms  \\\n",
       "0   gain;model_predictive_control;mpc;optimise;opt...   \n",
       "1   model_predictive_control;mpc;optimization;opti...   \n",
       "2                   q_learning;reinforcement_learning   \n",
       "3            genetic_algorithm;optimization;optimizes   \n",
       "4                                                gave   \n",
       "5   gain;model_predictive_control;mpc;optimization...   \n",
       "6                                                  NM   \n",
       "7                            model_predictive_control   \n",
       "8                                             gap;gas   \n",
       "9                                             gap;gas   \n",
       "10                                        ga;gaussian   \n",
       "11                                     gap;gas;gather   \n",
       "12                                       optimization   \n",
       "13                   optimization;optimizer;optimizes   \n",
       "\n",
       "                                  applications_merged  \\\n",
       "0                      model_predictive_control (214)   \n",
       "1   model_predictive_control (321);performance_ana...   \n",
       "2                                 demand_response (2)   \n",
       "3                          performance_evaluation (2)   \n",
       "4        commissioning (8);performance_evaluation (2)   \n",
       "5    model_predictive_control (47);predictive_and (2)   \n",
       "6                                                  NM   \n",
       "7                                     predictive (15)   \n",
       "8                                  commissioning (10)   \n",
       "9                                  commissioning (12)   \n",
       "10                             predictive_control (1)   \n",
       "11                                                 NM   \n",
       "12                                 commissioning (12)   \n",
       "13                                                 NM   \n",
       "\n",
       "                                    data_types_merged  \\\n",
       "0   predicted_mean_vote (94);air_temperature (17);...   \n",
       "1   predicted_mean_vote (139);air_temperature (45)...   \n",
       "2   setpoint (7);room_temperature (6);thermal_comf...   \n",
       "3   thermal_comfort (22);setpoints (6);heater_stat...   \n",
       "4   occupancy (173);setpoint (24);thermal_comfort ...   \n",
       "5   setpoint (56);predicted_mean_vote (34);room_te...   \n",
       "6   thermal_comfort (27);air_flow (14);setpoint (1...   \n",
       "7       room_temperature (11);ambient_temperature (8)   \n",
       "8     occupancy (28);air_temperature (2);metering (2)   \n",
       "9   occupancy (22);metering (12);air_temperature (...   \n",
       "10  occupancy (47);setpoint (46);air_temperature (...   \n",
       "11  setpoint (29);occupancy (13);thermal_comfort (...   \n",
       "12  supply_water_temperature (35);water_flow (26);...   \n",
       "13  thermal_comfort (39);air_flow (15);indoor_air_...   \n",
       "\n",
       "   optimization_reinforcement_learning_ranked  \\\n",
       "0                                          NM   \n",
       "1                                          NM   \n",
       "2   q_learning (7);reinforcement_learning (2)   \n",
       "3                                          NM   \n",
       "4                                          NM   \n",
       "5                                          NM   \n",
       "6                                          NM   \n",
       "7                                          NM   \n",
       "8                                          NM   \n",
       "9                                          NM   \n",
       "10                                         NM   \n",
       "11                                         NM   \n",
       "12                                         NM   \n",
       "13                                         NM   \n",
       "\n",
       "        optimization_classical_control_and_mpc_ranked  \\\n",
       "0                      model_predictive_control (190)   \n",
       "1   model_predictive_control (287);proportional_in...   \n",
       "2                                                  NM   \n",
       "3                                                  NM   \n",
       "4                                                  NM   \n",
       "5     model_predictive_control (47);pi_controller (2)   \n",
       "6                                                  NM   \n",
       "7                       model_predictive_control (13)   \n",
       "8                                                  NM   \n",
       "9                                                  NM   \n",
       "10                                                 NM   \n",
       "11                                                 NM   \n",
       "12                                                 NM   \n",
       "13                                                 NM   \n",
       "\n",
       "         optimization_metaheuristics_and_optim_ranked  \n",
       "0                                                  NM  \n",
       "1   particle_swarm_optimization (4);genetic_algori...  \n",
       "2                                                  NM  \n",
       "3   genetic_algorithm (9);multi_objective_occupant...  \n",
       "4                                                  NM  \n",
       "5                              linear_programming (2)  \n",
       "6                                                  NM  \n",
       "7                             convex_optimization (3)  \n",
       "8                                                  NM  \n",
       "9                                                  NM  \n",
       "10                             genetic_algorithm (10)  \n",
       "11                                                 NM  \n",
       "12                                                 NM  \n",
       "13                             genetic_algorithm (21)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eplus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
