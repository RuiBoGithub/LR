{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c406ff",
   "metadata": {},
   "source": [
    "#### NLTK Text Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac31dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from _cpwords import compound_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c60ba76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Define a list of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Add specific terms to be removed\n",
    "    remove_terms = {'introduct', 'literature', 'review', 'figure', 'doi', 'fig', 'table', 'conclusion', \n",
    "                    'altimg', 'gif', 'png', 'discussion', 'acknowledgment', 'appendix','http', 'copyright'}\n",
    "    stop_words.update(remove_terms)\n",
    "\n",
    "    # Initialize lemmatizer and stemmer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    for original, compound in compound_keywords.items():\n",
    "        text = text.replace(original, compound)\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    sentence_tokens = sent_tokenize(text)\n",
    "    sentences = []\n",
    "\n",
    "    # Tokenize each sentence into words and remove stopwords\n",
    "    for sentence in sentence_tokens:\n",
    "        words = word_tokenize(sentence)\n",
    "        filtered_words = []\n",
    "        for word in words:\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            #word = stemmer.stem(word)\n",
    "            \n",
    "            # Remove non-alphabetic characters except underscores and hyphens\n",
    "            word = re.sub(r'[^\\w\\s\\-]', '', word)\n",
    "            \n",
    "            #if word.lower() not in stop_words:\n",
    "            if word.lower() not in stop_words and not word.isdigit() and 1 < len(word) <= 20:\n",
    "                filtered_words.append(word)\n",
    "            \n",
    "        sentences.append(filtered_words)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def process_json_files(directory):\n",
    "    all_sentences = []\n",
    "\n",
    "    # Iterate through each JSON file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                # Extract the original text from the JSON structure\n",
    "                text = data['full-text-retrieval-response']['originalText']\n",
    "                # Process the text to tokenize and remove stopwords\n",
    "                processed_sentences = preprocess_text(text)\n",
    "                # Append the processed sentences to the overall list\n",
    "                all_sentences.extend(processed_sentences)\n",
    "    \n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46702df",
   "metadata": {},
   "source": [
    "#### Train and Save Word2Vec Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fea57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'papers_json'\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=process_json_files(directory),  \n",
    "    vector_size=300,           # Size of the embedding vectors\n",
    "    window=20,                 # Context window size\n",
    "    min_count=2,               # Minimum occurrence in vocabulary\n",
    "    workers=4                  # Number of threads for model training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0404d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"word2vec.model\"\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45985cf6",
   "metadata": {},
   "source": [
    "#### Model Examination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "682d71dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from _cpwords import compound_keywords\n",
    "\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fabd2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'rl':\n",
      "drl: 0.8673951029777527\n",
      "pre-training: 0.7499464750289917\n",
      "dqn: 0.747580885887146\n",
      "ddpg: 0.7355743646621704\n",
      "maddpg: 0.7274594306945801\n",
      "model-free: 0.7238556146621704\n",
      "non-stationarity: 0.7223429679870605\n",
      "ddqn: 0.7099489569664001\n",
      "rl-based: 0.7083368301391602\n",
      "single-agent: 0.7039849758148193\n",
      "learns: 0.6998539566993713\n",
      "sac: 0.6940401196479797\n",
      "ddq: 0.6898158192634583\n",
      "ppo: 0.6891800761222839\n",
      "irl: 0.6855924129486084\n",
      "dyna-pinn: 0.6837936639785767\n",
      "bdq: 0.6813809871673584\n",
      "gail: 0.6684640049934387\n",
      "rc-ddq: 0.6657112240791321\n",
      "learn: 0.6630381941795349\n",
      "fashion: 0.6607140898704529\n",
      "learned: 0.6595422625541687\n",
      "well-trained: 0.657589852809906\n",
      "policy-gradient: 0.6567992568016052\n",
      "policy-based: 0.6558753252029419\n",
      "policy: 0.6531656980514526\n",
      "chose: 0.6458209156990051\n",
      "offline: 0.6437338590621948\n",
      "agent: 0.6435546278953552\n",
      "mcts: 0.6267192363739014\n",
      "marl: 0.6246955394744873\n",
      "value-based: 0.6217694282531738\n",
      "tl: 0.6204082369804382\n",
      "pre-trained: 0.6198127865791321\n",
      "observational: 0.6179376840591431\n",
      "q-network: 0.6168456077575684\n",
      "reinforce: 0.6165980100631714\n",
      "nn-ddq: 0.6137312054634094\n",
      "overestimation: 0.6127275228500366\n",
      "fine-tuning: 0.611228883266449\n",
      "otl: 0.6060397624969482\n",
      "mbrl: 0.6050821542739868\n",
      "drl-based: 0.604965090751648\n",
      "imitation_learning: 0.604762852191925\n",
      "a2c: 0.6005122661590576\n",
      "ddpg-based: 0.5993816256523132\n",
      "q-learning: 0.5965062975883484\n",
      "tsf: 0.5956545472145081\n",
      "non-stationary: 0.5951052904129028\n",
      "sarl: 0.5938380360603333\n",
      "relearning: 0.5886499881744385\n",
      "near-optimal_control: 0.5852896571159363\n",
      "try: 0.5840117931365967\n",
      "performing: 0.5825886726379395\n",
      "interact: 0.5798453688621521\n",
      "trained: 0.5771385431289673\n",
      "attempt: 0.5768823623657227\n",
      "citylearn: 0.573920488357544\n",
      "highlighting: 0.5736115574836731\n",
      "exploration: 0.5729409456253052\n",
      "deep_q-learning: 0.5722297430038452\n",
      "unsl: 0.5720763802528381\n",
      "cmarl: 0.5717414021492004\n",
      "sarsa: 0.5685867667198181\n",
      "action: 0.5636659860610962\n",
      "variant: 0.5596384406089783\n",
      "dcrlbb: 0.5585046410560608\n",
      "trial-and-error: 0.5576779842376709\n",
      "reward: 0.5562135577201843\n",
      "performs: 0.5560171008110046\n",
      "stateaction: 0.5557475686073303\n",
      "episodic: 0.5548930168151855\n",
      "cmarl-exne: 0.5537832975387573\n",
      "state-action: 0.5507919192314148\n",
      "fine-tuned: 0.548780620098114\n",
      "physic-based: 0.5487169027328491\n",
      "pomdp: 0.5477008819580078\n",
      "branching: 0.5465099811553955\n",
      "time-saving: 0.5435489416122437\n",
      "advantage: 0.5402511358261108\n",
      "deep_q-network: 0.5397502183914185\n",
      "low-diversity: 0.5389804840087891\n",
      "episode: 0.5386243462562561\n",
      "actor: 0.5381639003753662\n",
      "amend: 0.5372578501701355\n",
      "q-networks: 0.5352843403816223\n",
      "experience: 0.533896267414093\n",
      "interesting: 0.533894419670105\n",
      "mdp: 0.5332838296890259\n",
      "explore: 0.5330336689949036\n",
      "behind: 0.5329150557518005\n",
      "cmarl-ex: 0.5327144861221313\n",
      "dueling: 0.5319538712501526\n",
      "rl-mpc: 0.5316309332847595\n",
      "unfamiliar: 0.5312100648880005\n",
      "mpcs: 0.530961811542511\n",
      "refining: 0.5293176770210266\n",
      "tianshou: 0.5277135968208313\n",
      "terminology: 0.5275562405586243\n",
      "outperform: 0.5269502401351929\n",
      "\n",
      "Words similar to 'carbon':\n",
      "economy: 0.8117650151252747\n",
      "emission: 0.7643256783485413\n",
      "urbanization: 0.7074873447418213\n",
      "decarbonizing: 0.7001002430915833\n",
      "net-zero: 0.6990650296211243\n",
      "low-carbon: 0.690885603427887\n",
      "carbon_emissions: 0.6859979629516602\n",
      "ghg: 0.663462221622467\n",
      "greenhouse: 0.6613112688064575\n",
      "mitigation: 0.6608213782310486\n",
      "reform: 0.6607627868652344\n",
      "peaking: 0.6601114273071289\n",
      "carbon_footprint: 0.6536933183670044\n",
      "risen: 0.6487932801246643\n",
      "country: 0.6465206146240234\n",
      "neutrality: 0.6288716793060303\n",
      "climate_change: 0.6247106790542603\n",
      "eschborn: 0.6217330694198608\n",
      "growth: 0.6195126175880432\n",
      "social: 0.6179429292678833\n"
     ]
    }
   ],
   "source": [
    "# Find similar words \n",
    "similar_words_rl = model.wv.most_similar('rl', topn=100)\n",
    "similar_words_carbon = model.wv.most_similar('carbon', topn=20)\n",
    "\n",
    "print(\"Words similar to 'rl':\")\n",
    "for word, similarity in similar_words_rl:\n",
    "    print(f\"{word}: {similarity}\")\n",
    "\n",
    "print(\"\\nWords similar to 'carbon':\")\n",
    "for word, similarity in similar_words_carbon:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adda3e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35463747\n"
     ]
    }
   ],
   "source": [
    "# Find similarity\n",
    "similarity_1 = model.wv.similarity('control', 'operation')\n",
    "print(similarity_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833edd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eplus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
