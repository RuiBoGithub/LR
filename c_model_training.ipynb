{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c406ff",
   "metadata": {},
   "source": [
    "#### NLTK Text Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac31dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from _cpwords import compound_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c60ba76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Define a list of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Add specific terms to be removed\n",
    "    remove_terms = {'introduct', 'literature', 'review', 'figure', 'doi', 'fig', 'table', 'conclusion', \n",
    "                    'altimg', 'gif', 'png', 'discussion', 'acknowledgment', 'appendix','http', 'copyright'}\n",
    "    stop_words.update(remove_terms)\n",
    "\n",
    "    # Initialize lemmatizer and stemmer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    for original, compound in compound_keywords.items():\n",
    "        text = text.replace(original, compound)\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    sentence_tokens = sent_tokenize(text)\n",
    "    sentences = []\n",
    "\n",
    "    # Tokenize each sentence into words and remove stopwords\n",
    "    for sentence in sentence_tokens:\n",
    "        words = word_tokenize(sentence)\n",
    "        filtered_words = []\n",
    "        for word in words:\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            #word = stemmer.stem(word)\n",
    "            \n",
    "            # Remove non-alphabetic characters except underscores and hyphens\n",
    "            word = re.sub(r'[^\\w\\s\\-]', '', word)\n",
    "            \n",
    "            #if word.lower() not in stop_words:\n",
    "            if word.lower() not in stop_words and not word.isdigit() and 1 < len(word) <= 20:\n",
    "                filtered_words.append(word)\n",
    "            \n",
    "        sentences.append(filtered_words)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def process_json_files(directory):\n",
    "    all_sentences = []\n",
    "\n",
    "    # Iterate through each JSON file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                # Extract the original text from the JSON structure\n",
    "                text = data['full-text-retrieval-response']['originalText']\n",
    "                # Process the text to tokenize and remove stopwords\n",
    "                processed_sentences = preprocess_text(text)\n",
    "                # Append the processed sentences to the overall list\n",
    "                all_sentences.extend(processed_sentences)\n",
    "    \n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46702df",
   "metadata": {},
   "source": [
    "#### Train and Save Word2Vec Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fea57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'papers_json'\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=process_json_files(directory),  \n",
    "    vector_size=300,           # Size of the embedding vectors\n",
    "    window=20,                 # Context window size\n",
    "    min_count=2,               # Minimum occurrence in vocabulary\n",
    "    workers=4                  # Number of threads for model training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0404d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"word2vec.model\"\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45985cf6",
   "metadata": {},
   "source": [
    "#### Model Examination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "682d71dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from _cpwords import compound_keywords\n",
    "\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fabd2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'rl':\n",
      "drl: 0.8592274785041809\n",
      "dqn: 0.7611899375915527\n",
      "learns: 0.729836642742157\n",
      "model-free: 0.724945604801178\n",
      "ddpg: 0.7184646129608154\n",
      "ddqn: 0.6960535049438477\n",
      "maddpg: 0.6896633505821228\n",
      "rl-based: 0.6895571947097778\n",
      "policy-gradient: 0.6804182529449463\n",
      "learned: 0.6797705292701721\n",
      "single-agent: 0.6784573197364807\n",
      "sac: 0.6734286546707153\n",
      "policy: 0.6724432110786438\n",
      "pre-training: 0.6674242615699768\n",
      "ppo: 0.6663268208503723\n",
      "irl: 0.664676308631897\n",
      "drl-based: 0.6629266738891602\n",
      "bdq: 0.6615206003189087\n",
      "gail: 0.6553522348403931\n",
      "mcts: 0.655299186706543\n",
      "reinforce: 0.6524274945259094\n",
      "dyna-pinn: 0.6515287160873413\n",
      "rc-ddq: 0.6482321619987488\n",
      "learn: 0.6480127573013306\n",
      "non-stationarity: 0.6472879648208618\n",
      "warm-starting: 0.646366536617279\n",
      "agent: 0.6379923224449158\n",
      "marl: 0.6373324394226074\n",
      "fashion: 0.631766676902771\n",
      "deep_q-learning: 0.6290871500968933\n",
      "pre-trained: 0.6253461837768555\n",
      "well-trained: 0.624663233757019\n",
      "tl: 0.6225482821464539\n",
      "observational: 0.615376889705658\n",
      "ddq: 0.6138175129890442\n",
      "q-network: 0.609434187412262\n",
      "value-based: 0.6050118803977966\n",
      "offline: 0.6011968851089478\n",
      "overestimation: 0.6001412868499756\n",
      "imitation_learning: 0.598689079284668\n",
      "variant: 0.5933249592781067\n",
      "exploration: 0.5926468968391418\n",
      "dcrlbb: 0.5925242900848389\n",
      "trained: 0.5923573970794678\n",
      "performs: 0.5909609794616699\n",
      "q-learning: 0.5903527736663818\n",
      "near-optimal_control: 0.5858339667320251\n",
      "rl-mpc: 0.5833780169487\n",
      "unsl: 0.5816968083381653\n",
      "explore: 0.5806102156639099\n",
      "action: 0.580292820930481\n",
      "policy-based: 0.5800146460533142\n",
      "fine-tuning: 0.5776270627975464\n",
      "relbot: 0.575825035572052\n",
      "amend: 0.5755454301834106\n",
      "surprising: 0.5753592848777771\n",
      "episodic: 0.5737823247909546\n",
      "deep_q-network: 0.5734846591949463\n",
      "non-stationary: 0.5690034627914429\n",
      "branching: 0.5671020150184631\n",
      "d3qn: 0.5638238787651062\n",
      "reward: 0.5616147518157959\n",
      "performing: 0.561191976070404\n",
      "episode: 0.5610678195953369\n",
      "physnet: 0.5609528422355652\n",
      "trajectory-ranked: 0.5588980317115784\n",
      "attempt: 0.5588908195495605\n",
      "sarsa: 0.5583983063697815\n",
      "tianshou: 0.5580869913101196\n",
      "sarl: 0.5560688376426697\n",
      "relearning: 0.5556584000587463\n",
      "fine-tuned: 0.5556067228317261\n",
      "nn-ddq: 0.5556036829948425\n",
      "dueling: 0.5550705790519714\n",
      "physics-aware: 0.5506302118301392\n",
      "otl: 0.5480120778083801\n",
      "transfer_learning: 0.5478568077087402\n",
      "a-priori: 0.5471043586730957\n",
      "cmarl: 0.5433614253997803\n",
      "stateaction: 0.5429306626319885\n",
      "counterpart: 0.5416972637176514\n",
      "training: 0.5403083562850952\n",
      "highlighting: 0.5391932129859924\n",
      "deep: 0.5388401746749878\n",
      "action-reward: 0.5378947854042053\n",
      "specifically: 0.5348877310752869\n",
      "pre-train: 0.5345008969306946\n",
      "citylearn: 0.5340993404388428\n",
      "appealing: 0.5337744355201721\n",
      "actor: 0.5332733988761902\n",
      "imitation: 0.5331220626831055\n",
      "unfamiliar: 0.5323131084442139\n",
      "deploying: 0.5319939255714417\n",
      "chose: 0.5317856073379517\n",
      "try: 0.5317809581756592\n",
      "trial-and-error: 0.531590461730957\n",
      "mpc-ode: 0.5298129916191101\n",
      "mpcs: 0.5289455056190491\n",
      "adaptability: 0.5270431637763977\n",
      "state-action: 0.5263131856918335\n",
      "\n",
      "Words similar to 'carbon':\n",
      "economy: 0.8032695055007935\n",
      "emission: 0.7849946022033691\n",
      "carbon_emissions: 0.7282648086547852\n",
      "peaking: 0.6923617720603943\n",
      "net-zero: 0.6821886897087097\n",
      "ghg: 0.6778658032417297\n",
      "carbon_footprint: 0.6724563837051392\n",
      "greenhouse: 0.6712405681610107\n",
      "low-carbon: 0.6607341766357422\n",
      "climate_change: 0.6507430076599121\n",
      "decarbonizing: 0.6451995968818665\n",
      "eu: 0.6425002813339233\n",
      "country: 0.6419606804847717\n",
      "risen: 0.6402339935302734\n",
      "mitigation: 0.6344324350357056\n",
      "neutrality: 0.6250779032707214\n",
      "reform: 0.6230491399765015\n",
      "nearly: 0.622559666633606\n",
      "urbanization: 0.6200520992279053\n",
      "australian: 0.6113796830177307\n"
     ]
    }
   ],
   "source": [
    "# Find similar words \n",
    "similar_words_rl = model.wv.most_similar('rl', topn=100)\n",
    "similar_words_carbon = model.wv.most_similar('carbon', topn=20)\n",
    "\n",
    "print(\"Words similar to 'rl':\")\n",
    "for word, similarity in similar_words_rl:\n",
    "    print(f\"{word}: {similarity}\")\n",
    "\n",
    "print(\"\\nWords similar to 'carbon':\")\n",
    "for word, similarity in similar_words_carbon:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adda3e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40574217\n"
     ]
    }
   ],
   "source": [
    "# Find similarity\n",
    "similarity_1 = model.wv.similarity('control', 'operation')\n",
    "print(similarity_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833edd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eplus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
