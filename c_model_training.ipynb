{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c406ff",
   "metadata": {},
   "source": [
    "#### NLTK Text Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac31dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from cp_words import compound_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c60ba76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Define a list of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Add specific terms to be removed\n",
    "    remove_terms = {'introduct', 'literature', 'review', 'figure', 'doi', 'fig', 'table', 'conclusion', \n",
    "                    'altimg', 'gif', 'png', 'discussion', 'acknowledgment', 'appendix','http', 'copyright'}\n",
    "    stop_words.update(remove_terms)\n",
    "\n",
    "    # Initialize lemmatizer and stemmer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    for original, compound in compound_keywords.items():\n",
    "        text = text.replace(original, compound)\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    sentence_tokens = sent_tokenize(text)\n",
    "    sentences = []\n",
    "\n",
    "    # Tokenize each sentence into words and remove stopwords\n",
    "    for sentence in sentence_tokens:\n",
    "        words = word_tokenize(sentence)\n",
    "        filtered_words = []\n",
    "        for word in words:\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            #word = stemmer.stem(word)\n",
    "            \n",
    "            # Remove non-alphabetic characters except underscores and hyphens\n",
    "            word = re.sub(r'[^\\w\\s\\-]', '', word)\n",
    "            \n",
    "            #if word.lower() not in stop_words:\n",
    "            if word.lower() not in stop_words and not word.isdigit() and 1 < len(word) <= 20:\n",
    "                filtered_words.append(word)\n",
    "            \n",
    "        sentences.append(filtered_words)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def process_json_files(directory):\n",
    "    all_sentences = []\n",
    "\n",
    "    # Iterate through each JSON file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                # Extract the original text from the JSON structure\n",
    "                text = data['full-text-retrieval-response']['originalText']\n",
    "                # Process the text to tokenize and remove stopwords\n",
    "                processed_sentences = preprocess_text(text)\n",
    "                # Append the processed sentences to the overall list\n",
    "                all_sentences.extend(processed_sentences)\n",
    "    \n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46702df",
   "metadata": {},
   "source": [
    "#### Train and Save Word2Vec Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'downloaded_articles'\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=process_json_files(directory),  \n",
    "    vector_size=300,           # Size of the embedding vectors\n",
    "    window=20,                 # Context window size\n",
    "    min_count=2,               # Minimum occurrence in vocabulary\n",
    "    workers=4                  # Number of threads for model training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e0404d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"word2vec_model.model\"\n",
    "model.save(model_save_path)\n",
    "model = Word2Vec.load(\"word2vec_model.model\")\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45985cf6",
   "metadata": {},
   "source": [
    "#### Model Examination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fabd2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'rl':\n",
      "drl: 0.7762457728385925\n",
      "model-free_rl: 0.7014486193656921\n",
      "model-based_rl: 0.6874682307243347\n",
      "rl-based: 0.6831478476524353\n",
      "marl: 0.629423201084137\n",
      "dqn: 0.6275638937950134\n",
      "ppo: 0.6267271637916565\n",
      "sac: 0.6256202459335327\n",
      "ddpg: 0.6214520335197449\n",
      "ddq: 0.6198261380195618\n",
      "single-agent: 0.6155815124511719\n",
      "mcts: 0.6127952933311462\n",
      "irl: 0.6084239482879639\n",
      "policy-based: 0.6080467104911804\n",
      "maddpg: 0.6001636981964111\n",
      "learns: 0.597644567489624\n",
      "value-based: 0.5944128036499023\n",
      "drl-based: 0.5910117626190186\n",
      "on-policy: 0.5886929631233215\n",
      "agent: 0.5880126357078552\n",
      "learn: 0.5829527974128723\n",
      "ddqn: 0.5671823024749756\n",
      "unsl: 0.5661435127258301\n",
      "behavioral_cloning: 0.564849317073822\n",
      "dyna-pinn: 0.5625392198562622\n",
      "mdp: 0.5622559785842896\n",
      "sarl: 0.5586027503013611\n",
      "model-free: 0.5563459992408752\n",
      "policy: 0.5514605641365051\n",
      "dyna-style: 0.5495252013206482\n",
      "dnns: 0.5471894145011902\n",
      "gail: 0.5425839424133301\n",
      "bdq: 0.5411962270736694\n",
      "pomdp: 0.5403475165367126\n",
      "q-learning: 0.5390430092811584\n",
      "pre-training: 0.532287061214447\n",
      "actor-critic: 0.5245136618614197\n",
      "offline: 0.5163769721984863\n",
      "exploration: 0.5148068070411682\n",
      "building_control: 0.5143107175827026\n",
      "mbrl: 0.5113481283187866\n",
      "learned: 0.5055752396583557\n",
      "dpg: 0.5011007785797119\n",
      "citylearn: 0.49806293845176697\n",
      "alphazero: 0.4970717132091522\n",
      "imitation_learning: 0.4951348900794983\n",
      "cmarl: 0.4928797781467438\n",
      "bmes: 0.4900660216808319\n",
      "non-stationarity: 0.48946839570999146\n",
      "relearning: 0.48702681064605713\n",
      "supervised_learning: 0.48651784658432007\n",
      "tl: 0.48616909980773926\n",
      "multi-agent: 0.4847521483898163\n",
      "ars-ann: 0.48442256450653076\n",
      "fdppo: 0.4843639135360718\n",
      "q-network: 0.48098763823509216\n",
      "deep_q-learning: 0.48080170154571533\n",
      "rl-mpc: 0.4788026511669159\n",
      "classical: 0.47837498784065247\n",
      "rc-ddq: 0.4760149419307709\n",
      "ctde: 0.4749850034713745\n",
      "vanilla: 0.47382986545562744\n",
      "trained: 0.4733944237232208\n",
      "mat-transfer: 0.4733125865459442\n",
      "d3qn: 0.4716169238090515\n",
      "building_controls: 0.4704665541648865\n",
      "variant: 0.47005054354667664\n",
      "reward: 0.46988150477409363\n",
      "a3c: 0.46984028816223145\n",
      "mdps: 0.4684391915798187\n",
      "vqc-based: 0.46830958127975464\n",
      "deep-rl: 0.4672524631023407\n",
      "cnn-sac: 0.46709388494491577\n",
      "madrl: 0.4670472741127014\n",
      "pre-trained: 0.46626362204551697\n",
      "branching: 0.4651811718940735\n",
      "portability: 0.4598877727985382\n",
      "action: 0.4592874348163605\n",
      "relbot: 0.45914608240127563\n",
      "state-action: 0.4574700593948364\n",
      "stateaction: 0.45714834332466125\n",
      "ml: 0.4564671814441681\n",
      "aamas: 0.45574164390563965\n",
      "ma-ocddpg: 0.45532605051994324\n",
      "asynchronous: 0.45382729172706604\n",
      "canili: 0.45239272713661194\n",
      "dfc: 0.4455544352531433\n",
      "episode: 0.44544729590415955\n",
      "pre-train: 0.44512081146240234\n",
      "adversary: 0.4447714686393738\n",
      "gameplay: 0.44465121626853943\n",
      "q-table: 0.4444282650947571\n",
      "outperform: 0.4439376890659332\n",
      "sarsa: 0.44389161467552185\n",
      "deep_q-networks: 0.4433198571205139\n",
      "exploitation: 0.4429627060890198\n",
      "appealing: 0.4417645335197449\n",
      "soft_actor-critic: 0.4413074553012848\n",
      "building_controllers: 0.44101592898368835\n",
      "straightforward: 0.4403025805950165\n",
      "\n",
      "Words similar to 'carbon':\n",
      "net-zero: 0.6549789905548096\n",
      "emission: 0.6463376879692078\n",
      "carbon_emissions: 0.6244805455207825\n",
      "decarbonization: 0.6109007000923157\n",
      "tax: 0.6040686368942261\n",
      "clean: 0.5959605574607849\n",
      "carbon_emission: 0.5932415723800659\n",
      "co2_emissions: 0.5890739560127258\n",
      "emission_reduction: 0.5867074728012085\n",
      "pathway: 0.5806291103363037\n",
      "fuel: 0.5712292790412903\n",
      "low-carbon: 0.561467707157135\n",
      "economy: 0.5574164986610413\n",
      "carbon_peak: 0.5547912120819092\n",
      "mitigation: 0.5542004704475403\n",
      "biomass: 0.5531633496284485\n",
      "carbon_neutrality: 0.5446597337722778\n",
      "decarbonizing: 0.5388709306716919\n",
      "hydrogen: 0.5383027195930481\n",
      "life-cycle: 0.5381681323051453\n"
     ]
    }
   ],
   "source": [
    "# Find similar words \n",
    "similar_words_rl = model.wv.most_similar('rl', topn=100)\n",
    "similar_words_carbon = model.wv.most_similar('carbon', topn=20)\n",
    "\n",
    "print(\"Words similar to 'rl':\")\n",
    "for word, similarity in similar_words_rl:\n",
    "    print(f\"{word}: {similarity}\")\n",
    "\n",
    "print(\"\\nWords similar to 'carbon':\")\n",
    "for word, similarity in similar_words_carbon:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adda3e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23131573\n"
     ]
    }
   ],
   "source": [
    "# Find similarity\n",
    "similarity_1 = model.wv.similarity('build', 'hous')\n",
    "print(similarity_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eplus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
