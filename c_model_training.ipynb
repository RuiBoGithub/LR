{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c406ff",
   "metadata": {},
   "source": [
    "#### NLTK Text Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac31dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from _cpwords import compound_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60ba76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Define a list of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Add specific terms to be removed\n",
    "    remove_terms = {'introduct', 'literature', 'review', 'figure', 'doi', 'fig', 'table', 'conclusion', \n",
    "                    'altimg', 'gif', 'png', 'discussion', 'acknowledgment', 'appendix','http', 'copyright'}\n",
    "    stop_words.update(remove_terms)\n",
    "\n",
    "    # Initialize lemmatizer and stemmer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    for original, compound in compound_keywords.items():\n",
    "        text = text.replace(original, compound)\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    sentence_tokens = sent_tokenize(text)\n",
    "    sentences = []\n",
    "\n",
    "    # Tokenize each sentence into words and remove stopwords\n",
    "    for sentence in sentence_tokens:\n",
    "        words = word_tokenize(sentence)\n",
    "        filtered_words = []\n",
    "        for word in words:\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            #word = stemmer.stem(word)\n",
    "            \n",
    "            # Remove non-alphabetic characters except underscores and hyphens\n",
    "            word = re.sub(r'[^\\w\\s\\-]', '', word)\n",
    "            \n",
    "            #if word.lower() not in stop_words:\n",
    "            if word.lower() not in stop_words and not word.isdigit() and 1 < len(word) <= 20:\n",
    "                filtered_words.append(word)\n",
    "            \n",
    "        sentences.append(filtered_words)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def process_json_files(directory):\n",
    "    all_sentences = []\n",
    "\n",
    "    # Iterate through each JSON file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                # Extract the original text from the JSON structure\n",
    "                text = data['full-text-retrieval-response']['originalText']\n",
    "                # Process the text to tokenize and remove stopwords\n",
    "                processed_sentences = preprocess_text(text)\n",
    "                # Append the processed sentences to the overall list\n",
    "                all_sentences.extend(processed_sentences)\n",
    "    \n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46702df",
   "metadata": {},
   "source": [
    "#### Train and Save Word2Vec Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fea57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'papers_json'\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=process_json_files(directory),  \n",
    "    vector_size=300,           # Size of the embedding vectors\n",
    "    window=20,                 # Context window size\n",
    "    min_count=2,               # Minimum occurrence in vocabulary\n",
    "    workers=4                  # Number of threads for model training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0404d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlp_input/word2vec_model.model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(model_save_path)\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlp_input/word2vec_model.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model_save_path = \"word2vec_model.model\"\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45985cf6",
   "metadata": {},
   "source": [
    "#### Model Examination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682d71dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from _cpwords import compound_keywords\n",
    "\n",
    "model = Word2Vec.load(\"word2vec_model.model\")\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fabd2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'rl':\n",
      "drl: 0.8500821590423584\n",
      "dqn: 0.7308868169784546\n",
      "ddpg: 0.7207165360450745\n",
      "single-agent: 0.7065060138702393\n",
      "cmarl: 0.7046096920967102\n",
      "ppo: 0.6917793154716492\n",
      "maddpg: 0.6907556056976318\n",
      "model-free: 0.6855440735816956\n",
      "learns: 0.6803081631660461\n",
      "cmarl-exne: 0.671204149723053\n",
      "learned: 0.6618658304214478\n",
      "ddqn: 0.6617934703826904\n",
      "marl: 0.648317813873291\n",
      "sac: 0.6459187865257263\n",
      "learn: 0.644686758518219\n",
      "q-network: 0.6402889490127563\n",
      "rl-based: 0.6401153802871704\n",
      "pre-trained: 0.6353899240493774\n",
      "irl: 0.6348980665206909\n",
      "dyna-pinn: 0.6346455216407776\n",
      "agent: 0.6309409141540527\n",
      "policy: 0.626416802406311\n",
      "gail: 0.6236981749534607\n",
      "bdq: 0.6234402656555176\n",
      "q-learning: 0.622119128704071\n",
      "ma-cwsc: 0.616926372051239\n",
      "rl-mpc: 0.6125118732452393\n",
      "exploration: 0.597885251045227\n",
      "tl: 0.5975538492202759\n",
      "trained: 0.5969592332839966\n",
      "relbot: 0.5934699177742004\n",
      "drl-based: 0.5888583660125732\n",
      "sarl: 0.5883486866950989\n",
      "dyna-style: 0.5852611660957336\n",
      "unsl: 0.5841846466064453\n",
      "pre-training: 0.5795066952705383\n",
      "offline: 0.5789167881011963\n",
      "fine-tuning: 0.5773125290870667\n",
      "ddq: 0.5771332383155823\n",
      "otl: 0.5742914080619812\n",
      "rc-ddq: 0.5731993913650513\n",
      "d3qn: 0.5688465237617493\n",
      "non-stationarity: 0.5664620995521545\n",
      "action: 0.5662857294082642\n",
      "alphazero: 0.560524582862854\n",
      "mcts: 0.55796217918396\n",
      "reinforce: 0.5514577627182007\n",
      "policy-based: 0.5513401627540588\n",
      "max-boltzmann: 0.5503404140472412\n",
      "value-based: 0.5502915978431702\n",
      "advantage: 0.5430359840393066\n",
      "fashion: 0.5426287055015564\n",
      "reward: 0.5400857925415039\n",
      "observational: 0.5398841500282288\n",
      "cooperate: 0.5389478206634521\n",
      "action-reward: 0.53815758228302\n",
      "Îµ-greedy: 0.5364702939987183\n",
      "unseen: 0.5309299826622009\n",
      "outperform: 0.5308528542518616\n",
      "dtl: 0.530758798122406\n",
      "ddpg-based: 0.527938187122345\n",
      "cmarl-ex: 0.5275560021400452\n",
      "best-performing: 0.5237777829170227\n",
      "learning: 0.5222693085670471\n",
      "explore: 0.5206877589225769\n",
      "transfer_learning: 0.5203071236610413\n",
      "re-utilization: 0.5201845169067383\n",
      "successfully: 0.5201222896575928\n",
      "eventually: 0.5175248980522156\n",
      "powerful: 0.5167036056518555\n",
      "state-action: 0.5165669918060303\n",
      "training: 0.516095757484436\n",
      "utilize: 0.5160829424858093\n",
      "proved: 0.5156129598617554\n",
      "terminology: 0.5151914954185486\n",
      "converge: 0.5144240260124207\n",
      "vqc-based: 0.513727068901062\n",
      "well-trained: 0.5093222260475159\n",
      "tabular: 0.5093130469322205\n",
      "rl3: 0.5090210437774658\n",
      "performing: 0.5063934326171875\n",
      "mbrl: 0.5049538612365723\n",
      "retraining: 0.5043991208076477\n",
      "variant: 0.5039837956428528\n",
      "overestimation: 0.5032301545143127\n",
      "citylearn: 0.5029323697090149\n",
      "highlighting: 0.5025712251663208\n",
      "experience: 0.5013641119003296\n",
      "ppg: 0.5009491443634033\n",
      "episode: 0.49957552552223206\n",
      "explored: 0.4987853169441223\n",
      "a3c: 0.49801045656204224\n",
      "q-values: 0.4967619776725769\n",
      "knowledge-sharing: 0.4962572455406189\n",
      "refine: 0.49590662121772766\n",
      "kind: 0.49552181363105774\n",
      "dnn: 0.4950784146785736\n",
      "dcrlbb: 0.49409380555152893\n",
      "real-world: 0.49402591586112976\n",
      "solitary: 0.4926210641860962\n",
      "\n",
      "Words similar to 'carbon':\n",
      "emission: 0.7918479442596436\n",
      "footprint: 0.7078036665916443\n",
      "dioxide: 0.6933639645576477\n",
      "ghg: 0.6898908019065857\n",
      "u22b20112: 0.6554229259490967\n",
      "greenhouse: 0.6368588805198669\n",
      "be2022606: 0.6355401873588562\n",
      "tax: 0.6153460144996643\n",
      "formaldehyde: 0.6063855886459351\n",
      "reform: 0.5982508063316345\n",
      "lcoe: 0.5884442329406738\n",
      "2022-zz-095: 0.5884289145469666\n",
      "kgmwh: 0.565898597240448\n",
      "fossil: 0.5653235912322998\n",
      "economy: 0.5643928647041321\n",
      "ecological: 0.5637581944465637\n",
      "urbanization: 0.557378351688385\n",
      "peaking: 0.555651843547821\n",
      "neutrality: 0.5550875067710876\n",
      "praddeep: 0.5415111184120178\n"
     ]
    }
   ],
   "source": [
    "# Find similar words \n",
    "similar_words_rl = model.wv.most_similar('rl', topn=100)\n",
    "similar_words_carbon = model.wv.most_similar('carbon', topn=20)\n",
    "\n",
    "print(\"Words similar to 'rl':\")\n",
    "for word, similarity in similar_words_rl:\n",
    "    print(f\"{word}: {similarity}\")\n",
    "\n",
    "print(\"\\nWords similar to 'carbon':\")\n",
    "for word, similarity in similar_words_carbon:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adda3e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08287287\n"
     ]
    }
   ],
   "source": [
    "# Find similarity\n",
    "similarity_1 = model.wv.similarity('control', 'hour')\n",
    "print(similarity_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833edd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eplus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
